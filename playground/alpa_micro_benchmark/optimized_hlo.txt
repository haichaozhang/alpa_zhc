HloModule train_step_shard_parallel.6, input_output_alias={ {0}: (0, {}, may-alias), {1}: (1, {}, may-alias), {2}: (2, {}, may-alias), {3}: (3, {}, may-alias), {4}: (4, {}, may-alias), {5}: (5, {}, may-alias), {6}: (6, {}, may-alias), {7}: (7, {}, may-alias), {8}: (8, {}, may-alias), {9}: (9, {}, may-alias), {10}: (10, {}, may-alias), {11}: (11, {}, may-alias), {12}: (12, {}, may-alias), {13}: (13, {}, may-alias), {14}: (14, {}, may-alias), {15}: (15, {}, may-alias), {16}: (16, {}, may-alias), {17}: (17, {}, may-alias), {18}: (18, {}, may-alias), {19}: (19, {}, may-alias), {20}: (20, {}, may-alias), {21}: (21, {}, may-alias), {22}: (22, {}, may-alias), {23}: (23, {}, may-alias), {24}: (24, {}, may-alias), {25}: (25, {}, may-alias), {26}: (26, {}, may-alias), {27}: (27, {}, may-alias), {28}: (28, {}, may-alias), {29}: (29, {}, may-alias), {30}: (30, {}, may-alias), {31}: (31, {}, may-alias), {32}: (32, {}, may-alias), {33}: (33, {}, may-alias), {34}: (34, {}, may-alias), {35}: (35, {}, may-alias), {36}: (36, {}, may-alias), {37}: (37, {}, may-alias), {38}: (38, {}, may-alias), {39}: (39, {}, may-alias), {40}: (40, {}, may-alias), {41}: (41, {}, may-alias), {42}: (42, {}, may-alias), {43}: (43, {}, may-alias), {44}: (44, {}, may-alias), {45}: (45, {}, may-alias), {46}: (46, {}, may-alias), {47}: (47, {}, may-alias), {48}: (48, {}, may-alias), {49}: (49, {}, may-alias), {50}: (50, {}, may-alias), {51}: (51, {}, may-alias), {52}: (52, {}, may-alias), {53}: (53, {}, may-alias), {54}: (54, {}, may-alias), {55}: (55, {}, may-alias), {56}: (56, {}, may-alias), {57}: (57, {}, may-alias), {58}: (58, {}, may-alias), {59}: (59, {}, may-alias), {60}: (60, {}, may-alias), {61}: (61, {}, may-alias), {62}: (62, {}, may-alias), {63}: (63, {}, may-alias), {64}: (64, {}, may-alias), {65}: (65, {}, may-alias), {66}: (66, {}, may-alias), {67}: (67, {}, may-alias), {68}: (68, {}, may-alias), {69}: (69, {}, may-alias), {70}: (70, {}, may-alias), {71}: (71, {}, may-alias), {72}: (72, {}, may-alias), {73}: (73, {}, may-alias), {74}: (74, {}, may-alias), {75}: (75, {}, may-alias), {76}: (76, {}, may-alias), {77}: (77, {}, may-alias), {78}: (78, {}, may-alias), {79}: (79, {}, may-alias), {80}: (80, {}, may-alias), {81}: (81, {}, may-alias), {82}: (82, {}, may-alias), {83}: (83, {}, may-alias), {84}: (84, {}, may-alias), {85}: (85, {}, may-alias), {86}: (86, {}, may-alias), {87}: (87, {}, may-alias), {88}: (88, {}, may-alias), {89}: (89, {}, may-alias), {90}: (90, {}, may-alias), {91}: (91, {}, may-alias), {92}: (92, {}, may-alias), {93}: (93, {}, may-alias), {94}: (94, {}, may-alias), {95}: (95, {}, may-alias), {96}: (96, {}, may-alias), {97}: (97, {}, may-alias), {98}: (98, {}, may-alias), {99}: (99, {}, may-alias), {100}: (100, {}, may-alias), {101}: (101, {}, may-alias), {102}: (102, {}, may-alias), {103}: (103, {}, may-alias), {104}: (104, {}, may-alias), {105}: (105, {}, may-alias), {106}: (106, {}, may-alias), {107}: (107, {}, may-alias), {108}: (108, {}, may-alias), {109}: (109, {}, may-alias), {110}: (110, {}, may-alias), {111}: (111, {}, may-alias), {112}: (112, {}, may-alias), {113}: (113, {}, may-alias), {114}: (114, {}, may-alias), {115}: (115, {}, may-alias), {116}: (116, {}, may-alias), {117}: (117, {}, may-alias), {118}: (118, {}, may-alias), {119}: (119, {}, may-alias), {120}: (120, {}, may-alias), {121}: (121, {}, may-alias), {122}: (122, {}, may-alias), {123}: (123, {}, may-alias), {124}: (124, {}, may-alias), {125}: (125, {}, may-alias), {126}: (126, {}, may-alias), {127}: (127, {}, may-alias), {128}: (128, {}, may-alias), {129}: (129, {}, may-alias), {130}: (130, {}, may-alias), {131}: (131, {}, may-alias), {132}: (132, {}, may-alias), {133}: (133, {}, may-alias), {134}: (134, {}, may-alias), {135}: (135, {}, may-alias), {136}: (136, {}, may-alias), {137}: (137, {}, may-alias), {138}: (138, {}, may-alias), {139}: (139, {}, may-alias), {140}: (140, {}, may-alias), {141}: (141, {}, may-alias), {142}: (142, {}, may-alias), {143}: (143, {}, may-alias), {144}: (144, {}, may-alias), {145}: (145, {}, may-alias), {146}: (146, {}, may-alias), {147}: (147, {}, may-alias), {148}: (148, {}, may-alias), {149}: (149, {}, may-alias), {150}: (150, {}, may-alias), {151}: (151, {}, may-alias), {152}: (152, {}, may-alias), {153}: (153, {}, may-alias), {154}: (154, {}, may-alias), {155}: (155, {}, may-alias), {156}: (156, {}, may-alias), {157}: (157, {}, may-alias), {158}: (158, {}, may-alias), {159}: (159, {}, may-alias), {160}: (160, {}, may-alias), {161}: (161, {}, may-alias), {162}: (162, {}, may-alias), {163}: (163, {}, may-alias), {164}: (164, {}, may-alias), {165}: (165, {}, may-alias), {166}: (166, {}, may-alias), {167}: (167, {}, may-alias), {168}: (168, {}, may-alias), {169}: (169, {}, may-alias), {170}: (170, {}, may-alias), {171}: (171, {}, may-alias), {172}: (172, {}, may-alias), {173}: (173, {}, may-alias), {174}: (174, {}, may-alias), {175}: (175, {}, may-alias), {176}: (176, {}, may-alias), {177}: (177, {}, may-alias), {178}: (178, {}, may-alias), {179}: (179, {}, may-alias), {180}: (180, {}, may-alias), {181}: (181, {}, may-alias), {182}: (182, {}, may-alias), {183}: (183, {}, may-alias), {184}: (184, {}, may-alias), {185}: (185, {}, may-alias), {186}: (186, {}, may-alias), {187}: (187, {}, may-alias), {188}: (188, {}, may-alias), {189}: (189, {}, may-alias), {190}: (190, {}, may-alias), {191}: (191, {}, may-alias), {192}: (192, {}, may-alias), {193}: (193, {}, may-alias), {194}: (194, {}, may-alias), {195}: (195, {}, may-alias), {196}: (196, {}, may-alias), {197}: (197, {}, may-alias), {198}: (198, {}, may-alias), {199}: (199, {}, may-alias), {200}: (200, {}, may-alias), {201}: (201, {}, may-alias), {202}: (202, {}, may-alias), {203}: (203, {}, may-alias), {204}: (204, {}, may-alias), {205}: (205, {}, may-alias), {206}: (206, {}, may-alias), {207}: (207, {}, may-alias), {208}: (208, {}, may-alias), {209}: (209, {}, may-alias), {210}: (210, {}, may-alias), {211}: (211, {}, may-alias), {212}: (212, {}, may-alias), {213}: (213, {}, may-alias), {214}: (214, {}, may-alias), {215}: (215, {}, may-alias), {216}: (216, {}, may-alias), {217}: (217, {}, may-alias), {218}: (218, {}, may-alias), {219}: (219, {}, may-alias), {220}: (220, {}, may-alias), {221}: (221, {}, may-alias), {222}: (222, {}, may-alias), {223}: (223, {}, may-alias), {224}: (224, {}, may-alias), {225}: (225, {}, may-alias), {226}: (226, {}, may-alias), {227}: (227, {}, may-alias), {228}: (228, {}, may-alias), {229}: (229, {}, may-alias), {230}: (230, {}, may-alias), {231}: (231, {}, may-alias), {232}: (232, {}, may-alias) }, entry_computation_layout={(s32[],f32[6400]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[6400,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},s32[],f32[6400]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[6400,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},f32[6400]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[6400,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},s32[4,1024]{1,0},s32[4,1024]{1,0},s32[4,1024]{1,0},s32[4,1024]{1,0},s32[4,1024]{1,0},u32[1]{0})->(s32[], f32[6400]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, /*index=5*/f32[6400,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, /*index=10*/f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, f32[1024,512]{1,0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=25*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, /*index=30*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, /*index=35*/f32[1024,384]{1,0}, f32[512]{0}, f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=40*/f32[1024]{0}, f32[512,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=45*/f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, f32[1024,512]{1,0}, /*index=50*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f32[1024]{0}, /*index=55*/f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, /*index=60*/f32[512]{0}, f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=65*/f32[512,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, /*index=70*/f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, f32[1024,512]{1,0}, f32[1024]{0}, /*index=75*/f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, s32[], f32[6400]{0}, /*index=80*/f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, f32[6400,1024]{1,0}, f32[1024]{0}, /*index=85*/f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, /*index=90*/f32[512]{0}, f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=95*/f32[512,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, /*index=100*/f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, f32[1024,512]{1,0}, f32[1024]{0}, /*index=105*/f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=110*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=115*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, /*index=120*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, /*index=125*/f32[1024,384]{1,0}, f32[512]{0}, f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=130*/f32[1024]{0}, f32[512,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=135*/f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, f32[1024,512]{1,0}, /*index=140*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f32[1024]{0}, /*index=145*/f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, /*index=150*/f32[512]{0}, f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=155*/f32[512,1024]{1,0}, f32[6400]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, /*index=160*/f32[6400,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, /*index=165*/f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, f32[1024,512]{1,0}, f32[1024]{0}, /*index=170*/f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=175*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=180*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, /*index=185*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, /*index=190*/f32[1024,384]{1,0}, f32[512]{0}, f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=195*/f32[1024]{0}, f32[512,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=200*/f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, f32[1024,512]{1,0}, /*index=205*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f32[1024]{0}, /*index=210*/f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, /*index=215*/f32[512]{0}, f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=220*/f32[512,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, /*index=225*/f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, f32[1024,512]{1,0}, f32[1024]{0}, /*index=230*/f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0})}

%region_40.1707 (Arg_0.1708: f32[], Arg_1.1709: f32[]) -> f32[] {
  %Arg_0.1708 = f32[] parameter(0)
  %Arg_1.1709 = f32[] parameter(1)
  ROOT %add.1710 = f32[] add(f32[] %Arg_0.1708, f32[] %Arg_1.1709), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(0, 1)]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=90}
}

%add.2 (x.2: f16[], y.2: f16[]) -> f16[] {
  %x.2 = f16[] parameter(0)
  %y.2 = f16[] parameter(1)
  ROOT %add.28 = f16[] add(f16[] %x.2, f16[] %y.2)
}

%add.3 (x.3: f16[], y.3: f16[]) -> f16[] {
  %x.3 = f16[] parameter(0)
  %y.3 = f16[] parameter(1)
  ROOT %add.29 = f16[] add(f16[] %x.3, f16[] %y.3)
}

%add.4 (x.4: f16[], y.4: f16[]) -> f16[] {
  %x.4 = f16[] parameter(0)
  %y.4 = f16[] parameter(1)
  ROOT %add.30 = f16[] add(f16[] %x.4, f16[] %y.4)
}

%add.5 (x.5: f16[], y.5: f16[]) -> f16[] {
  %x.5 = f16[] parameter(0)
  %y.5 = f16[] parameter(1)
  ROOT %add.31 = f16[] add(f16[] %x.5, f16[] %y.5)
}

%add.6 (x.6: f16[], y.6: f16[]) -> f16[] {
  %x.6 = f16[] parameter(0)
  %y.6 = f16[] parameter(1)
  ROOT %add.32 = f16[] add(f16[] %x.6, f16[] %y.6)
}

%add.7 (x.7: f16[], y.7: f16[]) -> f16[] {
  %x.7 = f16[] parameter(0)
  %y.7 = f16[] parameter(1)
  ROOT %add.33 = f16[] add(f16[] %x.7, f16[] %y.7)
}

%add.8 (x.8: f16[], y.8: f16[]) -> f16[] {
  %x.8 = f16[] parameter(0)
  %y.8 = f16[] parameter(1)
  ROOT %add.34 = f16[] add(f16[] %x.8, f16[] %y.8)
}

%add.9 (x.9: f16[], y.9: f16[]) -> f16[] {
  %x.9 = f16[] parameter(0)
  %y.9 = f16[] parameter(1)
  ROOT %add.35 = f16[] add(f16[] %x.9, f16[] %y.9)
}

%add.10 (x.10: f16[], y.10: f16[]) -> f16[] {
  %x.10 = f16[] parameter(0)
  %y.10 = f16[] parameter(1)
  ROOT %add.36 = f16[] add(f16[] %x.10, f16[] %y.10)
}

%add.11 (x.11: f16[], y.11: f16[]) -> f16[] {
  %x.11 = f16[] parameter(0)
  %y.11 = f16[] parameter(1)
  ROOT %add.37 = f16[] add(f16[] %x.11, f16[] %y.11)
}

%add.12 (x.12: f16[], y.12: f16[]) -> f16[] {
  %x.12 = f16[] parameter(0)
  %y.12 = f16[] parameter(1)
  ROOT %add.38 = f16[] add(f16[] %x.12, f16[] %y.12)
}

%add.13 (x.13: f16[], y.13: f16[]) -> f16[] {
  %x.13 = f16[] parameter(0)
  %y.13 = f16[] parameter(1)
  ROOT %add.39 = f16[] add(f16[] %x.13, f16[] %y.13)
}

%region_38.1688 (Arg_0.1689: f16[], Arg_1.1690: f16[]) -> f16[] {
  %Arg_0.1689 = f16[] parameter(0)
  %Arg_1.1690 = f16[] parameter(1)
  ROOT %maximum.1691 = f16[] maximum(f16[] %Arg_0.1689, f16[] %Arg_1.1690), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/reduce_max[axes=(2,)]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
}

%region_39.1700 (Arg_0.1701: f32[], Arg_1.1702: f32[]) -> f32[] {
  %Arg_0.1701 = f32[] parameter(0)
  %Arg_1.1702 = f32[] parameter(1)
  ROOT %add.1703 = f32[] add(f32[] %Arg_0.1701, f32[] %Arg_1.1702), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(2,)]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
}

%add.14 (x.14: f16[], y.14: f16[]) -> f16[] {
  %x.14 = f16[] parameter(0)
  %y.14 = f16[] parameter(1)
  ROOT %add.40 = f16[] add(f16[] %x.14, f16[] %y.14)
}

%region_45.1817 (Arg_0.1818: f16[], Arg_1.1819: f16[]) -> f16[] {
  %Arg_0.1818 = f16[] parameter(0)
  %Arg_1.1819 = f16[] parameter(1)
  ROOT %maximum.1820 = f16[] maximum(f16[] %Arg_0.1818, f16[] %Arg_1.1819), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%add.15 (x.15: f16[], y.15: f16[]) -> f16[] {
  %x.15 = f16[] parameter(0)
  %y.15 = f16[] parameter(1)
  ROOT %add.41 = f16[] add(f16[] %x.15, f16[] %y.15)
}

%add.16 (x.16: f16[], y.16: f16[]) -> f16[] {
  %x.16 = f16[] parameter(0)
  %y.16 = f16[] parameter(1)
  ROOT %add.42 = f16[] add(f16[] %x.16, f16[] %y.16)
}

%add.17 (x.17: f16[], y.17: f16[]) -> f16[] {
  %x.17 = f16[] parameter(0)
  %y.17 = f16[] parameter(1)
  ROOT %add.43 = f16[] add(f16[] %x.17, f16[] %y.17)
}

%add.18 (x.18: f16[], y.18: f16[]) -> f16[] {
  %x.18 = f16[] parameter(0)
  %y.18 = f16[] parameter(1)
  ROOT %add.44 = f16[] add(f16[] %x.18, f16[] %y.18)
}

%region_65.2276 (Arg_0.2277: f16[], Arg_1.2278: f16[]) -> f16[] {
  %Arg_0.2277 = f16[] parameter(0)
  %Arg_1.2278 = f16[] parameter(1)
  ROOT %maximum.2279 = f16[] maximum(f16[] %Arg_0.2277, f16[] %Arg_1.2278), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%add.19 (x.19: f16[], y.19: f16[]) -> f16[] {
  %x.19 = f16[] parameter(0)
  %y.19 = f16[] parameter(1)
  ROOT %add.45 = f16[] add(f16[] %x.19, f16[] %y.19)
}

%add.20 (x.20: f16[], y.20: f16[]) -> f16[] {
  %x.20 = f16[] parameter(0)
  %y.20 = f16[] parameter(1)
  ROOT %add.46 = f16[] add(f16[] %x.20, f16[] %y.20)
}

%add.21 (x.21: f16[], y.21: f16[]) -> f16[] {
  %x.21 = f16[] parameter(0)
  %y.21 = f16[] parameter(1)
  ROOT %add.47 = f16[] add(f16[] %x.21, f16[] %y.21)
}

%add.22 (x.22: f16[], y.22: f16[]) -> f16[] {
  %x.22 = f16[] parameter(0)
  %y.22 = f16[] parameter(1)
  ROOT %add.48 = f16[] add(f16[] %x.22, f16[] %y.22)
}

%region_85.2735 (Arg_0.2736: f16[], Arg_1.2737: f16[]) -> f16[] {
  %Arg_0.2736 = f16[] parameter(0)
  %Arg_1.2737 = f16[] parameter(1)
  ROOT %maximum.2738 = f16[] maximum(f16[] %Arg_0.2736, f16[] %Arg_1.2737), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%add.23 (x.23: f16[], y.23: f16[]) -> f16[] {
  %x.23 = f16[] parameter(0)
  %y.23 = f16[] parameter(1)
  ROOT %add.49 = f16[] add(f16[] %x.23, f16[] %y.23)
}

%add.24 (x.24: f16[], y.24: f16[]) -> f16[] {
  %x.24 = f16[] parameter(0)
  %y.24 = f16[] parameter(1)
  ROOT %add.50 = f16[] add(f16[] %x.24, f16[] %y.24)
}

%add.25 (x.25: f16[], y.25: f16[]) -> f16[] {
  %x.25 = f16[] parameter(0)
  %y.25 = f16[] parameter(1)
  ROOT %add.51 = f16[] add(f16[] %x.25, f16[] %y.25)
}

%add.26 (x.26: f16[], y.26: f16[]) -> f16[] {
  %x.26 = f16[] parameter(0)
  %y.26 = f16[] parameter(1)
  ROOT %add.52 = f16[] add(f16[] %x.26, f16[] %y.26)
}

%region_105.3194 (Arg_0.3195: f16[], Arg_1.3196: f16[]) -> f16[] {
  %Arg_0.3195 = f16[] parameter(0)
  %Arg_1.3196 = f16[] parameter(1)
  ROOT %maximum.3197 = f16[] maximum(f16[] %Arg_0.3195, f16[] %Arg_1.3196), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%add.27 (x.27: f16[], y.27: f16[]) -> f16[] {
  %x.27 = f16[] parameter(0)
  %y.27 = f16[] parameter(1)
  ROOT %add.53 = f16[] add(f16[] %x.27, f16[] %y.27)
}

%add.28 (x.28: f16[], y.28: f16[]) -> f16[] {
  %x.28 = f16[] parameter(0)
  %y.28 = f16[] parameter(1)
  ROOT %add.54 = f16[] add(f16[] %x.28, f16[] %y.28)
}

%add.29 (x.29: f16[], y.29: f16[]) -> f16[] {
  %x.29 = f16[] parameter(0)
  %y.29 = f16[] parameter(1)
  ROOT %add.55 = f16[] add(f16[] %x.29, f16[] %y.29)
}

%add.30 (x.30: f16[], y.30: f16[]) -> f16[] {
  %x.30 = f16[] parameter(0)
  %y.30 = f16[] parameter(1)
  ROOT %add.56 = f16[] add(f16[] %x.30, f16[] %y.30)
}

%region_125.3653 (Arg_0.3654: f16[], Arg_1.3655: f16[]) -> f16[] {
  %Arg_0.3654 = f16[] parameter(0)
  %Arg_1.3655 = f16[] parameter(1)
  ROOT %maximum.3656 = f16[] maximum(f16[] %Arg_0.3654, f16[] %Arg_1.3655), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%add.31 (x.31: f16[], y.31: f16[]) -> f16[] {
  %x.31 = f16[] parameter(0)
  %y.31 = f16[] parameter(1)
  ROOT %add.57 = f16[] add(f16[] %x.31, f16[] %y.31)
}

%add.32 (x.32: f16[], y.32: f16[]) -> f16[] {
  %x.32 = f16[] parameter(0)
  %y.32 = f16[] parameter(1)
  ROOT %add.58 = f16[] add(f16[] %x.32, f16[] %y.32)
}

%add.33 (x.33: f16[], y.33: f16[]) -> f16[] {
  %x.33 = f16[] parameter(0)
  %y.33 = f16[] parameter(1)
  ROOT %add.59 = f16[] add(f16[] %x.33, f16[] %y.33)
}

%add.34 (x.34: f16[], y.34: f16[]) -> f16[] {
  %x.34 = f16[] parameter(0)
  %y.34 = f16[] parameter(1)
  ROOT %add.60 = f16[] add(f16[] %x.34, f16[] %y.34)
}

%region_145.4112 (Arg_0.4113: f16[], Arg_1.4114: f16[]) -> f16[] {
  %Arg_0.4113 = f16[] parameter(0)
  %Arg_1.4114 = f16[] parameter(1)
  ROOT %maximum.4115 = f16[] maximum(f16[] %Arg_0.4113, f16[] %Arg_1.4114), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%add.35 (x.35: f16[], y.35: f16[]) -> f16[] {
  %x.35 = f16[] parameter(0)
  %y.35 = f16[] parameter(1)
  ROOT %add.61 = f16[] add(f16[] %x.35, f16[] %y.35)
}

%add.36 (x.36: f16[], y.36: f16[]) -> f16[] {
  %x.36 = f16[] parameter(0)
  %y.36 = f16[] parameter(1)
  ROOT %add.62 = f16[] add(f16[] %x.36, f16[] %y.36)
}

%add.37 (x.37: f16[], y.37: f16[]) -> f16[] {
  %x.37 = f16[] parameter(0)
  %y.37 = f16[] parameter(1)
  ROOT %add.63 = f16[] add(f16[] %x.37, f16[] %y.37)
}

%add.38 (x.38: f16[], y.38: f16[]) -> f16[] {
  %x.38 = f16[] parameter(0)
  %y.38 = f16[] parameter(1)
  ROOT %add.64 = f16[] add(f16[] %x.38, f16[] %y.38)
}

%region_52.2025 (Arg_0.2026: f32[], Arg_1.2027: f32[]) -> f32[] {
  %Arg_0.2026 = f32[] parameter(0)
  %Arg_1.2027 = f32[] parameter(1)
  ROOT %add.2028 = f32[] add(f32[] %Arg_0.2026, f32[] %Arg_1.2027), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_51.2015 (Arg_0.2016: f32[], Arg_1.2017: f32[]) -> f32[] {
  %Arg_0.2016 = f32[] parameter(0)
  %Arg_1.2017 = f32[] parameter(1)
  ROOT %add.2018 = f32[] add(f32[] %Arg_0.2016, f32[] %Arg_1.2017), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
}

%fused_computation.9 (param_0.1997: f32[], param_1.2425: f32[4,1024], param_2.2188: f32[], param_3.1652: f32[4,1024], param_4.1113: f32[], param_5.1119: f32[], param_6.951: f16[4,1024,1024], param_7.575: f16[4,1024,1024]) -> (f32[1024], f32[1024]) {
  %param_5.1119 = f32[] parameter(5)
  %broadcast.2185 = f32[4,1024]{1,0} broadcast(f32[] %param_5.1119), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_3.1652 = f32[4,1024]{1,0} parameter(3)
  %param_4.1113 = f32[] parameter(4)
  %broadcast.2184 = f32[4,1024]{1,0} broadcast(f32[] %param_4.1113), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.701 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_3.1652, f32[4,1024]{1,0} %broadcast.2184), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_1.2425 = f32[4,1024]{1,0} parameter(1)
  %param_2.2188 = f32[] parameter(2)
  %broadcast.2183 = f32[4,1024]{1,0} broadcast(f32[] %param_2.2188), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.700 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_1.2425, f32[4,1024]{1,0} %broadcast.2183), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2292 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.700, f32[4,1024]{1,0} %divide.700), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.201 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.701, f32[4,1024]{1,0} %multiply.2292), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.90 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.2185, f32[4,1024]{1,0} %subtract.201), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_0.1997 = f32[] parameter(0)
  %broadcast.2182 = f32[4,1024]{1,0} broadcast(f32[] %param_0.1997), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.1726 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.90, f32[4,1024]{1,0} %broadcast.2182), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1240 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.1726), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.78 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1240), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1239 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.78), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.2181 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1239), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_7.575 = f16[4,1024,1024]{2,1,0} parameter(7)
  %convert.792 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_7.575), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %broadcast.2216 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.700), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.212 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.792, f32[4,1024,1024]{2,1,0} %broadcast.2216), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_6.951 = f16[4,1024,1024]{2,1,0} parameter(6)
  %convert.791 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_6.951), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %multiply.2308 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.212, f32[4,1024,1024]{2,1,0} %convert.791), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.24 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.2181, f32[4,1024,1024]{2,1,0} %multiply.2308), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %bitcast.622 = f32[4096,1024]{1,0} bitcast(f32[4,1024,1024]{2,1,0} %multiply.24)
  %constant_31 = f32[] constant(0)
  %reduce.213 = f32[1024]{0} reduce(f32[4096,1024]{1,0} %bitcast.622, f32[] %constant_31), dimensions={0}, to_apply=%region_52.2025, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %bitcast.623.clone.1 = f32[4096,1024]{1,0} bitcast(f32[4,1024,1024]{2,1,0} %convert.791)
  %reduce.214.clone.1 = f32[1024]{0} reduce(f32[4096,1024]{1,0} %bitcast.623.clone.1, f32[] %constant_31), dimensions={0}, to_apply=%region_51.2015, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  ROOT %tuple.215 = (f32[1024]{0}, f32[1024]{0}) tuple(f32[1024]{0} %reduce.213, f32[1024]{0} %reduce.214.clone.1)
}

%region_58.2105 (Arg_0.2106: f32[], Arg_1.2107: f32[]) -> f32[] {
  %Arg_0.2106 = f32[] parameter(0)
  %Arg_1.2107 = f32[] parameter(1)
  ROOT %add.2108 = f32[] add(f32[] %Arg_0.2106, f32[] %Arg_1.2107), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_57.2095 (Arg_0.2096: f32[], Arg_1.2097: f32[]) -> f32[] {
  %Arg_0.2096 = f32[] parameter(0)
  %Arg_1.2097 = f32[] parameter(1)
  ROOT %add.2098 = f32[] add(f32[] %Arg_0.2096, f32[] %Arg_1.2097), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
}

%fused_computation.35 (param_0.2012: f32[], param_1.2444: f32[4,1024], param_2.2209: f32[], param_3.1669: f32[4,1024], param_4.1131: f32[], param_5.1133: f32[], param_6.962: f16[4,1024,1024], param_7.588: f16[4096,1024], param_8.328: f16[4,1024,1024]) -> (f32[1024], f32[1024]) {
  %param_5.1133 = f32[] parameter(5)
  %broadcast.2063 = f32[4,1024]{1,0} broadcast(f32[] %param_5.1133), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_3.1669 = f32[4,1024]{1,0} parameter(3)
  %param_4.1131 = f32[] parameter(4)
  %broadcast.2062 = f32[4,1024]{1,0} broadcast(f32[] %param_4.1131), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.665 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_3.1669, f32[4,1024]{1,0} %broadcast.2062), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_1.2444 = f32[4,1024]{1,0} parameter(1)
  %param_2.2209 = f32[] parameter(2)
  %broadcast.2061 = f32[4,1024]{1,0} broadcast(f32[] %param_2.2209), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.664 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_1.2444, f32[4,1024]{1,0} %broadcast.2061), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2226 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.664, f32[4,1024]{1,0} %divide.664), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.185 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.665, f32[4,1024]{1,0} %multiply.2226), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.80 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.2063, f32[4,1024]{1,0} %subtract.185), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_0.2012 = f32[] parameter(0)
  %broadcast.2060 = f32[4,1024]{1,0} broadcast(f32[] %param_0.2012), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.1661 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.80, f32[4,1024]{1,0} %broadcast.2060), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1220 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.1661), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.72 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1220), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1219 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.72), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.2059 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1219), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_8.328 = f16[4,1024,1024]{2,1,0} parameter(8)
  %convert.804 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_8.328), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %broadcast.2248 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.664), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.220 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.804, f32[4,1024,1024]{2,1,0} %broadcast.2248), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_6.962 = f16[4,1024,1024]{2,1,0} parameter(6)
  %param_7.588 = f16[4096,1024]{1,0} parameter(7)
  %bitcast.1270 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_7.588), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.1749 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %param_6.962, f16[4,1024,1024]{2,1,0} %bitcast.1270), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.803 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1749), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %multiply.2324 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.220, f32[4,1024,1024]{2,1,0} %convert.803), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.84 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.2059, f32[4,1024,1024]{2,1,0} %multiply.2324), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %bitcast.624 = f32[4096,1024]{1,0} bitcast(f32[4,1024,1024]{2,1,0} %multiply.84)
  %constant_97 = f32[] constant(0)
  %reduce.215 = f32[1024]{0} reduce(f32[4096,1024]{1,0} %bitcast.624, f32[] %constant_97), dimensions={0}, to_apply=%region_58.2105, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %bitcast.625.clone.1 = f32[4096,1024]{1,0} bitcast(f32[4,1024,1024]{2,1,0} %convert.803)
  %reduce.216.clone.1 = f32[1024]{0} reduce(f32[4096,1024]{1,0} %bitcast.625.clone.1, f32[] %constant_97), dimensions={0}, to_apply=%region_57.2095, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  ROOT %tuple.213 = (f32[1024]{0}, f32[1024]{0}) tuple(f32[1024]{0} %reduce.215, f32[1024]{0} %reduce.216.clone.1)
}

%fused_computation.46 (param_0.78: f32[1024], param_1.2939: f32[], param_2.2784: f32[], param_3.2216: f32[1024], param_4.1651: f32[1024], param_5.1716: f32[1024], param_6.1928: f32[1024], param_7.1225: f32[1024], param_8.662: f32[1024], param_9.575: f32[1024], param_10.530: f32[1024], param_11.455: f32[1024], param_12.380: f16[1024], param_13.414: f32[1024], param_14.529: f32[1024], param_15.521: f32[1024], param_16.471: f32[1024], param_17.373: f32[1024], param_18.283: f32[1024], param_19.145: f32[1024], param_20.39: f32[1024], param_21.75: f32[1024], param_22.155: f32[1024], param_23.109: f32[1024], param_24.39: f16[1024], param_25.75: f32[1024], param_26.135: f32[1024], param_27.89: f32[1024], param_28.14: f32[1024], param_29.27: f32[1024], param_30.52: f32[1024], param_31.33: f32[1024], param_32.9: f32[1024], param_33.17: f32[1024]) -> (f32[1024], f32[1024], f32[1024], f32[1024], f32[1024], /*index=5*/f32[1024], f32[1024], f32[1024], f32[1024], f32[1024], /*index=10*/f32[1024], f32[1024], f32[1024], f32[1024], f32[1024], /*index=15*/f32[1024], f32[1024], f32[1024], f32[1024], f32[1024], /*index=20*/f32[1024], f32[1024], f32[1024], f32[1024]) {
  %param_0.78 = f32[1024]{0} parameter(0)
  %param_4.1651 = f32[1024]{0} parameter(4)
  %constant_127_clone_1 = f32[] constant(0.1)
  %broadcast.399.clone.1 = f32[1024]{0} broadcast(f32[] %constant_127_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.113.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_4.1651, f32[1024]{0} %broadcast.399.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_5.1716 = f32[1024]{0} parameter(5)
  %constant_128_clone_1 = f32[] constant(0.9)
  %broadcast.400.clone.1 = f32[1024]{0} broadcast(f32[] %constant_128_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.112.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_5.1716, f32[1024]{0} %broadcast.400.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.867.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.113.clone.1, f32[1024]{0} %multiply.112.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_2.2784 = f32[] parameter(2)
  %broadcast.395 = f32[1024]{0} broadcast(f32[] %param_2.2784), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %multiply.111.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_4.1651, f32[1024]{0} %param_4.1651), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %constant_125_clone_1 = f32[] constant(0.001)
  %broadcast.397.clone.1 = f32[1024]{0} broadcast(f32[] %constant_125_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %multiply.110.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.111.clone.1, f32[1024]{0} %broadcast.397.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_3.2216 = f32[1024]{0} parameter(3)
  %constant_126_clone_1 = f32[] constant(0.999)
  %broadcast.398.clone.1 = f32[1024]{0} broadcast(f32[] %constant_126_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %multiply.109.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_3.2216, f32[1024]{0} %broadcast.398.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.866.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.110.clone.1, f32[1024]{0} %multiply.109.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_1.2939 = f32[] parameter(1)
  %broadcast.393 = f32[1024]{0} broadcast(f32[] %param_1.2939), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %divide.29 = f32[1024]{0} divide(f32[1024]{0} %add.866.clone.1, f32[1024]{0} %broadcast.393), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.91 = f32[1024]{0} sqrt(f32[1024]{0} %divide.29), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %constant_122 = f32[] constant(1e-08)
  %broadcast.394 = f32[1024]{0} broadcast(f32[] %constant_122), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.865 = f32[1024]{0} add(f32[1024]{0} %sqrt.91, f32[1024]{0} %broadcast.394), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.108 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.395, f32[1024]{0} %add.865)
  %divide.28 = f32[1024]{0} divide(f32[1024]{0} %add.867.clone.1, f32[1024]{0} %multiply.108), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %constant_123 = f32[] constant(-0.01)
  %broadcast.396 = f32[1024]{0} broadcast(f32[] %constant_123), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %multiply.107 = f32[1024]{0} multiply(f32[1024]{0} %divide.28, f32[1024]{0} %broadcast.396), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.864 = f32[1024]{0} add(f32[1024]{0} %param_0.78, f32[1024]{0} %multiply.107), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_6.1928 = f32[1024]{0} parameter(6)
  %param_8.662 = f32[1024]{0} parameter(8)
  %multiply.121.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_8.662, f32[1024]{0} %broadcast.399.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_9.575 = f32[1024]{0} parameter(9)
  %multiply.120.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_9.575, f32[1024]{0} %broadcast.400.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.871.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.121.clone.1.clone.1, f32[1024]{0} %multiply.120.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.119.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_8.662, f32[1024]{0} %param_8.662), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.118.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.119.clone.1.clone.1, f32[1024]{0} %broadcast.397.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_7.1225 = f32[1024]{0} parameter(7)
  %multiply.117.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_7.1225, f32[1024]{0} %broadcast.398.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.870.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.118.clone.1.clone.1, f32[1024]{0} %multiply.117.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.31.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.870.clone.1.clone.1, f32[1024]{0} %broadcast.393), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.92.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.31.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.869.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.92.clone.1, f32[1024]{0} %broadcast.394), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.116.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.395, f32[1024]{0} %add.869.clone.1)
  %divide.30.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.871.clone.1.clone.1, f32[1024]{0} %multiply.116.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.115.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.30.clone.1, f32[1024]{0} %broadcast.396), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.868.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_6.1928, f32[1024]{0} %multiply.115.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_10.530 = f32[1024]{0} parameter(10)
  %param_12.380 = f16[1024]{0} parameter(12)
  %convert.35.clone.1.clone.1 = f32[1024]{0} convert(f16[1024]{0} %param_12.380), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.166.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %convert.35.clone.1.clone.1, f32[1024]{0} %broadcast.399.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_13.414 = f32[1024]{0} parameter(13)
  %multiply.165.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_13.414, f32[1024]{0} %broadcast.400.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.899.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.166.clone.1.clone.1, f32[1024]{0} %multiply.165.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.164.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %convert.35.clone.1.clone.1, f32[1024]{0} %convert.35.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.163.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.164.clone.1.clone.1, f32[1024]{0} %broadcast.397.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_11.455 = f32[1024]{0} parameter(11)
  %multiply.162.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_11.455, f32[1024]{0} %broadcast.398.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.898.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.163.clone.1.clone.1, f32[1024]{0} %multiply.162.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.43.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.898.clone.1.clone.1, f32[1024]{0} %broadcast.393), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.98.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.43.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.897.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.98.clone.1, f32[1024]{0} %broadcast.394), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.161.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.395, f32[1024]{0} %add.897.clone.1)
  %divide.42.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.899.clone.1.clone.1, f32[1024]{0} %multiply.161.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.160.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.42.clone.1, f32[1024]{0} %broadcast.396), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.896.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_10.530, f32[1024]{0} %multiply.160.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_14.529 = f32[1024]{0} parameter(14)
  %param_16.471 = f32[1024]{0} parameter(16)
  %multiply.480.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_16.471, f32[1024]{0} %broadcast.399.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_17.373 = f32[1024]{0} parameter(17)
  %multiply.473.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_17.373, f32[1024]{0} %broadcast.400.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.903.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.480.clone.1.clone.1, f32[1024]{0} %multiply.473.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.472.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_16.471, f32[1024]{0} %param_16.471), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.170.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.472.clone.1.clone.1, f32[1024]{0} %broadcast.397.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_15.521 = f32[1024]{0} parameter(15)
  %multiply.169.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_15.521, f32[1024]{0} %broadcast.398.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.902.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.170.clone.1.clone.1, f32[1024]{0} %multiply.169.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.45.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.902.clone.1.clone.1, f32[1024]{0} %broadcast.393), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.99.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.45.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.901.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.99.clone.1, f32[1024]{0} %broadcast.394), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.168.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.395, f32[1024]{0} %add.901.clone.1)
  %divide.44.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.903.clone.1.clone.1, f32[1024]{0} %multiply.168.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.167.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.44.clone.1, f32[1024]{0} %broadcast.396), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.900.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_14.529, f32[1024]{0} %multiply.167.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_18.283 = f32[1024]{0} parameter(18)
  %param_20.39 = f32[1024]{0} parameter(20)
  %multiply.608.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_20.39, f32[1024]{0} %broadcast.399.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_21.75 = f32[1024]{0} parameter(21)
  %multiply.607.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_21.75, f32[1024]{0} %broadcast.400.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.907.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.608.clone.1.clone.1, f32[1024]{0} %multiply.607.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.594.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_20.39, f32[1024]{0} %param_20.39), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.587.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.594.clone.1.clone.1, f32[1024]{0} %broadcast.397.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_19.145 = f32[1024]{0} parameter(19)
  %multiply.508.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_19.145, f32[1024]{0} %broadcast.398.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.906.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.587.clone.1.clone.1, f32[1024]{0} %multiply.508.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.47.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.906.clone.1.clone.1, f32[1024]{0} %broadcast.393), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.100.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.47.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.905.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.100.clone.1, f32[1024]{0} %broadcast.394), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.507.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.395, f32[1024]{0} %add.905.clone.1)
  %divide.46.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.907.clone.1.clone.1, f32[1024]{0} %multiply.507.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.499.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.46.clone.1, f32[1024]{0} %broadcast.396), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.904.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_18.283, f32[1024]{0} %multiply.499.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_22.155 = f32[1024]{0} parameter(22)
  %param_24.39 = f16[1024]{0} parameter(24)
  %convert.40.clone.1.clone.1 = f32[1024]{0} convert(f16[1024]{0} %param_24.39), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.655.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %convert.40.clone.1.clone.1, f32[1024]{0} %broadcast.399.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_25.75 = f32[1024]{0} parameter(25)
  %multiply.651.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_25.75, f32[1024]{0} %broadcast.400.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.916.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.655.clone.1.clone.1, f32[1024]{0} %multiply.651.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.649.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %convert.40.clone.1.clone.1, f32[1024]{0} %convert.40.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.647.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.649.clone.1.clone.1, f32[1024]{0} %broadcast.397.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_23.109 = f32[1024]{0} parameter(23)
  %multiply.645.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_23.109, f32[1024]{0} %broadcast.398.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.915.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.647.clone.1.clone.1, f32[1024]{0} %multiply.645.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.51.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.915.clone.1.clone.1, f32[1024]{0} %broadcast.393), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.102.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.51.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.914.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.102.clone.1, f32[1024]{0} %broadcast.394), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.643.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.395, f32[1024]{0} %add.914.clone.1)
  %divide.50.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.916.clone.1.clone.1, f32[1024]{0} %multiply.643.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.641.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.50.clone.1, f32[1024]{0} %broadcast.396), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.913.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_22.155, f32[1024]{0} %multiply.641.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_26.135 = f32[1024]{0} parameter(26)
  %param_28.14 = f32[1024]{0} parameter(28)
  %multiply.799.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_28.14, f32[1024]{0} %broadcast.399.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_29.27 = f32[1024]{0} parameter(29)
  %multiply.786.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_29.27, f32[1024]{0} %broadcast.400.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.920.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.799.clone.1.clone.1, f32[1024]{0} %multiply.786.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.779.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_28.14, f32[1024]{0} %param_28.14), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.700.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.779.clone.1.clone.1, f32[1024]{0} %broadcast.397.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_27.89 = f32[1024]{0} parameter(27)
  %multiply.699.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_27.89, f32[1024]{0} %broadcast.398.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.919.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.700.clone.1.clone.1, f32[1024]{0} %multiply.699.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.53.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.919.clone.1.clone.1, f32[1024]{0} %broadcast.393), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.103.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.53.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.918.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.103.clone.1, f32[1024]{0} %broadcast.394), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.686.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.395, f32[1024]{0} %add.918.clone.1)
  %divide.52.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.920.clone.1.clone.1, f32[1024]{0} %multiply.686.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.679.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.52.clone.1, f32[1024]{0} %broadcast.396), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.917.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_26.135, f32[1024]{0} %multiply.679.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_30.52 = f32[1024]{0} parameter(30)
  %param_32.9 = f32[1024]{0} parameter(32)
  %multiply.829.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_32.9, f32[1024]{0} %broadcast.399.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_33.17 = f32[1024]{0} parameter(33)
  %multiply.827.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_33.17, f32[1024]{0} %broadcast.400.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.924.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.829.clone.1.clone.1, f32[1024]{0} %multiply.827.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.825.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_32.9, f32[1024]{0} %param_32.9), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.823.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.825.clone.1.clone.1, f32[1024]{0} %broadcast.397.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_31.33 = f32[1024]{0} parameter(31)
  %multiply.821.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_31.33, f32[1024]{0} %broadcast.398.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.923.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.823.clone.1.clone.1, f32[1024]{0} %multiply.821.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.55.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.923.clone.1.clone.1, f32[1024]{0} %broadcast.393), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.104.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.55.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.922.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.104.clone.1, f32[1024]{0} %broadcast.394), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.819.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.395, f32[1024]{0} %add.922.clone.1)
  %divide.54.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.924.clone.1.clone.1, f32[1024]{0} %multiply.819.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.818.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.54.clone.1, f32[1024]{0} %broadcast.396), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.921.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_30.52, f32[1024]{0} %multiply.818.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  ROOT %tuple.267 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) tuple(f32[1024]{0} %add.864, f32[1024]{0} %add.866.clone.1, f32[1024]{0} %add.867.clone.1, f32[1024]{0} %add.868.clone.1, f32[1024]{0} %add.870.clone.1.clone.1, /*index=5*/f32[1024]{0} %add.871.clone.1.clone.1, f32[1024]{0} %add.896.clone.1, f32[1024]{0} %add.898.clone.1.clone.1, f32[1024]{0} %add.899.clone.1.clone.1, f32[1024]{0} %add.900.clone.1, /*index=10*/f32[1024]{0} %add.902.clone.1.clone.1, f32[1024]{0} %add.903.clone.1.clone.1, f32[1024]{0} %add.904.clone.1, f32[1024]{0} %add.906.clone.1.clone.1, f32[1024]{0} %add.907.clone.1.clone.1, /*index=15*/f32[1024]{0} %add.913.clone.1, f32[1024]{0} %add.915.clone.1.clone.1, f32[1024]{0} %add.916.clone.1.clone.1, f32[1024]{0} %add.917.clone.1, f32[1024]{0} %add.919.clone.1.clone.1, /*index=20*/f32[1024]{0} %add.920.clone.1.clone.1, f32[1024]{0} %add.921.clone.1, f32[1024]{0} %add.923.clone.1.clone.1, f32[1024]{0} %add.924.clone.1.clone.1)
}

%region_72.2484 (Arg_0.2485: f32[], Arg_1.2486: f32[]) -> f32[] {
  %Arg_0.2485 = f32[] parameter(0)
  %Arg_1.2486 = f32[] parameter(1)
  ROOT %add.2487 = f32[] add(f32[] %Arg_0.2485, f32[] %Arg_1.2486), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_71.2474 (Arg_0.2475: f32[], Arg_1.2476: f32[]) -> f32[] {
  %Arg_0.2475 = f32[] parameter(0)
  %Arg_1.2476 = f32[] parameter(1)
  ROOT %add.2477 = f32[] add(f32[] %Arg_0.2475, f32[] %Arg_1.2476), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
}

%fused_computation.49 (param_0.2071: f32[], param_1.2516: f32[4,1024], param_2.2291: f32[], param_3.1750: f32[4,1024], param_4.1204: f32[], param_5.1214: f32[], param_6.1022: f16[4,1024,1024], param_7.641: f16[4,1024,1024]) -> (f32[1024], f32[1024]) {
  %param_5.1214 = f32[] parameter(5)
  %broadcast.2431 = f32[4,1024]{1,0} broadcast(f32[] %param_5.1214), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_3.1750 = f32[4,1024]{1,0} parameter(3)
  %param_4.1204 = f32[] parameter(4)
  %broadcast.2430 = f32[4,1024]{1,0} broadcast(f32[] %param_4.1204), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.789 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_3.1750, f32[4,1024]{1,0} %broadcast.2430), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_1.2516 = f32[4,1024]{1,0} parameter(1)
  %param_2.2291 = f32[] parameter(2)
  %broadcast.2429 = f32[4,1024]{1,0} broadcast(f32[] %param_2.2291), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.788 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_1.2516, f32[4,1024]{1,0} %broadcast.2429), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2402 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.788, f32[4,1024]{1,0} %divide.788), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.265 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.789, f32[4,1024]{1,0} %multiply.2402), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.116 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.2431, f32[4,1024]{1,0} %subtract.265), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_0.2071 = f32[] parameter(0)
  %broadcast.2428 = f32[4,1024]{1,0} broadcast(f32[] %param_0.2071), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.1819 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.116, f32[4,1024]{1,0} %broadcast.2428), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1318 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.1819), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.96 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1318), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1317 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.96), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.2427 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1317), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_7.641 = f16[4,1024,1024]{2,1,0} parameter(7)
  %convert.828 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_7.641), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %broadcast.2462 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.788), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.273 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.828, f32[4,1024,1024]{2,1,0} %broadcast.2462), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_6.1022 = f16[4,1024,1024]{2,1,0} parameter(6)
  %convert.827 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_6.1022), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %multiply.2418 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.273, f32[4,1024,1024]{2,1,0} %convert.827), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.114 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.2427, f32[4,1024,1024]{2,1,0} %multiply.2418), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %bitcast.626 = f32[4096,1024]{1,0} bitcast(f32[4,1024,1024]{2,1,0} %multiply.114)
  %constant_129 = f32[] constant(0)
  %reduce.217 = f32[1024]{0} reduce(f32[4096,1024]{1,0} %bitcast.626, f32[] %constant_129), dimensions={0}, to_apply=%region_72.2484, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %bitcast.627.clone.1 = f32[4096,1024]{1,0} bitcast(f32[4,1024,1024]{2,1,0} %convert.827)
  %reduce.218.clone.1 = f32[1024]{0} reduce(f32[4096,1024]{1,0} %bitcast.627.clone.1, f32[] %constant_129), dimensions={0}, to_apply=%region_71.2474, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  ROOT %tuple.205 = (f32[1024]{0}, f32[1024]{0}) tuple(f32[1024]{0} %reduce.217, f32[1024]{0} %reduce.218.clone.1)
}

%region_78.2564 (Arg_0.2565: f32[], Arg_1.2566: f32[]) -> f32[] {
  %Arg_0.2565 = f32[] parameter(0)
  %Arg_1.2566 = f32[] parameter(1)
  ROOT %add.2567 = f32[] add(f32[] %Arg_0.2565, f32[] %Arg_1.2566), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_77.2554 (Arg_0.2555: f32[], Arg_1.2556: f32[]) -> f32[] {
  %Arg_0.2555 = f32[] parameter(0)
  %Arg_1.2556 = f32[] parameter(1)
  ROOT %add.2557 = f32[] add(f32[] %Arg_0.2555, f32[] %Arg_1.2556), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
}

%fused_computation.75 (param_0.2086: f32[], param_1.2535: f32[4,1024], param_2.2312: f32[], param_3.1767: f32[4,1024], param_4.1222: f32[], param_5.1228: f32[], param_6.1033: f16[4,1024,1024], param_7.654: f16[4096,1024], param_8.376: f16[4,1024,1024]) -> (f32[1024], f32[1024]) {
  %param_5.1228 = f32[] parameter(5)
  %broadcast.2309 = f32[4,1024]{1,0} broadcast(f32[] %param_5.1228), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_3.1767 = f32[4,1024]{1,0} parameter(3)
  %param_4.1222 = f32[] parameter(4)
  %broadcast.2308 = f32[4,1024]{1,0} broadcast(f32[] %param_4.1222), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.753 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_3.1767, f32[4,1024]{1,0} %broadcast.2308), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_1.2535 = f32[4,1024]{1,0} parameter(1)
  %param_2.2312 = f32[] parameter(2)
  %broadcast.2307 = f32[4,1024]{1,0} broadcast(f32[] %param_2.2312), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.752 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_1.2535, f32[4,1024]{1,0} %broadcast.2307), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2336 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.752, f32[4,1024]{1,0} %divide.752), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.249 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.753, f32[4,1024]{1,0} %multiply.2336), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.106 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.2309, f32[4,1024]{1,0} %subtract.249), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_0.2086 = f32[] parameter(0)
  %broadcast.2306 = f32[4,1024]{1,0} broadcast(f32[] %param_0.2086), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.1757 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.106, f32[4,1024]{1,0} %broadcast.2306), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1298 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.1757), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.90 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1298), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1297 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.90), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.2305 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1297), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_8.376 = f16[4,1024,1024]{2,1,0} parameter(8)
  %convert.840 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_8.376), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %broadcast.2494 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.752), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.284 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.840, f32[4,1024,1024]{2,1,0} %broadcast.2494), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_6.1033 = f16[4,1024,1024]{2,1,0} parameter(6)
  %param_7.654 = f16[4096,1024]{1,0} parameter(7)
  %bitcast.1348 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_7.654), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.1842 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %param_6.1033, f16[4,1024,1024]{2,1,0} %bitcast.1348), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.839 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %multiply.2434 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.284, f32[4,1024,1024]{2,1,0} %convert.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.481 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.2305, f32[4,1024,1024]{2,1,0} %multiply.2434), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %bitcast.628 = f32[4096,1024]{1,0} bitcast(f32[4,1024,1024]{2,1,0} %multiply.481)
  %constant_187 = f32[] constant(0)
  %reduce.219 = f32[1024]{0} reduce(f32[4096,1024]{1,0} %bitcast.628, f32[] %constant_187), dimensions={0}, to_apply=%region_78.2564, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %bitcast.629.clone.1 = f32[4096,1024]{1,0} bitcast(f32[4,1024,1024]{2,1,0} %convert.839)
  %reduce.220.clone.1 = f32[1024]{0} reduce(f32[4096,1024]{1,0} %bitcast.629.clone.1, f32[] %constant_187), dimensions={0}, to_apply=%region_77.2554, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  ROOT %tuple.203 = (f32[1024]{0}, f32[1024]{0}) tuple(f32[1024]{0} %reduce.219, f32[1024]{0} %reduce.220.clone.1)
}

%region_92.2943 (Arg_0.2944: f32[], Arg_1.2945: f32[]) -> f32[] {
  %Arg_0.2944 = f32[] parameter(0)
  %Arg_1.2945 = f32[] parameter(1)
  ROOT %add.2946 = f32[] add(f32[] %Arg_0.2944, f32[] %Arg_1.2945), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_91.2933 (Arg_0.2934: f32[], Arg_1.2935: f32[]) -> f32[] {
  %Arg_0.2934 = f32[] parameter(0)
  %Arg_1.2935 = f32[] parameter(1)
  ROOT %add.2936 = f32[] add(f32[] %Arg_0.2934, f32[] %Arg_1.2935), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
}

%fused_computation.89 (param_0.2145: f32[], param_1.2607: f32[4,1024], param_2.2394: f32[], param_3.1848: f32[4,1024], param_4.1295: f32[], param_5.1309: f32[], param_6.1093: f16[4,1024,1024], param_7.707: f16[4,1024,1024]) -> (f32[1024], f32[1024]) {
  %param_5.1309 = f32[] parameter(5)
  %broadcast.2677 = f32[4,1024]{1,0} broadcast(f32[] %param_5.1309), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_3.1848 = f32[4,1024]{1,0} parameter(3)
  %param_4.1295 = f32[] parameter(4)
  %broadcast.2676 = f32[4,1024]{1,0} broadcast(f32[] %param_4.1295), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.877 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_3.1848, f32[4,1024]{1,0} %broadcast.2676), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_1.2607 = f32[4,1024]{1,0} parameter(1)
  %param_2.2394 = f32[] parameter(2)
  %broadcast.2675 = f32[4,1024]{1,0} broadcast(f32[] %param_2.2394), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.876 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_1.2607, f32[4,1024]{1,0} %broadcast.2675), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2512 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.876, f32[4,1024]{1,0} %divide.876), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.324 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.877, f32[4,1024]{1,0} %multiply.2512), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.142 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.2677, f32[4,1024]{1,0} %subtract.324), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_0.2145 = f32[] parameter(0)
  %broadcast.2674 = f32[4,1024]{1,0} broadcast(f32[] %param_0.2145), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.1914 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.142, f32[4,1024]{1,0} %broadcast.2674), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1396 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.1914), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.114 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1396), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1395 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.114), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.2673 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1395), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_7.707 = f16[4,1024,1024]{2,1,0} parameter(7)
  %convert.864 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_7.707), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %broadcast.2708 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.876), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.332 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.864, f32[4,1024,1024]{2,1,0} %broadcast.2708), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_6.1093 = f16[4,1024,1024]{2,1,0} parameter(6)
  %convert.863 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_6.1093), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %multiply.2528 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.332, f32[4,1024,1024]{2,1,0} %convert.863), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.800 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.2673, f32[4,1024,1024]{2,1,0} %multiply.2528), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %bitcast.630 = f32[4096,1024]{1,0} bitcast(f32[4,1024,1024]{2,1,0} %multiply.800)
  %constant_215 = f32[] constant(0)
  %reduce.221 = f32[1024]{0} reduce(f32[4096,1024]{1,0} %bitcast.630, f32[] %constant_215), dimensions={0}, to_apply=%region_92.2943, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %bitcast.631.clone.1 = f32[4096,1024]{1,0} bitcast(f32[4,1024,1024]{2,1,0} %convert.863)
  %reduce.222.clone.1 = f32[1024]{0} reduce(f32[4096,1024]{1,0} %bitcast.631.clone.1, f32[] %constant_215), dimensions={0}, to_apply=%region_91.2933, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  ROOT %tuple.195 = (f32[1024]{0}, f32[1024]{0}) tuple(f32[1024]{0} %reduce.221, f32[1024]{0} %reduce.222.clone.1)
}

%fused_computation.109 (param_0.185: f32[1024], param_1.2958: f32[], param_2.2822: f32[], param_3.2254: f32[1024], param_4.1708: f16[1024], param_5.1844: f32[1024], param_6.1932: f32[1024], param_7.1229: f32[1024], param_8.667: f32[1024], param_9.584: f32[1024], param_10.548: f32[1024], param_11.467: f32[1024], param_12.385: f32[1024], param_13.423: f32[1024], param_14.547: f32[1024], param_15.533: f32[1024], param_16.476: f16[1024], param_17.382: f32[1024], param_18.301: f32[1024], param_19.159: f32[1024], param_20.44: f32[1024], param_21.84: f32[1024], param_22.173: f32[1024], param_23.121: f32[1024], param_24.44: f32[1024], param_25.84: f32[1024], param_26.153: f32[1024], param_27.101: f32[1024], param_28.19: f16[1024], param_29.36: f32[1024], param_30.70: f32[1024], param_31.47: f32[1024], param_32.14: f32[1024], param_33.26: f32[1024]) -> (f32[1024], f32[1024], f32[1024], f32[1024], f32[1024], /*index=5*/f32[1024], f32[1024], f32[1024], f32[1024], f32[1024], /*index=10*/f32[1024], f32[1024], f32[1024], f32[1024], f32[1024], /*index=15*/f32[1024], f32[1024], f32[1024], f32[1024], f32[1024], /*index=20*/f32[1024], f32[1024], f32[1024], f32[1024]) {
  %param_0.185 = f32[1024]{0} parameter(0)
  %param_4.1708 = f16[1024]{0} parameter(4)
  %convert.343.clone.1 = f32[1024]{0} convert(f16[1024]{0} %param_4.1708), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %constant_269_clone_1 = f32[] constant(0.1)
  %broadcast.623.clone.1 = f32[1024]{0} broadcast(f32[] %constant_269_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1211.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %convert.343.clone.1, f32[1024]{0} %broadcast.623.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_5.1844 = f32[1024]{0} parameter(5)
  %constant_270_clone_1 = f32[] constant(0.9)
  %broadcast.624.clone.1 = f32[1024]{0} broadcast(f32[] %constant_270_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1209.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_5.1844, f32[1024]{0} %broadcast.624.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.952.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1211.clone.1, f32[1024]{0} %multiply.1209.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_2.2822 = f32[] parameter(2)
  %broadcast.613 = f32[1024]{0} broadcast(f32[] %param_2.2822), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %multiply.1207.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %convert.343.clone.1, f32[1024]{0} %convert.343.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %constant_267_clone_1 = f32[] constant(0.001)
  %broadcast.619.clone.1 = f32[1024]{0} broadcast(f32[] %constant_267_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %multiply.1205.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1207.clone.1, f32[1024]{0} %broadcast.619.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_3.2254 = f32[1024]{0} parameter(3)
  %constant_268_clone_1 = f32[] constant(0.999)
  %broadcast.621.clone.1 = f32[1024]{0} broadcast(f32[] %constant_268_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %multiply.1203.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_3.2254, f32[1024]{0} %broadcast.621.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.951.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1205.clone.1, f32[1024]{0} %multiply.1203.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_1.2958 = f32[] parameter(1)
  %broadcast.610 = f32[1024]{0} broadcast(f32[] %param_1.2958), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %divide.67 = f32[1024]{0} divide(f32[1024]{0} %add.951.clone.1, f32[1024]{0} %broadcast.610), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.110 = f32[1024]{0} sqrt(f32[1024]{0} %divide.67), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %constant_264 = f32[] constant(1e-08)
  %broadcast.612 = f32[1024]{0} broadcast(f32[] %constant_264), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.950 = f32[1024]{0} add(f32[1024]{0} %sqrt.110, f32[1024]{0} %broadcast.612), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1202 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.613, f32[1024]{0} %add.950)
  %divide.66 = f32[1024]{0} divide(f32[1024]{0} %add.952.clone.1, f32[1024]{0} %multiply.1202), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %constant_266 = f32[] constant(-0.01)
  %broadcast.617 = f32[1024]{0} broadcast(f32[] %constant_266), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %multiply.1184 = f32[1024]{0} multiply(f32[1024]{0} %divide.66, f32[1024]{0} %broadcast.617), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.949 = f32[1024]{0} add(f32[1024]{0} %param_0.185, f32[1024]{0} %multiply.1184), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_6.1932 = f32[1024]{0} parameter(6)
  %param_8.667 = f32[1024]{0} parameter(8)
  %multiply.1225.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_8.667, f32[1024]{0} %broadcast.623.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_9.584 = f32[1024]{0} parameter(9)
  %multiply.1223.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_9.584, f32[1024]{0} %broadcast.624.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.956.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1225.clone.1.clone.1, f32[1024]{0} %multiply.1223.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1221.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_8.667, f32[1024]{0} %param_8.667), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1219.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1221.clone.1.clone.1, f32[1024]{0} %broadcast.619.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_7.1229 = f32[1024]{0} parameter(7)
  %multiply.1217.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_7.1229, f32[1024]{0} %broadcast.621.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.955.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1219.clone.1.clone.1, f32[1024]{0} %multiply.1217.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.69.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.955.clone.1.clone.1, f32[1024]{0} %broadcast.610), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.111.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.69.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.954.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.111.clone.1, f32[1024]{0} %broadcast.612), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1215.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.613, f32[1024]{0} %add.954.clone.1)
  %divide.68.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.956.clone.1.clone.1, f32[1024]{0} %multiply.1215.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1213.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.68.clone.1, f32[1024]{0} %broadcast.617), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.953.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_6.1932, f32[1024]{0} %multiply.1213.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_10.548 = f32[1024]{0} parameter(10)
  %param_12.385 = f32[1024]{0} parameter(12)
  %multiply.1344.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_12.385, f32[1024]{0} %broadcast.623.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_13.423 = f32[1024]{0} parameter(13)
  %multiply.1343.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_13.423, f32[1024]{0} %broadcast.624.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.960.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1344.clone.1.clone.1, f32[1024]{0} %multiply.1343.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1276.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_12.385, f32[1024]{0} %param_12.385), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1275.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1276.clone.1.clone.1, f32[1024]{0} %broadcast.619.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_11.467 = f32[1024]{0} parameter(11)
  %multiply.1262.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_11.467, f32[1024]{0} %broadcast.621.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.959.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1275.clone.1.clone.1, f32[1024]{0} %multiply.1262.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.71.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.959.clone.1.clone.1, f32[1024]{0} %broadcast.610), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.112.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.71.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.958.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.112.clone.1, f32[1024]{0} %broadcast.612), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1255.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.613, f32[1024]{0} %add.958.clone.1)
  %divide.70.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.960.clone.1.clone.1, f32[1024]{0} %multiply.1255.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1231.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.70.clone.1, f32[1024]{0} %broadcast.617), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.957.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_10.548, f32[1024]{0} %multiply.1231.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_14.547 = f32[1024]{0} parameter(14)
  %param_16.476 = f16[1024]{0} parameter(16)
  %convert.348.clone.1.clone.1 = f32[1024]{0} convert(f16[1024]{0} %param_16.476), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1359.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %convert.348.clone.1.clone.1, f32[1024]{0} %broadcast.623.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_17.382 = f32[1024]{0} parameter(17)
  %multiply.1358.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_17.382, f32[1024]{0} %broadcast.624.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.970.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1359.clone.1.clone.1, f32[1024]{0} %multiply.1358.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1357.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %convert.348.clone.1.clone.1, f32[1024]{0} %convert.348.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1356.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1357.clone.1.clone.1, f32[1024]{0} %broadcast.619.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_15.533 = f32[1024]{0} parameter(15)
  %multiply.1355.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_15.533, f32[1024]{0} %broadcast.621.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.969.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1356.clone.1.clone.1, f32[1024]{0} %multiply.1355.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.75.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.969.clone.1.clone.1, f32[1024]{0} %broadcast.610), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.114.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.75.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.968.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.114.clone.1, f32[1024]{0} %broadcast.612), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1354.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.613, f32[1024]{0} %add.968.clone.1)
  %divide.74.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.970.clone.1.clone.1, f32[1024]{0} %multiply.1354.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1353.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.74.clone.1, f32[1024]{0} %broadcast.617), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.967.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_14.547, f32[1024]{0} %multiply.1353.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_18.301 = f32[1024]{0} parameter(18)
  %param_20.44 = f32[1024]{0} parameter(20)
  %multiply.1366.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_20.44, f32[1024]{0} %broadcast.623.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_21.84 = f32[1024]{0} parameter(21)
  %multiply.1365.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_21.84, f32[1024]{0} %broadcast.624.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.974.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1366.clone.1.clone.1, f32[1024]{0} %multiply.1365.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1364.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_20.44, f32[1024]{0} %param_20.44), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1363.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1364.clone.1.clone.1, f32[1024]{0} %broadcast.619.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_19.159 = f32[1024]{0} parameter(19)
  %multiply.1362.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_19.159, f32[1024]{0} %broadcast.621.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.973.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1363.clone.1.clone.1, f32[1024]{0} %multiply.1362.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.77.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.973.clone.1.clone.1, f32[1024]{0} %broadcast.610), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.115.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.77.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.972.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.115.clone.1, f32[1024]{0} %broadcast.612), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1361.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.613, f32[1024]{0} %add.972.clone.1)
  %divide.76.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.974.clone.1.clone.1, f32[1024]{0} %multiply.1361.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1360.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.76.clone.1, f32[1024]{0} %broadcast.617), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.971.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_18.301, f32[1024]{0} %multiply.1360.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_22.173 = f32[1024]{0} parameter(22)
  %param_24.44 = f32[1024]{0} parameter(24)
  %multiply.1374.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_24.44, f32[1024]{0} %broadcast.623.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_25.84 = f32[1024]{0} parameter(25)
  %multiply.1373.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_25.84, f32[1024]{0} %broadcast.624.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.979.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1374.clone.1.clone.1, f32[1024]{0} %multiply.1373.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1372.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_24.44, f32[1024]{0} %param_24.44), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1371.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1372.clone.1.clone.1, f32[1024]{0} %broadcast.619.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_23.121 = f32[1024]{0} parameter(23)
  %multiply.1370.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_23.121, f32[1024]{0} %broadcast.621.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.978.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1371.clone.1.clone.1, f32[1024]{0} %multiply.1370.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.79.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.978.clone.1.clone.1, f32[1024]{0} %broadcast.610), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.116.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.79.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.977.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.116.clone.1, f32[1024]{0} %broadcast.612), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1369.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.613, f32[1024]{0} %add.977.clone.1)
  %divide.78.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.979.clone.1.clone.1, f32[1024]{0} %multiply.1369.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1368.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.78.clone.1, f32[1024]{0} %broadcast.617), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.976.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_22.173, f32[1024]{0} %multiply.1368.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_26.153 = f32[1024]{0} parameter(26)
  %param_28.19 = f16[1024]{0} parameter(28)
  %convert.361.clone.1.clone.1 = f32[1024]{0} convert(f16[1024]{0} %param_28.19), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1419.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %convert.361.clone.1.clone.1, f32[1024]{0} %broadcast.623.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_29.36 = f32[1024]{0} parameter(29)
  %multiply.1418.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_29.36, f32[1024]{0} %broadcast.624.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1006.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1419.clone.1.clone.1, f32[1024]{0} %multiply.1418.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1417.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %convert.361.clone.1.clone.1, f32[1024]{0} %convert.361.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1416.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1417.clone.1.clone.1, f32[1024]{0} %broadcast.619.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_27.101 = f32[1024]{0} parameter(27)
  %multiply.1415.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_27.101, f32[1024]{0} %broadcast.621.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1005.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1416.clone.1.clone.1, f32[1024]{0} %multiply.1415.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.91.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.1005.clone.1.clone.1, f32[1024]{0} %broadcast.610), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.122.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.91.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.1004.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.122.clone.1, f32[1024]{0} %broadcast.612), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1414.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.613, f32[1024]{0} %add.1004.clone.1)
  %divide.90.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.1006.clone.1.clone.1, f32[1024]{0} %multiply.1414.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1413.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.90.clone.1, f32[1024]{0} %broadcast.617), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.1003.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_26.153, f32[1024]{0} %multiply.1413.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_30.70 = f32[1024]{0} parameter(30)
  %param_32.14 = f32[1024]{0} parameter(32)
  %multiply.1426.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_32.14, f32[1024]{0} %broadcast.623.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_33.26 = f32[1024]{0} parameter(33)
  %multiply.1425.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_33.26, f32[1024]{0} %broadcast.624.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1010.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1426.clone.1.clone.1, f32[1024]{0} %multiply.1425.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1424.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_32.14, f32[1024]{0} %param_32.14), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1423.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1424.clone.1.clone.1, f32[1024]{0} %broadcast.619.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_31.47 = f32[1024]{0} parameter(31)
  %multiply.1422.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_31.47, f32[1024]{0} %broadcast.621.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1009.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1423.clone.1.clone.1, f32[1024]{0} %multiply.1422.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.93.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.1009.clone.1.clone.1, f32[1024]{0} %broadcast.610), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.123.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.93.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.1008.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.123.clone.1, f32[1024]{0} %broadcast.612), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1421.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.613, f32[1024]{0} %add.1008.clone.1)
  %divide.92.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.1010.clone.1.clone.1, f32[1024]{0} %multiply.1421.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1420.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.92.clone.1, f32[1024]{0} %broadcast.617), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.1007.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_30.70, f32[1024]{0} %multiply.1420.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  ROOT %tuple.274 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) tuple(f32[1024]{0} %add.949, f32[1024]{0} %add.951.clone.1, f32[1024]{0} %add.952.clone.1, f32[1024]{0} %add.953.clone.1, f32[1024]{0} %add.955.clone.1.clone.1, /*index=5*/f32[1024]{0} %add.956.clone.1.clone.1, f32[1024]{0} %add.957.clone.1, f32[1024]{0} %add.959.clone.1.clone.1, f32[1024]{0} %add.960.clone.1.clone.1, f32[1024]{0} %add.967.clone.1, /*index=10*/f32[1024]{0} %add.969.clone.1.clone.1, f32[1024]{0} %add.970.clone.1.clone.1, f32[1024]{0} %add.971.clone.1, f32[1024]{0} %add.973.clone.1.clone.1, f32[1024]{0} %add.974.clone.1.clone.1, /*index=15*/f32[1024]{0} %add.976.clone.1, f32[1024]{0} %add.978.clone.1.clone.1, f32[1024]{0} %add.979.clone.1.clone.1, f32[1024]{0} %add.1003.clone.1, f32[1024]{0} %add.1005.clone.1.clone.1, /*index=20*/f32[1024]{0} %add.1006.clone.1.clone.1, f32[1024]{0} %add.1007.clone.1, f32[1024]{0} %add.1009.clone.1.clone.1, f32[1024]{0} %add.1010.clone.1.clone.1)
}

%region_98.3023 (Arg_0.3024: f32[], Arg_1.3025: f32[]) -> f32[] {
  %Arg_0.3024 = f32[] parameter(0)
  %Arg_1.3025 = f32[] parameter(1)
  ROOT %add.3026 = f32[] add(f32[] %Arg_0.3024, f32[] %Arg_1.3025), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_97.3013 (Arg_0.3014: f32[], Arg_1.3015: f32[]) -> f32[] {
  %Arg_0.3014 = f32[] parameter(0)
  %Arg_1.3015 = f32[] parameter(1)
  ROOT %add.3016 = f32[] add(f32[] %Arg_0.3014, f32[] %Arg_1.3015), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
}

%fused_computation.115 (param_0.2160: f32[], param_1.2626: f32[4,1024], param_2.2415: f32[], param_3.1865: f32[4,1024], param_4.1313: f32[], param_5.1323: f32[], param_6.1104: f16[4,1024,1024], param_7.720: f16[4096,1024], param_8.424: f16[4,1024,1024]) -> (f32[1024], f32[1024]) {
  %param_5.1323 = f32[] parameter(5)
  %broadcast.2555 = f32[4,1024]{1,0} broadcast(f32[] %param_5.1323), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_3.1865 = f32[4,1024]{1,0} parameter(3)
  %param_4.1313 = f32[] parameter(4)
  %broadcast.2554 = f32[4,1024]{1,0} broadcast(f32[] %param_4.1313), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.841 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_3.1865, f32[4,1024]{1,0} %broadcast.2554), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_1.2626 = f32[4,1024]{1,0} parameter(1)
  %param_2.2415 = f32[] parameter(2)
  %broadcast.2553 = f32[4,1024]{1,0} broadcast(f32[] %param_2.2415), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.840 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_1.2626, f32[4,1024]{1,0} %broadcast.2553), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2446 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.840, f32[4,1024]{1,0} %divide.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.310 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.841, f32[4,1024]{1,0} %multiply.2446), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.132 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.2555, f32[4,1024]{1,0} %subtract.310), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_0.2160 = f32[] parameter(0)
  %broadcast.2552 = f32[4,1024]{1,0} broadcast(f32[] %param_0.2160), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.1850 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.132, f32[4,1024]{1,0} %broadcast.2552), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1376 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.1850), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.108 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1376), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1375 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.108), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.2551 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1375), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_8.424 = f16[4,1024,1024]{2,1,0} parameter(8)
  %convert.876 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_8.424), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %broadcast.2740 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.840), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.340 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.876, f32[4,1024,1024]{2,1,0} %broadcast.2740), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_6.1104 = f16[4,1024,1024]{2,1,0} parameter(6)
  %param_7.720 = f16[4096,1024]{1,0} parameter(7)
  %bitcast.1426 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_7.720), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.1936 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %param_6.1104, f16[4,1024,1024]{2,1,0} %bitcast.1426), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.875 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1936), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %multiply.2544 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.340, f32[4,1024,1024]{2,1,0} %convert.875), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.1227 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.2551, f32[4,1024,1024]{2,1,0} %multiply.2544), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %bitcast.632 = f32[4096,1024]{1,0} bitcast(f32[4,1024,1024]{2,1,0} %multiply.1227)
  %constant_279 = f32[] constant(0)
  %reduce.223 = f32[1024]{0} reduce(f32[4096,1024]{1,0} %bitcast.632, f32[] %constant_279), dimensions={0}, to_apply=%region_98.3023, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %bitcast.633.clone.1 = f32[4096,1024]{1,0} bitcast(f32[4,1024,1024]{2,1,0} %convert.875)
  %reduce.224.clone.1 = f32[1024]{0} reduce(f32[4096,1024]{1,0} %bitcast.633.clone.1, f32[] %constant_279), dimensions={0}, to_apply=%region_97.3013, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  ROOT %tuple.193 = (f32[1024]{0}, f32[1024]{0}) tuple(f32[1024]{0} %reduce.223, f32[1024]{0} %reduce.224.clone.1)
}

%region_112.3402 (Arg_0.3403: f32[], Arg_1.3404: f32[]) -> f32[] {
  %Arg_0.3403 = f32[] parameter(0)
  %Arg_1.3404 = f32[] parameter(1)
  ROOT %add.3405 = f32[] add(f32[] %Arg_0.3403, f32[] %Arg_1.3404), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_111.3392 (Arg_0.3393: f32[], Arg_1.3394: f32[]) -> f32[] {
  %Arg_0.3393 = f32[] parameter(0)
  %Arg_1.3394 = f32[] parameter(1)
  ROOT %add.3395 = f32[] add(f32[] %Arg_0.3393, f32[] %Arg_1.3394), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
}

%fused_computation.129 (param_0.2219: f32[], param_1.2698: f32[4,1024], param_2.2497: f32[], param_3.1946: f32[4,1024], param_4.1386: f32[], param_5.1404: f32[], param_6.1164: f16[4,1024,1024], param_7.773: f16[4,1024,1024]) -> (f32[1024], f32[1024]) {
  %param_5.1404 = f32[] parameter(5)
  %broadcast.2923 = f32[4,1024]{1,0} broadcast(f32[] %param_5.1404), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_3.1946 = f32[4,1024]{1,0} parameter(3)
  %param_4.1386 = f32[] parameter(4)
  %broadcast.2922 = f32[4,1024]{1,0} broadcast(f32[] %param_4.1386), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.965 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_3.1946, f32[4,1024]{1,0} %broadcast.2922), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_1.2698 = f32[4,1024]{1,0} parameter(1)
  %param_2.2497 = f32[] parameter(2)
  %broadcast.2921 = f32[4,1024]{1,0} broadcast(f32[] %param_2.2497), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.964 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_1.2698, f32[4,1024]{1,0} %broadcast.2921), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2622 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.964, f32[4,1024]{1,0} %divide.964), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.378 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.965, f32[4,1024]{1,0} %multiply.2622), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.168 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.2923, f32[4,1024]{1,0} %subtract.378), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_0.2219 = f32[] parameter(0)
  %broadcast.2920 = f32[4,1024]{1,0} broadcast(f32[] %param_0.2219), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.2008 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.168, f32[4,1024]{1,0} %broadcast.2920), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1474 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.2008), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.132 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1474), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1473 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.132), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.2919 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1473), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_7.773 = f16[4,1024,1024]{2,1,0} parameter(7)
  %convert.900 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_7.773), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %broadcast.2954 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.964), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.386 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.900, f32[4,1024,1024]{2,1,0} %broadcast.2954), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_6.1164 = f16[4,1024,1024]{2,1,0} parameter(6)
  %convert.899 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_6.1164), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %multiply.2638 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.386, f32[4,1024,1024]{2,1,0} %convert.899), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.1367 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.2919, f32[4,1024,1024]{2,1,0} %multiply.2638), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %bitcast.634 = f32[4096,1024]{1,0} bitcast(f32[4,1024,1024]{2,1,0} %multiply.1367)
  %constant_311 = f32[] constant(0)
  %reduce.225 = f32[1024]{0} reduce(f32[4096,1024]{1,0} %bitcast.634, f32[] %constant_311), dimensions={0}, to_apply=%region_112.3402, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %bitcast.635.clone.1 = f32[4096,1024]{1,0} bitcast(f32[4,1024,1024]{2,1,0} %convert.899)
  %reduce.226.clone.1 = f32[1024]{0} reduce(f32[4096,1024]{1,0} %bitcast.635.clone.1, f32[] %constant_311), dimensions={0}, to_apply=%region_111.3392, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  ROOT %tuple.185 = (f32[1024]{0}, f32[1024]{0}) tuple(f32[1024]{0} %reduce.225, f32[1024]{0} %reduce.226.clone.1)
}

%region_118.3482 (Arg_0.3483: f32[], Arg_1.3484: f32[]) -> f32[] {
  %Arg_0.3483 = f32[] parameter(0)
  %Arg_1.3484 = f32[] parameter(1)
  ROOT %add.3485 = f32[] add(f32[] %Arg_0.3483, f32[] %Arg_1.3484), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_117.3472 (Arg_0.3473: f32[], Arg_1.3474: f32[]) -> f32[] {
  %Arg_0.3473 = f32[] parameter(0)
  %Arg_1.3474 = f32[] parameter(1)
  ROOT %add.3475 = f32[] add(f32[] %Arg_0.3473, f32[] %Arg_1.3474), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
}

%fused_computation.155 (param_0.2234: f32[], param_1.2717: f32[4,1024], param_2.2518: f32[], param_3.1963: f32[4,1024], param_4.1404: f32[], param_5.1418: f32[], param_6.1175: f16[4,1024,1024], param_7.786: f16[4096,1024], param_8.472: f16[4,1024,1024]) -> (f32[1024], f32[1024]) {
  %param_5.1418 = f32[] parameter(5)
  %broadcast.2801 = f32[4,1024]{1,0} broadcast(f32[] %param_5.1418), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_3.1963 = f32[4,1024]{1,0} parameter(3)
  %param_4.1404 = f32[] parameter(4)
  %broadcast.2800 = f32[4,1024]{1,0} broadcast(f32[] %param_4.1404), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.929 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_3.1963, f32[4,1024]{1,0} %broadcast.2800), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_1.2717 = f32[4,1024]{1,0} parameter(1)
  %param_2.2518 = f32[] parameter(2)
  %broadcast.2799 = f32[4,1024]{1,0} broadcast(f32[] %param_2.2518), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.928 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_1.2717, f32[4,1024]{1,0} %broadcast.2799), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2556 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.928, f32[4,1024]{1,0} %divide.928), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.364 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.929, f32[4,1024]{1,0} %multiply.2556), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.158 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.2801, f32[4,1024]{1,0} %subtract.364), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_0.2234 = f32[] parameter(0)
  %broadcast.2798 = f32[4,1024]{1,0} broadcast(f32[] %param_0.2234), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.1944 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.158, f32[4,1024]{1,0} %broadcast.2798), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1454 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.1944), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.126 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1454), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1453 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.126), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.2797 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1453), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_8.472 = f16[4,1024,1024]{2,1,0} parameter(8)
  %convert.912 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_8.472), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %broadcast.2986 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.928), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.394 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.912, f32[4,1024,1024]{2,1,0} %broadcast.2986), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_6.1175 = f16[4,1024,1024]{2,1,0} parameter(6)
  %param_7.786 = f16[4096,1024]{1,0} parameter(7)
  %bitcast.1504 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_7.786), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.2032 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %param_6.1175, f16[4,1024,1024]{2,1,0} %bitcast.1504), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.911 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.2032), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %multiply.2654 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.394, f32[4,1024,1024]{2,1,0} %convert.911), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.1427 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.2797, f32[4,1024,1024]{2,1,0} %multiply.2654), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %bitcast.636 = f32[4096,1024]{1,0} bitcast(f32[4,1024,1024]{2,1,0} %multiply.1427)
  %constant_372 = f32[] constant(0)
  %reduce.227 = f32[1024]{0} reduce(f32[4096,1024]{1,0} %bitcast.636, f32[] %constant_372), dimensions={0}, to_apply=%region_118.3482, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %bitcast.637.clone.1 = f32[4096,1024]{1,0} bitcast(f32[4,1024,1024]{2,1,0} %convert.911)
  %reduce.228.clone.1 = f32[1024]{0} reduce(f32[4096,1024]{1,0} %bitcast.637.clone.1, f32[] %constant_372), dimensions={0}, to_apply=%region_117.3472, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  ROOT %tuple.183 = (f32[1024]{0}, f32[1024]{0}) tuple(f32[1024]{0} %reduce.227, f32[1024]{0} %reduce.228.clone.1)
}

%fused_computation.156 (param_0.265: f32[1024], param_1.2972: f32[], param_2.2850: f32[], param_3.2282: f32[1024], param_4.1750: f32[1024], param_5.1936: f32[1024], param_6.1936: f32[1024], param_7.1233: f32[1024], param_8.672: f16[1024], param_9.593: f32[1024], param_10.566: f32[1024], param_11.481: f32[1024], param_12.390: f32[1024], param_13.432: f32[1024], param_14.565: f32[1024], param_15.545: f32[1024], param_16.481: f32[1024], param_17.391: f32[1024], param_18.319: f32[1024], param_19.171: f32[1024], param_20.49: f16[1024], param_21.93: f32[1024], param_22.191: f32[1024], param_23.135: f32[1024], param_24.49: f32[1024], param_25.93: f32[1024], param_26.171: f32[1024], param_27.113: f32[1024], param_28.24: f32[1024], param_29.45: f32[1024], param_30.88: f32[1024], param_31.59: f32[1024], param_32.19: f16[1024], param_33.35: f32[1024]) -> (f32[1024], f32[1024], f32[1024], f32[1024], f32[1024], /*index=5*/f32[1024], f32[1024], f32[1024], f32[1024], f32[1024], /*index=10*/f32[1024], f32[1024], f32[1024], f32[1024], f32[1024], /*index=15*/f32[1024], f32[1024], f32[1024], f32[1024], f32[1024], /*index=20*/f32[1024], f32[1024], f32[1024], f32[1024]) {
  %param_0.265 = f32[1024]{0} parameter(0)
  %param_4.1750 = f32[1024]{0} parameter(4)
  %constant_377_clone_1 = f32[] constant(0.1)
  %broadcast.862.clone.1 = f32[1024]{0} broadcast(f32[] %constant_377_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1434.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_4.1750, f32[1024]{0} %broadcast.862.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_5.1936 = f32[1024]{0} parameter(5)
  %constant_378_clone_1 = f32[] constant(0.9)
  %broadcast.863.clone.1 = f32[1024]{0} broadcast(f32[] %constant_378_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1433.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_5.1936, f32[1024]{0} %broadcast.863.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1014.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1434.clone.1, f32[1024]{0} %multiply.1433.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_2.2850 = f32[] parameter(2)
  %broadcast.857 = f32[1024]{0} broadcast(f32[] %param_2.2850), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %multiply.1432.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_4.1750, f32[1024]{0} %param_4.1750), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %constant_375_clone_1 = f32[] constant(0.001)
  %broadcast.859.clone.1 = f32[1024]{0} broadcast(f32[] %constant_375_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %multiply.1431.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1432.clone.1, f32[1024]{0} %broadcast.859.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_3.2282 = f32[1024]{0} parameter(3)
  %constant_376_clone_1 = f32[] constant(0.999)
  %broadcast.861.clone.1 = f32[1024]{0} broadcast(f32[] %constant_376_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %multiply.1430.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_3.2282, f32[1024]{0} %broadcast.861.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1013.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1431.clone.1, f32[1024]{0} %multiply.1430.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_1.2972 = f32[] parameter(1)
  %broadcast.854 = f32[1024]{0} broadcast(f32[] %param_1.2972), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %divide.95 = f32[1024]{0} divide(f32[1024]{0} %add.1013.clone.1, f32[1024]{0} %broadcast.854), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.124 = f32[1024]{0} sqrt(f32[1024]{0} %divide.95), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %constant_373 = f32[] constant(1e-08)
  %broadcast.855 = f32[1024]{0} broadcast(f32[] %constant_373), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.1012 = f32[1024]{0} add(f32[1024]{0} %sqrt.124, f32[1024]{0} %broadcast.855), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1429 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.857, f32[1024]{0} %add.1012)
  %divide.94 = f32[1024]{0} divide(f32[1024]{0} %add.1014.clone.1, f32[1024]{0} %multiply.1429), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %constant_374 = f32[] constant(-0.01)
  %broadcast.858 = f32[1024]{0} broadcast(f32[] %constant_374), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %multiply.1428 = f32[1024]{0} multiply(f32[1024]{0} %divide.94, f32[1024]{0} %broadcast.858), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.1011 = f32[1024]{0} add(f32[1024]{0} %param_0.265, f32[1024]{0} %multiply.1428), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_6.1936 = f32[1024]{0} parameter(6)
  %param_8.672 = f16[1024]{0} parameter(8)
  %convert.366.clone.1.clone.1 = f32[1024]{0} convert(f16[1024]{0} %param_8.672), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1449.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %convert.366.clone.1.clone.1, f32[1024]{0} %broadcast.862.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_9.593 = f32[1024]{0} parameter(9)
  %multiply.1448.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_9.593, f32[1024]{0} %broadcast.863.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1023.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1449.clone.1.clone.1, f32[1024]{0} %multiply.1448.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1447.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %convert.366.clone.1.clone.1, f32[1024]{0} %convert.366.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1446.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1447.clone.1.clone.1, f32[1024]{0} %broadcast.859.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_7.1233 = f32[1024]{0} parameter(7)
  %multiply.1445.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_7.1233, f32[1024]{0} %broadcast.861.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1022.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1446.clone.1.clone.1, f32[1024]{0} %multiply.1445.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.99.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.1022.clone.1.clone.1, f32[1024]{0} %broadcast.854), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.126.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.99.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.1021.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.126.clone.1, f32[1024]{0} %broadcast.855), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1444.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.857, f32[1024]{0} %add.1021.clone.1)
  %divide.98.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.1023.clone.1.clone.1, f32[1024]{0} %multiply.1444.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1443.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.98.clone.1, f32[1024]{0} %broadcast.858), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.1020.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_6.1936, f32[1024]{0} %multiply.1443.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_10.566 = f32[1024]{0} parameter(10)
  %param_12.390 = f32[1024]{0} parameter(12)
  %multiply.1456.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_12.390, f32[1024]{0} %broadcast.862.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_13.432 = f32[1024]{0} parameter(13)
  %multiply.1455.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_13.432, f32[1024]{0} %broadcast.863.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1027.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1456.clone.1.clone.1, f32[1024]{0} %multiply.1455.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1454.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_12.390, f32[1024]{0} %param_12.390), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1453.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1454.clone.1.clone.1, f32[1024]{0} %broadcast.859.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_11.481 = f32[1024]{0} parameter(11)
  %multiply.1452.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_11.481, f32[1024]{0} %broadcast.861.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1026.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1453.clone.1.clone.1, f32[1024]{0} %multiply.1452.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.101.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.1026.clone.1.clone.1, f32[1024]{0} %broadcast.854), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.127.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.101.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.1025.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.127.clone.1, f32[1024]{0} %broadcast.855), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1451.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.857, f32[1024]{0} %add.1025.clone.1)
  %divide.100.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.1027.clone.1.clone.1, f32[1024]{0} %multiply.1451.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1450.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.100.clone.1, f32[1024]{0} %broadcast.858), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.1024.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_10.566, f32[1024]{0} %multiply.1450.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_14.565 = f32[1024]{0} parameter(14)
  %param_16.481 = f32[1024]{0} parameter(16)
  %multiply.1464.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_16.481, f32[1024]{0} %broadcast.862.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_17.391 = f32[1024]{0} parameter(17)
  %multiply.1463.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_17.391, f32[1024]{0} %broadcast.863.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1031.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1464.clone.1.clone.1, f32[1024]{0} %multiply.1463.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1462.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_16.481, f32[1024]{0} %param_16.481), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1461.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1462.clone.1.clone.1, f32[1024]{0} %broadcast.859.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_15.545 = f32[1024]{0} parameter(15)
  %multiply.1460.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_15.545, f32[1024]{0} %broadcast.861.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1030.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1461.clone.1.clone.1, f32[1024]{0} %multiply.1460.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.405.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.1030.clone.1.clone.1, f32[1024]{0} %broadcast.854), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.128.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.405.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.1029.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.128.clone.1, f32[1024]{0} %broadcast.855), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1459.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.857, f32[1024]{0} %add.1029.clone.1)
  %divide.404.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.1031.clone.1.clone.1, f32[1024]{0} %multiply.1459.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1458.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.404.clone.1, f32[1024]{0} %broadcast.858), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.1028.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_14.565, f32[1024]{0} %multiply.1458.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_18.319 = f32[1024]{0} parameter(18)
  %param_20.49 = f16[1024]{0} parameter(20)
  %convert.379.clone.1.clone.1 = f32[1024]{0} convert(f16[1024]{0} %param_20.49), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1509.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %convert.379.clone.1.clone.1, f32[1024]{0} %broadcast.862.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_21.93 = f32[1024]{0} parameter(21)
  %multiply.1508.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_21.93, f32[1024]{0} %broadcast.863.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1059.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1509.clone.1.clone.1, f32[1024]{0} %multiply.1508.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1507.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %convert.379.clone.1.clone.1, f32[1024]{0} %convert.379.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1506.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1507.clone.1.clone.1, f32[1024]{0} %broadcast.859.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_19.171 = f32[1024]{0} parameter(19)
  %multiply.1505.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_19.171, f32[1024]{0} %broadcast.861.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1058.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1506.clone.1.clone.1, f32[1024]{0} %multiply.1505.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.417.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.1058.clone.1.clone.1, f32[1024]{0} %broadcast.854), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.134.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.417.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.1057.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.134.clone.1, f32[1024]{0} %broadcast.855), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1504.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.857, f32[1024]{0} %add.1057.clone.1)
  %divide.416.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.1059.clone.1.clone.1, f32[1024]{0} %multiply.1504.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1503.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.416.clone.1, f32[1024]{0} %broadcast.858), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.1056.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_18.319, f32[1024]{0} %multiply.1503.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_22.191 = f32[1024]{0} parameter(22)
  %param_24.49 = f32[1024]{0} parameter(24)
  %multiply.1516.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_24.49, f32[1024]{0} %broadcast.862.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_25.93 = f32[1024]{0} parameter(25)
  %multiply.1515.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_25.93, f32[1024]{0} %broadcast.863.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1063.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1516.clone.1.clone.1, f32[1024]{0} %multiply.1515.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1514.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_24.49, f32[1024]{0} %param_24.49), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1513.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1514.clone.1.clone.1, f32[1024]{0} %broadcast.859.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_23.135 = f32[1024]{0} parameter(23)
  %multiply.1512.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_23.135, f32[1024]{0} %broadcast.861.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1062.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1513.clone.1.clone.1, f32[1024]{0} %multiply.1512.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.419.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.1062.clone.1.clone.1, f32[1024]{0} %broadcast.854), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.135.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.419.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.1061.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.135.clone.1, f32[1024]{0} %broadcast.855), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1511.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.857, f32[1024]{0} %add.1061.clone.1)
  %divide.418.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.1063.clone.1.clone.1, f32[1024]{0} %multiply.1511.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1510.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.418.clone.1, f32[1024]{0} %broadcast.858), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.1060.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_22.191, f32[1024]{0} %multiply.1510.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_26.171 = f32[1024]{0} parameter(26)
  %param_28.24 = f32[1024]{0} parameter(28)
  %multiply.1524.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_28.24, f32[1024]{0} %broadcast.862.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_29.45 = f32[1024]{0} parameter(29)
  %multiply.1523.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_29.45, f32[1024]{0} %broadcast.863.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1068.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1524.clone.1.clone.1, f32[1024]{0} %multiply.1523.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1522.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_28.24, f32[1024]{0} %param_28.24), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1521.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1522.clone.1.clone.1, f32[1024]{0} %broadcast.859.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_27.113 = f32[1024]{0} parameter(27)
  %multiply.1520.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_27.113, f32[1024]{0} %broadcast.861.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1066.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1521.clone.1.clone.1, f32[1024]{0} %multiply.1520.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.421.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.1066.clone.1.clone.1, f32[1024]{0} %broadcast.854), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.136.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.421.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.1065.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.136.clone.1, f32[1024]{0} %broadcast.855), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1519.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.857, f32[1024]{0} %add.1065.clone.1)
  %divide.420.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.1068.clone.1.clone.1, f32[1024]{0} %multiply.1519.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1518.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.420.clone.1, f32[1024]{0} %broadcast.858), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.1064.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_26.171, f32[1024]{0} %multiply.1518.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_30.88 = f32[1024]{0} parameter(30)
  %param_32.19 = f16[1024]{0} parameter(32)
  %convert.384.clone.1.clone.1 = f32[1024]{0} convert(f16[1024]{0} %param_32.19), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1539.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %convert.384.clone.1.clone.1, f32[1024]{0} %broadcast.862.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_33.35 = f32[1024]{0} parameter(33)
  %multiply.1538.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_33.35, f32[1024]{0} %broadcast.863.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1077.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1539.clone.1.clone.1, f32[1024]{0} %multiply.1538.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1537.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %convert.384.clone.1.clone.1, f32[1024]{0} %convert.384.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1536.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1537.clone.1.clone.1, f32[1024]{0} %broadcast.859.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_31.59 = f32[1024]{0} parameter(31)
  %multiply.1535.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_31.59, f32[1024]{0} %broadcast.861.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1076.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1536.clone.1.clone.1, f32[1024]{0} %multiply.1535.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.425.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.1076.clone.1.clone.1, f32[1024]{0} %broadcast.854), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.138.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.425.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.1075.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.138.clone.1, f32[1024]{0} %broadcast.855), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1534.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.857, f32[1024]{0} %add.1075.clone.1)
  %divide.424.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.1077.clone.1.clone.1, f32[1024]{0} %multiply.1534.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1533.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.424.clone.1, f32[1024]{0} %broadcast.858), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.1074.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_30.88, f32[1024]{0} %multiply.1533.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  ROOT %tuple.281 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) tuple(f32[1024]{0} %add.1011, f32[1024]{0} %add.1013.clone.1, f32[1024]{0} %add.1014.clone.1, f32[1024]{0} %add.1020.clone.1, f32[1024]{0} %add.1022.clone.1.clone.1, /*index=5*/f32[1024]{0} %add.1023.clone.1.clone.1, f32[1024]{0} %add.1024.clone.1, f32[1024]{0} %add.1026.clone.1.clone.1, f32[1024]{0} %add.1027.clone.1.clone.1, f32[1024]{0} %add.1028.clone.1, /*index=10*/f32[1024]{0} %add.1030.clone.1.clone.1, f32[1024]{0} %add.1031.clone.1.clone.1, f32[1024]{0} %add.1056.clone.1, f32[1024]{0} %add.1058.clone.1.clone.1, f32[1024]{0} %add.1059.clone.1.clone.1, /*index=15*/f32[1024]{0} %add.1060.clone.1, f32[1024]{0} %add.1062.clone.1.clone.1, f32[1024]{0} %add.1063.clone.1.clone.1, f32[1024]{0} %add.1064.clone.1, f32[1024]{0} %add.1066.clone.1.clone.1, /*index=20*/f32[1024]{0} %add.1068.clone.1.clone.1, f32[1024]{0} %add.1074.clone.1, f32[1024]{0} %add.1076.clone.1.clone.1, f32[1024]{0} %add.1077.clone.1.clone.1)
}

%region_132.3861 (Arg_0.3862: f32[], Arg_1.3863: f32[]) -> f32[] {
  %Arg_0.3862 = f32[] parameter(0)
  %Arg_1.3863 = f32[] parameter(1)
  ROOT %add.3864 = f32[] add(f32[] %Arg_0.3862, f32[] %Arg_1.3863), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_131.3851 (Arg_0.3852: f32[], Arg_1.3853: f32[]) -> f32[] {
  %Arg_0.3852 = f32[] parameter(0)
  %Arg_1.3853 = f32[] parameter(1)
  ROOT %add.3854 = f32[] add(f32[] %Arg_0.3852, f32[] %Arg_1.3853), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
}

%fused_computation.169 (param_0.2293: f32[], param_1.2789: f32[4,1024], param_2.2600: f32[], param_3.2044: f32[4,1024], param_4.1477: f32[], param_5.1499: f32[], param_6.1235: f16[4,1024,1024], param_7.839: f16[4,1024,1024]) -> (f32[1024], f32[1024]) {
  %param_5.1499 = f32[] parameter(5)
  %broadcast.3169 = f32[4,1024]{1,0} broadcast(f32[] %param_5.1499), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_3.2044 = f32[4,1024]{1,0} parameter(3)
  %param_4.1477 = f32[] parameter(4)
  %broadcast.3168 = f32[4,1024]{1,0} broadcast(f32[] %param_4.1477), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.1053 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_3.2044, f32[4,1024]{1,0} %broadcast.3168), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_1.2789 = f32[4,1024]{1,0} parameter(1)
  %param_2.2600 = f32[] parameter(2)
  %broadcast.3167 = f32[4,1024]{1,0} broadcast(f32[] %param_2.2600), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.1052 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_1.2789, f32[4,1024]{1,0} %broadcast.3167), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2732 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.1052, f32[4,1024]{1,0} %divide.1052), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.432 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.1053, f32[4,1024]{1,0} %multiply.2732), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.194 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.3169, f32[4,1024]{1,0} %subtract.432), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_0.2293 = f32[] parameter(0)
  %broadcast.3166 = f32[4,1024]{1,0} broadcast(f32[] %param_0.2293), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.2107 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.194, f32[4,1024]{1,0} %broadcast.3166), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1552 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.2107), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.150 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1552), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1551 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.150), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.3165 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1551), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_7.839 = f16[4,1024,1024]{2,1,0} parameter(7)
  %convert.936 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_7.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %broadcast.3200 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.1052), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.440 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.936, f32[4,1024,1024]{2,1,0} %broadcast.3200), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_6.1235 = f16[4,1024,1024]{2,1,0} parameter(6)
  %convert.935 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_6.1235), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %multiply.2748 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.440, f32[4,1024,1024]{2,1,0} %convert.935), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.1457 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.3165, f32[4,1024,1024]{2,1,0} %multiply.2748), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %bitcast.638 = f32[4096,1024]{1,0} bitcast(f32[4,1024,1024]{2,1,0} %multiply.1457)
  %constant_400 = f32[] constant(0)
  %reduce.229 = f32[1024]{0} reduce(f32[4096,1024]{1,0} %bitcast.638, f32[] %constant_400), dimensions={0}, to_apply=%region_132.3861, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %bitcast.639.clone.1 = f32[4096,1024]{1,0} bitcast(f32[4,1024,1024]{2,1,0} %convert.935)
  %reduce.230.clone.1 = f32[1024]{0} reduce(f32[4096,1024]{1,0} %bitcast.639.clone.1, f32[] %constant_400), dimensions={0}, to_apply=%region_131.3851, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  ROOT %tuple.175 = (f32[1024]{0}, f32[1024]{0}) tuple(f32[1024]{0} %reduce.229, f32[1024]{0} %reduce.230.clone.1)
}

%region_138.3941 (Arg_0.3942: f32[], Arg_1.3943: f32[]) -> f32[] {
  %Arg_0.3942 = f32[] parameter(0)
  %Arg_1.3943 = f32[] parameter(1)
  ROOT %add.3944 = f32[] add(f32[] %Arg_0.3942, f32[] %Arg_1.3943), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_137.3931 (Arg_0.3932: f32[], Arg_1.3933: f32[]) -> f32[] {
  %Arg_0.3932 = f32[] parameter(0)
  %Arg_1.3933 = f32[] parameter(1)
  ROOT %add.3934 = f32[] add(f32[] %Arg_0.3932, f32[] %Arg_1.3933), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
}

%fused_computation.195 (param_0.2308: f32[], param_1.2808: f32[4,1024], param_2.2621: f32[], param_3.2061: f32[4,1024], param_4.1495: f32[], param_5.1513: f32[], param_6.1246: f16[4,1024,1024], param_7.852: f16[4096,1024], param_8.520: f16[4,1024,1024]) -> (f32[1024], f32[1024]) {
  %param_5.1513 = f32[] parameter(5)
  %broadcast.3047 = f32[4,1024]{1,0} broadcast(f32[] %param_5.1513), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_3.2061 = f32[4,1024]{1,0} parameter(3)
  %param_4.1495 = f32[] parameter(4)
  %broadcast.3046 = f32[4,1024]{1,0} broadcast(f32[] %param_4.1495), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.1017 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_3.2061, f32[4,1024]{1,0} %broadcast.3046), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_1.2808 = f32[4,1024]{1,0} parameter(1)
  %param_2.2621 = f32[] parameter(2)
  %broadcast.3045 = f32[4,1024]{1,0} broadcast(f32[] %param_2.2621), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.1016 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_1.2808, f32[4,1024]{1,0} %broadcast.3045), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2666 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.1016, f32[4,1024]{1,0} %divide.1016), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.418 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.1017, f32[4,1024]{1,0} %multiply.2666), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.184 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.3047, f32[4,1024]{1,0} %subtract.418), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_0.2308 = f32[] parameter(0)
  %broadcast.3044 = f32[4,1024]{1,0} broadcast(f32[] %param_0.2308), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.2041 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.184, f32[4,1024]{1,0} %broadcast.3044), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1532 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.2041), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.144 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1532), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1531 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.144), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.3043 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1531), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_8.520 = f16[4,1024,1024]{2,1,0} parameter(8)
  %convert.948 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_8.520), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %broadcast.3232 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.1016), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.448 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.948, f32[4,1024,1024]{2,1,0} %broadcast.3232), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_6.1246 = f16[4,1024,1024]{2,1,0} parameter(6)
  %param_7.852 = f16[4096,1024]{1,0} parameter(7)
  %bitcast.1582 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_7.852), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.2131 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %param_6.1246, f16[4,1024,1024]{2,1,0} %bitcast.1582), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.947 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.2131), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %multiply.2764 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.448, f32[4,1024,1024]{2,1,0} %convert.947), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.1517 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.3043, f32[4,1024,1024]{2,1,0} %multiply.2764), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %bitcast.640 = f32[4096,1024]{1,0} bitcast(f32[4,1024,1024]{2,1,0} %multiply.1517)
  %constant_456 = f32[] constant(0)
  %reduce.231 = f32[1024]{0} reduce(f32[4096,1024]{1,0} %bitcast.640, f32[] %constant_456), dimensions={0}, to_apply=%region_138.3941, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %bitcast.641.clone.1 = f32[4096,1024]{1,0} bitcast(f32[4,1024,1024]{2,1,0} %convert.947)
  %reduce.232.clone.1 = f32[1024]{0} reduce(f32[4096,1024]{1,0} %bitcast.641.clone.1, f32[] %constant_456), dimensions={0}, to_apply=%region_137.3931, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  ROOT %tuple.173 = (f32[1024]{0}, f32[1024]{0}) tuple(f32[1024]{0} %reduce.231, f32[1024]{0} %reduce.232.clone.1)
}

%fused_computation.200 (param_0.340: f32[512,1024], param_1.2985: f32[], param_2.2876: f32[], param_3.2308: f32[512,1024], param_4.1789: f16[512,1024], param_5.2023: f32[512,1024], param_6.1924: f32[512,1024], param_7.1221: f32[512,1024], param_8.657: f16[512,1024], param_9.566: f32[512,1024], param_10.512: f32[512,1024], param_11.443: f32[512,1024], param_12.375: f16[512,1024], param_13.405: f32[512,1024], param_14.511: f32[512,1024], param_15.507: f32[512,1024], param_16.466: f16[512,1024], param_17.364: f32[512,1024], param_18.265: f32[512,1024], param_19.133: f32[512,1024], param_20.34: f16[512,1024], param_21.66: f32[512,1024], param_22.137: f32[512,1024], param_23.97: f32[512,1024], param_24.34: f16[512,1024], param_25.66: f32[512,1024]) -> (f32[512,1024], f32[512,1024], f32[512,1024], f32[512,1024], f32[512,1024], /*index=5*/f32[512,1024], f32[512,1024], f32[512,1024], f32[512,1024], f32[512,1024], /*index=10*/f32[512,1024], f32[512,1024], f32[512,1024], f32[512,1024], f32[512,1024], /*index=15*/f32[512,1024], f32[512,1024], f32[512,1024]) {
  %param_0.340 = f32[512,1024]{1,0} parameter(0)
  %param_4.1789 = f16[512,1024]{1,0} parameter(4)
  %convert.382.clone.1 = f32[512,1024]{1,0} convert(f16[512,1024]{1,0} %param_4.1789), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %constant_469_clone_1 = f32[] constant(0.1)
  %broadcast.1212.clone.1 = f32[512,1024]{1,0} broadcast(f32[] %constant_469_clone_1), dimensions={}
  %multiply.1532.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %convert.382.clone.1, f32[512,1024]{1,0} %broadcast.1212.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_5.2023 = f32[512,1024]{1,0} parameter(5)
  %constant_470_clone_1 = f32[] constant(0.9)
  %broadcast.1213.clone.1 = f32[512,1024]{1,0} broadcast(f32[] %constant_470_clone_1), dimensions={}
  %multiply.1531.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param_5.2023, f32[512,1024]{1,0} %broadcast.1213.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1073.clone.1 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %multiply.1532.clone.1, f32[512,1024]{1,0} %multiply.1531.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_2.2876 = f32[] parameter(2)
  %broadcast.1207 = f32[512,1024]{1,0} broadcast(f32[] %param_2.2876), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %multiply.1530.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %convert.382.clone.1, f32[512,1024]{1,0} %convert.382.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %constant_467_clone_1 = f32[] constant(0.001)
  %broadcast.1210.clone.1 = f32[512,1024]{1,0} broadcast(f32[] %constant_467_clone_1), dimensions={}
  %multiply.1529.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %multiply.1530.clone.1, f32[512,1024]{1,0} %broadcast.1210.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_3.2308 = f32[512,1024]{1,0} parameter(3)
  %constant_468_clone_1 = f32[] constant(0.999)
  %broadcast.1211.clone.1 = f32[512,1024]{1,0} broadcast(f32[] %constant_468_clone_1), dimensions={}
  %multiply.1528.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param_3.2308, f32[512,1024]{1,0} %broadcast.1211.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1072.clone.1 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %multiply.1529.clone.1, f32[512,1024]{1,0} %multiply.1528.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_1.2985 = f32[] parameter(1)
  %broadcast.1205 = f32[512,1024]{1,0} broadcast(f32[] %param_1.2985), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %divide.423 = f32[512,1024]{1,0} divide(f32[512,1024]{1,0} %add.1072.clone.1, f32[512,1024]{1,0} %broadcast.1205), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.137 = f32[512,1024]{1,0} sqrt(f32[512,1024]{1,0} %divide.423), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %constant_465 = f32[] constant(1e-08)
  %broadcast.1206 = f32[512,1024]{1,0} broadcast(f32[] %constant_465), dimensions={}
  %add.1071 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %sqrt.137, f32[512,1024]{1,0} %broadcast.1206), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1527 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %broadcast.1207, f32[512,1024]{1,0} %add.1071)
  %divide.422 = f32[512,1024]{1,0} divide(f32[512,1024]{1,0} %add.1073.clone.1, f32[512,1024]{1,0} %multiply.1527), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %constant_464 = f32[] constant(0.0001)
  %broadcast.1208 = f32[512,1024]{1,0} broadcast(f32[] %constant_464), dimensions={}
  %multiply.1526 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param_0.340, f32[512,1024]{1,0} %broadcast.1208), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.1070 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %divide.422, f32[512,1024]{1,0} %multiply.1526), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %constant_466 = f32[] constant(-0.01)
  %broadcast.1209 = f32[512,1024]{1,0} broadcast(f32[] %constant_466), dimensions={}
  %multiply.1525 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %add.1070, f32[512,1024]{1,0} %broadcast.1209), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.1069 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %param_0.340, f32[512,1024]{1,0} %multiply.1525), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_6.1924 = f32[512,1024]{1,0} parameter(6)
  %param_8.657 = f16[512,1024]{1,0} parameter(8)
  %convert.2.clone.1.clone.1 = f32[512,1024]{1,0} convert(f16[512,1024]{1,0} %param_8.657), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.9.clone.1.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %convert.2.clone.1.clone.1, f32[512,1024]{1,0} %broadcast.1212.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_9.566 = f32[512,1024]{1,0} parameter(9)
  %multiply.8.clone.1.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param_9.566, f32[512,1024]{1,0} %broadcast.1213.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.513.clone.1.clone.1 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %multiply.9.clone.1.clone.1, f32[512,1024]{1,0} %multiply.8.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.7.clone.1.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %convert.2.clone.1.clone.1, f32[512,1024]{1,0} %convert.2.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.6.clone.1.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %multiply.7.clone.1.clone.1, f32[512,1024]{1,0} %broadcast.1210.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_7.1221 = f32[512,1024]{1,0} parameter(7)
  %multiply.5.clone.1.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param_7.1221, f32[512,1024]{1,0} %broadcast.1211.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.496.clone.1.clone.1 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %multiply.6.clone.1.clone.1, f32[512,1024]{1,0} %multiply.5.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.1.clone.1 = f32[512,1024]{1,0} divide(f32[512,1024]{1,0} %add.496.clone.1.clone.1, f32[512,1024]{1,0} %broadcast.1205), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.77.clone.1 = f32[512,1024]{1,0} sqrt(f32[512,1024]{1,0} %divide.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.464.clone.1 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %sqrt.77.clone.1, f32[512,1024]{1,0} %broadcast.1206), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.4.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %broadcast.1207, f32[512,1024]{1,0} %add.464.clone.1)
  %divide.0.clone.1 = f32[512,1024]{1,0} divide(f32[512,1024]{1,0} %add.513.clone.1.clone.1, f32[512,1024]{1,0} %multiply.4.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.3.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param_6.1924, f32[512,1024]{1,0} %broadcast.1208), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.95.clone.1 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %divide.0.clone.1, f32[512,1024]{1,0} %multiply.3.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.2.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %add.95.clone.1, f32[512,1024]{1,0} %broadcast.1209), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.25.clone.1 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %param_6.1924, f32[512,1024]{1,0} %multiply.2.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_10.512 = f32[512,1024]{1,0} parameter(10)
  %param_12.375 = f16[512,1024]{1,0} parameter(12)
  %convert.20.clone.1.clone.1 = f32[512,1024]{1,0} convert(f16[512,1024]{1,0} %param_12.375), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.99.clone.1.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %convert.20.clone.1.clone.1, f32[512,1024]{1,0} %broadcast.1212.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_13.405 = f32[512,1024]{1,0} parameter(13)
  %multiply.98.clone.1.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param_13.405, f32[512,1024]{1,0} %broadcast.1213.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.858.clone.1.clone.1 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %multiply.99.clone.1.clone.1, f32[512,1024]{1,0} %multiply.98.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.97.clone.1.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %convert.20.clone.1.clone.1, f32[512,1024]{1,0} %convert.20.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.96.clone.1.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %multiply.97.clone.1.clone.1, f32[512,1024]{1,0} %broadcast.1210.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_11.443 = f32[512,1024]{1,0} parameter(11)
  %multiply.95.clone.1.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param_11.443, f32[512,1024]{1,0} %broadcast.1211.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.857.clone.1.clone.1 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %multiply.96.clone.1.clone.1, f32[512,1024]{1,0} %multiply.95.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.25.clone.1 = f32[512,1024]{1,0} divide(f32[512,1024]{1,0} %add.857.clone.1.clone.1, f32[512,1024]{1,0} %broadcast.1205), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.89.clone.1 = f32[512,1024]{1,0} sqrt(f32[512,1024]{1,0} %divide.25.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.856.clone.1 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %sqrt.89.clone.1, f32[512,1024]{1,0} %broadcast.1206), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.94.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %broadcast.1207, f32[512,1024]{1,0} %add.856.clone.1)
  %divide.24.clone.1 = f32[512,1024]{1,0} divide(f32[512,1024]{1,0} %add.858.clone.1.clone.1, f32[512,1024]{1,0} %multiply.94.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.93.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param_10.512, f32[512,1024]{1,0} %broadcast.1208), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.855.clone.1 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %divide.24.clone.1, f32[512,1024]{1,0} %multiply.93.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.92.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %add.855.clone.1, f32[512,1024]{1,0} %broadcast.1209), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.854.clone.1 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %param_10.512, f32[512,1024]{1,0} %multiply.92.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_14.511 = f32[512,1024]{1,0} parameter(14)
  %param_16.466 = f16[512,1024]{1,0} parameter(16)
  %convert.38.clone.1.clone.1 = f32[512,1024]{1,0} convert(f16[512,1024]{1,0} %param_16.466), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.639.clone.1.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %convert.38.clone.1.clone.1, f32[512,1024]{1,0} %broadcast.1212.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_17.364 = f32[512,1024]{1,0} parameter(17)
  %multiply.637.clone.1.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param_17.364, f32[512,1024]{1,0} %broadcast.1213.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.912.clone.1.clone.1 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %multiply.639.clone.1.clone.1, f32[512,1024]{1,0} %multiply.637.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.635.clone.1.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %convert.38.clone.1.clone.1, f32[512,1024]{1,0} %convert.38.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.633.clone.1.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %multiply.635.clone.1.clone.1, f32[512,1024]{1,0} %broadcast.1210.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_15.507 = f32[512,1024]{1,0} parameter(15)
  %multiply.631.clone.1.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param_15.507, f32[512,1024]{1,0} %broadcast.1211.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.911.clone.1.clone.1 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %multiply.633.clone.1.clone.1, f32[512,1024]{1,0} %multiply.631.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.49.clone.1 = f32[512,1024]{1,0} divide(f32[512,1024]{1,0} %add.911.clone.1.clone.1, f32[512,1024]{1,0} %broadcast.1205), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.101.clone.1 = f32[512,1024]{1,0} sqrt(f32[512,1024]{1,0} %divide.49.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.910.clone.1 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %sqrt.101.clone.1, f32[512,1024]{1,0} %broadcast.1206), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.629.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %broadcast.1207, f32[512,1024]{1,0} %add.910.clone.1)
  %divide.48.clone.1 = f32[512,1024]{1,0} divide(f32[512,1024]{1,0} %add.912.clone.1.clone.1, f32[512,1024]{1,0} %multiply.629.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.627.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param_14.511, f32[512,1024]{1,0} %broadcast.1208), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.909.clone.1 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %divide.48.clone.1, f32[512,1024]{1,0} %multiply.627.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.626.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %add.909.clone.1, f32[512,1024]{1,0} %broadcast.1209), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.908.clone.1 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %param_14.511, f32[512,1024]{1,0} %multiply.626.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_18.265 = f32[512,1024]{1,0} parameter(18)
  %param_20.34 = f16[512,1024]{1,0} parameter(20)
  %convert.346.clone.1.clone.1 = f32[512,1024]{1,0} convert(f16[512,1024]{1,0} %param_20.34), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1352.clone.1.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %convert.346.clone.1.clone.1, f32[512,1024]{1,0} %broadcast.1212.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_21.66 = f32[512,1024]{1,0} parameter(21)
  %multiply.1351.clone.1.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param_21.66, f32[512,1024]{1,0} %broadcast.1213.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.966.clone.1.clone.1 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %multiply.1352.clone.1.clone.1, f32[512,1024]{1,0} %multiply.1351.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1350.clone.1.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %convert.346.clone.1.clone.1, f32[512,1024]{1,0} %convert.346.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1349.clone.1.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %multiply.1350.clone.1.clone.1, f32[512,1024]{1,0} %broadcast.1210.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_19.133 = f32[512,1024]{1,0} parameter(19)
  %multiply.1348.clone.1.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param_19.133, f32[512,1024]{1,0} %broadcast.1211.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.965.clone.1.clone.1 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %multiply.1349.clone.1.clone.1, f32[512,1024]{1,0} %multiply.1348.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.73.clone.1 = f32[512,1024]{1,0} divide(f32[512,1024]{1,0} %add.965.clone.1.clone.1, f32[512,1024]{1,0} %broadcast.1205), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.113.clone.1 = f32[512,1024]{1,0} sqrt(f32[512,1024]{1,0} %divide.73.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.964.clone.1 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %sqrt.113.clone.1, f32[512,1024]{1,0} %broadcast.1206), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1347.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %broadcast.1207, f32[512,1024]{1,0} %add.964.clone.1)
  %divide.72.clone.1 = f32[512,1024]{1,0} divide(f32[512,1024]{1,0} %add.966.clone.1.clone.1, f32[512,1024]{1,0} %multiply.1347.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1346.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param_18.265, f32[512,1024]{1,0} %broadcast.1208), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.962.clone.1 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %divide.72.clone.1, f32[512,1024]{1,0} %multiply.1346.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.1345.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %add.962.clone.1, f32[512,1024]{1,0} %broadcast.1209), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.961.clone.1 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %param_18.265, f32[512,1024]{1,0} %multiply.1345.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_22.137 = f32[512,1024]{1,0} parameter(22)
  %param_24.34 = f16[512,1024]{1,0} parameter(24)
  %convert.364.clone.1.clone.1 = f32[512,1024]{1,0} convert(f16[512,1024]{1,0} %param_24.34), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1442.clone.1.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %convert.364.clone.1.clone.1, f32[512,1024]{1,0} %broadcast.1212.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_25.66 = f32[512,1024]{1,0} parameter(25)
  %multiply.1441.clone.1.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param_25.66, f32[512,1024]{1,0} %broadcast.1213.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1019.clone.1.clone.1 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %multiply.1442.clone.1.clone.1, f32[512,1024]{1,0} %multiply.1441.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1440.clone.1.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %convert.364.clone.1.clone.1, f32[512,1024]{1,0} %convert.364.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1439.clone.1.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %multiply.1440.clone.1.clone.1, f32[512,1024]{1,0} %broadcast.1210.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_23.97 = f32[512,1024]{1,0} parameter(23)
  %multiply.1438.clone.1.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param_23.97, f32[512,1024]{1,0} %broadcast.1211.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1018.clone.1.clone.1 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %multiply.1439.clone.1.clone.1, f32[512,1024]{1,0} %multiply.1438.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.97.clone.1 = f32[512,1024]{1,0} divide(f32[512,1024]{1,0} %add.1018.clone.1.clone.1, f32[512,1024]{1,0} %broadcast.1205), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.125.clone.1 = f32[512,1024]{1,0} sqrt(f32[512,1024]{1,0} %divide.97.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.1017.clone.1 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %sqrt.125.clone.1, f32[512,1024]{1,0} %broadcast.1206), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1437.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %broadcast.1207, f32[512,1024]{1,0} %add.1017.clone.1)
  %divide.96.clone.1 = f32[512,1024]{1,0} divide(f32[512,1024]{1,0} %add.1019.clone.1.clone.1, f32[512,1024]{1,0} %multiply.1437.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1436.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param_22.137, f32[512,1024]{1,0} %broadcast.1208), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.1016.clone.1 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %divide.96.clone.1, f32[512,1024]{1,0} %multiply.1436.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.1435.clone.1 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %add.1016.clone.1, f32[512,1024]{1,0} %broadcast.1209), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.1015.clone.1 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %param_22.137, f32[512,1024]{1,0} %multiply.1435.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  ROOT %tuple.260 = (f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=5*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=10*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=15*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}) tuple(f32[512,1024]{1,0} %add.1069, f32[512,1024]{1,0} %add.1072.clone.1, f32[512,1024]{1,0} %add.1073.clone.1, f32[512,1024]{1,0} %add.25.clone.1, f32[512,1024]{1,0} %add.496.clone.1.clone.1, /*index=5*/f32[512,1024]{1,0} %add.513.clone.1.clone.1, f32[512,1024]{1,0} %add.854.clone.1, f32[512,1024]{1,0} %add.857.clone.1.clone.1, f32[512,1024]{1,0} %add.858.clone.1.clone.1, f32[512,1024]{1,0} %add.908.clone.1, /*index=10*/f32[512,1024]{1,0} %add.911.clone.1.clone.1, f32[512,1024]{1,0} %add.912.clone.1.clone.1, f32[512,1024]{1,0} %add.961.clone.1, f32[512,1024]{1,0} %add.965.clone.1.clone.1, f32[512,1024]{1,0} %add.966.clone.1.clone.1, /*index=15*/f32[512,1024]{1,0} %add.1015.clone.1, f32[512,1024]{1,0} %add.1018.clone.1.clone.1, f32[512,1024]{1,0} %add.1019.clone.1.clone.1)
}

%fused_computation.206 (param_0.350: f32[1024], param_1.2987: f32[], param_2.2880: f32[], param_3.2312: f32[1024], param_4.1795: f32[1024], param_5.2036: f32[1024], param_6.1940: f32[1024], param_7.1237: f32[1024], param_8.677: f32[1024], param_9.602: f32[1024], param_10.584: f32[1024], param_11.493: f32[1024], param_12.395: f16[1024], param_13.441: f32[1024], param_14.583: f32[1024], param_15.559: f32[1024], param_16.486: f32[1024], param_17.400: f32[1024], param_18.337: f32[1024], param_19.183: f32[1024], param_20.54: f32[1024], param_21.102: f32[1024], param_22.209: f32[1024], param_23.147: f32[1024], param_24.54: f32[1024], param_25.102: f32[1024]) -> (f32[1024], f32[1024], f32[1024], f32[1024], f32[1024], /*index=5*/f32[1024], f32[1024], f32[1024], f32[1024], f32[1024], /*index=10*/f32[1024], f32[1024], f32[1024], f32[1024], f32[1024], /*index=15*/f32[1024], f32[1024], f32[1024]) {
  %param_0.350 = f32[1024]{0} parameter(0)
  %param_4.1795 = f32[1024]{0} parameter(4)
  %constant_482_clone_1 = f32[] constant(0.1)
  %broadcast.1228.clone.1 = f32[1024]{0} broadcast(f32[] %constant_482_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1546.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_4.1795, f32[1024]{0} %broadcast.1228.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_5.2036 = f32[1024]{0} parameter(5)
  %constant_483_clone_1 = f32[] constant(0.9)
  %broadcast.1229.clone.1 = f32[1024]{0} broadcast(f32[] %constant_483_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1545.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_5.2036, f32[1024]{0} %broadcast.1229.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1081.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1546.clone.1, f32[1024]{0} %multiply.1545.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_2.2880 = f32[] parameter(2)
  %broadcast.1224 = f32[1024]{0} broadcast(f32[] %param_2.2880), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %multiply.1544.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_4.1795, f32[1024]{0} %param_4.1795), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %constant_480_clone_1 = f32[] constant(0.001)
  %broadcast.1226.clone.1 = f32[1024]{0} broadcast(f32[] %constant_480_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %multiply.1543.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1544.clone.1, f32[1024]{0} %broadcast.1226.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_3.2312 = f32[1024]{0} parameter(3)
  %constant_481_clone_1 = f32[] constant(0.999)
  %broadcast.1227.clone.1 = f32[1024]{0} broadcast(f32[] %constant_481_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %multiply.1542.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_3.2312, f32[1024]{0} %broadcast.1227.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1080.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1543.clone.1, f32[1024]{0} %multiply.1542.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_1.2987 = f32[] parameter(1)
  %broadcast.1222 = f32[1024]{0} broadcast(f32[] %param_1.2987), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %divide.427 = f32[1024]{0} divide(f32[1024]{0} %add.1080.clone.1, f32[1024]{0} %broadcast.1222), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.139 = f32[1024]{0} sqrt(f32[1024]{0} %divide.427), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %constant_478 = f32[] constant(1e-08)
  %broadcast.1223 = f32[1024]{0} broadcast(f32[] %constant_478), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.1079 = f32[1024]{0} add(f32[1024]{0} %sqrt.139, f32[1024]{0} %broadcast.1223), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1541 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.1224, f32[1024]{0} %add.1079)
  %divide.426 = f32[1024]{0} divide(f32[1024]{0} %add.1081.clone.1, f32[1024]{0} %multiply.1541), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %constant_479 = f32[] constant(-0.01)
  %broadcast.1225 = f32[1024]{0} broadcast(f32[] %constant_479), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %multiply.1540 = f32[1024]{0} multiply(f32[1024]{0} %divide.426, f32[1024]{0} %broadcast.1225), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.1078 = f32[1024]{0} add(f32[1024]{0} %param_0.350, f32[1024]{0} %multiply.1540), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_6.1940 = f32[1024]{0} parameter(6)
  %param_8.677 = f32[1024]{0} parameter(8)
  %multiply.1554.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_8.677, f32[1024]{0} %broadcast.1228.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_9.602 = f32[1024]{0} parameter(9)
  %multiply.1553.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_9.602, f32[1024]{0} %broadcast.1229.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1085.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1554.clone.1.clone.1, f32[1024]{0} %multiply.1553.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1552.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_8.677, f32[1024]{0} %param_8.677), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1551.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1552.clone.1.clone.1, f32[1024]{0} %broadcast.1226.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_7.1237 = f32[1024]{0} parameter(7)
  %multiply.1550.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_7.1237, f32[1024]{0} %broadcast.1227.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1084.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1551.clone.1.clone.1, f32[1024]{0} %multiply.1550.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.429.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.1084.clone.1.clone.1, f32[1024]{0} %broadcast.1222), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.140.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.429.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.1083.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.140.clone.1, f32[1024]{0} %broadcast.1223), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1549.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.1224, f32[1024]{0} %add.1083.clone.1)
  %divide.428.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.1085.clone.1.clone.1, f32[1024]{0} %multiply.1549.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1548.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.428.clone.1, f32[1024]{0} %broadcast.1225), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.1082.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_6.1940, f32[1024]{0} %multiply.1548.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_10.584 = f32[1024]{0} parameter(10)
  %param_12.395 = f16[1024]{0} parameter(12)
  %convert.397.clone.1.clone.1 = f32[1024]{0} convert(f16[1024]{0} %param_12.395), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1599.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %convert.397.clone.1.clone.1, f32[1024]{0} %broadcast.1228.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_13.441 = f32[1024]{0} parameter(13)
  %multiply.1598.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_13.441, f32[1024]{0} %broadcast.1229.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1112.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1599.clone.1.clone.1, f32[1024]{0} %multiply.1598.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1597.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %convert.397.clone.1.clone.1, f32[1024]{0} %convert.397.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1596.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1597.clone.1.clone.1, f32[1024]{0} %broadcast.1226.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_11.493 = f32[1024]{0} parameter(11)
  %multiply.1595.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_11.493, f32[1024]{0} %broadcast.1227.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1111.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1596.clone.1.clone.1, f32[1024]{0} %multiply.1595.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.441.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.1111.clone.1.clone.1, f32[1024]{0} %broadcast.1222), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.146.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.441.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.1110.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.146.clone.1, f32[1024]{0} %broadcast.1223), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1594.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.1224, f32[1024]{0} %add.1110.clone.1)
  %divide.440.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.1112.clone.1.clone.1, f32[1024]{0} %multiply.1594.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1593.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.440.clone.1, f32[1024]{0} %broadcast.1225), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.1109.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_10.584, f32[1024]{0} %multiply.1593.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_14.583 = f32[1024]{0} parameter(14)
  %param_16.486 = f32[1024]{0} parameter(16)
  %multiply.1606.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_16.486, f32[1024]{0} %broadcast.1228.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_17.400 = f32[1024]{0} parameter(17)
  %multiply.1605.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_17.400, f32[1024]{0} %broadcast.1229.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1116.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1606.clone.1.clone.1, f32[1024]{0} %multiply.1605.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1604.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_16.486, f32[1024]{0} %param_16.486), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1603.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1604.clone.1.clone.1, f32[1024]{0} %broadcast.1226.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_15.559 = f32[1024]{0} parameter(15)
  %multiply.1602.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_15.559, f32[1024]{0} %broadcast.1227.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1115.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1603.clone.1.clone.1, f32[1024]{0} %multiply.1602.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.443.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.1115.clone.1.clone.1, f32[1024]{0} %broadcast.1222), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.147.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.443.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.1114.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.147.clone.1, f32[1024]{0} %broadcast.1223), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1601.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.1224, f32[1024]{0} %add.1114.clone.1)
  %divide.442.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.1116.clone.1.clone.1, f32[1024]{0} %multiply.1601.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1600.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.442.clone.1, f32[1024]{0} %broadcast.1225), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.1113.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_14.583, f32[1024]{0} %multiply.1600.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_18.337 = f32[1024]{0} parameter(18)
  %param_20.54 = f32[1024]{0} parameter(20)
  %multiply.1614.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_20.54, f32[1024]{0} %broadcast.1228.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_21.102 = f32[1024]{0} parameter(21)
  %multiply.1613.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_21.102, f32[1024]{0} %broadcast.1229.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1120.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1614.clone.1.clone.1, f32[1024]{0} %multiply.1613.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1612.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_20.54, f32[1024]{0} %param_20.54), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1611.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1612.clone.1.clone.1, f32[1024]{0} %broadcast.1226.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_19.183 = f32[1024]{0} parameter(19)
  %multiply.1610.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_19.183, f32[1024]{0} %broadcast.1227.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1119.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1611.clone.1.clone.1, f32[1024]{0} %multiply.1610.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.445.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.1119.clone.1.clone.1, f32[1024]{0} %broadcast.1222), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.148.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.445.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.1118.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.148.clone.1, f32[1024]{0} %broadcast.1223), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1609.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.1224, f32[1024]{0} %add.1118.clone.1)
  %divide.444.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.1120.clone.1.clone.1, f32[1024]{0} %multiply.1609.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1608.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.444.clone.1, f32[1024]{0} %broadcast.1225), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.1117.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_18.337, f32[1024]{0} %multiply.1608.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_22.209 = f32[1024]{0} parameter(22)
  %param_24.54 = f32[1024]{0} parameter(24)
  %multiply.1647.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_24.54, f32[1024]{0} %broadcast.1228.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_25.102 = f32[1024]{0} parameter(25)
  %multiply.1646.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_25.102, f32[1024]{0} %broadcast.1229.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1139.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1647.clone.1.clone.1, f32[1024]{0} %multiply.1646.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1645.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_24.54, f32[1024]{0} %param_24.54), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1644.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1645.clone.1.clone.1, f32[1024]{0} %broadcast.1226.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_23.147 = f32[1024]{0} parameter(23)
  %multiply.1643.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_23.147, f32[1024]{0} %broadcast.1227.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1138.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1644.clone.1.clone.1, f32[1024]{0} %multiply.1643.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.453.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.1138.clone.1.clone.1, f32[1024]{0} %broadcast.1222), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.151.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.453.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.1137.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.151.clone.1, f32[1024]{0} %broadcast.1223), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1642.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.1224, f32[1024]{0} %add.1137.clone.1)
  %divide.452.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.1139.clone.1.clone.1, f32[1024]{0} %multiply.1642.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1641.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.452.clone.1, f32[1024]{0} %broadcast.1225), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.1136.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_22.209, f32[1024]{0} %multiply.1641.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  ROOT %tuple.286 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) tuple(f32[1024]{0} %add.1078, f32[1024]{0} %add.1080.clone.1, f32[1024]{0} %add.1081.clone.1, f32[1024]{0} %add.1082.clone.1, f32[1024]{0} %add.1084.clone.1.clone.1, /*index=5*/f32[1024]{0} %add.1085.clone.1.clone.1, f32[1024]{0} %add.1109.clone.1, f32[1024]{0} %add.1111.clone.1.clone.1, f32[1024]{0} %add.1112.clone.1.clone.1, f32[1024]{0} %add.1113.clone.1, /*index=10*/f32[1024]{0} %add.1115.clone.1.clone.1, f32[1024]{0} %add.1116.clone.1.clone.1, f32[1024]{0} %add.1117.clone.1, f32[1024]{0} %add.1119.clone.1.clone.1, f32[1024]{0} %add.1120.clone.1.clone.1, /*index=15*/f32[1024]{0} %add.1136.clone.1, f32[1024]{0} %add.1138.clone.1.clone.1, f32[1024]{0} %add.1139.clone.1.clone.1)
}

%region_152.4320 (Arg_0.4321: f32[], Arg_1.4322: f32[]) -> f32[] {
  %Arg_0.4321 = f32[] parameter(0)
  %Arg_1.4322 = f32[] parameter(1)
  ROOT %add.4323 = f32[] add(f32[] %Arg_0.4321, f32[] %Arg_1.4322), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_151.4310 (Arg_0.4311: f32[], Arg_1.4312: f32[]) -> f32[] {
  %Arg_0.4311 = f32[] parameter(0)
  %Arg_1.4312 = f32[] parameter(1)
  ROOT %add.4313 = f32[] add(f32[] %Arg_0.4311, f32[] %Arg_1.4312), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
}

%fused_computation.209 (param_0.2367: f32[], param_1.2880: f32[4,1024], param_2.2703: f32[], param_3.2142: f32[4,1024], param_4.1568: f32[], param_5.1594: f32[], param_6.1306: f16[4,1024,1024], param_7.905: f16[4,1024,1024]) -> (f32[1024], f32[1024]) {
  %param_5.1594 = f32[] parameter(5)
  %broadcast.3415 = f32[4,1024]{1,0} broadcast(f32[] %param_5.1594), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_3.2142 = f32[4,1024]{1,0} parameter(3)
  %param_4.1568 = f32[] parameter(4)
  %broadcast.3414 = f32[4,1024]{1,0} broadcast(f32[] %param_4.1568), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.1141 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_3.2142, f32[4,1024]{1,0} %broadcast.3414), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_1.2880 = f32[4,1024]{1,0} parameter(1)
  %param_2.2703 = f32[] parameter(2)
  %broadcast.3413 = f32[4,1024]{1,0} broadcast(f32[] %param_2.2703), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.1140 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_1.2880, f32[4,1024]{1,0} %broadcast.3413), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2842 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.1140, f32[4,1024]{1,0} %divide.1140), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.486 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.1141, f32[4,1024]{1,0} %multiply.2842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.220 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.3415, f32[4,1024]{1,0} %subtract.486), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_0.2367 = f32[] parameter(0)
  %broadcast.3412 = f32[4,1024]{1,0} broadcast(f32[] %param_0.2367), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.2205 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.220, f32[4,1024]{1,0} %broadcast.3412), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1630 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.2205), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.168 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1630), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1629 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.168), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.3411 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1629), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_7.905 = f16[4,1024,1024]{2,1,0} parameter(7)
  %convert.972 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_7.905), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %broadcast.3446 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.1140), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.494 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.972, f32[4,1024,1024]{2,1,0} %broadcast.3446), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_6.1306 = f16[4,1024,1024]{2,1,0} parameter(6)
  %convert.971 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_6.1306), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %multiply.2858 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.494, f32[4,1024,1024]{2,1,0} %convert.971), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.1547 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.3411, f32[4,1024,1024]{2,1,0} %multiply.2858), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %bitcast.642 = f32[4096,1024]{1,0} bitcast(f32[4,1024,1024]{2,1,0} %multiply.1547)
  %constant_484 = f32[] constant(0)
  %reduce.233 = f32[1024]{0} reduce(f32[4096,1024]{1,0} %bitcast.642, f32[] %constant_484), dimensions={0}, to_apply=%region_152.4320, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %bitcast.643.clone.1 = f32[4096,1024]{1,0} bitcast(f32[4,1024,1024]{2,1,0} %convert.971)
  %reduce.234.clone.1 = f32[1024]{0} reduce(f32[4096,1024]{1,0} %bitcast.643.clone.1, f32[] %constant_484), dimensions={0}, to_apply=%region_151.4310, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  ROOT %tuple.165 = (f32[1024]{0}, f32[1024]{0}) tuple(f32[1024]{0} %reduce.233, f32[1024]{0} %reduce.234.clone.1)
}

%fused_computation.214 (param_0.364: f32[1024,512], param_1.2989: f32[], param_2.2884: f32[], param_3.2316: f32[1024,512], param_4.1801: f16[1024,512], param_5.2049: f32[1024,512], param_6.1920: f32[1024,512], param_7.1217: f32[1024,512], param_8.652: f16[1024,512], param_9.556: f32[1024,512], param_10.491: f32[1024,512], param_11.429: f32[1024,512], param_12.370: f16[1024,512], param_13.395: f32[1024,512], param_14.490: f32[1024,512], param_15.493: f32[1024,512], param_16.461: f16[1024,512], param_17.354: f32[1024,512], param_18.244: f32[1024,512], param_19.119: f32[1024,512], param_20.29: f16[1024,512], param_21.56: f32[1024,512], param_22.116: f32[1024,512], param_23.83: f32[1024,512], param_24.29: f16[1024,512], param_25.56: f32[1024,512]) -> (f32[1024,512], f32[1024,512], f32[1024,512], f32[1024,512], f32[1024,512], /*index=5*/f32[1024,512], f32[1024,512], f32[1024,512], f32[1024,512], f32[1024,512], /*index=10*/f32[1024,512], f32[1024,512], f32[1024,512], f32[1024,512], f32[1024,512], /*index=15*/f32[1024,512], f32[1024,512], f32[1024,512]) {
  %param_0.364 = f32[1024,512]{1,0} parameter(0)
  %param_4.1801 = f16[1024,512]{1,0} parameter(4)
  %convert.387.clone.1 = f32[1024,512]{1,0} convert(f16[1024,512]{1,0} %param_4.1801), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %constant_497_clone_1 = f32[] constant(0.1)
  %broadcast.1245.clone.1 = f32[1024,512]{1,0} broadcast(f32[] %constant_497_clone_1), dimensions={}
  %multiply.1562.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %convert.387.clone.1, f32[1024,512]{1,0} %broadcast.1245.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_5.2049 = f32[1024,512]{1,0} parameter(5)
  %constant_498_clone_1 = f32[] constant(0.9)
  %broadcast.1246.clone.1 = f32[1024,512]{1,0} broadcast(f32[] %constant_498_clone_1), dimensions={}
  %multiply.1561.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param_5.2049, f32[1024,512]{1,0} %broadcast.1246.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1090.clone.1 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %multiply.1562.clone.1, f32[1024,512]{1,0} %multiply.1561.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_2.2884 = f32[] parameter(2)
  %broadcast.1240 = f32[1024,512]{1,0} broadcast(f32[] %param_2.2884), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %multiply.1560.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %convert.387.clone.1, f32[1024,512]{1,0} %convert.387.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %constant_495_clone_1 = f32[] constant(0.001)
  %broadcast.1243.clone.1 = f32[1024,512]{1,0} broadcast(f32[] %constant_495_clone_1), dimensions={}
  %multiply.1559.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %multiply.1560.clone.1, f32[1024,512]{1,0} %broadcast.1243.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_3.2316 = f32[1024,512]{1,0} parameter(3)
  %constant_496_clone_1 = f32[] constant(0.999)
  %broadcast.1244.clone.1 = f32[1024,512]{1,0} broadcast(f32[] %constant_496_clone_1), dimensions={}
  %multiply.1558.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param_3.2316, f32[1024,512]{1,0} %broadcast.1244.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1089.clone.1 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %multiply.1559.clone.1, f32[1024,512]{1,0} %multiply.1558.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_1.2989 = f32[] parameter(1)
  %broadcast.1238 = f32[1024,512]{1,0} broadcast(f32[] %param_1.2989), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %divide.431 = f32[1024,512]{1,0} divide(f32[1024,512]{1,0} %add.1089.clone.1, f32[1024,512]{1,0} %broadcast.1238), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.141 = f32[1024,512]{1,0} sqrt(f32[1024,512]{1,0} %divide.431), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %constant_493 = f32[] constant(1e-08)
  %broadcast.1239 = f32[1024,512]{1,0} broadcast(f32[] %constant_493), dimensions={}
  %add.1088 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %sqrt.141, f32[1024,512]{1,0} %broadcast.1239), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1557 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %broadcast.1240, f32[1024,512]{1,0} %add.1088)
  %divide.430 = f32[1024,512]{1,0} divide(f32[1024,512]{1,0} %add.1090.clone.1, f32[1024,512]{1,0} %multiply.1557), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %constant_492 = f32[] constant(0.0001)
  %broadcast.1241 = f32[1024,512]{1,0} broadcast(f32[] %constant_492), dimensions={}
  %multiply.1556 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param_0.364, f32[1024,512]{1,0} %broadcast.1241), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.1087 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %divide.430, f32[1024,512]{1,0} %multiply.1556), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %constant_494 = f32[] constant(-0.01)
  %broadcast.1242 = f32[1024,512]{1,0} broadcast(f32[] %constant_494), dimensions={}
  %multiply.1555 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %add.1087, f32[1024,512]{1,0} %broadcast.1242), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.1086 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %param_0.364, f32[1024,512]{1,0} %multiply.1555), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_6.1920 = f32[1024,512]{1,0} parameter(6)
  %param_8.652 = f16[1024,512]{1,0} parameter(8)
  %convert.7.clone.1.clone.1 = f32[1024,512]{1,0} convert(f16[1024,512]{1,0} %param_8.652), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.39.clone.1.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %convert.7.clone.1.clone.1, f32[1024,512]{1,0} %broadcast.1245.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_9.556 = f32[1024,512]{1,0} parameter(9)
  %multiply.38.clone.1.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param_9.556, f32[1024,512]{1,0} %broadcast.1246.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.646.clone.1.clone.1 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %multiply.39.clone.1.clone.1, f32[1024,512]{1,0} %multiply.38.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.37.clone.1.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %convert.7.clone.1.clone.1, f32[1024,512]{1,0} %convert.7.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.36.clone.1.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %multiply.37.clone.1.clone.1, f32[1024,512]{1,0} %broadcast.1243.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_7.1217 = f32[1024,512]{1,0} parameter(7)
  %multiply.35.clone.1.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param_7.1217, f32[1024,512]{1,0} %broadcast.1244.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.644.clone.1.clone.1 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %multiply.36.clone.1.clone.1, f32[1024,512]{1,0} %multiply.35.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.9.clone.1 = f32[1024,512]{1,0} divide(f32[1024,512]{1,0} %add.644.clone.1.clone.1, f32[1024,512]{1,0} %broadcast.1238), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.81.clone.1 = f32[1024,512]{1,0} sqrt(f32[1024,512]{1,0} %divide.9.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.642.clone.1 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %sqrt.81.clone.1, f32[1024,512]{1,0} %broadcast.1239), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.34.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %broadcast.1240, f32[1024,512]{1,0} %add.642.clone.1)
  %divide.8.clone.1 = f32[1024,512]{1,0} divide(f32[1024,512]{1,0} %add.646.clone.1.clone.1, f32[1024,512]{1,0} %multiply.34.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.33.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param_6.1920, f32[1024,512]{1,0} %broadcast.1241), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.640.clone.1 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %divide.8.clone.1, f32[1024,512]{1,0} %multiply.33.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.32.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %add.640.clone.1, f32[1024,512]{1,0} %broadcast.1242), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.638.clone.1 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %param_6.1920, f32[1024,512]{1,0} %multiply.32.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_10.491 = f32[1024,512]{1,0} parameter(10)
  %param_12.370 = f16[1024,512]{1,0} parameter(12)
  %convert.25.clone.1.clone.1 = f32[1024,512]{1,0} convert(f16[1024,512]{1,0} %param_12.370), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.129.clone.1.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %convert.25.clone.1.clone.1, f32[1024,512]{1,0} %broadcast.1245.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_13.395 = f32[1024,512]{1,0} parameter(13)
  %multiply.128.clone.1.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param_13.395, f32[1024,512]{1,0} %broadcast.1246.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.877.clone.1.clone.1 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %multiply.129.clone.1.clone.1, f32[1024,512]{1,0} %multiply.128.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.127.clone.1.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %convert.25.clone.1.clone.1, f32[1024,512]{1,0} %convert.25.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.126.clone.1.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %multiply.127.clone.1.clone.1, f32[1024,512]{1,0} %broadcast.1243.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_11.429 = f32[1024,512]{1,0} parameter(11)
  %multiply.125.clone.1.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param_11.429, f32[1024,512]{1,0} %broadcast.1244.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.876.clone.1.clone.1 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %multiply.126.clone.1.clone.1, f32[1024,512]{1,0} %multiply.125.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.33.clone.1 = f32[1024,512]{1,0} divide(f32[1024,512]{1,0} %add.876.clone.1.clone.1, f32[1024,512]{1,0} %broadcast.1238), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.93.clone.1 = f32[1024,512]{1,0} sqrt(f32[1024,512]{1,0} %divide.33.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.874.clone.1 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %sqrt.93.clone.1, f32[1024,512]{1,0} %broadcast.1239), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.124.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %broadcast.1240, f32[1024,512]{1,0} %add.874.clone.1)
  %divide.32.clone.1 = f32[1024,512]{1,0} divide(f32[1024,512]{1,0} %add.877.clone.1.clone.1, f32[1024,512]{1,0} %multiply.124.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.123.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param_10.491, f32[1024,512]{1,0} %broadcast.1241), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.873.clone.1 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %divide.32.clone.1, f32[1024,512]{1,0} %multiply.123.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.122.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %add.873.clone.1, f32[1024,512]{1,0} %broadcast.1242), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.872.clone.1 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %param_10.491, f32[1024,512]{1,0} %multiply.122.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_14.490 = f32[1024,512]{1,0} parameter(14)
  %param_16.461 = f16[1024,512]{1,0} parameter(16)
  %convert.333.clone.1.clone.1 = f32[1024,512]{1,0} convert(f16[1024,512]{1,0} %param_16.461), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.847.clone.1.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %convert.333.clone.1.clone.1, f32[1024,512]{1,0} %broadcast.1245.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_17.354 = f32[1024,512]{1,0} parameter(17)
  %multiply.843.clone.1.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param_17.354, f32[1024,512]{1,0} %broadcast.1246.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.929.clone.1.clone.1 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %multiply.847.clone.1.clone.1, f32[1024,512]{1,0} %multiply.843.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.841.clone.1.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %convert.333.clone.1.clone.1, f32[1024,512]{1,0} %convert.333.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.839.clone.1.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %multiply.841.clone.1.clone.1, f32[1024,512]{1,0} %broadcast.1243.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_15.493 = f32[1024,512]{1,0} parameter(15)
  %multiply.837.clone.1.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param_15.493, f32[1024,512]{1,0} %broadcast.1244.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.928.clone.1.clone.1 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %multiply.839.clone.1.clone.1, f32[1024,512]{1,0} %multiply.837.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.57.clone.1 = f32[1024,512]{1,0} divide(f32[1024,512]{1,0} %add.928.clone.1.clone.1, f32[1024,512]{1,0} %broadcast.1238), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.105.clone.1 = f32[1024,512]{1,0} sqrt(f32[1024,512]{1,0} %divide.57.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.927.clone.1 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %sqrt.105.clone.1, f32[1024,512]{1,0} %broadcast.1239), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.835.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %broadcast.1240, f32[1024,512]{1,0} %add.927.clone.1)
  %divide.56.clone.1 = f32[1024,512]{1,0} divide(f32[1024,512]{1,0} %add.929.clone.1.clone.1, f32[1024,512]{1,0} %multiply.835.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.833.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param_14.490, f32[1024,512]{1,0} %broadcast.1241), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.926.clone.1 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %divide.56.clone.1, f32[1024,512]{1,0} %multiply.833.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.831.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %add.926.clone.1, f32[1024,512]{1,0} %broadcast.1242), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.925.clone.1 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %param_14.490, f32[1024,512]{1,0} %multiply.831.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_18.244 = f32[1024,512]{1,0} parameter(18)
  %param_20.29 = f16[1024,512]{1,0} parameter(20)
  %convert.351.clone.1.clone.1 = f32[1024,512]{1,0} convert(f16[1024,512]{1,0} %param_20.29), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1382.clone.1.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %convert.351.clone.1.clone.1, f32[1024,512]{1,0} %broadcast.1245.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_21.56 = f32[1024,512]{1,0} parameter(21)
  %multiply.1381.clone.1.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param_21.56, f32[1024,512]{1,0} %broadcast.1246.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.984.clone.1.clone.1 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %multiply.1382.clone.1.clone.1, f32[1024,512]{1,0} %multiply.1381.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1380.clone.1.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %convert.351.clone.1.clone.1, f32[1024,512]{1,0} %convert.351.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1379.clone.1.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %multiply.1380.clone.1.clone.1, f32[1024,512]{1,0} %broadcast.1243.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_19.119 = f32[1024,512]{1,0} parameter(19)
  %multiply.1378.clone.1.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param_19.119, f32[1024,512]{1,0} %broadcast.1244.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.983.clone.1.clone.1 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %multiply.1379.clone.1.clone.1, f32[1024,512]{1,0} %multiply.1378.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.81.clone.1 = f32[1024,512]{1,0} divide(f32[1024,512]{1,0} %add.983.clone.1.clone.1, f32[1024,512]{1,0} %broadcast.1238), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.117.clone.1 = f32[1024,512]{1,0} sqrt(f32[1024,512]{1,0} %divide.81.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.982.clone.1 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %sqrt.117.clone.1, f32[1024,512]{1,0} %broadcast.1239), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1377.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %broadcast.1240, f32[1024,512]{1,0} %add.982.clone.1)
  %divide.80.clone.1 = f32[1024,512]{1,0} divide(f32[1024,512]{1,0} %add.984.clone.1.clone.1, f32[1024,512]{1,0} %multiply.1377.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1376.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param_18.244, f32[1024,512]{1,0} %broadcast.1241), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.981.clone.1 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %divide.80.clone.1, f32[1024,512]{1,0} %multiply.1376.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.1375.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %add.981.clone.1, f32[1024,512]{1,0} %broadcast.1242), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.980.clone.1 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %param_18.244, f32[1024,512]{1,0} %multiply.1375.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_22.116 = f32[1024,512]{1,0} parameter(22)
  %param_24.29 = f16[1024,512]{1,0} parameter(24)
  %convert.369.clone.1.clone.1 = f32[1024,512]{1,0} convert(f16[1024,512]{1,0} %param_24.29), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1472.clone.1.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %convert.369.clone.1.clone.1, f32[1024,512]{1,0} %broadcast.1245.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_25.56 = f32[1024,512]{1,0} parameter(25)
  %multiply.1471.clone.1.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param_25.56, f32[1024,512]{1,0} %broadcast.1246.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1036.clone.1.clone.1 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %multiply.1472.clone.1.clone.1, f32[1024,512]{1,0} %multiply.1471.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1470.clone.1.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %convert.369.clone.1.clone.1, f32[1024,512]{1,0} %convert.369.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1469.clone.1.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %multiply.1470.clone.1.clone.1, f32[1024,512]{1,0} %broadcast.1243.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_23.83 = f32[1024,512]{1,0} parameter(23)
  %multiply.1468.clone.1.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param_23.83, f32[1024,512]{1,0} %broadcast.1244.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1035.clone.1.clone.1 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %multiply.1469.clone.1.clone.1, f32[1024,512]{1,0} %multiply.1468.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.407.clone.1 = f32[1024,512]{1,0} divide(f32[1024,512]{1,0} %add.1035.clone.1.clone.1, f32[1024,512]{1,0} %broadcast.1238), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.129.clone.1 = f32[1024,512]{1,0} sqrt(f32[1024,512]{1,0} %divide.407.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.1034.clone.1 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %sqrt.129.clone.1, f32[1024,512]{1,0} %broadcast.1239), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1467.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %broadcast.1240, f32[1024,512]{1,0} %add.1034.clone.1)
  %divide.406.clone.1 = f32[1024,512]{1,0} divide(f32[1024,512]{1,0} %add.1036.clone.1.clone.1, f32[1024,512]{1,0} %multiply.1467.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1466.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param_22.116, f32[1024,512]{1,0} %broadcast.1241), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.1033.clone.1 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %divide.406.clone.1, f32[1024,512]{1,0} %multiply.1466.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.1465.clone.1 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %add.1033.clone.1, f32[1024,512]{1,0} %broadcast.1242), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.1032.clone.1 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %param_22.116, f32[1024,512]{1,0} %multiply.1465.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  ROOT %tuple.255 = (f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=5*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=10*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=15*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}) tuple(f32[1024,512]{1,0} %add.1086, f32[1024,512]{1,0} %add.1089.clone.1, f32[1024,512]{1,0} %add.1090.clone.1, f32[1024,512]{1,0} %add.638.clone.1, f32[1024,512]{1,0} %add.644.clone.1.clone.1, /*index=5*/f32[1024,512]{1,0} %add.646.clone.1.clone.1, f32[1024,512]{1,0} %add.872.clone.1, f32[1024,512]{1,0} %add.876.clone.1.clone.1, f32[1024,512]{1,0} %add.877.clone.1.clone.1, f32[1024,512]{1,0} %add.925.clone.1, /*index=10*/f32[1024,512]{1,0} %add.928.clone.1.clone.1, f32[1024,512]{1,0} %add.929.clone.1.clone.1, f32[1024,512]{1,0} %add.980.clone.1, f32[1024,512]{1,0} %add.983.clone.1.clone.1, f32[1024,512]{1,0} %add.984.clone.1.clone.1, /*index=15*/f32[1024,512]{1,0} %add.1032.clone.1, f32[1024,512]{1,0} %add.1035.clone.1.clone.1, f32[1024,512]{1,0} %add.1036.clone.1.clone.1)
}

%fused_computation.217 (param_0.369: f32[512], param_1.2990: f32[], param_2.2886: f32[], param_3.2318: f32[512], param_4.1804: f16[512], param_5.2056: f32[512], param_6.1916: f32[512], param_7.1213: f32[512], param_8.647: f16[512], param_9.546: f32[512], param_10.470: f32[512], param_11.415: f32[512], param_12.365: f16[512], param_13.385: f32[512], param_14.469: f32[512], param_15.479: f32[512], param_16.456: f16[512], param_17.344: f32[512], param_18.223: f32[512], param_19.105: f32[512], param_20.24: f16[512], param_21.46: f32[512], param_22.95: f32[512], param_23.69: f32[512], param_24.24: f16[512], param_25.46: f32[512]) -> (f32[512], f32[512], f32[512], f32[512], f32[512], /*index=5*/f32[512], f32[512], f32[512], f32[512], f32[512], /*index=10*/f32[512], f32[512], f32[512], f32[512], f32[512], /*index=15*/f32[512], f32[512], f32[512]) {
  %param_0.369 = f32[512]{0} parameter(0)
  %param_4.1804 = f16[512]{0} parameter(4)
  %convert.389.clone.1 = f32[512]{0} convert(f16[512]{0} %param_4.1804), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %constant_503_clone_1 = f32[] constant(0.1)
  %broadcast.1253.clone.1 = f32[512]{0} broadcast(f32[] %constant_503_clone_1), dimensions={}
  %multiply.1569.clone.1 = f32[512]{0} multiply(f32[512]{0} %convert.389.clone.1, f32[512]{0} %broadcast.1253.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_5.2056 = f32[512]{0} parameter(5)
  %constant_504_clone_1 = f32[] constant(0.9)
  %broadcast.1254.clone.1 = f32[512]{0} broadcast(f32[] %constant_504_clone_1), dimensions={}
  %multiply.1568.clone.1 = f32[512]{0} multiply(f32[512]{0} %param_5.2056, f32[512]{0} %broadcast.1254.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1094.clone.1 = f32[512]{0} add(f32[512]{0} %multiply.1569.clone.1, f32[512]{0} %multiply.1568.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_2.2886 = f32[] parameter(2)
  %broadcast.1249 = f32[512]{0} broadcast(f32[] %param_2.2886), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %multiply.1567.clone.1 = f32[512]{0} multiply(f32[512]{0} %convert.389.clone.1, f32[512]{0} %convert.389.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %constant_501_clone_1 = f32[] constant(0.001)
  %broadcast.1251.clone.1 = f32[512]{0} broadcast(f32[] %constant_501_clone_1), dimensions={}
  %multiply.1566.clone.1 = f32[512]{0} multiply(f32[512]{0} %multiply.1567.clone.1, f32[512]{0} %broadcast.1251.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_3.2318 = f32[512]{0} parameter(3)
  %constant_502_clone_1 = f32[] constant(0.999)
  %broadcast.1252.clone.1 = f32[512]{0} broadcast(f32[] %constant_502_clone_1), dimensions={}
  %multiply.1565.clone.1 = f32[512]{0} multiply(f32[512]{0} %param_3.2318, f32[512]{0} %broadcast.1252.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1093.clone.1 = f32[512]{0} add(f32[512]{0} %multiply.1566.clone.1, f32[512]{0} %multiply.1565.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_1.2990 = f32[] parameter(1)
  %broadcast.1247 = f32[512]{0} broadcast(f32[] %param_1.2990), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %divide.433 = f32[512]{0} divide(f32[512]{0} %add.1093.clone.1, f32[512]{0} %broadcast.1247), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.142 = f32[512]{0} sqrt(f32[512]{0} %divide.433), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %constant_499 = f32[] constant(1e-08)
  %broadcast.1248 = f32[512]{0} broadcast(f32[] %constant_499), dimensions={}
  %add.1092 = f32[512]{0} add(f32[512]{0} %sqrt.142, f32[512]{0} %broadcast.1248), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1564 = f32[512]{0} multiply(f32[512]{0} %broadcast.1249, f32[512]{0} %add.1092)
  %divide.432 = f32[512]{0} divide(f32[512]{0} %add.1094.clone.1, f32[512]{0} %multiply.1564), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %constant_500 = f32[] constant(-0.01)
  %broadcast.1250 = f32[512]{0} broadcast(f32[] %constant_500), dimensions={}
  %multiply.1563 = f32[512]{0} multiply(f32[512]{0} %divide.432, f32[512]{0} %broadcast.1250), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.1091 = f32[512]{0} add(f32[512]{0} %param_0.369, f32[512]{0} %multiply.1563), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_6.1916 = f32[512]{0} parameter(6)
  %param_8.647 = f16[512]{0} parameter(8)
  %convert.9.clone.1.clone.1 = f32[512]{0} convert(f16[512]{0} %param_8.647), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.46.clone.1.clone.1 = f32[512]{0} multiply(f32[512]{0} %convert.9.clone.1.clone.1, f32[512]{0} %broadcast.1253.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_9.546 = f32[512]{0} parameter(9)
  %multiply.45.clone.1.clone.1 = f32[512]{0} multiply(f32[512]{0} %param_9.546, f32[512]{0} %broadcast.1254.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.664.clone.1.clone.1 = f32[512]{0} add(f32[512]{0} %multiply.46.clone.1.clone.1, f32[512]{0} %multiply.45.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.44.clone.1.clone.1 = f32[512]{0} multiply(f32[512]{0} %convert.9.clone.1.clone.1, f32[512]{0} %convert.9.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.43.clone.1.clone.1 = f32[512]{0} multiply(f32[512]{0} %multiply.44.clone.1.clone.1, f32[512]{0} %broadcast.1251.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_7.1213 = f32[512]{0} parameter(7)
  %multiply.42.clone.1.clone.1 = f32[512]{0} multiply(f32[512]{0} %param_7.1213, f32[512]{0} %broadcast.1252.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.654.clone.1.clone.1 = f32[512]{0} add(f32[512]{0} %multiply.43.clone.1.clone.1, f32[512]{0} %multiply.42.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.11.clone.1 = f32[512]{0} divide(f32[512]{0} %add.654.clone.1.clone.1, f32[512]{0} %broadcast.1247), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.82.clone.1 = f32[512]{0} sqrt(f32[512]{0} %divide.11.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.650.clone.1 = f32[512]{0} add(f32[512]{0} %sqrt.82.clone.1, f32[512]{0} %broadcast.1248), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.41.clone.1 = f32[512]{0} multiply(f32[512]{0} %broadcast.1249, f32[512]{0} %add.650.clone.1)
  %divide.10.clone.1 = f32[512]{0} divide(f32[512]{0} %add.664.clone.1.clone.1, f32[512]{0} %multiply.41.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.40.clone.1 = f32[512]{0} multiply(f32[512]{0} %divide.10.clone.1, f32[512]{0} %broadcast.1250), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.648.clone.1 = f32[512]{0} add(f32[512]{0} %param_6.1916, f32[512]{0} %multiply.40.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_10.470 = f32[512]{0} parameter(10)
  %param_12.365 = f16[512]{0} parameter(12)
  %convert.27.clone.1.clone.1 = f32[512]{0} convert(f16[512]{0} %param_12.365), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.136.clone.1.clone.1 = f32[512]{0} multiply(f32[512]{0} %convert.27.clone.1.clone.1, f32[512]{0} %broadcast.1253.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_13.385 = f32[512]{0} parameter(13)
  %multiply.135.clone.1.clone.1 = f32[512]{0} multiply(f32[512]{0} %param_13.385, f32[512]{0} %broadcast.1254.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.881.clone.1.clone.1 = f32[512]{0} add(f32[512]{0} %multiply.136.clone.1.clone.1, f32[512]{0} %multiply.135.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.134.clone.1.clone.1 = f32[512]{0} multiply(f32[512]{0} %convert.27.clone.1.clone.1, f32[512]{0} %convert.27.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.133.clone.1.clone.1 = f32[512]{0} multiply(f32[512]{0} %multiply.134.clone.1.clone.1, f32[512]{0} %broadcast.1251.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_11.415 = f32[512]{0} parameter(11)
  %multiply.132.clone.1.clone.1 = f32[512]{0} multiply(f32[512]{0} %param_11.415, f32[512]{0} %broadcast.1252.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.880.clone.1.clone.1 = f32[512]{0} add(f32[512]{0} %multiply.133.clone.1.clone.1, f32[512]{0} %multiply.132.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.35.clone.1 = f32[512]{0} divide(f32[512]{0} %add.880.clone.1.clone.1, f32[512]{0} %broadcast.1247), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.94.clone.1 = f32[512]{0} sqrt(f32[512]{0} %divide.35.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.879.clone.1 = f32[512]{0} add(f32[512]{0} %sqrt.94.clone.1, f32[512]{0} %broadcast.1248), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.131.clone.1 = f32[512]{0} multiply(f32[512]{0} %broadcast.1249, f32[512]{0} %add.879.clone.1)
  %divide.34.clone.1 = f32[512]{0} divide(f32[512]{0} %add.881.clone.1.clone.1, f32[512]{0} %multiply.131.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.130.clone.1 = f32[512]{0} multiply(f32[512]{0} %divide.34.clone.1, f32[512]{0} %broadcast.1250), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.878.clone.1 = f32[512]{0} add(f32[512]{0} %param_10.470, f32[512]{0} %multiply.130.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_14.469 = f32[512]{0} parameter(14)
  %param_16.456 = f16[512]{0} parameter(16)
  %convert.335.clone.1.clone.1 = f32[512]{0} convert(f16[512]{0} %param_16.456), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.991.clone.1.clone.1 = f32[512]{0} multiply(f32[512]{0} %convert.335.clone.1.clone.1, f32[512]{0} %broadcast.1253.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_17.344 = f32[512]{0} parameter(17)
  %multiply.978.clone.1.clone.1 = f32[512]{0} multiply(f32[512]{0} %param_17.344, f32[512]{0} %broadcast.1254.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.933.clone.1.clone.1 = f32[512]{0} add(f32[512]{0} %multiply.991.clone.1.clone.1, f32[512]{0} %multiply.978.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.971.clone.1.clone.1 = f32[512]{0} multiply(f32[512]{0} %convert.335.clone.1.clone.1, f32[512]{0} %convert.335.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.892.clone.1.clone.1 = f32[512]{0} multiply(f32[512]{0} %multiply.971.clone.1.clone.1, f32[512]{0} %broadcast.1251.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_15.479 = f32[512]{0} parameter(15)
  %multiply.891.clone.1.clone.1 = f32[512]{0} multiply(f32[512]{0} %param_15.479, f32[512]{0} %broadcast.1252.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.932.clone.1.clone.1 = f32[512]{0} add(f32[512]{0} %multiply.892.clone.1.clone.1, f32[512]{0} %multiply.891.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.59.clone.1 = f32[512]{0} divide(f32[512]{0} %add.932.clone.1.clone.1, f32[512]{0} %broadcast.1247), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.106.clone.1 = f32[512]{0} sqrt(f32[512]{0} %divide.59.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.931.clone.1 = f32[512]{0} add(f32[512]{0} %sqrt.106.clone.1, f32[512]{0} %broadcast.1248), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.878.clone.1 = f32[512]{0} multiply(f32[512]{0} %broadcast.1249, f32[512]{0} %add.931.clone.1)
  %divide.58.clone.1 = f32[512]{0} divide(f32[512]{0} %add.933.clone.1.clone.1, f32[512]{0} %multiply.878.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.871.clone.1 = f32[512]{0} multiply(f32[512]{0} %divide.58.clone.1, f32[512]{0} %broadcast.1250), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.930.clone.1 = f32[512]{0} add(f32[512]{0} %param_14.469, f32[512]{0} %multiply.871.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_18.223 = f32[512]{0} parameter(18)
  %param_20.24 = f16[512]{0} parameter(20)
  %convert.353.clone.1.clone.1 = f32[512]{0} convert(f16[512]{0} %param_20.24), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1389.clone.1.clone.1 = f32[512]{0} multiply(f32[512]{0} %convert.353.clone.1.clone.1, f32[512]{0} %broadcast.1253.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_21.46 = f32[512]{0} parameter(21)
  %multiply.1388.clone.1.clone.1 = f32[512]{0} multiply(f32[512]{0} %param_21.46, f32[512]{0} %broadcast.1254.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.988.clone.1.clone.1 = f32[512]{0} add(f32[512]{0} %multiply.1389.clone.1.clone.1, f32[512]{0} %multiply.1388.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1387.clone.1.clone.1 = f32[512]{0} multiply(f32[512]{0} %convert.353.clone.1.clone.1, f32[512]{0} %convert.353.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1386.clone.1.clone.1 = f32[512]{0} multiply(f32[512]{0} %multiply.1387.clone.1.clone.1, f32[512]{0} %broadcast.1251.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_19.105 = f32[512]{0} parameter(19)
  %multiply.1385.clone.1.clone.1 = f32[512]{0} multiply(f32[512]{0} %param_19.105, f32[512]{0} %broadcast.1252.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.987.clone.1.clone.1 = f32[512]{0} add(f32[512]{0} %multiply.1386.clone.1.clone.1, f32[512]{0} %multiply.1385.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.83.clone.1 = f32[512]{0} divide(f32[512]{0} %add.987.clone.1.clone.1, f32[512]{0} %broadcast.1247), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.118.clone.1 = f32[512]{0} sqrt(f32[512]{0} %divide.83.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.986.clone.1 = f32[512]{0} add(f32[512]{0} %sqrt.118.clone.1, f32[512]{0} %broadcast.1248), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1384.clone.1 = f32[512]{0} multiply(f32[512]{0} %broadcast.1249, f32[512]{0} %add.986.clone.1)
  %divide.82.clone.1 = f32[512]{0} divide(f32[512]{0} %add.988.clone.1.clone.1, f32[512]{0} %multiply.1384.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1383.clone.1 = f32[512]{0} multiply(f32[512]{0} %divide.82.clone.1, f32[512]{0} %broadcast.1250), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.985.clone.1 = f32[512]{0} add(f32[512]{0} %param_18.223, f32[512]{0} %multiply.1383.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_22.95 = f32[512]{0} parameter(22)
  %param_24.24 = f16[512]{0} parameter(24)
  %convert.371.clone.1.clone.1 = f32[512]{0} convert(f16[512]{0} %param_24.24), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1479.clone.1.clone.1 = f32[512]{0} multiply(f32[512]{0} %convert.371.clone.1.clone.1, f32[512]{0} %broadcast.1253.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_25.46 = f32[512]{0} parameter(25)
  %multiply.1478.clone.1.clone.1 = f32[512]{0} multiply(f32[512]{0} %param_25.46, f32[512]{0} %broadcast.1254.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1040.clone.1.clone.1 = f32[512]{0} add(f32[512]{0} %multiply.1479.clone.1.clone.1, f32[512]{0} %multiply.1478.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1477.clone.1.clone.1 = f32[512]{0} multiply(f32[512]{0} %convert.371.clone.1.clone.1, f32[512]{0} %convert.371.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1476.clone.1.clone.1 = f32[512]{0} multiply(f32[512]{0} %multiply.1477.clone.1.clone.1, f32[512]{0} %broadcast.1251.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_23.69 = f32[512]{0} parameter(23)
  %multiply.1475.clone.1.clone.1 = f32[512]{0} multiply(f32[512]{0} %param_23.69, f32[512]{0} %broadcast.1252.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1039.clone.1.clone.1 = f32[512]{0} add(f32[512]{0} %multiply.1476.clone.1.clone.1, f32[512]{0} %multiply.1475.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.409.clone.1 = f32[512]{0} divide(f32[512]{0} %add.1039.clone.1.clone.1, f32[512]{0} %broadcast.1247), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.130.clone.1 = f32[512]{0} sqrt(f32[512]{0} %divide.409.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.1038.clone.1 = f32[512]{0} add(f32[512]{0} %sqrt.130.clone.1, f32[512]{0} %broadcast.1248), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1474.clone.1 = f32[512]{0} multiply(f32[512]{0} %broadcast.1249, f32[512]{0} %add.1038.clone.1)
  %divide.408.clone.1 = f32[512]{0} divide(f32[512]{0} %add.1040.clone.1.clone.1, f32[512]{0} %multiply.1474.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1473.clone.1 = f32[512]{0} multiply(f32[512]{0} %divide.408.clone.1, f32[512]{0} %broadcast.1250), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.1037.clone.1 = f32[512]{0} add(f32[512]{0} %param_22.95, f32[512]{0} %multiply.1473.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  ROOT %tuple.250 = (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=5*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=10*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=15*/f32[512]{0}, f32[512]{0}, f32[512]{0}) tuple(f32[512]{0} %add.1091, f32[512]{0} %add.1093.clone.1, f32[512]{0} %add.1094.clone.1, f32[512]{0} %add.648.clone.1, f32[512]{0} %add.654.clone.1.clone.1, /*index=5*/f32[512]{0} %add.664.clone.1.clone.1, f32[512]{0} %add.878.clone.1, f32[512]{0} %add.880.clone.1.clone.1, f32[512]{0} %add.881.clone.1.clone.1, f32[512]{0} %add.930.clone.1, /*index=10*/f32[512]{0} %add.932.clone.1.clone.1, f32[512]{0} %add.933.clone.1.clone.1, f32[512]{0} %add.985.clone.1, f32[512]{0} %add.987.clone.1.clone.1, f32[512]{0} %add.988.clone.1.clone.1, /*index=15*/f32[512]{0} %add.1037.clone.1, f32[512]{0} %add.1039.clone.1.clone.1, f32[512]{0} %add.1040.clone.1.clone.1)
}

%fused_computation.220 (param_0.374: f32[1024,384], param_1.2991: f32[], param_2.2888: f32[], param_3.2320: f32[1024,384], param_4.1807: f16[1024,384], param_5.2063: f32[1024,384], param_6.1912: f32[1024,384], param_7.1209: f32[1024,384], param_8.642: f16[1024,384], param_9.537: f32[1024,384], param_10.452: f32[1024,384], param_11.401: f32[1024,384], param_12.360: f16[1024,384], param_13.376: f32[1024,384], param_14.451: f32[1024,384], param_15.465: f32[1024,384], param_16.451: f16[1024,384], param_17.335: f32[1024,384], param_18.205: f32[1024,384], param_19.91: f32[1024,384], param_20.19: f16[1024,384], param_21.37: f32[1024,384], param_22.77: f32[1024,384], param_23.55: f32[1024,384], param_24.19: f16[1024,384], param_25.37: f32[1024,384]) -> (f32[1024,384], f32[1024,384], f32[1024,384], f32[1024,384], f32[1024,384], /*index=5*/f32[1024,384], f32[1024,384], f32[1024,384], f32[1024,384], f32[1024,384], /*index=10*/f32[1024,384], f32[1024,384], f32[1024,384], f32[1024,384], f32[1024,384], /*index=15*/f32[1024,384], f32[1024,384], f32[1024,384]) {
  %param_0.374 = f32[1024,384]{1,0} parameter(0)
  %param_4.1807 = f16[1024,384]{1,0} parameter(4)
  %convert.391.clone.1 = f32[1024,384]{1,0} convert(f16[1024,384]{1,0} %param_4.1807), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %constant_511_clone_1 = f32[] constant(0.1)
  %broadcast.1262.clone.1 = f32[1024,384]{1,0} broadcast(f32[] %constant_511_clone_1), dimensions={}
  %multiply.1577.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %convert.391.clone.1, f32[1024,384]{1,0} %broadcast.1262.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_5.2063 = f32[1024,384]{1,0} parameter(5)
  %constant_512_clone_1 = f32[] constant(0.9)
  %broadcast.1263.clone.1 = f32[1024,384]{1,0} broadcast(f32[] %constant_512_clone_1), dimensions={}
  %multiply.1576.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param_5.2063, f32[1024,384]{1,0} %broadcast.1263.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1099.clone.1 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %multiply.1577.clone.1, f32[1024,384]{1,0} %multiply.1576.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_2.2888 = f32[] parameter(2)
  %broadcast.1257 = f32[1024,384]{1,0} broadcast(f32[] %param_2.2888), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %multiply.1575.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %convert.391.clone.1, f32[1024,384]{1,0} %convert.391.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %constant_509_clone_1 = f32[] constant(0.001)
  %broadcast.1260.clone.1 = f32[1024,384]{1,0} broadcast(f32[] %constant_509_clone_1), dimensions={}
  %multiply.1574.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %multiply.1575.clone.1, f32[1024,384]{1,0} %broadcast.1260.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_3.2320 = f32[1024,384]{1,0} parameter(3)
  %constant_510_clone_1 = f32[] constant(0.999)
  %broadcast.1261.clone.1 = f32[1024,384]{1,0} broadcast(f32[] %constant_510_clone_1), dimensions={}
  %multiply.1573.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param_3.2320, f32[1024,384]{1,0} %broadcast.1261.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1098.clone.1 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %multiply.1574.clone.1, f32[1024,384]{1,0} %multiply.1573.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_1.2991 = f32[] parameter(1)
  %broadcast.1255 = f32[1024,384]{1,0} broadcast(f32[] %param_1.2991), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %divide.435 = f32[1024,384]{1,0} divide(f32[1024,384]{1,0} %add.1098.clone.1, f32[1024,384]{1,0} %broadcast.1255), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.143 = f32[1024,384]{1,0} sqrt(f32[1024,384]{1,0} %divide.435), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %constant_507 = f32[] constant(1e-08)
  %broadcast.1256 = f32[1024,384]{1,0} broadcast(f32[] %constant_507), dimensions={}
  %add.1097 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %sqrt.143, f32[1024,384]{1,0} %broadcast.1256), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1572 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %broadcast.1257, f32[1024,384]{1,0} %add.1097)
  %divide.434 = f32[1024,384]{1,0} divide(f32[1024,384]{1,0} %add.1099.clone.1, f32[1024,384]{1,0} %multiply.1572), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %constant_506 = f32[] constant(0.0001)
  %broadcast.1258 = f32[1024,384]{1,0} broadcast(f32[] %constant_506), dimensions={}
  %multiply.1571 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param_0.374, f32[1024,384]{1,0} %broadcast.1258), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.1096 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %divide.434, f32[1024,384]{1,0} %multiply.1571), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %constant_508 = f32[] constant(-0.01)
  %broadcast.1259 = f32[1024,384]{1,0} broadcast(f32[] %constant_508), dimensions={}
  %multiply.1570 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %add.1096, f32[1024,384]{1,0} %broadcast.1259), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.1095 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %param_0.374, f32[1024,384]{1,0} %multiply.1570), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_6.1912 = f32[1024,384]{1,0} parameter(6)
  %param_8.642 = f16[1024,384]{1,0} parameter(8)
  %convert.11.clone.1.clone.1 = f32[1024,384]{1,0} convert(f16[1024,384]{1,0} %param_8.642), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.54.clone.1.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %convert.11.clone.1.clone.1, f32[1024,384]{1,0} %broadcast.1262.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_9.537 = f32[1024,384]{1,0} parameter(9)
  %multiply.53.clone.1.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param_9.537, f32[1024,384]{1,0} %broadcast.1263.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.730.clone.1.clone.1 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %multiply.54.clone.1.clone.1, f32[1024,384]{1,0} %multiply.53.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.52.clone.1.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %convert.11.clone.1.clone.1, f32[1024,384]{1,0} %convert.11.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.51.clone.1.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %multiply.52.clone.1.clone.1, f32[1024,384]{1,0} %broadcast.1260.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_7.1209 = f32[1024,384]{1,0} parameter(7)
  %multiply.50.clone.1.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param_7.1209, f32[1024,384]{1,0} %broadcast.1261.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.714.clone.1.clone.1 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %multiply.51.clone.1.clone.1, f32[1024,384]{1,0} %multiply.50.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.13.clone.1 = f32[1024,384]{1,0} divide(f32[1024,384]{1,0} %add.714.clone.1.clone.1, f32[1024,384]{1,0} %broadcast.1255), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.83.clone.1 = f32[1024,384]{1,0} sqrt(f32[1024,384]{1,0} %divide.13.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.705.clone.1 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %sqrt.83.clone.1, f32[1024,384]{1,0} %broadcast.1256), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.49.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %broadcast.1257, f32[1024,384]{1,0} %add.705.clone.1)
  %divide.12.clone.1 = f32[1024,384]{1,0} divide(f32[1024,384]{1,0} %add.730.clone.1.clone.1, f32[1024,384]{1,0} %multiply.49.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.48.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param_6.1912, f32[1024,384]{1,0} %broadcast.1258), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.690.clone.1 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %divide.12.clone.1, f32[1024,384]{1,0} %multiply.48.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.47.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %add.690.clone.1, f32[1024,384]{1,0} %broadcast.1259), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.665.clone.1 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %param_6.1912, f32[1024,384]{1,0} %multiply.47.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_10.452 = f32[1024,384]{1,0} parameter(10)
  %param_12.360 = f16[1024,384]{1,0} parameter(12)
  %convert.29.clone.1.clone.1 = f32[1024,384]{1,0} convert(f16[1024,384]{1,0} %param_12.360), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.144.clone.1.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %convert.29.clone.1.clone.1, f32[1024,384]{1,0} %broadcast.1262.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_13.376 = f32[1024,384]{1,0} parameter(13)
  %multiply.143.clone.1.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param_13.376, f32[1024,384]{1,0} %broadcast.1263.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.886.clone.1.clone.1 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %multiply.144.clone.1.clone.1, f32[1024,384]{1,0} %multiply.143.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.142.clone.1.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %convert.29.clone.1.clone.1, f32[1024,384]{1,0} %convert.29.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.141.clone.1.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %multiply.142.clone.1.clone.1, f32[1024,384]{1,0} %broadcast.1260.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_11.401 = f32[1024,384]{1,0} parameter(11)
  %multiply.140.clone.1.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param_11.401, f32[1024,384]{1,0} %broadcast.1261.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.885.clone.1.clone.1 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %multiply.141.clone.1.clone.1, f32[1024,384]{1,0} %multiply.140.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.37.clone.1 = f32[1024,384]{1,0} divide(f32[1024,384]{1,0} %add.885.clone.1.clone.1, f32[1024,384]{1,0} %broadcast.1255), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.95.clone.1 = f32[1024,384]{1,0} sqrt(f32[1024,384]{1,0} %divide.37.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.884.clone.1 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %sqrt.95.clone.1, f32[1024,384]{1,0} %broadcast.1256), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.139.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %broadcast.1257, f32[1024,384]{1,0} %add.884.clone.1)
  %divide.36.clone.1 = f32[1024,384]{1,0} divide(f32[1024,384]{1,0} %add.886.clone.1.clone.1, f32[1024,384]{1,0} %multiply.139.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.138.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param_10.452, f32[1024,384]{1,0} %broadcast.1258), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.883.clone.1 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %divide.36.clone.1, f32[1024,384]{1,0} %multiply.138.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.137.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %add.883.clone.1, f32[1024,384]{1,0} %broadcast.1259), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.882.clone.1 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %param_10.452, f32[1024,384]{1,0} %multiply.137.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_14.451 = f32[1024,384]{1,0} parameter(14)
  %param_16.451 = f16[1024,384]{1,0} parameter(16)
  %convert.337.clone.1.clone.1 = f32[1024,384]{1,0} convert(f16[1024,384]{1,0} %param_16.451), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1021.clone.1.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %convert.337.clone.1.clone.1, f32[1024,384]{1,0} %broadcast.1262.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_17.335 = f32[1024,384]{1,0} parameter(17)
  %multiply.1019.clone.1.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param_17.335, f32[1024,384]{1,0} %broadcast.1263.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.939.clone.1.clone.1 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %multiply.1021.clone.1.clone.1, f32[1024,384]{1,0} %multiply.1019.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1017.clone.1.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %convert.337.clone.1.clone.1, f32[1024,384]{1,0} %convert.337.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1015.clone.1.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %multiply.1017.clone.1.clone.1, f32[1024,384]{1,0} %broadcast.1260.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_15.465 = f32[1024,384]{1,0} parameter(15)
  %multiply.1013.clone.1.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param_15.465, f32[1024,384]{1,0} %broadcast.1261.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.937.clone.1.clone.1 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %multiply.1015.clone.1.clone.1, f32[1024,384]{1,0} %multiply.1013.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.61.clone.1 = f32[1024,384]{1,0} divide(f32[1024,384]{1,0} %add.937.clone.1.clone.1, f32[1024,384]{1,0} %broadcast.1255), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.107.clone.1 = f32[1024,384]{1,0} sqrt(f32[1024,384]{1,0} %divide.61.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.936.clone.1 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %sqrt.107.clone.1, f32[1024,384]{1,0} %broadcast.1256), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1011.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %broadcast.1257, f32[1024,384]{1,0} %add.936.clone.1)
  %divide.60.clone.1 = f32[1024,384]{1,0} divide(f32[1024,384]{1,0} %add.939.clone.1.clone.1, f32[1024,384]{1,0} %multiply.1011.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1010.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param_14.451, f32[1024,384]{1,0} %broadcast.1258), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.935.clone.1 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %divide.60.clone.1, f32[1024,384]{1,0} %multiply.1010.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.992.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %add.935.clone.1, f32[1024,384]{1,0} %broadcast.1259), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.934.clone.1 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %param_14.451, f32[1024,384]{1,0} %multiply.992.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_18.205 = f32[1024,384]{1,0} parameter(18)
  %param_20.19 = f16[1024,384]{1,0} parameter(20)
  %convert.355.clone.1.clone.1 = f32[1024,384]{1,0} convert(f16[1024,384]{1,0} %param_20.19), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1397.clone.1.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %convert.355.clone.1.clone.1, f32[1024,384]{1,0} %broadcast.1262.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_21.37 = f32[1024,384]{1,0} parameter(21)
  %multiply.1396.clone.1.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param_21.37, f32[1024,384]{1,0} %broadcast.1263.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.993.clone.1.clone.1 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %multiply.1397.clone.1.clone.1, f32[1024,384]{1,0} %multiply.1396.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1395.clone.1.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %convert.355.clone.1.clone.1, f32[1024,384]{1,0} %convert.355.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1394.clone.1.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %multiply.1395.clone.1.clone.1, f32[1024,384]{1,0} %broadcast.1260.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_19.91 = f32[1024,384]{1,0} parameter(19)
  %multiply.1393.clone.1.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param_19.91, f32[1024,384]{1,0} %broadcast.1261.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.992.clone.1.clone.1 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %multiply.1394.clone.1.clone.1, f32[1024,384]{1,0} %multiply.1393.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.85.clone.1 = f32[1024,384]{1,0} divide(f32[1024,384]{1,0} %add.992.clone.1.clone.1, f32[1024,384]{1,0} %broadcast.1255), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.119.clone.1 = f32[1024,384]{1,0} sqrt(f32[1024,384]{1,0} %divide.85.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.991.clone.1 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %sqrt.119.clone.1, f32[1024,384]{1,0} %broadcast.1256), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1392.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %broadcast.1257, f32[1024,384]{1,0} %add.991.clone.1)
  %divide.84.clone.1 = f32[1024,384]{1,0} divide(f32[1024,384]{1,0} %add.993.clone.1.clone.1, f32[1024,384]{1,0} %multiply.1392.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1391.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param_18.205, f32[1024,384]{1,0} %broadcast.1258), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.990.clone.1 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %divide.84.clone.1, f32[1024,384]{1,0} %multiply.1391.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.1390.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %add.990.clone.1, f32[1024,384]{1,0} %broadcast.1259), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.989.clone.1 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %param_18.205, f32[1024,384]{1,0} %multiply.1390.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_22.77 = f32[1024,384]{1,0} parameter(22)
  %param_24.19 = f16[1024,384]{1,0} parameter(24)
  %convert.373.clone.1.clone.1 = f32[1024,384]{1,0} convert(f16[1024,384]{1,0} %param_24.19), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1487.clone.1.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %convert.373.clone.1.clone.1, f32[1024,384]{1,0} %broadcast.1262.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_25.37 = f32[1024,384]{1,0} parameter(25)
  %multiply.1486.clone.1.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param_25.37, f32[1024,384]{1,0} %broadcast.1263.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1045.clone.1.clone.1 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %multiply.1487.clone.1.clone.1, f32[1024,384]{1,0} %multiply.1486.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1485.clone.1.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %convert.373.clone.1.clone.1, f32[1024,384]{1,0} %convert.373.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1484.clone.1.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %multiply.1485.clone.1.clone.1, f32[1024,384]{1,0} %broadcast.1260.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_23.55 = f32[1024,384]{1,0} parameter(23)
  %multiply.1483.clone.1.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param_23.55, f32[1024,384]{1,0} %broadcast.1261.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1044.clone.1.clone.1 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %multiply.1484.clone.1.clone.1, f32[1024,384]{1,0} %multiply.1483.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.411.clone.1 = f32[1024,384]{1,0} divide(f32[1024,384]{1,0} %add.1044.clone.1.clone.1, f32[1024,384]{1,0} %broadcast.1255), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.131.clone.1 = f32[1024,384]{1,0} sqrt(f32[1024,384]{1,0} %divide.411.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.1043.clone.1 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %sqrt.131.clone.1, f32[1024,384]{1,0} %broadcast.1256), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1482.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %broadcast.1257, f32[1024,384]{1,0} %add.1043.clone.1)
  %divide.410.clone.1 = f32[1024,384]{1,0} divide(f32[1024,384]{1,0} %add.1045.clone.1.clone.1, f32[1024,384]{1,0} %multiply.1482.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1481.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param_22.77, f32[1024,384]{1,0} %broadcast.1258), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.1042.clone.1 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %divide.410.clone.1, f32[1024,384]{1,0} %multiply.1481.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.1480.clone.1 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %add.1042.clone.1, f32[1024,384]{1,0} %broadcast.1259), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.1041.clone.1 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %param_22.77, f32[1024,384]{1,0} %multiply.1480.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  ROOT %tuple.245 = (f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=5*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=10*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=15*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}) tuple(f32[1024,384]{1,0} %add.1095, f32[1024,384]{1,0} %add.1098.clone.1, f32[1024,384]{1,0} %add.1099.clone.1, f32[1024,384]{1,0} %add.665.clone.1, f32[1024,384]{1,0} %add.714.clone.1.clone.1, /*index=5*/f32[1024,384]{1,0} %add.730.clone.1.clone.1, f32[1024,384]{1,0} %add.882.clone.1, f32[1024,384]{1,0} %add.885.clone.1.clone.1, f32[1024,384]{1,0} %add.886.clone.1.clone.1, f32[1024,384]{1,0} %add.934.clone.1, /*index=10*/f32[1024,384]{1,0} %add.937.clone.1.clone.1, f32[1024,384]{1,0} %add.939.clone.1.clone.1, f32[1024,384]{1,0} %add.989.clone.1, f32[1024,384]{1,0} %add.992.clone.1.clone.1, f32[1024,384]{1,0} %add.993.clone.1.clone.1, /*index=15*/f32[1024,384]{1,0} %add.1041.clone.1, f32[1024,384]{1,0} %add.1044.clone.1.clone.1, f32[1024,384]{1,0} %add.1045.clone.1.clone.1)
}

%fused_computation.223 (param_0.379: f32[384], param_1.2992: f32[], param_2.2890: f32[], param_3.2322: f32[384], param_4.1810: f16[384], param_5.2070: f32[384], param_6.1908: f32[384], param_7.1205: f32[384], param_8.637: f16[384], param_9.527: f32[384], param_10.431: f32[384], param_11.387: f32[384], param_12.355: f16[384], param_13.366: f32[384], param_14.430: f32[384], param_15.451: f32[384], param_16.446: f16[384], param_17.325: f32[384], param_18.184: f32[384], param_19.77: f32[384], param_20.14: f16[384], param_21.27: f32[384], param_22.56: f32[384], param_23.41: f32[384], param_24.14: f16[384], param_25.27: f32[384]) -> (f32[384], f32[384], f32[384], f32[384], f32[384], /*index=5*/f32[384], f32[384], f32[384], f32[384], f32[384], /*index=10*/f32[384], f32[384], f32[384], f32[384], f32[384], /*index=15*/f32[384], f32[384], f32[384]) {
  %param_0.379 = f32[384]{0} parameter(0)
  %param_4.1810 = f16[384]{0} parameter(4)
  %convert.393.clone.1 = f32[384]{0} convert(f16[384]{0} %param_4.1810), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %constant_517_clone_1 = f32[] constant(0.1)
  %broadcast.1270.clone.1 = f32[384]{0} broadcast(f32[] %constant_517_clone_1), dimensions={}
  %multiply.1584.clone.1 = f32[384]{0} multiply(f32[384]{0} %convert.393.clone.1, f32[384]{0} %broadcast.1270.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_5.2070 = f32[384]{0} parameter(5)
  %constant_518_clone_1 = f32[] constant(0.9)
  %broadcast.1271.clone.1 = f32[384]{0} broadcast(f32[] %constant_518_clone_1), dimensions={}
  %multiply.1583.clone.1 = f32[384]{0} multiply(f32[384]{0} %param_5.2070, f32[384]{0} %broadcast.1271.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1103.clone.1 = f32[384]{0} add(f32[384]{0} %multiply.1584.clone.1, f32[384]{0} %multiply.1583.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_2.2890 = f32[] parameter(2)
  %broadcast.1266 = f32[384]{0} broadcast(f32[] %param_2.2890), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %multiply.1582.clone.1 = f32[384]{0} multiply(f32[384]{0} %convert.393.clone.1, f32[384]{0} %convert.393.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %constant_515_clone_1 = f32[] constant(0.001)
  %broadcast.1268.clone.1 = f32[384]{0} broadcast(f32[] %constant_515_clone_1), dimensions={}
  %multiply.1581.clone.1 = f32[384]{0} multiply(f32[384]{0} %multiply.1582.clone.1, f32[384]{0} %broadcast.1268.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_3.2322 = f32[384]{0} parameter(3)
  %constant_516_clone_1 = f32[] constant(0.999)
  %broadcast.1269.clone.1 = f32[384]{0} broadcast(f32[] %constant_516_clone_1), dimensions={}
  %multiply.1580.clone.1 = f32[384]{0} multiply(f32[384]{0} %param_3.2322, f32[384]{0} %broadcast.1269.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1102.clone.1 = f32[384]{0} add(f32[384]{0} %multiply.1581.clone.1, f32[384]{0} %multiply.1580.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_1.2992 = f32[] parameter(1)
  %broadcast.1264 = f32[384]{0} broadcast(f32[] %param_1.2992), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %divide.437 = f32[384]{0} divide(f32[384]{0} %add.1102.clone.1, f32[384]{0} %broadcast.1264), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.144 = f32[384]{0} sqrt(f32[384]{0} %divide.437), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %constant_513 = f32[] constant(1e-08)
  %broadcast.1265 = f32[384]{0} broadcast(f32[] %constant_513), dimensions={}
  %add.1101 = f32[384]{0} add(f32[384]{0} %sqrt.144, f32[384]{0} %broadcast.1265), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1579 = f32[384]{0} multiply(f32[384]{0} %broadcast.1266, f32[384]{0} %add.1101)
  %divide.436 = f32[384]{0} divide(f32[384]{0} %add.1103.clone.1, f32[384]{0} %multiply.1579), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %constant_514 = f32[] constant(-0.01)
  %broadcast.1267 = f32[384]{0} broadcast(f32[] %constant_514), dimensions={}
  %multiply.1578 = f32[384]{0} multiply(f32[384]{0} %divide.436, f32[384]{0} %broadcast.1267), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.1100 = f32[384]{0} add(f32[384]{0} %param_0.379, f32[384]{0} %multiply.1578), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_6.1908 = f32[384]{0} parameter(6)
  %param_8.637 = f16[384]{0} parameter(8)
  %convert.13.clone.1.clone.1 = f32[384]{0} convert(f16[384]{0} %param_8.637), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.61.clone.1.clone.1 = f32[384]{0} multiply(f32[384]{0} %convert.13.clone.1.clone.1, f32[384]{0} %broadcast.1270.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_9.527 = f32[384]{0} parameter(9)
  %multiply.60.clone.1.clone.1 = f32[384]{0} multiply(f32[384]{0} %param_9.527, f32[384]{0} %broadcast.1271.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.805.clone.1.clone.1 = f32[384]{0} add(f32[384]{0} %multiply.61.clone.1.clone.1, f32[384]{0} %multiply.60.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.59.clone.1.clone.1 = f32[384]{0} multiply(f32[384]{0} %convert.13.clone.1.clone.1, f32[384]{0} %convert.13.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.58.clone.1.clone.1 = f32[384]{0} multiply(f32[384]{0} %multiply.59.clone.1.clone.1, f32[384]{0} %broadcast.1268.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_7.1205 = f32[384]{0} parameter(7)
  %multiply.57.clone.1.clone.1 = f32[384]{0} multiply(f32[384]{0} %param_7.1205, f32[384]{0} %broadcast.1269.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.790.clone.1.clone.1 = f32[384]{0} add(f32[384]{0} %multiply.58.clone.1.clone.1, f32[384]{0} %multiply.57.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.15.clone.1 = f32[384]{0} divide(f32[384]{0} %add.790.clone.1.clone.1, f32[384]{0} %broadcast.1264), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.84.clone.1 = f32[384]{0} sqrt(f32[384]{0} %divide.15.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.765.clone.1 = f32[384]{0} add(f32[384]{0} %sqrt.84.clone.1, f32[384]{0} %broadcast.1265), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.56.clone.1 = f32[384]{0} multiply(f32[384]{0} %broadcast.1266, f32[384]{0} %add.765.clone.1)
  %divide.14.clone.1 = f32[384]{0} divide(f32[384]{0} %add.805.clone.1.clone.1, f32[384]{0} %multiply.56.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.55.clone.1 = f32[384]{0} multiply(f32[384]{0} %divide.14.clone.1, f32[384]{0} %broadcast.1267), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.764.clone.1 = f32[384]{0} add(f32[384]{0} %param_6.1908, f32[384]{0} %multiply.55.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_10.431 = f32[384]{0} parameter(10)
  %param_12.355 = f16[384]{0} parameter(12)
  %convert.31.clone.1.clone.1 = f32[384]{0} convert(f16[384]{0} %param_12.355), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.151.clone.1.clone.1 = f32[384]{0} multiply(f32[384]{0} %convert.31.clone.1.clone.1, f32[384]{0} %broadcast.1270.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_13.366 = f32[384]{0} parameter(13)
  %multiply.150.clone.1.clone.1 = f32[384]{0} multiply(f32[384]{0} %param_13.366, f32[384]{0} %broadcast.1271.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.890.clone.1.clone.1 = f32[384]{0} add(f32[384]{0} %multiply.151.clone.1.clone.1, f32[384]{0} %multiply.150.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.149.clone.1.clone.1 = f32[384]{0} multiply(f32[384]{0} %convert.31.clone.1.clone.1, f32[384]{0} %convert.31.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.148.clone.1.clone.1 = f32[384]{0} multiply(f32[384]{0} %multiply.149.clone.1.clone.1, f32[384]{0} %broadcast.1268.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_11.387 = f32[384]{0} parameter(11)
  %multiply.147.clone.1.clone.1 = f32[384]{0} multiply(f32[384]{0} %param_11.387, f32[384]{0} %broadcast.1269.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.889.clone.1.clone.1 = f32[384]{0} add(f32[384]{0} %multiply.148.clone.1.clone.1, f32[384]{0} %multiply.147.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.39.clone.1 = f32[384]{0} divide(f32[384]{0} %add.889.clone.1.clone.1, f32[384]{0} %broadcast.1264), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.96.clone.1 = f32[384]{0} sqrt(f32[384]{0} %divide.39.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.888.clone.1 = f32[384]{0} add(f32[384]{0} %sqrt.96.clone.1, f32[384]{0} %broadcast.1265), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.146.clone.1 = f32[384]{0} multiply(f32[384]{0} %broadcast.1266, f32[384]{0} %add.888.clone.1)
  %divide.38.clone.1 = f32[384]{0} divide(f32[384]{0} %add.890.clone.1.clone.1, f32[384]{0} %multiply.146.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.145.clone.1 = f32[384]{0} multiply(f32[384]{0} %divide.38.clone.1, f32[384]{0} %broadcast.1267), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.887.clone.1 = f32[384]{0} add(f32[384]{0} %param_10.431, f32[384]{0} %multiply.145.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_14.430 = f32[384]{0} parameter(14)
  %param_16.446 = f16[384]{0} parameter(16)
  %convert.339.clone.1.clone.1 = f32[384]{0} convert(f16[384]{0} %param_16.446), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1035.clone.1.clone.1 = f32[384]{0} multiply(f32[384]{0} %convert.339.clone.1.clone.1, f32[384]{0} %broadcast.1270.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_17.325 = f32[384]{0} parameter(17)
  %multiply.1033.clone.1.clone.1 = f32[384]{0} multiply(f32[384]{0} %param_17.325, f32[384]{0} %broadcast.1271.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.943.clone.1.clone.1 = f32[384]{0} add(f32[384]{0} %multiply.1035.clone.1.clone.1, f32[384]{0} %multiply.1033.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1031.clone.1.clone.1 = f32[384]{0} multiply(f32[384]{0} %convert.339.clone.1.clone.1, f32[384]{0} %convert.339.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1029.clone.1.clone.1 = f32[384]{0} multiply(f32[384]{0} %multiply.1031.clone.1.clone.1, f32[384]{0} %broadcast.1268.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_15.451 = f32[384]{0} parameter(15)
  %multiply.1027.clone.1.clone.1 = f32[384]{0} multiply(f32[384]{0} %param_15.451, f32[384]{0} %broadcast.1269.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.942.clone.1.clone.1 = f32[384]{0} add(f32[384]{0} %multiply.1029.clone.1.clone.1, f32[384]{0} %multiply.1027.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.63.clone.1 = f32[384]{0} divide(f32[384]{0} %add.942.clone.1.clone.1, f32[384]{0} %broadcast.1264), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.108.clone.1 = f32[384]{0} sqrt(f32[384]{0} %divide.63.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.941.clone.1 = f32[384]{0} add(f32[384]{0} %sqrt.108.clone.1, f32[384]{0} %broadcast.1265), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1025.clone.1 = f32[384]{0} multiply(f32[384]{0} %broadcast.1266, f32[384]{0} %add.941.clone.1)
  %divide.62.clone.1 = f32[384]{0} divide(f32[384]{0} %add.943.clone.1.clone.1, f32[384]{0} %multiply.1025.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1023.clone.1 = f32[384]{0} multiply(f32[384]{0} %divide.62.clone.1, f32[384]{0} %broadcast.1267), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.940.clone.1 = f32[384]{0} add(f32[384]{0} %param_14.430, f32[384]{0} %multiply.1023.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_18.184 = f32[384]{0} parameter(18)
  %param_20.14 = f16[384]{0} parameter(20)
  %convert.357.clone.1.clone.1 = f32[384]{0} convert(f16[384]{0} %param_20.14), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1404.clone.1.clone.1 = f32[384]{0} multiply(f32[384]{0} %convert.357.clone.1.clone.1, f32[384]{0} %broadcast.1270.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_21.27 = f32[384]{0} parameter(21)
  %multiply.1403.clone.1.clone.1 = f32[384]{0} multiply(f32[384]{0} %param_21.27, f32[384]{0} %broadcast.1271.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.997.clone.1.clone.1 = f32[384]{0} add(f32[384]{0} %multiply.1404.clone.1.clone.1, f32[384]{0} %multiply.1403.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1402.clone.1.clone.1 = f32[384]{0} multiply(f32[384]{0} %convert.357.clone.1.clone.1, f32[384]{0} %convert.357.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1401.clone.1.clone.1 = f32[384]{0} multiply(f32[384]{0} %multiply.1402.clone.1.clone.1, f32[384]{0} %broadcast.1268.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_19.77 = f32[384]{0} parameter(19)
  %multiply.1400.clone.1.clone.1 = f32[384]{0} multiply(f32[384]{0} %param_19.77, f32[384]{0} %broadcast.1269.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.996.clone.1.clone.1 = f32[384]{0} add(f32[384]{0} %multiply.1401.clone.1.clone.1, f32[384]{0} %multiply.1400.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.87.clone.1 = f32[384]{0} divide(f32[384]{0} %add.996.clone.1.clone.1, f32[384]{0} %broadcast.1264), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.120.clone.1 = f32[384]{0} sqrt(f32[384]{0} %divide.87.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.995.clone.1 = f32[384]{0} add(f32[384]{0} %sqrt.120.clone.1, f32[384]{0} %broadcast.1265), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1399.clone.1 = f32[384]{0} multiply(f32[384]{0} %broadcast.1266, f32[384]{0} %add.995.clone.1)
  %divide.86.clone.1 = f32[384]{0} divide(f32[384]{0} %add.997.clone.1.clone.1, f32[384]{0} %multiply.1399.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1398.clone.1 = f32[384]{0} multiply(f32[384]{0} %divide.86.clone.1, f32[384]{0} %broadcast.1267), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.994.clone.1 = f32[384]{0} add(f32[384]{0} %param_18.184, f32[384]{0} %multiply.1398.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_22.56 = f32[384]{0} parameter(22)
  %param_24.14 = f16[384]{0} parameter(24)
  %convert.375.clone.1.clone.1 = f32[384]{0} convert(f16[384]{0} %param_24.14), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1494.clone.1.clone.1 = f32[384]{0} multiply(f32[384]{0} %convert.375.clone.1.clone.1, f32[384]{0} %broadcast.1270.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_25.27 = f32[384]{0} parameter(25)
  %multiply.1493.clone.1.clone.1 = f32[384]{0} multiply(f32[384]{0} %param_25.27, f32[384]{0} %broadcast.1271.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1049.clone.1.clone.1 = f32[384]{0} add(f32[384]{0} %multiply.1494.clone.1.clone.1, f32[384]{0} %multiply.1493.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1492.clone.1.clone.1 = f32[384]{0} multiply(f32[384]{0} %convert.375.clone.1.clone.1, f32[384]{0} %convert.375.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1491.clone.1.clone.1 = f32[384]{0} multiply(f32[384]{0} %multiply.1492.clone.1.clone.1, f32[384]{0} %broadcast.1268.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_23.41 = f32[384]{0} parameter(23)
  %multiply.1490.clone.1.clone.1 = f32[384]{0} multiply(f32[384]{0} %param_23.41, f32[384]{0} %broadcast.1269.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1048.clone.1.clone.1 = f32[384]{0} add(f32[384]{0} %multiply.1491.clone.1.clone.1, f32[384]{0} %multiply.1490.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.413.clone.1 = f32[384]{0} divide(f32[384]{0} %add.1048.clone.1.clone.1, f32[384]{0} %broadcast.1264), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.132.clone.1 = f32[384]{0} sqrt(f32[384]{0} %divide.413.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.1047.clone.1 = f32[384]{0} add(f32[384]{0} %sqrt.132.clone.1, f32[384]{0} %broadcast.1265), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1489.clone.1 = f32[384]{0} multiply(f32[384]{0} %broadcast.1266, f32[384]{0} %add.1047.clone.1)
  %divide.412.clone.1 = f32[384]{0} divide(f32[384]{0} %add.1049.clone.1.clone.1, f32[384]{0} %multiply.1489.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1488.clone.1 = f32[384]{0} multiply(f32[384]{0} %divide.412.clone.1, f32[384]{0} %broadcast.1267), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.1046.clone.1 = f32[384]{0} add(f32[384]{0} %param_22.56, f32[384]{0} %multiply.1488.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  ROOT %tuple.240 = (f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=5*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=10*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=15*/f32[384]{0}, f32[384]{0}, f32[384]{0}) tuple(f32[384]{0} %add.1100, f32[384]{0} %add.1102.clone.1, f32[384]{0} %add.1103.clone.1, f32[384]{0} %add.764.clone.1, f32[384]{0} %add.790.clone.1.clone.1, /*index=5*/f32[384]{0} %add.805.clone.1.clone.1, f32[384]{0} %add.887.clone.1, f32[384]{0} %add.889.clone.1.clone.1, f32[384]{0} %add.890.clone.1.clone.1, f32[384]{0} %add.940.clone.1, /*index=10*/f32[384]{0} %add.942.clone.1.clone.1, f32[384]{0} %add.943.clone.1.clone.1, f32[384]{0} %add.994.clone.1, f32[384]{0} %add.996.clone.1.clone.1, f32[384]{0} %add.997.clone.1.clone.1, /*index=15*/f32[384]{0} %add.1046.clone.1, f32[384]{0} %add.1048.clone.1.clone.1, f32[384]{0} %add.1049.clone.1.clone.1)
}

%region_158.4400 (Arg_0.4401: f32[], Arg_1.4402: f32[]) -> f32[] {
  %Arg_0.4401 = f32[] parameter(0)
  %Arg_1.4402 = f32[] parameter(1)
  ROOT %add.4403 = f32[] add(f32[] %Arg_0.4401, f32[] %Arg_1.4402), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_157.4390 (Arg_0.4391: f32[], Arg_1.4392: f32[]) -> f32[] {
  %Arg_0.4391 = f32[] parameter(0)
  %Arg_1.4392 = f32[] parameter(1)
  ROOT %add.4393 = f32[] add(f32[] %Arg_0.4391, f32[] %Arg_1.4392), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
}

%fused_computation.235 (param_0.2382: f32[], param_1.2899: f32[4,1024], param_2.2724: f32[], param_3.2159: f32[4,1024], param_4.1586: f32[], param_5.1608: f32[], param_6.1317: f16[4,1024,1024], param_7.918: f16[4096,1024], param_8.568: f16[4,1024,1024]) -> (f32[1024], f32[1024]) {
  %param_5.1608 = f32[] parameter(5)
  %broadcast.3293 = f32[4,1024]{1,0} broadcast(f32[] %param_5.1608), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_3.2159 = f32[4,1024]{1,0} parameter(3)
  %param_4.1586 = f32[] parameter(4)
  %broadcast.3292 = f32[4,1024]{1,0} broadcast(f32[] %param_4.1586), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.1105 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_3.2159, f32[4,1024]{1,0} %broadcast.3292), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_1.2899 = f32[4,1024]{1,0} parameter(1)
  %param_2.2724 = f32[] parameter(2)
  %broadcast.3291 = f32[4,1024]{1,0} broadcast(f32[] %param_2.2724), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.1104 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_1.2899, f32[4,1024]{1,0} %broadcast.3291), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2776 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.1104, f32[4,1024]{1,0} %divide.1104), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.472 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.1105, f32[4,1024]{1,0} %multiply.2776), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.210 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.3293, f32[4,1024]{1,0} %subtract.472), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_0.2382 = f32[] parameter(0)
  %broadcast.3290 = f32[4,1024]{1,0} broadcast(f32[] %param_0.2382), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.2140 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.210, f32[4,1024]{1,0} %broadcast.3290), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1610 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.2140), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.162 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1610), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1609 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.162), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.3289 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1609), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_8.568 = f16[4,1024,1024]{2,1,0} parameter(8)
  %convert.984 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_8.568), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %broadcast.3478 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.1104), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.502 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.984, f32[4,1024,1024]{2,1,0} %broadcast.3478), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_6.1317 = f16[4,1024,1024]{2,1,0} parameter(6)
  %param_7.918 = f16[4096,1024]{1,0} parameter(7)
  %bitcast.1660 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_7.918), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.2227 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %param_6.1317, f16[4,1024,1024]{2,1,0} %bitcast.1660), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.983 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.2227), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %multiply.2874 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.502, f32[4,1024,1024]{2,1,0} %convert.983), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.1607 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.3289, f32[4,1024,1024]{2,1,0} %multiply.2874), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %bitcast.644 = f32[4096,1024]{1,0} bitcast(f32[4,1024,1024]{2,1,0} %multiply.1607)
  %constant_540 = f32[] constant(0)
  %reduce.235 = f32[1024]{0} reduce(f32[4096,1024]{1,0} %bitcast.644, f32[] %constant_540), dimensions={0}, to_apply=%region_158.4400, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %bitcast.645.clone.1 = f32[4096,1024]{1,0} bitcast(f32[4,1024,1024]{2,1,0} %convert.983)
  %reduce.236.clone.1 = f32[1024]{0} reduce(f32[4096,1024]{1,0} %bitcast.645.clone.1, f32[] %constant_540), dimensions={0}, to_apply=%region_157.4390, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  ROOT %tuple.163 = (f32[1024]{0}, f32[1024]{0}) tuple(f32[1024]{0} %reduce.235, f32[1024]{0} %reduce.236.clone.1)
}

%fused_computation.240 (param_0.408: f32[6400,1024], param_1.2997: f32[], param_2.2900: f32[], param_3.2332: f32[6400,1024], param_4.1825: f16[6400,1024], param_5.2103: f16[6400,1024], param_6.1859: f32[6400,1024]) -> (f32[6400,1024], f32[6400,1024], f32[6400,1024]) {
  %param_0.408 = f32[6400,1024]{1,0} parameter(0)
  %param_5.2103 = f16[6400,1024]{1,0} parameter(5)
  %convert.1000.clone.1 = f32[6400,1024]{1,0} convert(f16[6400,1024]{1,0} %param_5.2103), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/word_embeddings.embed_call_one_hot/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=248}
  %param_4.1825 = f16[6400,1024]{1,0} parameter(4)
  %convert.999.clone.1 = f32[6400,1024]{1,0} convert(f16[6400,1024]{1,0} %param_4.1825), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/word_embeddings.embed_call_one_hot/remat(core_fn)/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=237}
  %add.2247.clone.1 = f32[6400,1024]{1,0} add(f32[6400,1024]{1,0} %convert.1000.clone.1, f32[6400,1024]{1,0} %convert.999.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/word_embeddings.embed_call_one_hot/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %constant_553_clone_1 = f32[] constant(0.1)
  %broadcast.9.clone.1 = f32[6400,1024]{1,0} broadcast(f32[] %constant_553_clone_1), dimensions={}
  %multiply.1622.clone.1 = f32[6400,1024]{1,0} multiply(f32[6400,1024]{1,0} %add.2247.clone.1, f32[6400,1024]{1,0} %broadcast.9.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_6.1859 = f32[6400,1024]{1,0} parameter(6)
  %constant_554_clone_1 = f32[] constant(0.9)
  %broadcast.8.clone.1 = f32[6400,1024]{1,0} broadcast(f32[] %constant_554_clone_1), dimensions={}
  %multiply.1621.clone.1 = f32[6400,1024]{1,0} multiply(f32[6400,1024]{1,0} %param_6.1859, f32[6400,1024]{1,0} %broadcast.8.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1125.clone.1 = f32[6400,1024]{1,0} add(f32[6400,1024]{1,0} %multiply.1622.clone.1, f32[6400,1024]{1,0} %multiply.1621.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_2.2900 = f32[] parameter(2)
  %broadcast.5 = f32[6400,1024]{1,0} broadcast(f32[] %param_2.2900), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %multiply.1620.clone.1 = f32[6400,1024]{1,0} multiply(f32[6400,1024]{1,0} %add.2247.clone.1, f32[6400,1024]{1,0} %add.2247.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %constant_551_clone_1 = f32[] constant(0.001)
  %broadcast.7.clone.1 = f32[6400,1024]{1,0} broadcast(f32[] %constant_551_clone_1), dimensions={}
  %multiply.1619.clone.1 = f32[6400,1024]{1,0} multiply(f32[6400,1024]{1,0} %multiply.1620.clone.1, f32[6400,1024]{1,0} %broadcast.7.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_3.2332 = f32[6400,1024]{1,0} parameter(3)
  %constant_552_clone_1 = f32[] constant(0.999)
  %broadcast.6.clone.1 = f32[6400,1024]{1,0} broadcast(f32[] %constant_552_clone_1), dimensions={}
  %multiply.1618.clone.1 = f32[6400,1024]{1,0} multiply(f32[6400,1024]{1,0} %param_3.2332, f32[6400,1024]{1,0} %broadcast.6.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1124.clone.1 = f32[6400,1024]{1,0} add(f32[6400,1024]{1,0} %multiply.1619.clone.1, f32[6400,1024]{1,0} %multiply.1618.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_1.2997 = f32[] parameter(1)
  %broadcast.4 = f32[6400,1024]{1,0} broadcast(f32[] %param_1.2997), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %divide.447 = f32[6400,1024]{1,0} divide(f32[6400,1024]{1,0} %add.1124.clone.1, f32[6400,1024]{1,0} %broadcast.4), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.149 = f32[6400,1024]{1,0} sqrt(f32[6400,1024]{1,0} %divide.447), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %constant_549 = f32[] constant(1e-08)
  %broadcast.3 = f32[6400,1024]{1,0} broadcast(f32[] %constant_549), dimensions={}
  %add.1123 = f32[6400,1024]{1,0} add(f32[6400,1024]{1,0} %sqrt.149, f32[6400,1024]{1,0} %broadcast.3), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1617 = f32[6400,1024]{1,0} multiply(f32[6400,1024]{1,0} %broadcast.5, f32[6400,1024]{1,0} %add.1123)
  %divide.446 = f32[6400,1024]{1,0} divide(f32[6400,1024]{1,0} %add.1125.clone.1, f32[6400,1024]{1,0} %multiply.1617), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %constant_548 = f32[] constant(0.0001)
  %broadcast.2 = f32[6400,1024]{1,0} broadcast(f32[] %constant_548), dimensions={}
  %multiply.1616 = f32[6400,1024]{1,0} multiply(f32[6400,1024]{1,0} %param_0.408, f32[6400,1024]{1,0} %broadcast.2), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.1122 = f32[6400,1024]{1,0} add(f32[6400,1024]{1,0} %divide.446, f32[6400,1024]{1,0} %multiply.1616), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %constant_550 = f32[] constant(-0.01)
  %broadcast.1 = f32[6400,1024]{1,0} broadcast(f32[] %constant_550), dimensions={}
  %multiply.1615 = f32[6400,1024]{1,0} multiply(f32[6400,1024]{1,0} %add.1122, f32[6400,1024]{1,0} %broadcast.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.1121 = f32[6400,1024]{1,0} add(f32[6400,1024]{1,0} %param_0.408, f32[6400,1024]{1,0} %multiply.1615), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  ROOT %tuple.154 = (f32[6400,1024]{1,0}, f32[6400,1024]{1,0}, f32[6400,1024]{1,0}) tuple(f32[6400,1024]{1,0} %add.1121, f32[6400,1024]{1,0} %add.1124.clone.1, f32[6400,1024]{1,0} %add.1125.clone.1)
}

%fused_computation.244 (param_0.1481: s32[16], param_1.1922: u32[], param_2.1609: s32[1,1,51200], param_3.1288: s32[4,1024]) -> f16[4096,6400] {
  %param_3.1288 = s32[4,1024]{1,0} parameter(3)
  %broadcast.11 = s32[4,1024,51200]{2,1,0} broadcast(s32[4,1024]{1,0} %param_3.1288), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/word_embeddings.embed_call_one_hot/remat(core_fn)/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %param_2.1609 = s32[1,1,51200]{2,1,0} parameter(2)
  %bitcast.648 = s32[51200]{0} bitcast(s32[1,1,51200]{2,1,0} %param_2.1609), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/word_embeddings.embed_call_one_hot/remat(core_fn)/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %broadcast.10 = s32[4,1024,51200]{2,1,0} broadcast(s32[51200]{0} %bitcast.648), dimensions={2}
  %compare.0 = pred[4,1024,51200]{2,1,0} compare(s32[4,1024,51200]{2,1,0} %broadcast.11, s32[4,1024,51200]{2,1,0} %broadcast.10), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/word_embeddings.embed_call_one_hot/remat(core_fn)/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %convert.401 = f16[4,1024,51200]{2,1,0} convert(pred[4,1024,51200]{2,1,0} %compare.0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/word_embeddings.embed_call_one_hot/remat(core_fn)/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %bitcast.647 = f16[4096,51200]{1,0} bitcast(f16[4,1024,51200]{2,1,0} %convert.401)
  %constant_555 = s32[] constant(0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %param_0.1481 = s32[16]{0} parameter(0)
  %param_1.1922 = u32[] parameter(1)
  %dynamic-slice.2 = s32[1]{0} dynamic-slice(s32[16]{0} %param_0.1481, u32[] %param_1.1922), dynamic_slice_sizes={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %bitcast.646 = s32[] bitcast(s32[1]{0} %dynamic-slice.2), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  ROOT %dynamic-slice.0 = f16[4096,6400]{1,0} dynamic-slice(f16[4096,51200]{1,0} %bitcast.647, s32[] %constant_555, s32[] %bitcast.646), dynamic_slice_sizes={4096,6400}
}

%fused_computation.245 (param_0.417: f32[128,1024], param_1.2998: f32[], param_2.2902: f32[], param_3.2334: f32[128,1024], param_4.1828: f16[128,1024], param_5.2110: f32[128,1024], param_6.1904: f32[128,1024], param_7.1201: f32[128,1024], param_8.632: f16[128,1024], param_9.518: f32[128,1024], param_10.413: f32[128,1024], param_11.373: f32[128,1024], param_12.350: f16[128,1024], param_13.357: f32[128,1024], param_14.412: f32[128,1024], param_15.437: f32[128,1024], param_16.441: f16[128,1024], param_17.316: f32[128,1024], param_18.166: f32[128,1024], param_19.63: f32[128,1024], param_20.9: f16[128,1024], param_21.18: f32[128,1024], param_22.38: f32[128,1024], param_23.27: f32[128,1024], param_24.9: f16[128,1024], param_25.18: f32[128,1024], param_26.38: f32[128,1024], param_27.25: f32[128,1024], param_28.9: f16[128,1024], param_29.18: f32[128,1024]) -> (f32[128,1024], f32[128,1024], f32[128,1024], f32[128,1024], f32[128,1024], /*index=5*/f32[128,1024], f32[128,1024], f32[128,1024], f32[128,1024], f32[128,1024], /*index=10*/f32[128,1024], f32[128,1024], f32[128,1024], f32[128,1024], f32[128,1024], /*index=15*/f32[128,1024], f32[128,1024], f32[128,1024], f32[128,1024], f32[128,1024], /*index=20*/f32[128,1024]) {
  %param_0.417 = f32[128,1024]{1,0} parameter(0)
  %param_4.1828 = f16[128,1024]{1,0} parameter(4)
  %convert.403.clone.1 = f32[128,1024]{1,0} convert(f16[128,1024]{1,0} %param_4.1828), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/position_embeddings.embed_call_one_hot/remat(core_fn)/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=237}
  %constant_561_clone_1 = f32[] constant(0.1)
  %broadcast.1312.clone.1 = f32[128,1024]{1,0} broadcast(f32[] %constant_561_clone_1), dimensions={}
  %multiply.1630.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %convert.403.clone.1, f32[128,1024]{1,0} %broadcast.1312.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_5.2110 = f32[128,1024]{1,0} parameter(5)
  %constant_562_clone_1 = f32[] constant(0.9)
  %broadcast.1313.clone.1 = f32[128,1024]{1,0} broadcast(f32[] %constant_562_clone_1), dimensions={}
  %multiply.1629.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param_5.2110, f32[128,1024]{1,0} %broadcast.1313.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1132.clone.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %multiply.1630.clone.1, f32[128,1024]{1,0} %multiply.1629.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_2.2902 = f32[] parameter(2)
  %broadcast.1307 = f32[128,1024]{1,0} broadcast(f32[] %param_2.2902), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %multiply.1628.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %convert.403.clone.1, f32[128,1024]{1,0} %convert.403.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %constant_559_clone_1 = f32[] constant(0.001)
  %broadcast.1310.clone.1 = f32[128,1024]{1,0} broadcast(f32[] %constant_559_clone_1), dimensions={}
  %multiply.1627.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %multiply.1628.clone.1, f32[128,1024]{1,0} %broadcast.1310.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_3.2334 = f32[128,1024]{1,0} parameter(3)
  %constant_560_clone_1 = f32[] constant(0.999)
  %broadcast.1311.clone.1 = f32[128,1024]{1,0} broadcast(f32[] %constant_560_clone_1), dimensions={}
  %multiply.1626.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param_3.2334, f32[128,1024]{1,0} %broadcast.1311.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1131.clone.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %multiply.1627.clone.1, f32[128,1024]{1,0} %multiply.1626.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_1.2998 = f32[] parameter(1)
  %broadcast.1305 = f32[128,1024]{1,0} broadcast(f32[] %param_1.2998), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %divide.449 = f32[128,1024]{1,0} divide(f32[128,1024]{1,0} %add.1131.clone.1, f32[128,1024]{1,0} %broadcast.1305), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.150 = f32[128,1024]{1,0} sqrt(f32[128,1024]{1,0} %divide.449), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %constant_557 = f32[] constant(1e-08)
  %broadcast.1306 = f32[128,1024]{1,0} broadcast(f32[] %constant_557), dimensions={}
  %add.1129 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %sqrt.150, f32[128,1024]{1,0} %broadcast.1306), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1625 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %broadcast.1307, f32[128,1024]{1,0} %add.1129)
  %divide.448 = f32[128,1024]{1,0} divide(f32[128,1024]{1,0} %add.1132.clone.1, f32[128,1024]{1,0} %multiply.1625), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %constant_556 = f32[] constant(0.0001)
  %broadcast.1308 = f32[128,1024]{1,0} broadcast(f32[] %constant_556), dimensions={}
  %multiply.1624 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param_0.417, f32[128,1024]{1,0} %broadcast.1308), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.1128 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %divide.448, f32[128,1024]{1,0} %multiply.1624), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %constant_558 = f32[] constant(-0.01)
  %broadcast.1309 = f32[128,1024]{1,0} broadcast(f32[] %constant_558), dimensions={}
  %multiply.1623 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.1128, f32[128,1024]{1,0} %broadcast.1309), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.1127 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %param_0.417, f32[128,1024]{1,0} %multiply.1623), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_6.1904 = f32[128,1024]{1,0} parameter(6)
  %param_8.632 = f16[128,1024]{1,0} parameter(8)
  %convert.15.clone.1.clone.1 = f32[128,1024]{1,0} convert(f16[128,1024]{1,0} %param_8.632), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.69.clone.1.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %convert.15.clone.1.clone.1, f32[128,1024]{1,0} %broadcast.1312.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_9.518 = f32[128,1024]{1,0} parameter(9)
  %multiply.68.clone.1.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param_9.518, f32[128,1024]{1,0} %broadcast.1313.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.826.clone.1.clone.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %multiply.69.clone.1.clone.1, f32[128,1024]{1,0} %multiply.68.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.67.clone.1.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %convert.15.clone.1.clone.1, f32[128,1024]{1,0} %convert.15.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.66.clone.1.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %multiply.67.clone.1.clone.1, f32[128,1024]{1,0} %broadcast.1310.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_7.1201 = f32[128,1024]{1,0} parameter(7)
  %multiply.65.clone.1.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param_7.1201, f32[128,1024]{1,0} %broadcast.1311.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.824.clone.1.clone.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %multiply.66.clone.1.clone.1, f32[128,1024]{1,0} %multiply.65.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.17.clone.1 = f32[128,1024]{1,0} divide(f32[128,1024]{1,0} %add.824.clone.1.clone.1, f32[128,1024]{1,0} %broadcast.1305), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.85.clone.1 = f32[128,1024]{1,0} sqrt(f32[128,1024]{1,0} %divide.17.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.822.clone.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %sqrt.85.clone.1, f32[128,1024]{1,0} %broadcast.1306), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.64.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %broadcast.1307, f32[128,1024]{1,0} %add.822.clone.1)
  %divide.16.clone.1 = f32[128,1024]{1,0} divide(f32[128,1024]{1,0} %add.826.clone.1.clone.1, f32[128,1024]{1,0} %multiply.64.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.63.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param_6.1904, f32[128,1024]{1,0} %broadcast.1308), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.820.clone.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %divide.16.clone.1, f32[128,1024]{1,0} %multiply.63.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.62.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.820.clone.1, f32[128,1024]{1,0} %broadcast.1309), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.814.clone.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %param_6.1904, f32[128,1024]{1,0} %multiply.62.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_10.413 = f32[128,1024]{1,0} parameter(10)
  %param_12.350 = f16[128,1024]{1,0} parameter(12)
  %convert.33.clone.1.clone.1 = f32[128,1024]{1,0} convert(f16[128,1024]{1,0} %param_12.350), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.159.clone.1.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %convert.33.clone.1.clone.1, f32[128,1024]{1,0} %broadcast.1312.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_13.357 = f32[128,1024]{1,0} parameter(13)
  %multiply.158.clone.1.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param_13.357, f32[128,1024]{1,0} %broadcast.1313.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.895.clone.1.clone.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %multiply.159.clone.1.clone.1, f32[128,1024]{1,0} %multiply.158.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.157.clone.1.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %convert.33.clone.1.clone.1, f32[128,1024]{1,0} %convert.33.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.156.clone.1.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %multiply.157.clone.1.clone.1, f32[128,1024]{1,0} %broadcast.1310.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_11.373 = f32[128,1024]{1,0} parameter(11)
  %multiply.155.clone.1.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param_11.373, f32[128,1024]{1,0} %broadcast.1311.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.894.clone.1.clone.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %multiply.156.clone.1.clone.1, f32[128,1024]{1,0} %multiply.155.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.41.clone.1 = f32[128,1024]{1,0} divide(f32[128,1024]{1,0} %add.894.clone.1.clone.1, f32[128,1024]{1,0} %broadcast.1305), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.97.clone.1 = f32[128,1024]{1,0} sqrt(f32[128,1024]{1,0} %divide.41.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.893.clone.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %sqrt.97.clone.1, f32[128,1024]{1,0} %broadcast.1306), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.154.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %broadcast.1307, f32[128,1024]{1,0} %add.893.clone.1)
  %divide.40.clone.1 = f32[128,1024]{1,0} divide(f32[128,1024]{1,0} %add.895.clone.1.clone.1, f32[128,1024]{1,0} %multiply.154.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.153.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param_10.413, f32[128,1024]{1,0} %broadcast.1308), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.892.clone.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %divide.40.clone.1, f32[128,1024]{1,0} %multiply.153.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.152.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.892.clone.1, f32[128,1024]{1,0} %broadcast.1309), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.891.clone.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %param_10.413, f32[128,1024]{1,0} %multiply.152.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_14.412 = f32[128,1024]{1,0} parameter(14)
  %param_16.441 = f16[128,1024]{1,0} parameter(16)
  %convert.341.clone.1.clone.1 = f32[128,1024]{1,0} convert(f16[128,1024]{1,0} %param_16.441), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1183.clone.1.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %convert.341.clone.1.clone.1, f32[128,1024]{1,0} %broadcast.1312.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_17.316 = f32[128,1024]{1,0} parameter(17)
  %multiply.1170.clone.1.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param_17.316, f32[128,1024]{1,0} %broadcast.1313.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.948.clone.1.clone.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %multiply.1183.clone.1.clone.1, f32[128,1024]{1,0} %multiply.1170.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1163.clone.1.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %convert.341.clone.1.clone.1, f32[128,1024]{1,0} %convert.341.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1084.clone.1.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %multiply.1163.clone.1.clone.1, f32[128,1024]{1,0} %broadcast.1310.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_15.437 = f32[128,1024]{1,0} parameter(15)
  %multiply.1083.clone.1.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param_15.437, f32[128,1024]{1,0} %broadcast.1311.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.947.clone.1.clone.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %multiply.1084.clone.1.clone.1, f32[128,1024]{1,0} %multiply.1083.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.65.clone.1 = f32[128,1024]{1,0} divide(f32[128,1024]{1,0} %add.947.clone.1.clone.1, f32[128,1024]{1,0} %broadcast.1305), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.109.clone.1 = f32[128,1024]{1,0} sqrt(f32[128,1024]{1,0} %divide.65.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.946.clone.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %sqrt.109.clone.1, f32[128,1024]{1,0} %broadcast.1306), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1070.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %broadcast.1307, f32[128,1024]{1,0} %add.946.clone.1)
  %divide.64.clone.1 = f32[128,1024]{1,0} divide(f32[128,1024]{1,0} %add.948.clone.1.clone.1, f32[128,1024]{1,0} %multiply.1070.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1063.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param_14.412, f32[128,1024]{1,0} %broadcast.1308), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.945.clone.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %divide.64.clone.1, f32[128,1024]{1,0} %multiply.1063.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.1039.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.945.clone.1, f32[128,1024]{1,0} %broadcast.1309), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.944.clone.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %param_14.412, f32[128,1024]{1,0} %multiply.1039.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_18.166 = f32[128,1024]{1,0} parameter(18)
  %param_20.9 = f16[128,1024]{1,0} parameter(20)
  %convert.359.clone.1.clone.1 = f32[128,1024]{1,0} convert(f16[128,1024]{1,0} %param_20.9), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1412.clone.1.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %convert.359.clone.1.clone.1, f32[128,1024]{1,0} %broadcast.1312.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_21.18 = f32[128,1024]{1,0} parameter(21)
  %multiply.1411.clone.1.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param_21.18, f32[128,1024]{1,0} %broadcast.1313.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1002.clone.1.clone.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %multiply.1412.clone.1.clone.1, f32[128,1024]{1,0} %multiply.1411.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1410.clone.1.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %convert.359.clone.1.clone.1, f32[128,1024]{1,0} %convert.359.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1409.clone.1.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %multiply.1410.clone.1.clone.1, f32[128,1024]{1,0} %broadcast.1310.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_19.63 = f32[128,1024]{1,0} parameter(19)
  %multiply.1408.clone.1.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param_19.63, f32[128,1024]{1,0} %broadcast.1311.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1001.clone.1.clone.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %multiply.1409.clone.1.clone.1, f32[128,1024]{1,0} %multiply.1408.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.89.clone.1 = f32[128,1024]{1,0} divide(f32[128,1024]{1,0} %add.1001.clone.1.clone.1, f32[128,1024]{1,0} %broadcast.1305), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.121.clone.1 = f32[128,1024]{1,0} sqrt(f32[128,1024]{1,0} %divide.89.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.1000.clone.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %sqrt.121.clone.1, f32[128,1024]{1,0} %broadcast.1306), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1407.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %broadcast.1307, f32[128,1024]{1,0} %add.1000.clone.1)
  %divide.88.clone.1 = f32[128,1024]{1,0} divide(f32[128,1024]{1,0} %add.1002.clone.1.clone.1, f32[128,1024]{1,0} %multiply.1407.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1406.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param_18.166, f32[128,1024]{1,0} %broadcast.1308), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.999.clone.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %divide.88.clone.1, f32[128,1024]{1,0} %multiply.1406.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.1405.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.999.clone.1, f32[128,1024]{1,0} %broadcast.1309), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.998.clone.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %param_18.166, f32[128,1024]{1,0} %multiply.1405.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_22.38 = f32[128,1024]{1,0} parameter(22)
  %param_24.9 = f16[128,1024]{1,0} parameter(24)
  %convert.377.clone.1.clone.1 = f32[128,1024]{1,0} convert(f16[128,1024]{1,0} %param_24.9), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1502.clone.1.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %convert.377.clone.1.clone.1, f32[128,1024]{1,0} %broadcast.1312.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_25.18 = f32[128,1024]{1,0} parameter(25)
  %multiply.1501.clone.1.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param_25.18, f32[128,1024]{1,0} %broadcast.1313.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1054.clone.1.clone.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %multiply.1502.clone.1.clone.1, f32[128,1024]{1,0} %multiply.1501.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1500.clone.1.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %convert.377.clone.1.clone.1, f32[128,1024]{1,0} %convert.377.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1499.clone.1.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %multiply.1500.clone.1.clone.1, f32[128,1024]{1,0} %broadcast.1310.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_23.27 = f32[128,1024]{1,0} parameter(23)
  %multiply.1498.clone.1.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param_23.27, f32[128,1024]{1,0} %broadcast.1311.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1053.clone.1.clone.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %multiply.1499.clone.1.clone.1, f32[128,1024]{1,0} %multiply.1498.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.415.clone.1 = f32[128,1024]{1,0} divide(f32[128,1024]{1,0} %add.1053.clone.1.clone.1, f32[128,1024]{1,0} %broadcast.1305), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.133.clone.1 = f32[128,1024]{1,0} sqrt(f32[128,1024]{1,0} %divide.415.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.1052.clone.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %sqrt.133.clone.1, f32[128,1024]{1,0} %broadcast.1306), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1497.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %broadcast.1307, f32[128,1024]{1,0} %add.1052.clone.1)
  %divide.414.clone.1 = f32[128,1024]{1,0} divide(f32[128,1024]{1,0} %add.1054.clone.1.clone.1, f32[128,1024]{1,0} %multiply.1497.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1496.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param_22.38, f32[128,1024]{1,0} %broadcast.1308), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.1051.clone.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %divide.414.clone.1, f32[128,1024]{1,0} %multiply.1496.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.1495.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.1051.clone.1, f32[128,1024]{1,0} %broadcast.1309), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.1050.clone.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %param_22.38, f32[128,1024]{1,0} %multiply.1495.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_26.38 = f32[128,1024]{1,0} parameter(26)
  %param_28.9 = f16[128,1024]{1,0} parameter(28)
  %convert.395.clone.1.clone.1 = f32[128,1024]{1,0} convert(f16[128,1024]{1,0} %param_28.9), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1592.clone.1.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %convert.395.clone.1.clone.1, f32[128,1024]{1,0} %broadcast.1312.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_29.18 = f32[128,1024]{1,0} parameter(29)
  %multiply.1591.clone.1.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param_29.18, f32[128,1024]{1,0} %broadcast.1313.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1108.clone.1.clone.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %multiply.1592.clone.1.clone.1, f32[128,1024]{1,0} %multiply.1591.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1590.clone.1.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %convert.395.clone.1.clone.1, f32[128,1024]{1,0} %convert.395.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1589.clone.1.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %multiply.1590.clone.1.clone.1, f32[128,1024]{1,0} %broadcast.1310.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_27.25 = f32[128,1024]{1,0} parameter(27)
  %multiply.1588.clone.1.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param_27.25, f32[128,1024]{1,0} %broadcast.1311.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1107.clone.1.clone.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %multiply.1589.clone.1.clone.1, f32[128,1024]{1,0} %multiply.1588.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.439.clone.1 = f32[128,1024]{1,0} divide(f32[128,1024]{1,0} %add.1107.clone.1.clone.1, f32[128,1024]{1,0} %broadcast.1305), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.145.clone.1 = f32[128,1024]{1,0} sqrt(f32[128,1024]{1,0} %divide.439.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.1106.clone.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %sqrt.145.clone.1, f32[128,1024]{1,0} %broadcast.1306), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1587.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %broadcast.1307, f32[128,1024]{1,0} %add.1106.clone.1)
  %divide.438.clone.1 = f32[128,1024]{1,0} divide(f32[128,1024]{1,0} %add.1108.clone.1.clone.1, f32[128,1024]{1,0} %multiply.1587.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1586.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param_26.38, f32[128,1024]{1,0} %broadcast.1308), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.1105.clone.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %divide.438.clone.1, f32[128,1024]{1,0} %multiply.1586.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.1585.clone.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.1105.clone.1, f32[128,1024]{1,0} %broadcast.1309), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.1104.clone.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %param_26.38, f32[128,1024]{1,0} %multiply.1585.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  ROOT %tuple.235 = (f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=5*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=10*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=15*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=20*/f32[128,1024]{1,0}) tuple(f32[128,1024]{1,0} %add.1127, f32[128,1024]{1,0} %add.1131.clone.1, f32[128,1024]{1,0} %add.1132.clone.1, f32[128,1024]{1,0} %add.814.clone.1, f32[128,1024]{1,0} %add.824.clone.1.clone.1, /*index=5*/f32[128,1024]{1,0} %add.826.clone.1.clone.1, f32[128,1024]{1,0} %add.891.clone.1, f32[128,1024]{1,0} %add.894.clone.1.clone.1, f32[128,1024]{1,0} %add.895.clone.1.clone.1, f32[128,1024]{1,0} %add.944.clone.1, /*index=10*/f32[128,1024]{1,0} %add.947.clone.1.clone.1, f32[128,1024]{1,0} %add.948.clone.1.clone.1, f32[128,1024]{1,0} %add.998.clone.1, f32[128,1024]{1,0} %add.1001.clone.1.clone.1, f32[128,1024]{1,0} %add.1002.clone.1.clone.1, /*index=15*/f32[128,1024]{1,0} %add.1050.clone.1, f32[128,1024]{1,0} %add.1053.clone.1.clone.1, f32[128,1024]{1,0} %add.1054.clone.1.clone.1, f32[128,1024]{1,0} %add.1104.clone.1, f32[128,1024]{1,0} %add.1107.clone.1.clone.1, /*index=20*/f32[128,1024]{1,0} %add.1108.clone.1.clone.1)
}

%fused_computation.248 (param_0.1482: s32[16], param_1.1926: u32[], param_2.1618: s32[1,1,1024], param_3.1295: s32[4,1024]) -> f16[4096,128] {
  %param_3.1295 = s32[4,1024]{1,0} parameter(3)
  %broadcast.13 = s32[4,1024,1024]{2,1,0} broadcast(s32[4,1024]{1,0} %param_3.1295), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/position_embeddings.embed_call_one_hot/remat(core_fn)/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %param_2.1618 = s32[1,1,1024]{2,1,0} parameter(2)
  %bitcast.651 = s32[1024]{0} bitcast(s32[1,1,1024]{2,1,0} %param_2.1618), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/position_embeddings.embed_call_one_hot/remat(core_fn)/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %broadcast.12 = s32[4,1024,1024]{2,1,0} broadcast(s32[1024]{0} %bitcast.651), dimensions={2}
  %compare.1 = pred[4,1024,1024]{2,1,0} compare(s32[4,1024,1024]{2,1,0} %broadcast.13, s32[4,1024,1024]{2,1,0} %broadcast.12), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/position_embeddings.embed_call_one_hot/remat(core_fn)/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %convert.404 = f16[4,1024,1024]{2,1,0} convert(pred[4,1024,1024]{2,1,0} %compare.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/position_embeddings.embed_call_one_hot/remat(core_fn)/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %bitcast.650 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %convert.404)
  %constant_563 = s32[] constant(0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %param_0.1482 = s32[16]{0} parameter(0)
  %param_1.1926 = u32[] parameter(1)
  %dynamic-slice.5 = s32[1]{0} dynamic-slice(s32[16]{0} %param_0.1482, u32[] %param_1.1926), dynamic_slice_sizes={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/position_embeddings.embed_call_one_hot/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %bitcast.649 = s32[] bitcast(s32[1]{0} %dynamic-slice.5), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/position_embeddings.embed_call_one_hot/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  ROOT %dynamic-slice.3 = f16[4096,128]{1,0} dynamic-slice(f16[4096,1024]{1,0} %bitcast.650, s32[] %constant_563, s32[] %bitcast.649), dynamic_slice_sizes={4096,128}
}

%fused_computation.249 (param_0.2397: f32[4,1024], param_1.2918: f32[4,1024], param_2.2748: f32[4,1024], param_3.2186: f16[4096,1024], param_4.1606: f32[4,1024], param_5.1616: f32[1024], param_6.1322: f16[4,1024,1024], param_7.931: f16[4096,1024]) -> f16[4,1024,1024] {
  %param_3.2186 = f16[4096,1024]{1,0} parameter(3)
  %convert.692 = f32[4096,1024]{1,0} convert(f16[4096,1024]{1,0} %param_3.2186), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %bitcast.1024 = f32[4,1024,1024]{2,1,0} bitcast(f32[4096,1024]{1,0} %convert.692), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.2748 = f32[4,1024]{1,0} parameter(2)
  %bitcast.653 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %param_2.2748), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/reshape[new_sizes=(8, 1024, 1) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_4.1606 = f32[4,1024]{1,0} parameter(4)
  %constant_566 = f32[] constant(0.0009765625)
  %broadcast.1831 = f32[4,1024]{1,0} broadcast(f32[] %constant_566), dimensions={}
  %multiply.2196 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_4.1606, f32[4,1024]{1,0} %broadcast.1831), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_1.2918 = f32[4,1024]{1,0} parameter(1)
  %multiply.2195 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_1.2918, f32[4,1024]{1,0} %broadcast.1831), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2194 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %multiply.2195, f32[4,1024]{1,0} %multiply.2195), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.116 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %multiply.2196, f32[4,1024]{1,0} %multiply.2194), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_568 = f32[] constant(0)
  %broadcast.1830 = f32[4,1024]{1,0} broadcast(f32[] %constant_568), dimensions={}
  %maximum.66 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %subtract.116, f32[4,1024]{1,0} %broadcast.1830), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_1094 = f32[] constant(1e-12)
  %broadcast.1829 = f32[4,1024]{1,0} broadcast(f32[] %constant_1094), dimensions={}
  %add.1492 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.66, f32[4,1024]{1,0} %broadcast.1829), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1032 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.1492), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.37 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1032), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %divide.451 = f32[4,1024,1]{1,0,2} divide(f32[4,1024,1]{1,0,2} %rsqrt.37, f32[4,1024,1]{1,0,2} %bitcast.1032), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %constant_564 = f32[] constant(-0.5)
  %broadcast.1314 = f32[4,1024,1]{1,0,2} broadcast(f32[] %constant_564), dimensions={}
  %multiply.1638 = f32[4,1024,1]{1,0,2} multiply(f32[4,1024,1]{1,0,2} %divide.451, f32[4,1024,1]{1,0,2} %broadcast.1314), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.1637 = f32[4,1024,1]{1,0,2} multiply(f32[4,1024,1]{1,0,2} %bitcast.653, f32[4,1024,1]{1,0,2} %multiply.1638), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.652 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %multiply.1637), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/reshape[new_sizes=(8, 1024) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=132}
  %compare.3 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %subtract.116, f32[4,1024]{1,0} %maximum.66), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_567 = f32[] constant(1)
  %broadcast.1318 = f32[4,1024]{1,0} broadcast(f32[] %constant_567), dimensions={}
  %select.1 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.3, f32[4,1024]{1,0} %broadcast.1318, f32[4,1024]{1,0} %broadcast.1830), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %compare.2 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %broadcast.1830, f32[4,1024]{1,0} %maximum.66), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_565 = f32[] constant(2)
  %broadcast.1315 = f32[4,1024]{1,0} broadcast(f32[] %constant_565), dimensions={}
  %select.0 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.2, f32[4,1024]{1,0} %broadcast.1315, f32[4,1024]{1,0} %broadcast.1318), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.450 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %select.1, f32[4,1024]{1,0} %select.0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.1636 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %bitcast.652, f32[4,1024]{1,0} %divide.450), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_4 = f32[] constant(0.001953125)
  %broadcast.16 = f32[4,1024]{1,0} broadcast(f32[] %constant_4), dimensions={}
  %multiply.1635 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %multiply.1636, f32[4,1024]{1,0} %broadcast.16), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %broadcast.15 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.1635), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.1634 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %bitcast.1024, f32[4,1024,1024]{2,1,0} %broadcast.15), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %negate.0 = f32[4,1024]{1,0} negate(f32[4,1024]{1,0} %multiply.1636), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.1633 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_1.2918, f32[4,1024]{1,0} %broadcast.16), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.1632 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %negate.0, f32[4,1024]{1,0} %multiply.1633), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %param_0.2397 = f32[4,1024]{1,0} parameter(0)
  %add.1135 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %multiply.1632, f32[4,1024]{1,0} %param_0.2397), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=131}
  %multiply.1631 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %add.1135, f32[4,1024]{1,0} %broadcast.1831), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.14 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.1631), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/broadcast_in_dim[shape=(8, 1024, 1024) broadcast_dimensions=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %add.1134 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.1634, f32[4,1024,1024]{2,1,0} %broadcast.14), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %convert.406 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.1134), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %param_6.1322 = f16[4,1024,1024]{2,1,0} parameter(6)
  %param_7.931 = f16[4096,1024]{1,0} parameter(7)
  %bitcast.1686 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_7.931), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.2245 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %param_6.1322, f16[4,1024,1024]{2,1,0} %bitcast.1686), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.996 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.2245), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %bitcast.1684 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.37), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.3504 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1684), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_5.1616 = f32[1024]{0} parameter(5)
  %broadcast.3503 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_5.1616), dimensions={2}
  %multiply.2899 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.3504, f32[4,1024,1024]{2,1,0} %broadcast.3503), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2898 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.996, f32[4,1024,1024]{2,1,0} %multiply.2899), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %convert.405 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %multiply.2898), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  ROOT %add.1133 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %convert.406, f16[4,1024,1024]{2,1,0} %convert.405), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%region_167.4526 (Arg_0.4527: f32[], Arg_1.4528: f32[]) -> f32[] {
  %Arg_0.4527 = f32[] parameter(0)
  %Arg_1.4528 = f32[] parameter(1)
  ROOT %add.4529 = f32[] add(f32[] %Arg_0.4527, f32[] %Arg_1.4528), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_168.4542 (Arg_0.4543: f32[], Arg_1.4544: f32[]) -> f32[] {
  %Arg_0.4543 = f32[] parameter(0)
  %Arg_1.4544 = f32[] parameter(1)
  ROOT %add.4545 = f32[] add(f32[] %Arg_0.4543, f32[] %Arg_1.4544), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%fused_computation.252 (param_0.2392: f32[1024], param_1.2911: f16[4,1024,1024], param_2.2734: f16[4096,1024], param_3.2172: f32[4,1024], param_4.1598: f16[4096,1024], param_5.2126: f32[4,1024]) -> (f32[4,1024], f32[4,1024]) {
  %param_4.1598 = f16[4096,1024]{1,0} parameter(4)
  %convert.992 = f32[4096,1024]{1,0} convert(f16[4096,1024]{1,0} %param_4.1598), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %bitcast.1674 = f32[4,1024,1024]{2,1,0} bitcast(f32[4096,1024]{1,0} %convert.992), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_3.2172 = f32[4,1024]{1,0} parameter(3)
  %constant_1812 = f32[] constant(0.0009765625)
  %broadcast.3487 = f32[4,1024]{1,0} broadcast(f32[] %constant_1812), dimensions={}
  %multiply.2882 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_3.2172, f32[4,1024]{1,0} %broadcast.3487), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.3486 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.2882), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.506 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %bitcast.1674, f32[4,1024,1024]{2,1,0} %broadcast.3486), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_1.2911 = f16[4,1024,1024]{2,1,0} parameter(1)
  %param_2.2734 = f16[4096,1024]{1,0} parameter(2)
  %bitcast.1673 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_2.2734), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.2237 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %param_1.2911, f16[4,1024,1024]{2,1,0} %bitcast.1673), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.991 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.2237), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %multiply.2881 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.506, f32[4,1024,1024]{2,1,0} %convert.991), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.2392 = f32[1024]{0} parameter(0)
  %broadcast.1319 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_0.2392), dimensions={2}
  %multiply.1640 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %multiply.2881, f32[4,1024,1024]{2,1,0} %broadcast.1319), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %constant_569 = f32[] constant(0)
  %reduce.238 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1640, f32[] %constant_569), dimensions={2}, to_apply=%region_167.4526, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_5.2126 = f32[4,1024]{1,0} parameter(5)
  %multiply.2892.clone.1 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_5.2126, f32[4,1024]{1,0} %broadcast.3487), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.2890.clone.1 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %multiply.2882, f32[4,1024]{1,0} %multiply.2882), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.508.clone.1 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %multiply.2892.clone.1, f32[4,1024]{1,0} %multiply.2890.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %broadcast.3496.clone.1 = f32[4,1024]{1,0} broadcast(f32[] %constant_569), dimensions={}
  %maximum.230.clone.1 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %subtract.508.clone.1, f32[4,1024]{1,0} %broadcast.3496.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_1817_clone_1 = f32[] constant(1e-12)
  %broadcast.3495.clone.1 = f32[4,1024]{1,0} broadcast(f32[] %constant_1817_clone_1), dimensions={}
  %add.2240.clone.1 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.230.clone.1, f32[4,1024]{1,0} %broadcast.3495.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1679.clone.1 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.2240.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.178.clone.1 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1679.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1678.clone.1 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.178.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.3494.clone.1 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1678.clone.1), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2889.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.3494.clone.1, f32[4,1024,1024]{2,1,0} %broadcast.1319), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2888.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.991, f32[4,1024,1024]{2,1,0} %multiply.2889.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %negate.1.clone.1 = f32[4,1024,1024]{2,1,0} negate(f32[4,1024,1024]{2,1,0} %multiply.2888.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reduce.237.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %negate.1.clone.1, f32[] %constant_569), dimensions={2}, to_apply=%region_168.4542, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  ROOT %tuple.161 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) tuple(f32[4,1024]{1,0} %reduce.238, f32[4,1024]{1,0} %reduce.237.clone.1)
}

%fused_computation.258 (param_0.440: f32[1024], param_1.3000: f32[], param_2.2906: f32[], param_3.2338: f32[1024], param_4.1834: f32[1024], param_5.2122: f32[1024], param_6.1900: f32[1024], param_7.1197: f32[1024], param_8.627: f16[1024], param_9.508: f32[1024], param_10.392: f32[1024], param_11.359: f32[1024], param_12.345: f32[1024], param_13.347: f32[1024], param_14.391: f32[1024], param_15.423: f32[1024], param_16.436: f32[1024], param_17.306: f32[1024], param_18.145: f32[1024], param_19.49: f32[1024], param_20.4: f16[1024], param_21.8: f32[1024], param_22.17: f32[1024], param_23.13: f32[1024], param_24.4: f32[1024], param_25.8: f32[1024], param_26.17: f32[1024], param_27.11: f32[1024], param_28.4: f32[1024], param_29.8: f32[1024], param_30.17: f32[1024], param_31.11: f32[1024], param_32.4: f16[1024], param_33.8: f32[1024]) -> (f32[1024], f32[1024], f32[1024], f32[1024], f32[1024], /*index=5*/f32[1024], f32[1024], f32[1024], f32[1024], f32[1024], /*index=10*/f32[1024], f32[1024], f32[1024], f32[1024], f32[1024], /*index=15*/f32[1024], f32[1024], f32[1024], f32[1024], f32[1024], /*index=20*/f32[1024], f32[1024], f32[1024], f32[1024]) {
  %param_0.440 = f32[1024]{0} parameter(0)
  %param_4.1834 = f32[1024]{0} parameter(4)
  %constant_582_clone_1 = f32[] constant(0.1)
  %broadcast.1335.clone.1 = f32[1024]{0} broadcast(f32[] %constant_582_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1656.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_4.1834, f32[1024]{0} %broadcast.1335.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_5.2122 = f32[1024]{0} parameter(5)
  %constant_583_clone_1 = f32[] constant(0.9)
  %broadcast.1336.clone.1 = f32[1024]{0} broadcast(f32[] %constant_583_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1655.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_5.2122, f32[1024]{0} %broadcast.1336.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1143.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1656.clone.1, f32[1024]{0} %multiply.1655.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_2.2906 = f32[] parameter(2)
  %broadcast.1331 = f32[1024]{0} broadcast(f32[] %param_2.2906), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %multiply.1654.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_4.1834, f32[1024]{0} %param_4.1834), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %constant_580_clone_1 = f32[] constant(0.001)
  %broadcast.1333.clone.1 = f32[1024]{0} broadcast(f32[] %constant_580_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %multiply.1653.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1654.clone.1, f32[1024]{0} %broadcast.1333.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_3.2338 = f32[1024]{0} parameter(3)
  %constant_581_clone_1 = f32[] constant(0.999)
  %broadcast.1334.clone.1 = f32[1024]{0} broadcast(f32[] %constant_581_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %multiply.1652.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_3.2338, f32[1024]{0} %broadcast.1334.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1142.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.1653.clone.1, f32[1024]{0} %multiply.1652.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_1.3000 = f32[] parameter(1)
  %broadcast.1329 = f32[1024]{0} broadcast(f32[] %param_1.3000), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %divide.455 = f32[1024]{0} divide(f32[1024]{0} %add.1142.clone.1, f32[1024]{0} %broadcast.1329), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.152 = f32[1024]{0} sqrt(f32[1024]{0} %divide.455), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %constant_578 = f32[] constant(1e-08)
  %broadcast.1330 = f32[1024]{0} broadcast(f32[] %constant_578), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.1141 = f32[1024]{0} add(f32[1024]{0} %sqrt.152, f32[1024]{0} %broadcast.1330), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1651 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.1331, f32[1024]{0} %add.1141)
  %divide.454 = f32[1024]{0} divide(f32[1024]{0} %add.1143.clone.1, f32[1024]{0} %multiply.1651), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %constant_579 = f32[] constant(-0.01)
  %broadcast.1332 = f32[1024]{0} broadcast(f32[] %constant_579), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %multiply.1650 = f32[1024]{0} multiply(f32[1024]{0} %divide.454, f32[1024]{0} %broadcast.1332), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.1140 = f32[1024]{0} add(f32[1024]{0} %param_0.440, f32[1024]{0} %multiply.1650), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_6.1900 = f32[1024]{0} parameter(6)
  %param_8.627 = f16[1024]{0} parameter(8)
  %convert.4.clone.1.clone.1 = f32[1024]{0} convert(f16[1024]{0} %param_8.627), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.16.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %convert.4.clone.1.clone.1, f32[1024]{0} %broadcast.1335.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_9.508 = f32[1024]{0} parameter(9)
  %multiply.15.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_9.508, f32[1024]{0} %broadcast.1336.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.573.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.16.clone.1.clone.1, f32[1024]{0} %multiply.15.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.14.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %convert.4.clone.1.clone.1, f32[1024]{0} %convert.4.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.13.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.14.clone.1.clone.1, f32[1024]{0} %broadcast.1333.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_7.1197 = f32[1024]{0} parameter(7)
  %multiply.12.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_7.1197, f32[1024]{0} %broadcast.1334.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.572.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.13.clone.1.clone.1, f32[1024]{0} %multiply.12.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.3.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.572.clone.1.clone.1, f32[1024]{0} %broadcast.1329), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.78.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.3.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.538.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.78.clone.1, f32[1024]{0} %broadcast.1330), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.11.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.1331, f32[1024]{0} %add.538.clone.1)
  %divide.2.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.573.clone.1.clone.1, f32[1024]{0} %multiply.11.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.10.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.2.clone.1, f32[1024]{0} %broadcast.1332), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.522.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_6.1900, f32[1024]{0} %multiply.10.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_10.392 = f32[1024]{0} parameter(10)
  %param_12.345 = f32[1024]{0} parameter(12)
  %multiply.23.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_12.345, f32[1024]{0} %broadcast.1335.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_13.347 = f32[1024]{0} parameter(13)
  %multiply.22.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_13.347, f32[1024]{0} %broadcast.1336.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.628.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.23.clone.1.clone.1, f32[1024]{0} %multiply.22.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.21.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_12.345, f32[1024]{0} %param_12.345), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.20.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.21.clone.1.clone.1, f32[1024]{0} %broadcast.1333.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_11.359 = f32[1024]{0} parameter(11)
  %multiply.19.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_11.359, f32[1024]{0} %broadcast.1334.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.622.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.20.clone.1.clone.1, f32[1024]{0} %multiply.19.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.5.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.622.clone.1.clone.1, f32[1024]{0} %broadcast.1329), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.79.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.5.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.613.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.79.clone.1, f32[1024]{0} %broadcast.1330), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.18.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.1331, f32[1024]{0} %add.613.clone.1)
  %divide.4.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.628.clone.1.clone.1, f32[1024]{0} %multiply.18.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.17.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.4.clone.1, f32[1024]{0} %broadcast.1332), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.598.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_10.392, f32[1024]{0} %multiply.17.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_14.391 = f32[1024]{0} parameter(14)
  %param_16.436 = f32[1024]{0} parameter(16)
  %multiply.31.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_16.436, f32[1024]{0} %broadcast.1335.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_17.306 = f32[1024]{0} parameter(17)
  %multiply.30.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_17.306, f32[1024]{0} %broadcast.1336.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.636.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.31.clone.1.clone.1, f32[1024]{0} %multiply.30.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.29.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_16.436, f32[1024]{0} %param_16.436), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.28.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.29.clone.1.clone.1, f32[1024]{0} %broadcast.1333.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_15.423 = f32[1024]{0} parameter(15)
  %multiply.27.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_15.423, f32[1024]{0} %broadcast.1334.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.634.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.28.clone.1.clone.1, f32[1024]{0} %multiply.27.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.7.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.634.clone.1.clone.1, f32[1024]{0} %broadcast.1329), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.80.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.7.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.632.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.80.clone.1, f32[1024]{0} %broadcast.1330), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.26.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.1331, f32[1024]{0} %add.632.clone.1)
  %divide.6.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.636.clone.1.clone.1, f32[1024]{0} %multiply.26.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.25.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.6.clone.1, f32[1024]{0} %broadcast.1332), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.630.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_14.391, f32[1024]{0} %multiply.25.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_18.145 = f32[1024]{0} parameter(18)
  %param_20.4 = f16[1024]{0} parameter(20)
  %convert.17.clone.1.clone.1 = f32[1024]{0} convert(f16[1024]{0} %param_20.4), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.76.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %convert.17.clone.1.clone.1, f32[1024]{0} %broadcast.1335.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_21.8 = f32[1024]{0} parameter(21)
  %multiply.75.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_21.8, f32[1024]{0} %broadcast.1336.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.834.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.76.clone.1.clone.1, f32[1024]{0} %multiply.75.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.74.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %convert.17.clone.1.clone.1, f32[1024]{0} %convert.17.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.73.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.74.clone.1.clone.1, f32[1024]{0} %broadcast.1333.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_19.49 = f32[1024]{0} parameter(19)
  %multiply.72.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_19.49, f32[1024]{0} %broadcast.1334.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.832.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.73.clone.1.clone.1, f32[1024]{0} %multiply.72.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.19.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.832.clone.1.clone.1, f32[1024]{0} %broadcast.1329), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.86.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.19.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.830.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.86.clone.1, f32[1024]{0} %broadcast.1330), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.71.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.1331, f32[1024]{0} %add.830.clone.1)
  %divide.18.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.834.clone.1.clone.1, f32[1024]{0} %multiply.71.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.70.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.18.clone.1, f32[1024]{0} %broadcast.1332), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.828.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_18.145, f32[1024]{0} %multiply.70.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_22.17 = f32[1024]{0} parameter(22)
  %param_24.4 = f32[1024]{0} parameter(24)
  %multiply.83.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_24.4, f32[1024]{0} %broadcast.1335.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_25.8 = f32[1024]{0} parameter(25)
  %multiply.82.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_25.8, f32[1024]{0} %broadcast.1336.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.842.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.83.clone.1.clone.1, f32[1024]{0} %multiply.82.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.81.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_24.4, f32[1024]{0} %param_24.4), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.80.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.81.clone.1.clone.1, f32[1024]{0} %broadcast.1333.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_23.13 = f32[1024]{0} parameter(23)
  %multiply.79.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_23.13, f32[1024]{0} %broadcast.1334.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.840.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.80.clone.1.clone.1, f32[1024]{0} %multiply.79.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.21.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.840.clone.1.clone.1, f32[1024]{0} %broadcast.1329), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.87.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.21.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.838.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.87.clone.1, f32[1024]{0} %broadcast.1330), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.78.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.1331, f32[1024]{0} %add.838.clone.1)
  %divide.20.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.842.clone.1.clone.1, f32[1024]{0} %multiply.78.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.77.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.20.clone.1, f32[1024]{0} %broadcast.1332), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.836.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_22.17, f32[1024]{0} %multiply.77.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_26.17 = f32[1024]{0} parameter(26)
  %param_28.4 = f32[1024]{0} parameter(28)
  %multiply.91.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_28.4, f32[1024]{0} %broadcast.1335.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_29.8 = f32[1024]{0} parameter(29)
  %multiply.90.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_29.8, f32[1024]{0} %broadcast.1336.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.853.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.91.clone.1.clone.1, f32[1024]{0} %multiply.90.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.89.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_28.4, f32[1024]{0} %param_28.4), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.88.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.89.clone.1.clone.1, f32[1024]{0} %broadcast.1333.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_27.11 = f32[1024]{0} parameter(27)
  %multiply.87.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_27.11, f32[1024]{0} %broadcast.1334.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.852.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.88.clone.1.clone.1, f32[1024]{0} %multiply.87.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.23.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.852.clone.1.clone.1, f32[1024]{0} %broadcast.1329), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.88.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.23.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.851.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.88.clone.1, f32[1024]{0} %broadcast.1330), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.86.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.1331, f32[1024]{0} %add.851.clone.1)
  %divide.22.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.853.clone.1.clone.1, f32[1024]{0} %multiply.86.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.85.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.22.clone.1, f32[1024]{0} %broadcast.1332), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.846.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_26.17, f32[1024]{0} %multiply.85.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %param_30.17 = f32[1024]{0} parameter(30)
  %param_32.4 = f16[1024]{0} parameter(32)
  %convert.22.clone.1.clone.1 = f32[1024]{0} convert(f16[1024]{0} %param_32.4), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.106.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %convert.22.clone.1.clone.1, f32[1024]{0} %broadcast.1335.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_33.8 = f32[1024]{0} parameter(33)
  %multiply.105.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_33.8, f32[1024]{0} %broadcast.1336.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.862.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.106.clone.1.clone.1, f32[1024]{0} %multiply.105.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.104.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %convert.22.clone.1.clone.1, f32[1024]{0} %convert.22.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.103.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %multiply.104.clone.1.clone.1, f32[1024]{0} %broadcast.1333.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_31.11 = f32[1024]{0} parameter(31)
  %multiply.102.clone.1.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %param_31.11, f32[1024]{0} %broadcast.1334.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.861.clone.1.clone.1 = f32[1024]{0} add(f32[1024]{0} %multiply.103.clone.1.clone.1, f32[1024]{0} %multiply.102.clone.1.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.27.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.861.clone.1.clone.1, f32[1024]{0} %broadcast.1329), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.90.clone.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.27.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.860.clone.1 = f32[1024]{0} add(f32[1024]{0} %sqrt.90.clone.1, f32[1024]{0} %broadcast.1330), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.101.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.1331, f32[1024]{0} %add.860.clone.1)
  %divide.26.clone.1 = f32[1024]{0} divide(f32[1024]{0} %add.862.clone.1.clone.1, f32[1024]{0} %multiply.101.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.100.clone.1 = f32[1024]{0} multiply(f32[1024]{0} %divide.26.clone.1, f32[1024]{0} %broadcast.1332), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.859.clone.1 = f32[1024]{0} add(f32[1024]{0} %param_30.17, f32[1024]{0} %multiply.100.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  ROOT %tuple.229 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) tuple(f32[1024]{0} %add.1140, f32[1024]{0} %add.1142.clone.1, f32[1024]{0} %add.1143.clone.1, f32[1024]{0} %add.522.clone.1, f32[1024]{0} %add.572.clone.1.clone.1, /*index=5*/f32[1024]{0} %add.573.clone.1.clone.1, f32[1024]{0} %add.598.clone.1, f32[1024]{0} %add.622.clone.1.clone.1, f32[1024]{0} %add.628.clone.1.clone.1, f32[1024]{0} %add.630.clone.1, /*index=10*/f32[1024]{0} %add.634.clone.1.clone.1, f32[1024]{0} %add.636.clone.1.clone.1, f32[1024]{0} %add.828.clone.1, f32[1024]{0} %add.832.clone.1.clone.1, f32[1024]{0} %add.834.clone.1.clone.1, /*index=15*/f32[1024]{0} %add.836.clone.1, f32[1024]{0} %add.840.clone.1.clone.1, f32[1024]{0} %add.842.clone.1.clone.1, f32[1024]{0} %add.846.clone.1, f32[1024]{0} %add.852.clone.1.clone.1, /*index=20*/f32[1024]{0} %add.853.clone.1.clone.1, f32[1024]{0} %add.859.clone.1, f32[1024]{0} %add.861.clone.1.clone.1, f32[1024]{0} %add.862.clone.1.clone.1)
}

%region_165.4507 (Arg_0.4508: f32[], Arg_1.4509: f32[]) -> f32[] {
  %Arg_0.4508 = f32[] parameter(0)
  %Arg_1.4509 = f32[] parameter(1)
  ROOT %add.4510 = f32[] add(f32[] %Arg_0.4508, f32[] %Arg_1.4509), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
}

%region_166.4517 (Arg_0.4518: f32[], Arg_1.4519: f32[]) -> f32[] {
  %Arg_0.4518 = f32[] parameter(0)
  %Arg_1.4519 = f32[] parameter(1)
  ROOT %add.4520 = f32[] add(f32[] %Arg_0.4518, f32[] %Arg_1.4519), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%fused_computation.261 (param_0.2384: f16[4,1024,1024], param_1.2902: f16[4096,1024], param_2.2910: f32[4,1024], param_3.2344: f16[4096,1024], param_4.1851: f32[4,1024]) -> (f32[1024], f32[1024]) {
  %param_0.2384 = f16[4,1024,1024]{2,1,0} parameter(0)
  %param_1.2902 = f16[4096,1024]{1,0} parameter(1)
  %bitcast.1662 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_1.2902), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.2229 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %param_0.2384, f16[4,1024,1024]{2,1,0} %bitcast.1662), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.409 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.2229), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %bitcast.655 = f32[4096,1024]{1,0} bitcast(f32[4,1024,1024]{2,1,0} %convert.409)
  %constant_584 = f32[] constant(0)
  %reduce.240 = f32[1024]{0} reduce(f32[4096,1024]{1,0} %bitcast.655, f32[] %constant_584), dimensions={0}, to_apply=%region_165.4507, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %param_4.1851 = f32[4,1024]{1,0} parameter(4)
  %constant_1809_clone_1 = f32[] constant(0.0009765625)
  %broadcast.1847.clone.1 = f32[4,1024]{1,0} broadcast(f32[] %constant_1809_clone_1), dimensions={}
  %multiply.2208.clone.1 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_4.1851, f32[4,1024]{1,0} %broadcast.1847.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.2910 = f32[4,1024]{1,0} parameter(2)
  %multiply.2207.clone.1 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_2.2910, f32[4,1024]{1,0} %broadcast.1847.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2206.clone.1 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %multiply.2207.clone.1, f32[4,1024]{1,0} %multiply.2207.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.120.clone.1 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %multiply.2208.clone.1, f32[4,1024]{1,0} %multiply.2206.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %broadcast.1846.clone.1 = f32[4,1024]{1,0} broadcast(f32[] %constant_584), dimensions={}
  %maximum.70.clone.1 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %subtract.120.clone.1, f32[4,1024]{1,0} %broadcast.1846.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_1108_clone_1 = f32[] constant(1e-12)
  %broadcast.1845.clone.1 = f32[4,1024]{1,0} broadcast(f32[] %constant_1108_clone_1), dimensions={}
  %add.1496.clone.1 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.70.clone.1, f32[4,1024]{1,0} %broadcast.1845.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1040.clone.1 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.1496.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.66.clone.1 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1040.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1039.clone.1 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.66.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.1844.clone.1 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1039.clone.1), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_3.2344 = f16[4096,1024]{1,0} parameter(3)
  %convert.988.clone.1 = f32[4096,1024]{1,0} convert(f16[4096,1024]{1,0} %param_3.2344), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %bitcast.1670.clone.1 = f32[4,1024,1024]{2,1,0} bitcast(f32[4096,1024]{1,0} %convert.988.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %broadcast.3482.clone.1 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.2207.clone.1), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.504.clone.1 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %bitcast.1670.clone.1, f32[4,1024,1024]{2,1,0} %broadcast.3482.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.2877.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.504.clone.1, f32[4,1024,1024]{2,1,0} %convert.409), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.1648.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.1844.clone.1, f32[4,1024,1024]{2,1,0} %multiply.2877.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %bitcast.654.clone.1 = f32[4096,1024]{1,0} bitcast(f32[4,1024,1024]{2,1,0} %multiply.1648.clone.1)
  %reduce.239.clone.1 = f32[1024]{0} reduce(f32[4096,1024]{1,0} %bitcast.654.clone.1, f32[] %constant_584), dimensions={0}, to_apply=%region_166.4517, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  ROOT %tuple.162 = (f32[1024]{0}, f32[1024]{0}) tuple(f32[1024]{0} %reduce.240, f32[1024]{0} %reduce.239.clone.1)
}

%fused_computation.263 (param_0.1488: f16[4,4,32,1024], param_1.1937: f16[4,4,1024,32], param_2.1635: f16[], param_3.1309: f16[4,4,1024,32]) -> f16[4,1024,128,3] {
  %param_3.1309 = f16[4,4,1024,32]{3,2,1,0} parameter(3)
  %transpose = f16[4,1024,4,32]{3,1,2,0} transpose(f16[4,4,1024,32]{3,2,1,0} %param_3.1309), dimensions={0,2,1,3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/transpose[permutation=(0, 2, 1, 3)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %copy.86 = f16[4,1024,4,32]{1,3,2,0} copy(f16[4,1024,4,32]{3,1,2,0} %transpose), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/transpose[permutation=(0, 2, 1, 3)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %bitcast.660 = f16[4,1024,128,1]{1,2,0,3} bitcast(f16[4,1024,4,32]{1,3,2,0} %copy.86), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=173}
  %constant_585 = f16[] constant(0)
  %pad.20 = f16[4,1024,128,3]{1,2,0,3} pad(f16[4,1024,128,1]{1,2,0,3} %bitcast.660, f16[] %constant_585), padding=0_0x0_0x0_0x2_0, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (2, 0, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %param_1.1937 = f16[4,4,1024,32]{3,2,1,0} parameter(1)
  %param_2.1635 = f16[] parameter(2)
  %broadcast.17 = f16[4,4,1024,32]{3,2,1,0} broadcast(f16[] %param_2.1635), dimensions={}
  %divide.456 = f16[4,4,1024,32]{3,2,1,0} divide(f16[4,4,1024,32]{3,2,1,0} %param_1.1937, f16[4,4,1024,32]{3,2,1,0} %broadcast.17), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %transpose.1 = f16[4,1024,4,32]{3,1,2,0} transpose(f16[4,4,1024,32]{3,2,1,0} %divide.456), dimensions={0,2,1,3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %copy.85 = f16[4,1024,4,32]{1,3,2,0} copy(f16[4,1024,4,32]{3,1,2,0} %transpose.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %bitcast.658 = f16[4,1024,128,1]{1,2,0,3} bitcast(f16[4,1024,4,32]{1,3,2,0} %copy.85), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=167}
  %pad.19 = f16[4,1024,128,3]{1,2,0,3} pad(f16[4,1024,128,1]{1,2,0,3} %bitcast.658, f16[] %constant_585), padding=0_0x0_0x0_0x0_2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 2, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %add.1146 = f16[4,1024,128,3]{1,2,0,3} add(f16[4,1024,128,3]{1,2,0,3} %pad.20, f16[4,1024,128,3]{1,2,0,3} %pad.19), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %param_0.1488 = f16[4,4,32,1024]{3,2,1,0} parameter(0)
  %bitcast.1855 = f16[1,4,128,1024]{3,2,1,0} bitcast(f16[4,4,32,1024]{3,2,1,0} %param_0.1488)
  %transpose.2 = f16[4,1024,128,1]{1,2,0,3} transpose(f16[1,4,128,1024]{3,2,1,0} %bitcast.1855), dimensions={1,3,2,0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=170}
  %pad.18 = f16[4,1024,128,3]{1,2,0,3} pad(f16[4,1024,128,1]{1,2,0,3} %transpose.2, f16[] %constant_585), padding=0_0x0_0x0_0x1_1, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (1, 1, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %add.1145 = f16[4,1024,128,3]{1,2,0,3} add(f16[4,1024,128,3]{1,2,0,3} %add.1146, f16[4,1024,128,3]{1,2,0,3} %pad.18), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  ROOT %copy.84 = f16[4,1024,128,3]{3,2,1,0} copy(f16[4,1024,128,3]{1,2,0,3} %add.1145), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
}

%fused_computation.264 (param_0.452: f16[4096,128]) -> f16[4,4,32,1024] {
  %param_0.452 = f16[4096,128]{1,0} parameter(0)
  %bitcast.1856 = f16[4,1024,4,32]{3,2,1,0} bitcast(f16[4096,128]{1,0} %param_0.452)
  %transpose.3 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %bitcast.1856), dimensions={0,2,3,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/transpose[permutation=(0, 2, 3, 1)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  ROOT %copy.87 = f16[4,4,32,1024]{3,2,1,0} copy(f16[4,4,32,1024]{2,1,3,0} %transpose.3), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/transpose[permutation=(0, 2, 3, 1)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
}

%fused_computation.265 (param_0.2321: f16[4096,384], param_1.3015: f16[]) -> (f16[4,4,1024,32], f16[4,4,1024,32]) {
  %param_0.2321 = f16[4096,384]{1,0} parameter(0)
  %bitcast.1598 = f16[4,1024,384]{2,1,0} bitcast(f16[4096,384]{1,0} %param_0.2321), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %bitcast.1597 = f16[4,1024,128,3]{3,2,1,0} bitcast(f16[4,1024,384]{2,1,0} %bitcast.1598), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %copy.239 = f16[4,1024,128,3]{1,2,0,3} copy(f16[4,1024,128,3]{3,2,1,0} %bitcast.1597), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %slice.66 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.239), slice={[0:4], [0:1024], [0:128], [2:3]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/slice[start_indices=(0, 0, 0, 2) limit_indices=(8, 1024, 1024, 3) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.4 = f16[1,4,128,1024]{3,2,1,0} transpose(f16[4,1024,128,1]{1,2,0,3} %slice.66), dimensions={3,0,2,1}
  %bitcast.1857 = f16[4,4,32,1024]{3,2,1,0} bitcast(f16[1,4,128,1024]{3,2,1,0} %transpose.4)
  %transpose.5 = f16[4,4,1024,32]{2,3,1,0} transpose(f16[4,4,32,1024]{3,2,1,0} %bitcast.1857), dimensions={0,1,3,2}
  %copy.88 = f16[4,4,1024,32]{3,2,1,0} copy(f16[4,4,1024,32]{2,3,1,0} %transpose.5)
  %slice.36.clone.1 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.239), slice={[0:4], [0:1024], [0:128], [0:1]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/slice[start_indices=(0, 0, 0, 0) limit_indices=(8, 1024, 1024, 1) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %param_1.3015 = f16[] parameter(1)
  %broadcast.40.clone.1 = f16[4,1024,128,1]{1,2,0,3} broadcast(f16[] %param_1.3015), dimensions={}
  %divide.473.clone.1 = f16[4,1024,128,1]{1,2,0,3} divide(f16[4,1024,128,1]{1,2,0,3} %slice.36.clone.1, f16[4,1024,128,1]{1,2,0,3} %broadcast.40.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %bitcast.685.clone.1 = f16[4,1024,4,32]{1,3,2,0} bitcast(f16[4,1024,128,1]{1,2,0,3} %divide.473.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %copy.91.clone.1 = f16[4,1024,4,32]{3,1,2,0} copy(f16[4,1024,4,32]{1,3,2,0} %bitcast.685.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %transpose.6 = f16[4,4,1024,32]{3,2,1,0} transpose(f16[4,1024,4,32]{3,1,2,0} %copy.91.clone.1), dimensions={0,2,1,3}
  ROOT %tuple.171 = (f16[4,4,1024,32]{3,2,1,0}, f16[4,4,1024,32]{3,2,1,0}) tuple(f16[4,4,1024,32]{3,2,1,0} %copy.88, f16[4,4,1024,32]{3,2,1,0} %transpose.6)
}

%fused_computation.266 (param_0.2330: f16[4,4,1024,1024], param_1.2821: f16[4,4,1024], param_2.2626: f32[4,4,1024], param_3.2065: f16[4,4,1024,1024], param_4.1497: f16[4,4,1024]) -> f16[4,4,1024,1024] {
  %param_1.2821 = f16[4,4,1024]{2,1,0} parameter(1)
  %negate.2 = f16[4,4,1024]{2,1,0} negate(f16[4,4,1024]{2,1,0} %param_1.2821), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.18 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %negate.2), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/broadcast_in_dim[shape=(8, 32, 1024, 1024) broadcast_dimensions=(0, 1, 2)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %param_0.2330 = f16[4,4,1024,1024]{3,2,1,0} parameter(0)
  %param_2.2626 = f32[4,4,1024]{2,1,0} parameter(2)
  %convert.661 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %param_2.2626), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.1337 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %convert.661), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.457 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %param_0.2330, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1337), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %add.1147 = f16[4,4,1024,1024]{3,2,1,0} add(f16[4,4,1024,1024]{3,2,1,0} %broadcast.18, f16[4,4,1024,1024]{3,2,1,0} %divide.457), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %param_3.2065 = f16[4,4,1024,1024]{3,2,1,0} parameter(3)
  %param_4.1497 = f16[4,4,1024]{2,1,0} parameter(4)
  %broadcast.3241 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_4.1497), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.456 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_3.2065, f16[4,4,1024,1024]{3,2,1,0} %broadcast.3241), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.113 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.456), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  ROOT %multiply.1657 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %add.1147, f16[4,4,1024,1024]{3,2,1,0} %exponential.113), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%region_162.4457 (Arg_0.4458: f16[], Arg_1.4459: f16[]) -> f16[] {
  %Arg_0.4458 = f16[] parameter(0)
  %Arg_1.4459 = f16[] parameter(1)
  ROOT %add.4460 = f16[] add(f16[] %Arg_0.4458, f16[] %Arg_1.4459), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.267 (param_0.2328: f16[4,4,1024,1024], param_1.2819: f32[4,4,1024], param_2.2625: f16[4,4,1024,1024], param_3.2063: f16[4,4,1024]) -> f16[4,4,1024] {
  %param_0.2328 = f16[4,4,1024,1024]{3,2,1,0} parameter(0)
  %constant_586 = f16[] constant(1)
  %broadcast.1338 = f16[4,4,1024]{2,1,0} broadcast(f16[] %constant_586), dimensions={}
  %param_1.2819 = f32[4,4,1024]{2,1,0} parameter(1)
  %convert.662 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %param_1.2819), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.1660 = f16[4,4,1024]{2,1,0} multiply(f16[4,4,1024]{2,1,0} %convert.662, f16[4,4,1024]{2,1,0} %convert.662), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.458 = f16[4,4,1024]{2,1,0} divide(f16[4,4,1024]{2,1,0} %broadcast.1338, f16[4,4,1024]{2,1,0} %multiply.1660), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.19 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %divide.458), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.1659 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %param_0.2328, f16[4,4,1024,1024]{3,2,1,0} %broadcast.19), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %param_2.2625 = f16[4,4,1024,1024]{3,2,1,0} parameter(2)
  %param_3.2063 = f16[4,4,1024]{2,1,0} parameter(3)
  %broadcast.3239 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_3.2063), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.454 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_2.2625, f16[4,4,1024,1024]{3,2,1,0} %broadcast.3239), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.111 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.454), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.1658 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %multiply.1659, f16[4,4,1024,1024]{3,2,1,0} %exponential.111), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %constant_587 = f16[] constant(0)
  ROOT %reduce.241 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %multiply.1658, f16[] %constant_587), dimensions={3}, to_apply=%region_162.4457, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.268 (param_0.461: f16[4096,128]) -> f16[4,4,1024,32] {
  %param_0.461 = f16[4096,128]{1,0} parameter(0)
  %bitcast.665 = f16[4,1024,4,32]{3,2,1,0} bitcast(f16[4096,128]{1,0} %param_0.461), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %copy.89 = f16[4,1024,4,32]{3,1,2,0} copy(f16[4,1024,4,32]{3,2,1,0} %bitcast.665), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  ROOT %transpose.7 = f16[4,4,1024,32]{3,2,1,0} transpose(f16[4,1024,4,32]{3,1,2,0} %copy.89), dimensions={0,2,1,3}
}

%fused_computation.269 (param_0.2378: f32[4,1024], param_1.2894: f32[8,1024], param_2.2717: s32[], param_3.2152: f32[8,1024], param_4.1579: f32[8,1024], param_5.1604: f32[8,1024], param_6.1315: f32[4,1024], param_7.914: f16[4,1024,1024], param_8.564: f32[], param_9.400: f32[4,1024], param_10.320: f32[], param_11.345: f32[], param_12.340: f32[4,1024], param_13.338: f32[], param_14.373: f32[1024], param_15.411: f16[4,1024,1024], param_16.431: f16[4096,1024]) -> f16[4,1024,1024] {
  %param_7.914 = f16[4,1024,1024]{2,1,0} parameter(7)
  %convert.412 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_7.914), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %param_6.1315 = f32[4,1024]{1,0} parameter(6)
  %bitcast.667 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %param_6.1315), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reshape[new_sizes=(8, 1024, 1) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_11.345 = f32[] parameter(11)
  %broadcast.3273 = f32[4,1024]{1,0} broadcast(f32[] %param_11.345), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_12.340 = f32[4,1024]{1,0} parameter(12)
  %param_8.564 = f32[] parameter(8)
  %broadcast.3272 = f32[4,1024]{1,0} broadcast(f32[] %param_8.564), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.1097 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_12.340, f32[4,1024]{1,0} %broadcast.3272), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_9.400 = f32[4,1024]{1,0} parameter(9)
  %param_10.320 = f32[] parameter(10)
  %broadcast.3271 = f32[4,1024]{1,0} broadcast(f32[] %param_10.320), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.1096 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_9.400, f32[4,1024]{1,0} %broadcast.3271), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2772 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.1096, f32[4,1024]{1,0} %divide.1096), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.468 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.1097, f32[4,1024]{1,0} %multiply.2772), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.206 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.3273, f32[4,1024]{1,0} %subtract.468), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_13.338 = f32[] parameter(13)
  %broadcast.3270 = f32[4,1024]{1,0} broadcast(f32[] %param_13.338), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.2136 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.206, f32[4,1024]{1,0} %broadcast.3270), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1602 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.2136), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.38 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1602), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %divide.462 = f32[4,1024,1]{1,0,2} divide(f32[4,1024,1]{1,0,2} %rsqrt.38, f32[4,1024,1]{1,0,2} %bitcast.1602), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %constant_588 = f32[] constant(-0.5)
  %broadcast.1342 = f32[4,1024,1]{1,0,2} broadcast(f32[] %constant_588), dimensions={}
  %multiply.1667 = f32[4,1024,1]{1,0,2} multiply(f32[4,1024,1]{1,0,2} %divide.462, f32[4,1024,1]{1,0,2} %broadcast.1342), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.1666 = f32[4,1024,1]{1,0,2} multiply(f32[4,1024,1]{1,0,2} %bitcast.667, f32[4,1024,1]{1,0,2} %multiply.1667), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.666 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %multiply.1666), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reshape[new_sizes=(8, 1024) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=132}
  %compare.5 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %subtract.468, f32[4,1024]{1,0} %maximum.206), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_5.1604 = f32[8,1024]{1,0} parameter(5)
  %param_2.2717 = s32[] parameter(2)
  %constant_590 = s32[] constant(0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %dynamic-slice.11 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_5.1604, s32[] %param_2.2717, s32[] %constant_590), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_4.1579 = f32[8,1024]{1,0} parameter(4)
  %dynamic-slice.10 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_4.1579, s32[] %param_2.2717, s32[] %constant_590), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.39 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.5, f32[4,1024]{1,0} %dynamic-slice.11, f32[4,1024]{1,0} %dynamic-slice.10), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %compare.4 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %broadcast.3273, f32[4,1024]{1,0} %maximum.206), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_3.2152 = f32[8,1024]{1,0} parameter(3)
  %dynamic-slice.9 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_3.2152, s32[] %param_2.2717, s32[] %constant_590), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_1.2894 = f32[8,1024]{1,0} parameter(1)
  %dynamic-slice.6 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_1.2894, s32[] %param_2.2717, s32[] %constant_590), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.38 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.4, f32[4,1024]{1,0} %dynamic-slice.9, f32[4,1024]{1,0} %dynamic-slice.6), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.461 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %select.39, f32[4,1024]{1,0} %select.38), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.1665 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %bitcast.666, f32[4,1024]{1,0} %divide.461), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.460 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %multiply.1665, f32[4,1024]{1,0} %broadcast.3272), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_589 = f32[] constant(2)
  %broadcast.1343 = f32[4,1024]{1,0} broadcast(f32[] %constant_589), dimensions={}
  %multiply.1664 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.460, f32[4,1024]{1,0} %broadcast.1343), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %broadcast.21 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.1664), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.1663 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.412, f32[4,1024,1024]{2,1,0} %broadcast.21), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %negate.3 = f32[4,1024]{1,0} negate(f32[4,1024]{1,0} %multiply.1665), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.1662 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.1096, f32[4,1024]{1,0} %broadcast.1343), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.1661 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %negate.3, f32[4,1024]{1,0} %multiply.1662), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %param_0.2378 = f32[4,1024]{1,0} parameter(0)
  %add.1150 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %multiply.1661, f32[4,1024]{1,0} %param_0.2378), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=131}
  %divide.459 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %add.1150, f32[4,1024]{1,0} %broadcast.3271), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.20 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.459), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/broadcast_in_dim[shape=(8, 1024, 1024) broadcast_dimensions=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %add.1149 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.1663, f32[4,1024,1024]{2,1,0} %broadcast.20), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %convert.411 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.1149), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %param_15.411 = f16[4,1024,1024]{2,1,0} parameter(15)
  %param_16.431 = f16[4096,1024]{1,0} parameter(16)
  %bitcast.1656 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_16.431), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.2223 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %param_15.411, f16[4,1024,1024]{2,1,0} %bitcast.1656), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.976 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.2223), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %bitcast.1654 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.38), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.3467 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1654), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_14.373 = f32[1024]{0} parameter(14)
  %broadcast.3466 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_14.373), dimensions={2}
  %multiply.2869 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.3467, f32[4,1024,1024]{2,1,0} %broadcast.3466), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2868 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.976, f32[4,1024,1024]{2,1,0} %multiply.2869), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %convert.410 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %multiply.2868), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  ROOT %add.1148 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %convert.411, f16[4,1024,1024]{2,1,0} %convert.410), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%region_160.4426 (Arg_0.4427: f32[], Arg_1.4428: f32[]) -> f32[] {
  %Arg_0.4427 = f32[] parameter(0)
  %Arg_1.4428 = f32[] parameter(1)
  ROOT %add.4429 = f32[] add(f32[] %Arg_0.4427, f32[] %Arg_1.4428), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%region_159.4409 (Arg_0.4410: f32[], Arg_1.4411: f32[]) -> f32[] {
  %Arg_0.4410 = f32[] parameter(0)
  %Arg_1.4411 = f32[] parameter(1)
  ROOT %add.4412 = f32[] add(f32[] %Arg_0.4410, f32[] %Arg_1.4411), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%fused_computation.270 (param_0.2376: f32[1024], param_1.2892: f32[], param_2.2715: f32[4,1024], param_3.2150: f32[], param_4.1577: f32[4,1024], param_5.1602: f32[], param_6.1313: f32[], param_7.912: f16[4,1024,1024], param_8.562: f16[4096,1024], param_9.412: f16[4,1024,1024]) -> (f32[4,1024], f32[4,1024]) {
  %param_7.912 = f16[4,1024,1024]{2,1,0} parameter(7)
  %param_8.562 = f16[4096,1024]{1,0} parameter(8)
  %bitcast.1650 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_8.562), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.2219 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %param_7.912, f16[4,1024,1024]{2,1,0} %bitcast.1650), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.974 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.2219), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %param_6.1313 = f32[] parameter(6)
  %broadcast.3459 = f32[4,1024]{1,0} broadcast(f32[] %param_6.1313), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_4.1577 = f32[4,1024]{1,0} parameter(4)
  %param_5.1602 = f32[] parameter(5)
  %broadcast.3458 = f32[4,1024]{1,0} broadcast(f32[] %param_5.1602), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.1157 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_4.1577, f32[4,1024]{1,0} %broadcast.3458), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.2715 = f32[4,1024]{1,0} parameter(2)
  %param_3.2150 = f32[] parameter(3)
  %broadcast.3457 = f32[4,1024]{1,0} broadcast(f32[] %param_3.2150), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.1156 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_2.2715, f32[4,1024]{1,0} %broadcast.3457), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2864 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.1156, f32[4,1024]{1,0} %divide.1156), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.496 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.1157, f32[4,1024]{1,0} %multiply.2864), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.226 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.3459, f32[4,1024]{1,0} %subtract.496), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_1.2892 = f32[] parameter(1)
  %broadcast.3456 = f32[4,1024]{1,0} broadcast(f32[] %param_1.2892), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.2218 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.226, f32[4,1024]{1,0} %broadcast.3456), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1649 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.2218), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.174 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1649), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1648 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.174), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.3455 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1648), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_0.2376 = f32[1024]{0} parameter(0)
  %broadcast.3454 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_0.2376), dimensions={2}
  %multiply.2863 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.3455, f32[4,1024,1024]{2,1,0} %broadcast.3454), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2862 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.974, f32[4,1024,1024]{2,1,0} %multiply.2863), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %negate.4 = f32[4,1024,1024]{2,1,0} negate(f32[4,1024,1024]{2,1,0} %multiply.2862), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %constant_592 = f32[] constant(0)
  %reduce.242 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %negate.4, f32[] %constant_592), dimensions={2}, to_apply=%region_160.4426, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_9.412 = f16[4,1024,1024]{2,1,0} parameter(9)
  %convert.980.clone.1 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_9.412), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %broadcast.3474.clone.1 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.1156), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.500.clone.1 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.980.clone.1, f32[4,1024,1024]{2,1,0} %broadcast.3474.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.2872.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.500.clone.1, f32[4,1024,1024]{2,1,0} %convert.974), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.1669.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %multiply.2872.clone.1, f32[4,1024,1024]{2,1,0} %broadcast.3454), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.243.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1669.clone.1, f32[] %constant_592), dimensions={2}, to_apply=%region_159.4409, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  ROOT %tuple.164 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) tuple(f32[4,1024]{1,0} %reduce.242, f32[4,1024]{1,0} %reduce.243.clone.1)
}

%fused_computation.275 (param_0.2347: f16[4096,512], param_1.2850: f16[], param_2.2662: f16[], param_3.2104: f16[4096,512], param_4.1536: f16[]) -> f16[4,1024,512] {
  %param_3.2104 = f16[4096,512]{1,0} parameter(3)
  %bitcast.978 = f16[4,1024,512]{2,1,0} bitcast(f16[4096,512]{1,0} %param_3.2104), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_0.2347 = f16[4096,512]{1,0} parameter(0)
  %param_1.2850 = f16[] parameter(1)
  %broadcast.22 = f16[4096,512]{1,0} broadcast(f16[] %param_1.2850), dimensions={}
  %divide.464 = f16[4096,512]{1,0} divide(f16[4096,512]{1,0} %param_0.2347, f16[4096,512]{1,0} %broadcast.22), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %bitcast.669 = f16[4,1024,512]{2,1,0} bitcast(f16[4096,512]{1,0} %divide.464), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1675 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %bitcast.978, f16[4,1024,512]{2,1,0} %bitcast.669), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_593 = f16[] constant(1.1279)
  %broadcast.1347 = f16[4,1024,512]{2,1,0} broadcast(f16[] %constant_593), dimensions={}
  %multiply.1674 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.1675, f16[4,1024,512]{2,1,0} %broadcast.1347), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %param_2.2662 = f16[] parameter(2)
  %broadcast.1346 = f16[4,1024,512]{2,1,0} broadcast(f16[] %param_2.2662), dimensions={}
  %divide.581 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %bitcast.978, f16[4,1024,512]{2,1,0} %broadcast.1346), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1673 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %divide.581, f16[4,1024,512]{2,1,0} %divide.581), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %negate.5 = f16[4,1024,512]{2,1,0} negate(f16[4,1024,512]{2,1,0} %multiply.1673), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/neg" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %exponential.19 = f16[4,1024,512]{2,1,0} exponential(f16[4,1024,512]{2,1,0} %negate.5), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/exp" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1672 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.1674, f16[4,1024,512]{2,1,0} %exponential.19), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.463 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %multiply.1672, f16[4,1024,512]{2,1,0} %broadcast.1346), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1786 = f32[] constant(-4)
  %broadcast.3371 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1786), dimensions={}
  %convert.960 = f32[4,1024,512]{2,1,0} convert(f16[4,1024,512]{2,1,0} %divide.581), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1785 = f32[] constant(4)
  %broadcast.3369 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1785), dimensions={}
  %clamp.49 = f32[4,1024,512]{2,1,0} clamp(f32[4,1024,512]{2,1,0} %broadcast.3371, f32[4,1024,512]{2,1,0} %convert.960, f32[4,1024,512]{2,1,0} %broadcast.3369), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2830 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.49, f32[4,1024,512]{2,1,0} %clamp.49), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1784 = f32[] constant(0)
  %broadcast.3368 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1784), dimensions={}
  %multiply.2829 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %multiply.2830, f32[4,1024,512]{2,1,0} %broadcast.3368), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1783 = f32[] constant(-2.72614237e-10)
  %broadcast.3367 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1783), dimensions={}
  %add.2196 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2829, f32[4,1024,512]{2,1,0} %broadcast.3367), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2828 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2196, f32[4,1024,512]{2,1,0} %multiply.2830), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1782 = f32[] constant(2.77068146e-08)
  %broadcast.3366 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1782), dimensions={}
  %add.2195 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2828, f32[4,1024,512]{2,1,0} %broadcast.3366), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2827 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2195, f32[4,1024,512]{2,1,0} %multiply.2830), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1781 = f32[] constant(-2.10102394e-06)
  %broadcast.3365 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1781), dimensions={}
  %add.2194 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2827, f32[4,1024,512]{2,1,0} %broadcast.3365), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2826 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2194, f32[4,1024,512]{2,1,0} %multiply.2830), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1780 = f32[] constant(-5.69250624e-05)
  %broadcast.3364 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1780), dimensions={}
  %add.2193 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2826, f32[4,1024,512]{2,1,0} %broadcast.3364), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2825 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2193, f32[4,1024,512]{2,1,0} %multiply.2830), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1779 = f32[] constant(-0.000734990637)
  %broadcast.3363 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1779), dimensions={}
  %add.2192 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2825, f32[4,1024,512]{2,1,0} %broadcast.3363), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2824 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2192, f32[4,1024,512]{2,1,0} %multiply.2830), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1778 = f32[] constant(-0.0029546)
  %broadcast.3362 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1778), dimensions={}
  %add.2191 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2824, f32[4,1024,512]{2,1,0} %broadcast.3362), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2823 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2191, f32[4,1024,512]{2,1,0} %multiply.2830), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1777 = f32[] constant(-0.0160960332)
  %broadcast.3361 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1777), dimensions={}
  %add.2190 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2823, f32[4,1024,512]{2,1,0} %broadcast.3361), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2822 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.49, f32[4,1024,512]{2,1,0} %add.2190), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1776 = f32[] constant(-1.45660715e-05)
  %broadcast.3360 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1776), dimensions={}
  %add.2189 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2829, f32[4,1024,512]{2,1,0} %broadcast.3360), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2821 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2189, f32[4,1024,512]{2,1,0} %multiply.2830), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1775 = f32[] constant(-0.000213374049)
  %broadcast.3359 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1775), dimensions={}
  %add.2188 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2821, f32[4,1024,512]{2,1,0} %broadcast.3359), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2820 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2188, f32[4,1024,512]{2,1,0} %multiply.2830), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1774 = f32[] constant(-0.00168282702)
  %broadcast.3358 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1774), dimensions={}
  %add.2187 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2820, f32[4,1024,512]{2,1,0} %broadcast.3358), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2819 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2187, f32[4,1024,512]{2,1,0} %multiply.2830), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1773 = f32[] constant(-0.00737332925)
  %broadcast.3357 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1773), dimensions={}
  %add.2186 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2819, f32[4,1024,512]{2,1,0} %broadcast.3357), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2818 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2186, f32[4,1024,512]{2,1,0} %multiply.2830), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1772 = f32[] constant(-0.0142647391)
  %broadcast.3356 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1772), dimensions={}
  %add.2185 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2818, f32[4,1024,512]{2,1,0} %broadcast.3356), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.1116 = f32[4,1024,512]{2,1,0} divide(f32[4,1024,512]{2,1,0} %multiply.2822, f32[4,1024,512]{2,1,0} %add.2185), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.959 = f16[4,1024,512]{2,1,0} convert(f32[4,1024,512]{2,1,0} %divide.1116), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %param_4.1536 = f16[] parameter(4)
  %broadcast.3355 = f16[4,1024,512]{2,1,0} broadcast(f16[] %param_4.1536), dimensions={}
  %add.2184 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %convert.959, f16[4,1024,512]{2,1,0} %broadcast.3355), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1671 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %bitcast.669, f16[4,1024,512]{2,1,0} %add.2184), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  ROOT %add.1152 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %divide.463, f16[4,1024,512]{2,1,0} %multiply.1671), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
}

%fused_computation.276 (param_0.2363: f32[4,1024], param_1.2875: f32[8,1024], param_2.2696: s32[], param_3.2135: f32[8,1024], param_4.1564: f32[8,1024], param_5.1593: f32[8,1024], param_6.1304: f32[4,1024], param_7.901: f16[4,1024,1024], param_8.554: f32[], param_9.399: f32[4,1024], param_10.319: f32[], param_11.344: f32[], param_12.339: f32[4,1024], param_13.337: f32[], param_14.371: f32[1024], param_15.406: f16[4,1024,1024]) -> f16[4,1024,1024] {
  %param_7.901 = f16[4,1024,1024]{2,1,0} parameter(7)
  %convert.417 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_7.901), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %param_6.1304 = f32[4,1024]{1,0} parameter(6)
  %bitcast.671 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %param_6.1304), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reshape[new_sizes=(8, 1024, 1) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_11.344 = f32[] parameter(11)
  %broadcast.3395 = f32[4,1024]{1,0} broadcast(f32[] %param_11.344), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_12.339 = f32[4,1024]{1,0} parameter(12)
  %param_8.554 = f32[] parameter(8)
  %broadcast.3394 = f32[4,1024]{1,0} broadcast(f32[] %param_8.554), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.1133 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_12.339, f32[4,1024]{1,0} %broadcast.3394), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_9.399 = f32[4,1024]{1,0} parameter(9)
  %param_10.319 = f32[] parameter(10)
  %broadcast.3393 = f32[4,1024]{1,0} broadcast(f32[] %param_10.319), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.1132 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_9.399, f32[4,1024]{1,0} %broadcast.3393), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2838 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.1132, f32[4,1024]{1,0} %divide.1132), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.482 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.1133, f32[4,1024]{1,0} %multiply.2838), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.216 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.3395, f32[4,1024]{1,0} %subtract.482), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_13.337 = f32[] parameter(13)
  %broadcast.3392 = f32[4,1024]{1,0} broadcast(f32[] %param_13.337), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.2200 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.216, f32[4,1024]{1,0} %broadcast.3392), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1622 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.2200), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.39 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1622), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %divide.468 = f32[4,1024,1]{1,0,2} divide(f32[4,1024,1]{1,0,2} %rsqrt.39, f32[4,1024,1]{1,0,2} %bitcast.1622), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %constant_594 = f32[] constant(-0.5)
  %broadcast.1351 = f32[4,1024,1]{1,0,2} broadcast(f32[] %constant_594), dimensions={}
  %multiply.1682 = f32[4,1024,1]{1,0,2} multiply(f32[4,1024,1]{1,0,2} %divide.468, f32[4,1024,1]{1,0,2} %broadcast.1351), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.1681 = f32[4,1024,1]{1,0,2} multiply(f32[4,1024,1]{1,0,2} %bitcast.671, f32[4,1024,1]{1,0,2} %multiply.1682), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.670 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %multiply.1681), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reshape[new_sizes=(8, 1024) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=132}
  %compare.7 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %subtract.482, f32[4,1024]{1,0} %maximum.216), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_5.1593 = f32[8,1024]{1,0} parameter(5)
  %param_2.2696 = s32[] parameter(2)
  %constant_596 = s32[] constant(0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %dynamic-slice.15 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_5.1593, s32[] %param_2.2696, s32[] %constant_596), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_4.1564 = f32[8,1024]{1,0} parameter(4)
  %dynamic-slice.14 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_4.1564, s32[] %param_2.2696, s32[] %constant_596), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.41 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.7, f32[4,1024]{1,0} %dynamic-slice.15, f32[4,1024]{1,0} %dynamic-slice.14), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %compare.6 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %broadcast.3395, f32[4,1024]{1,0} %maximum.216), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_3.2135 = f32[8,1024]{1,0} parameter(3)
  %dynamic-slice.13 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_3.2135, s32[] %param_2.2696, s32[] %constant_596), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_1.2875 = f32[8,1024]{1,0} parameter(1)
  %dynamic-slice.12 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_1.2875, s32[] %param_2.2696, s32[] %constant_596), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.40 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.6, f32[4,1024]{1,0} %dynamic-slice.13, f32[4,1024]{1,0} %dynamic-slice.12), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.467 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %select.41, f32[4,1024]{1,0} %select.40), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.1680 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %bitcast.670, f32[4,1024]{1,0} %divide.467), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.466 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %multiply.1680, f32[4,1024]{1,0} %broadcast.3394), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_595 = f32[] constant(2)
  %broadcast.1352 = f32[4,1024]{1,0} broadcast(f32[] %constant_595), dimensions={}
  %multiply.1679 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.466, f32[4,1024]{1,0} %broadcast.1352), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %broadcast.24 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.1679), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.1678 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.417, f32[4,1024,1024]{2,1,0} %broadcast.24), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %negate.46 = f32[4,1024]{1,0} negate(f32[4,1024]{1,0} %multiply.1680), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.1677 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.1132, f32[4,1024]{1,0} %broadcast.1352), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.1676 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %negate.46, f32[4,1024]{1,0} %multiply.1677), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %param_0.2363 = f32[4,1024]{1,0} parameter(0)
  %add.1156 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %multiply.1676, f32[4,1024]{1,0} %param_0.2363), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=131}
  %divide.465 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %add.1156, f32[4,1024]{1,0} %broadcast.3393), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.23 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.465), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/broadcast_in_dim[shape=(8, 1024, 1024) broadcast_dimensions=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %add.1154 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.1678, f32[4,1024,1024]{2,1,0} %broadcast.23), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %convert.416 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.1154), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %param_15.406 = f16[4,1024,1024]{2,1,0} parameter(15)
  %convert.964 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_15.406), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %bitcast.1637 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.39), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.3435 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1637), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_14.371 = f32[1024]{0} parameter(14)
  %broadcast.3434 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_14.371), dimensions={2}
  %multiply.2853 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.3435, f32[4,1024,1024]{2,1,0} %broadcast.3434), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2852 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.964, f32[4,1024,1024]{2,1,0} %multiply.2853), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %convert.415 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %multiply.2852), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  ROOT %add.1153 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %convert.416, f16[4,1024,1024]{2,1,0} %convert.415), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%region_154.4346 (Arg_0.4347: f32[], Arg_1.4348: f32[]) -> f32[] {
  %Arg_0.4347 = f32[] parameter(0)
  %Arg_1.4348 = f32[] parameter(1)
  ROOT %add.4349 = f32[] add(f32[] %Arg_0.4347, f32[] %Arg_1.4348), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%region_153.4329 (Arg_0.4330: f32[], Arg_1.4331: f32[]) -> f32[] {
  %Arg_0.4330 = f32[] parameter(0)
  %Arg_1.4331 = f32[] parameter(1)
  ROOT %add.4332 = f32[] add(f32[] %Arg_0.4330, f32[] %Arg_1.4331), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%fused_computation.277 (param_0.2361: f32[1024], param_1.2873: f32[], param_2.2694: f32[4,1024], param_3.2133: f32[], param_4.1562: f32[4,1024], param_5.1591: f32[], param_6.1302: f32[], param_7.899: f16[4,1024,1024], param_8.587: f16[4,1024,1024]) -> (f32[4,1024], f32[4,1024]) {
  %param_7.899 = f16[4,1024,1024]{2,1,0} parameter(7)
  %convert.962 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_7.899), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %param_6.1302 = f32[] parameter(6)
  %broadcast.3427 = f32[4,1024]{1,0} broadcast(f32[] %param_6.1302), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_4.1562 = f32[4,1024]{1,0} parameter(4)
  %param_5.1591 = f32[] parameter(5)
  %broadcast.3426 = f32[4,1024]{1,0} broadcast(f32[] %param_5.1591), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.1145 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_4.1562, f32[4,1024]{1,0} %broadcast.3426), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.2694 = f32[4,1024]{1,0} parameter(2)
  %param_3.2133 = f32[] parameter(3)
  %broadcast.3425 = f32[4,1024]{1,0} broadcast(f32[] %param_3.2133), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.1144 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_2.2694, f32[4,1024]{1,0} %broadcast.3425), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2848 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.1144, f32[4,1024]{1,0} %divide.1144), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.488 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.1145, f32[4,1024]{1,0} %multiply.2848), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.222 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.3427, f32[4,1024]{1,0} %subtract.488), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_1.2873 = f32[] parameter(1)
  %broadcast.3424 = f32[4,1024]{1,0} broadcast(f32[] %param_1.2873), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.2207 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.222, f32[4,1024]{1,0} %broadcast.3424), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1634 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.2207), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.170 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1634), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1633 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.170), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.3423 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1633), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_0.2361 = f32[1024]{0} parameter(0)
  %broadcast.3422 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_0.2361), dimensions={2}
  %multiply.2847 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.3423, f32[4,1024,1024]{2,1,0} %broadcast.3422), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2846 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.962, f32[4,1024,1024]{2,1,0} %multiply.2847), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %negate.47 = f32[4,1024,1024]{2,1,0} negate(f32[4,1024,1024]{2,1,0} %multiply.2846), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %constant_598 = f32[] constant(0)
  %reduce.244 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %negate.47, f32[] %constant_598), dimensions={2}, to_apply=%region_154.4346, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_8.587 = f16[4,1024,1024]{2,1,0} parameter(8)
  %convert.968.clone.1 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_8.587), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %broadcast.3442.clone.1 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.1144), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.492.clone.1 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.968.clone.1, f32[4,1024,1024]{2,1,0} %broadcast.3442.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.2856.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.492.clone.1, f32[4,1024,1024]{2,1,0} %convert.962), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.1687.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %multiply.2856.clone.1, f32[4,1024,1024]{2,1,0} %broadcast.3422), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.246.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1687.clone.1, f32[] %constant_598), dimensions={2}, to_apply=%region_153.4329, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  ROOT %tuple.166 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) tuple(f32[4,1024]{1,0} %reduce.244, f32[4,1024]{1,0} %reduce.246.clone.1)
}

%region_149.4248 (Arg_0.4249: f32[], Arg_1.4250: f32[]) -> f32[] {
  %Arg_0.4249 = f32[] parameter(0)
  %Arg_1.4250 = f32[] parameter(1)
  ROOT %add.4251 = f32[] add(f32[] %Arg_0.4249, f32[] %Arg_1.4250), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_150.4261 (Arg_0.4262: f32[], Arg_1.4263: f32[]) -> f32[] {
  %Arg_0.4262 = f32[] parameter(0)
  %Arg_1.4263 = f32[] parameter(1)
  ROOT %add.4264 = f32[] add(f32[] %Arg_0.4262, f32[] %Arg_1.4263), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%fused_computation.285 (param_0.2401: f16[4,1024,1024], param_1.3005: f32[1024], param_2.2915: f16[4096,1024]) -> (f32[4,1024], f32[4,1024], f16[4,1024,1024]) {
  %param_2.2915 = f16[4096,1024]{1,0} parameter(2)
  %bitcast.674.clone.1 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_2.2915), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_1.3005 = f32[1024]{0} parameter(1)
  %convert.423.clone.1 = f16[1024]{0} convert(f32[1024]{0} %param_1.3005), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.28.clone.1 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.423.clone.1), dimensions={2}
  %add.1159.clone.1 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.674.clone.1, f16[4,1024,1024]{2,1,0} %broadcast.28.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_0.2401 = f16[4,1024,1024]{2,1,0} parameter(0)
  %add.1158.clone.1 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1159.clone.1, f16[4,1024,1024]{2,1,0} %param_0.2401), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=310}
  %convert.422 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1158.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %constant_600 = f32[] constant(0)
  %reduce.247 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.422, f32[] %constant_600), dimensions={2}, to_apply=%region_149.4248, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.1686.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.422, f32[4,1024,1024]{2,1,0} %convert.422), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.245.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1686.clone.1, f32[] %constant_600), dimensions={2}, to_apply=%region_150.4261, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  ROOT %tuple.168 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) tuple(f32[4,1024]{1,0} %reduce.247, f32[4,1024]{1,0} %reduce.245.clone.1, f16[4,1024,1024]{2,1,0} %add.1158.clone.1)
}

%fused_computation.287 (param_0.499: f16[], param_1.2848: f16[4096,512], param_2.2660: f16[], param_3.2103: f16[]) -> f16[4096,512] {
  %param_1.2848 = f16[4096,512]{1,0} parameter(1)
  %bitcast.979 = f16[4,1024,512]{2,1,0} bitcast(f16[4096,512]{1,0} %param_1.2848), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %constant_1755 = f32[] constant(-4)
  %broadcast.3337 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1755), dimensions={}
  %param_3.2103 = f16[] parameter(3)
  %broadcast.3336 = f16[4,1024,512]{2,1,0} broadcast(f16[] %param_3.2103), dimensions={}
  %divide.1113 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %bitcast.979, f16[4,1024,512]{2,1,0} %broadcast.3336), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.956 = f32[4,1024,512]{2,1,0} convert(f16[4,1024,512]{2,1,0} %divide.1113), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1754 = f32[] constant(4)
  %broadcast.3335 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1754), dimensions={}
  %clamp.47 = f32[4,1024,512]{2,1,0} clamp(f32[4,1024,512]{2,1,0} %broadcast.3337, f32[4,1024,512]{2,1,0} %convert.956, f32[4,1024,512]{2,1,0} %broadcast.3335), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2804 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.47, f32[4,1024,512]{2,1,0} %clamp.47), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1753 = f32[] constant(0)
  %broadcast.3334 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1753), dimensions={}
  %multiply.2803 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %multiply.2804, f32[4,1024,512]{2,1,0} %broadcast.3334), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1752 = f32[] constant(-2.72614237e-10)
  %broadcast.3333 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1752), dimensions={}
  %add.2170 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2803, f32[4,1024,512]{2,1,0} %broadcast.3333), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2802 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2170, f32[4,1024,512]{2,1,0} %multiply.2804), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1751 = f32[] constant(2.77068146e-08)
  %broadcast.3332 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1751), dimensions={}
  %add.2169 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2802, f32[4,1024,512]{2,1,0} %broadcast.3332), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2801 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2169, f32[4,1024,512]{2,1,0} %multiply.2804), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1750 = f32[] constant(-2.10102394e-06)
  %broadcast.3331 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1750), dimensions={}
  %add.2168 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2801, f32[4,1024,512]{2,1,0} %broadcast.3331), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2800 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2168, f32[4,1024,512]{2,1,0} %multiply.2804), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1749 = f32[] constant(-5.69250624e-05)
  %broadcast.3330 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1749), dimensions={}
  %add.2167 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2800, f32[4,1024,512]{2,1,0} %broadcast.3330), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2799 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2167, f32[4,1024,512]{2,1,0} %multiply.2804), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1748 = f32[] constant(-0.000734990637)
  %broadcast.3329 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1748), dimensions={}
  %add.2166 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2799, f32[4,1024,512]{2,1,0} %broadcast.3329), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2798 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2166, f32[4,1024,512]{2,1,0} %multiply.2804), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1747 = f32[] constant(-0.0029546)
  %broadcast.3328 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1747), dimensions={}
  %add.2164 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2798, f32[4,1024,512]{2,1,0} %broadcast.3328), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2797 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2164, f32[4,1024,512]{2,1,0} %multiply.2804), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1746 = f32[] constant(-0.0160960332)
  %broadcast.3327 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1746), dimensions={}
  %add.2163 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2797, f32[4,1024,512]{2,1,0} %broadcast.3327), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2796 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.47, f32[4,1024,512]{2,1,0} %add.2163), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1745 = f32[] constant(-1.45660715e-05)
  %broadcast.3326 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1745), dimensions={}
  %add.2162 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2803, f32[4,1024,512]{2,1,0} %broadcast.3326), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2795 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2162, f32[4,1024,512]{2,1,0} %multiply.2804), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1744 = f32[] constant(-0.000213374049)
  %broadcast.3325 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1744), dimensions={}
  %add.2161 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2795, f32[4,1024,512]{2,1,0} %broadcast.3325), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2794 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2161, f32[4,1024,512]{2,1,0} %multiply.2804), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1743 = f32[] constant(-0.00168282702)
  %broadcast.3324 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1743), dimensions={}
  %add.2160 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2794, f32[4,1024,512]{2,1,0} %broadcast.3324), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2793 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2160, f32[4,1024,512]{2,1,0} %multiply.2804), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1742 = f32[] constant(-0.00737332925)
  %broadcast.3323 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1742), dimensions={}
  %add.2159 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2793, f32[4,1024,512]{2,1,0} %broadcast.3323), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2792 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2159, f32[4,1024,512]{2,1,0} %multiply.2804), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1741 = f32[] constant(-0.0142647391)
  %broadcast.3322 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1741), dimensions={}
  %add.2158 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2792, f32[4,1024,512]{2,1,0} %broadcast.3322), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.1112 = f32[4,1024,512]{2,1,0} divide(f32[4,1024,512]{2,1,0} %multiply.2796, f32[4,1024,512]{2,1,0} %add.2158), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.955 = f16[4,1024,512]{2,1,0} convert(f32[4,1024,512]{2,1,0} %divide.1112), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %param_2.2660 = f16[] parameter(2)
  %broadcast.3321 = f16[4,1024,512]{2,1,0} broadcast(f16[] %param_2.2660), dimensions={}
  %add.2157 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %convert.955, f16[4,1024,512]{2,1,0} %broadcast.3321), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1689 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %bitcast.979, f16[4,1024,512]{2,1,0} %add.2157), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %param_0.499 = f16[] parameter(0)
  %broadcast.29 = f16[4,1024,512]{2,1,0} broadcast(f16[] %param_0.499), dimensions={}
  %divide.470 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %multiply.1689, f16[4,1024,512]{2,1,0} %broadcast.29), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  ROOT %bitcast.675 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %divide.470)
}

%fused_computation.289 (param_0.504: f32[512]) -> f16[4096,512] {
  %param_0.504 = f32[512]{0} parameter(0)
  %convert.426 = f16[512]{0} convert(f32[512]{0} %param_0.504), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.31 = f16[4,1024,512]{2,1,0} broadcast(f16[512]{0} %convert.426), dimensions={2}
  ROOT %bitcast.676 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %broadcast.31)
}

%fused_computation.290 (param_0.507: f32[1024], param_1.2837: f32[1024], param_2.2645: f32[4,1024], param_3.2085: f32[], param_4.1512: f16[4,1024,1024], param_5.1530: f32[], param_6.1250: f32[4,1024], param_7.858: f32[], param_8.528: f32[]) -> f16[4,1024,1024] {
  %param_4.1512 = f16[4,1024,1024]{2,1,0} parameter(4)
  %convert.950 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_4.1512), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %param_2.2645 = f32[4,1024]{1,0} parameter(2)
  %param_3.2085 = f32[] parameter(3)
  %broadcast.3245 = f32[4,1024]{1,0} broadcast(f32[] %param_3.2085), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.1079 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_2.2645, f32[4,1024]{1,0} %broadcast.3245), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.3244 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.1079), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.458 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.950, f32[4,1024,1024]{2,1,0} %broadcast.3244), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_8.528 = f32[] parameter(8)
  %broadcast.3283 = f32[4,1024]{1,0} broadcast(f32[] %param_8.528), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_6.1250 = f32[4,1024]{1,0} parameter(6)
  %param_7.858 = f32[] parameter(7)
  %broadcast.3282 = f32[4,1024]{1,0} broadcast(f32[] %param_7.858), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.1101 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_6.1250, f32[4,1024]{1,0} %broadcast.3282), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.2774 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.1079, f32[4,1024]{1,0} %divide.1079), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.470 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.1101, f32[4,1024]{1,0} %multiply.2774), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.208 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.3283, f32[4,1024]{1,0} %subtract.470), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_5.1530 = f32[] parameter(5)
  %broadcast.3280 = f32[4,1024]{1,0} broadcast(f32[] %param_5.1530), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.2138 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.208, f32[4,1024]{1,0} %broadcast.3280), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1606 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.2138), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.160 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1606), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1605 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.160), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.3279 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1605), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_1.2837 = f32[1024]{0} parameter(1)
  %broadcast.1375 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_1.2837), dimensions={2}
  %multiply.2143 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.3279, f32[4,1024,1024]{2,1,0} %broadcast.1375), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.1703 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.458, f32[4,1024,1024]{2,1,0} %multiply.2143), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.507 = f32[1024]{0} parameter(0)
  %broadcast.32 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_0.507), dimensions={2}
  %add.1174 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.1703, f32[4,1024,1024]{2,1,0} %broadcast.32), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  ROOT %convert.427 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.1174), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
}

%region_147.4149 (Arg_0.4150: f32[], Arg_1.4151: f32[]) -> f32[] {
  %Arg_0.4150 = f32[] parameter(0)
  %Arg_1.4151 = f32[] parameter(1)
  ROOT %add.4152 = f32[] add(f32[] %Arg_0.4150, f32[] %Arg_1.4151), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_148.4162 (Arg_0.4163: f32[], Arg_1.4164: f32[]) -> f32[] {
  %Arg_0.4163 = f32[] parameter(0)
  %Arg_1.4164 = f32[] parameter(1)
  ROOT %add.4165 = f32[] add(f32[] %Arg_0.4163, f32[] %Arg_1.4164), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%fused_computation.296 (param_0.2403: f16[4,1024,1024], param_1.3010: f32[1024], param_2.2920: f16[4096,1024]) -> (f32[4,1024], f32[4,1024], f16[4,1024,1024]) {
  %param_2.2920 = f16[4096,1024]{1,0} parameter(2)
  %bitcast.679.clone.1 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_2.2920), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_1.3010 = f32[1024]{0} parameter(1)
  %convert.431.clone.1 = f16[1024]{0} convert(f32[1024]{0} %param_1.3010), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.36.clone.1 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.431.clone.1), dimensions={2}
  %add.1177.clone.1 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.679.clone.1, f16[4,1024,1024]{2,1,0} %broadcast.36.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_0.2403 = f16[4,1024,1024]{2,1,0} parameter(0)
  %add.1176.clone.1 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1177.clone.1, f16[4,1024,1024]{2,1,0} %param_0.2403), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=233}
  %convert.430 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1176.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %constant_617 = f32[] constant(0)
  %reduce.249 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.430, f32[] %constant_617), dimensions={2}, to_apply=%region_147.4149, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.1705.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.430, f32[4,1024,1024]{2,1,0} %convert.430), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.248.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1705.clone.1, f32[] %constant_617), dimensions={2}, to_apply=%region_148.4162, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  ROOT %tuple.170 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) tuple(f32[4,1024]{1,0} %reduce.249, f32[4,1024]{1,0} %reduce.248.clone.1, f16[4,1024,1024]{2,1,0} %add.1176.clone.1)
}

%fused_computation.298 (param_0.524: f16[4,4,32,1024]) -> f16[4096,128] {
  %param_0.524 = f16[4,4,32,1024]{3,2,1,0} parameter(0)
  %transpose.8 = f16[4,1024,4,32]{1,3,2,0} transpose(f16[4,4,32,1024]{3,2,1,0} %param_0.524), dimensions={0,3,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %copy.90 = f16[4,1024,4,32]{3,2,1,0} copy(f16[4,1024,4,32]{1,3,2,0} %transpose.8), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  ROOT %bitcast.680 = f16[4096,128]{1,0} bitcast(f16[4,1024,4,32]{3,2,1,0} %copy.90)
}

%region_146.4124 (Arg_0.4125: f32[], Arg_1.4126: f32[]) -> f32[] {
  %Arg_0.4125 = f32[] parameter(0)
  %Arg_1.4126 = f32[] parameter(1)
  ROOT %add.4127 = f32[] add(f32[] %Arg_0.4125, f32[] %Arg_1.4126), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.299 (param_0.2324: f16[4,4,1024,1024], param_1.2814: f16[4,4,1024]) -> f32[4,4,1024] {
  %param_0.2324 = f16[4,4,1024,1024]{3,2,1,0} parameter(0)
  %param_1.2814 = f16[4,4,1024]{2,1,0} parameter(1)
  %broadcast.3235 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_1.2814), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.450 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_0.2324, f16[4,4,1024,1024]{3,2,1,0} %broadcast.3235), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.107 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.450), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.432 = f32[4,4,1024,1024]{3,2,1,0} convert(f16[4,4,1024,1024]{3,2,1,0} %exponential.107), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %constant_618 = f32[] constant(0)
  ROOT %reduce.250 = f32[4,4,1024]{2,1,0} reduce(f32[4,4,1024,1024]{3,2,1,0} %convert.432, f32[] %constant_618), dimensions={3}, to_apply=%region_146.4124, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.301 (param_0.532: f16[8,1,1,1024], param_1.875: s32[], param_2.1666: f16[8,1,1,1024], param_3.1334: s32[4,1024], param_4.835: s32[]) -> f16[4,4,1024,1024] {
  %param_3.1334 = s32[4,1024]{1,0} parameter(3)
  %param_4.835 = s32[] parameter(4)
  %broadcast.39 = s32[4,1024]{1,0} broadcast(s32[] %param_4.835), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %compare.8 = pred[4,1024]{1,0} compare(s32[4,1024]{1,0} %param_3.1334, s32[4,1024]{1,0} %broadcast.39), direction=GT, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %bitcast.683 = pred[4,1,1,1024]{3,0,2,1} bitcast(pred[4,1024]{1,0} %compare.8), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %param_2.1666 = f16[8,1,1,1024]{3,0,2,1} parameter(2)
  %param_1.875 = s32[] parameter(1)
  %constant_619 = s32[] constant(0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %dynamic-slice.17 = f16[4,1,1,1024]{3,0,2,1} dynamic-slice(f16[8,1,1,1024]{3,0,2,1} %param_2.1666, s32[] %param_1.875, s32[] %constant_619, s32[] %constant_619, s32[] %constant_619), dynamic_slice_sizes={4,1,1,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %param_0.532 = f16[8,1,1,1024]{3,0,2,1} parameter(0)
  %dynamic-slice.16 = f16[4,1,1,1024]{3,0,2,1} dynamic-slice(f16[8,1,1,1024]{3,0,2,1} %param_0.532, s32[] %param_1.875, s32[] %constant_619, s32[] %constant_619, s32[] %constant_619), dynamic_slice_sizes={4,1,1,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %select.42 = f16[4,1,1,1024]{3,0,2,1} select(pred[4,1,1,1024]{3,0,2,1} %bitcast.683, f16[4,1,1,1024]{3,0,2,1} %dynamic-slice.17, f16[4,1,1,1024]{3,0,2,1} %dynamic-slice.16), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %bitcast.682 = f16[4,1024]{1,0} bitcast(f16[4,1,1,1024]{3,0,2,1} %select.42), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=100}
  ROOT %broadcast.38 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,1024]{1,0} %bitcast.682), dimensions={0,3}
}

%fused_computation.303 (param_0.2312: f16[4096,384]) -> (f16[4,4,32,1024], f16[4,4,32,1024]) {
  %param_0.2312 = f16[4096,384]{1,0} parameter(0)
  %bitcast.1586 = f16[4,1024,384]{2,1,0} bitcast(f16[4096,384]{1,0} %param_0.2312), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %bitcast.1585 = f16[4,1024,128,3]{3,2,1,0} bitcast(f16[4,1024,384]{2,1,0} %bitcast.1586), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %copy.233 = f16[4,1024,128,3]{1,2,0,3} copy(f16[4,1024,128,3]{3,2,1,0} %bitcast.1585), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %slice.37 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.233), slice={[0:4], [0:1024], [0:128], [1:2]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/slice[start_indices=(0, 0, 0, 1) limit_indices=(8, 1024, 1024, 2) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.9 = f16[1,4,128,1024]{3,2,1,0} transpose(f16[4,1024,128,1]{1,2,0,3} %slice.37), dimensions={3,0,2,1}
  %bitcast.686 = f16[4,4,32,1024]{3,2,1,0} bitcast(f16[1,4,128,1024]{3,2,1,0} %transpose.9)
  %slice.67.clone.1 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.233), slice={[0:4], [0:1024], [0:128], [2:3]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/slice[start_indices=(0, 0, 0, 2) limit_indices=(8, 1024, 1024, 3) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.10 = f16[1,4,128,1024]{3,2,1,0} transpose(f16[4,1024,128,1]{1,2,0,3} %slice.67.clone.1), dimensions={3,0,2,1}
  %bitcast.981.clone.1 = f16[4,4,32,1024]{3,2,1,0} bitcast(f16[1,4,128,1024]{3,2,1,0} %transpose.10)
  ROOT %tuple.172 = (f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) tuple(f16[4,4,32,1024]{3,2,1,0} %bitcast.686, f16[4,4,32,1024]{3,2,1,0} %bitcast.981.clone.1)
}

%fused_computation.305 (param_0.545: f32[384]) -> f16[4096,384] {
  %param_0.545 = f32[384]{0} parameter(0)
  %convert.433 = f16[384]{0} convert(f32[384]{0} %param_0.545), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.41 = f16[4,1024,384]{2,1,0} broadcast(f16[384]{0} %convert.433), dimensions={2}
  ROOT %bitcast.689 = f16[4096,384]{1,0} bitcast(f16[4,1024,384]{2,1,0} %broadcast.41)
}

%fused_computation.306 (param_0.546: f16[4,1024,1024], param_1.880: f16[4096,1024]) -> f16[4,1024,1024] {
  %param_0.546 = f16[4,1024,1024]{2,1,0} parameter(0)
  %param_1.880 = f16[4096,1024]{1,0} parameter(1)
  %bitcast.690 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_1.880), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  ROOT %add.1178 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %param_0.546, f16[4,1024,1024]{2,1,0} %bitcast.690), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
}

%fused_computation.307 (param_0.1511: f16[4,4,32,1024], param_1.1960: f16[4,4,1024,32], param_2.1667: f16[], param_3.1335: f16[4,4,1024,32]) -> f16[4,1024,128,3] {
  %param_3.1335 = f16[4,4,1024,32]{3,2,1,0} parameter(3)
  %transpose.11 = f16[4,1024,4,32]{3,1,2,0} transpose(f16[4,4,1024,32]{3,2,1,0} %param_3.1335), dimensions={0,2,1,3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/transpose[permutation=(0, 2, 1, 3)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %copy.95 = f16[4,1024,4,32]{1,3,2,0} copy(f16[4,1024,4,32]{3,1,2,0} %transpose.11), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/transpose[permutation=(0, 2, 1, 3)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %bitcast.694 = f16[4,1024,128,1]{1,2,0,3} bitcast(f16[4,1024,4,32]{1,3,2,0} %copy.95), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=173}
  %constant_620 = f16[] constant(0)
  %pad.23 = f16[4,1024,128,3]{1,2,0,3} pad(f16[4,1024,128,1]{1,2,0,3} %bitcast.694, f16[] %constant_620), padding=0_0x0_0x0_0x2_0, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (2, 0, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %param_1.1960 = f16[4,4,1024,32]{3,2,1,0} parameter(1)
  %param_2.1667 = f16[] parameter(2)
  %broadcast.42 = f16[4,4,1024,32]{3,2,1,0} broadcast(f16[] %param_2.1667), dimensions={}
  %divide.474 = f16[4,4,1024,32]{3,2,1,0} divide(f16[4,4,1024,32]{3,2,1,0} %param_1.1960, f16[4,4,1024,32]{3,2,1,0} %broadcast.42), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %transpose.12 = f16[4,1024,4,32]{3,1,2,0} transpose(f16[4,4,1024,32]{3,2,1,0} %divide.474), dimensions={0,2,1,3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %copy.94 = f16[4,1024,4,32]{1,3,2,0} copy(f16[4,1024,4,32]{3,1,2,0} %transpose.12), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %bitcast.692 = f16[4,1024,128,1]{1,2,0,3} bitcast(f16[4,1024,4,32]{1,3,2,0} %copy.94), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=167}
  %pad.22 = f16[4,1024,128,3]{1,2,0,3} pad(f16[4,1024,128,1]{1,2,0,3} %bitcast.692, f16[] %constant_620), padding=0_0x0_0x0_0x0_2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 2, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %add.1180 = f16[4,1024,128,3]{1,2,0,3} add(f16[4,1024,128,3]{1,2,0,3} %pad.23, f16[4,1024,128,3]{1,2,0,3} %pad.22), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %param_0.1511 = f16[4,4,32,1024]{3,2,1,0} parameter(0)
  %bitcast.1860 = f16[1,4,128,1024]{3,2,1,0} bitcast(f16[4,4,32,1024]{3,2,1,0} %param_0.1511)
  %transpose.13 = f16[4,1024,128,1]{1,2,0,3} transpose(f16[1,4,128,1024]{3,2,1,0} %bitcast.1860), dimensions={1,3,2,0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=170}
  %pad.21 = f16[4,1024,128,3]{1,2,0,3} pad(f16[4,1024,128,1]{1,2,0,3} %transpose.13, f16[] %constant_620), padding=0_0x0_0x0_0x1_1, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (1, 1, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %add.1179 = f16[4,1024,128,3]{1,2,0,3} add(f16[4,1024,128,3]{1,2,0,3} %add.1180, f16[4,1024,128,3]{1,2,0,3} %pad.21), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  ROOT %copy.93 = f16[4,1024,128,3]{3,2,1,0} copy(f16[4,1024,128,3]{1,2,0,3} %add.1179), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
}

%fused_computation.308 (param_0.551: f16[4096,128]) -> f16[4,4,32,1024] {
  %param_0.551 = f16[4096,128]{1,0} parameter(0)
  %bitcast.1861 = f16[4,1024,4,32]{3,2,1,0} bitcast(f16[4096,128]{1,0} %param_0.551)
  %transpose.14 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %bitcast.1861), dimensions={0,2,3,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/transpose[permutation=(0, 2, 3, 1)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  ROOT %copy.96 = f16[4,4,32,1024]{3,2,1,0} copy(f16[4,4,32,1024]{2,1,3,0} %transpose.14), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/transpose[permutation=(0, 2, 3, 1)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
}

%fused_computation.309 (param_0.2247: f16[4096,384], param_1.3034: f16[]) -> (f16[4,4,1024,32], f16[4,4,1024,32]) {
  %param_0.2247 = f16[4096,384]{1,0} parameter(0)
  %bitcast.1520 = f16[4,1024,384]{2,1,0} bitcast(f16[4096,384]{1,0} %param_0.2247), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %bitcast.1519 = f16[4,1024,128,3]{3,2,1,0} bitcast(f16[4,1024,384]{2,1,0} %bitcast.1520), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %copy.231 = f16[4,1024,128,3]{1,2,0,3} copy(f16[4,1024,128,3]{3,2,1,0} %bitcast.1519), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %slice.68 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.231), slice={[0:4], [0:1024], [0:128], [2:3]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/slice[start_indices=(0, 0, 0, 2) limit_indices=(8, 1024, 1024, 3) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.15 = f16[1,4,128,1024]{3,2,1,0} transpose(f16[4,1024,128,1]{1,2,0,3} %slice.68), dimensions={3,0,2,1}
  %bitcast.1862 = f16[4,4,32,1024]{3,2,1,0} bitcast(f16[1,4,128,1024]{3,2,1,0} %transpose.15)
  %transpose.16 = f16[4,4,1024,32]{2,3,1,0} transpose(f16[4,4,32,1024]{3,2,1,0} %bitcast.1862), dimensions={0,1,3,2}
  %copy.97 = f16[4,4,1024,32]{3,2,1,0} copy(f16[4,4,1024,32]{2,3,1,0} %transpose.16)
  %slice.38.clone.1 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.231), slice={[0:4], [0:1024], [0:128], [0:1]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/slice[start_indices=(0, 0, 0, 0) limit_indices=(8, 1024, 1024, 1) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %param_1.3034 = f16[] parameter(1)
  %broadcast.65.clone.1 = f16[4,1024,128,1]{1,2,0,3} broadcast(f16[] %param_1.3034), dimensions={}
  %divide.491.clone.1 = f16[4,1024,128,1]{1,2,0,3} divide(f16[4,1024,128,1]{1,2,0,3} %slice.38.clone.1, f16[4,1024,128,1]{1,2,0,3} %broadcast.65.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %bitcast.719.clone.1 = f16[4,1024,4,32]{1,3,2,0} bitcast(f16[4,1024,128,1]{1,2,0,3} %divide.491.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %copy.100.clone.1 = f16[4,1024,4,32]{3,1,2,0} copy(f16[4,1024,4,32]{1,3,2,0} %bitcast.719.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %transpose.17 = f16[4,4,1024,32]{3,2,1,0} transpose(f16[4,1024,4,32]{3,1,2,0} %copy.100.clone.1), dimensions={0,2,1,3}
  ROOT %tuple.181 = (f16[4,4,1024,32]{3,2,1,0}, f16[4,4,1024,32]{3,2,1,0}) tuple(f16[4,4,1024,32]{3,2,1,0} %copy.97, f16[4,4,1024,32]{3,2,1,0} %transpose.17)
}

%fused_computation.310 (param_0.2256: f16[4,4,1024,1024], param_1.2730: f16[4,4,1024], param_2.2523: f32[4,4,1024], param_3.1967: f16[4,4,1024,1024], param_4.1406: f16[4,4,1024]) -> f16[4,4,1024,1024] {
  %param_1.2730 = f16[4,4,1024]{2,1,0} parameter(1)
  %negate.48 = f16[4,4,1024]{2,1,0} negate(f16[4,4,1024]{2,1,0} %param_1.2730), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.43 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %negate.48), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/broadcast_in_dim[shape=(8, 32, 1024, 1024) broadcast_dimensions=(0, 1, 2)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %param_0.2256 = f16[4,4,1024,1024]{3,2,1,0} parameter(0)
  %param_2.2523 = f32[4,4,1024]{2,1,0} parameter(2)
  %convert.664 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %param_2.2523), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.1381 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %convert.664), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.475 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %param_0.2256, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1381), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %add.1181 = f16[4,4,1024,1024]{3,2,1,0} add(f16[4,4,1024,1024]{3,2,1,0} %broadcast.43, f16[4,4,1024,1024]{3,2,1,0} %divide.475), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %param_3.1967 = f16[4,4,1024,1024]{3,2,1,0} parameter(3)
  %param_4.1406 = f16[4,4,1024]{2,1,0} parameter(4)
  %broadcast.2995 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_4.1406), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.402 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_3.1967, f16[4,4,1024,1024]{3,2,1,0} %broadcast.2995), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.105 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.402), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  ROOT %multiply.1706 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %add.1181, f16[4,4,1024,1024]{3,2,1,0} %exponential.105), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%region_142.3998 (Arg_0.3999: f16[], Arg_1.4000: f16[]) -> f16[] {
  %Arg_0.3999 = f16[] parameter(0)
  %Arg_1.4000 = f16[] parameter(1)
  ROOT %add.4001 = f16[] add(f16[] %Arg_0.3999, f16[] %Arg_1.4000), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.311 (param_0.2254: f16[4,4,1024,1024], param_1.2728: f32[4,4,1024], param_2.2522: f16[4,4,1024,1024], param_3.1965: f16[4,4,1024]) -> f16[4,4,1024] {
  %param_0.2254 = f16[4,4,1024,1024]{3,2,1,0} parameter(0)
  %constant_621 = f16[] constant(1)
  %broadcast.1382 = f16[4,4,1024]{2,1,0} broadcast(f16[] %constant_621), dimensions={}
  %param_1.2728 = f32[4,4,1024]{2,1,0} parameter(1)
  %convert.665 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %param_1.2728), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.1709 = f16[4,4,1024]{2,1,0} multiply(f16[4,4,1024]{2,1,0} %convert.665, f16[4,4,1024]{2,1,0} %convert.665), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.476 = f16[4,4,1024]{2,1,0} divide(f16[4,4,1024]{2,1,0} %broadcast.1382, f16[4,4,1024]{2,1,0} %multiply.1709), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.44 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %divide.476), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.1708 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %param_0.2254, f16[4,4,1024,1024]{3,2,1,0} %broadcast.44), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %param_2.2522 = f16[4,4,1024,1024]{3,2,1,0} parameter(2)
  %param_3.1965 = f16[4,4,1024]{2,1,0} parameter(3)
  %broadcast.2993 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_3.1965), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.400 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_2.2522, f16[4,4,1024,1024]{3,2,1,0} %broadcast.2993), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.103 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.400), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.1707 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %multiply.1708, f16[4,4,1024,1024]{3,2,1,0} %exponential.103), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %constant_622 = f16[] constant(0)
  ROOT %reduce.251 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %multiply.1707, f16[] %constant_622), dimensions={3}, to_apply=%region_142.3998, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.312 (param_0.560: f16[4096,128]) -> f16[4,4,1024,32] {
  %param_0.560 = f16[4096,128]{1,0} parameter(0)
  %bitcast.699 = f16[4,1024,4,32]{3,2,1,0} bitcast(f16[4096,128]{1,0} %param_0.560), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %copy.98 = f16[4,1024,4,32]{3,1,2,0} copy(f16[4,1024,4,32]{3,2,1,0} %bitcast.699), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  ROOT %transpose.18 = f16[4,4,1024,32]{3,2,1,0} transpose(f16[4,1024,4,32]{3,1,2,0} %copy.98), dimensions={0,2,1,3}
}

%fused_computation.313 (param_0.2304: f32[4,1024], param_1.2803: f32[8,1024], param_2.2614: s32[], param_3.2054: f32[8,1024], param_4.1488: f32[8,1024], param_5.1509: f32[8,1024], param_6.1244: f32[4,1024], param_7.848: f16[4,1024,1024], param_8.516: f32[], param_9.379: f32[4,1024], param_10.314: f32[], param_11.339: f32[], param_12.334: f32[4,1024], param_13.332: f32[], param_14.359: f32[1024], param_15.388: f16[4,1024,1024], param_16.407: f16[4096,1024]) -> f16[4,1024,1024] {
  %param_7.848 = f16[4,1024,1024]{2,1,0} parameter(7)
  %convert.436 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_7.848), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %param_6.1244 = f32[4,1024]{1,0} parameter(6)
  %bitcast.701 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %param_6.1244), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reshape[new_sizes=(8, 1024, 1) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_11.339 = f32[] parameter(11)
  %broadcast.3027 = f32[4,1024]{1,0} broadcast(f32[] %param_11.339), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_12.334 = f32[4,1024]{1,0} parameter(12)
  %param_8.516 = f32[] parameter(8)
  %broadcast.3026 = f32[4,1024]{1,0} broadcast(f32[] %param_8.516), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.1009 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_12.334, f32[4,1024]{1,0} %broadcast.3026), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_9.379 = f32[4,1024]{1,0} parameter(9)
  %param_10.314 = f32[] parameter(10)
  %broadcast.3025 = f32[4,1024]{1,0} broadcast(f32[] %param_10.314), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.1008 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_9.379, f32[4,1024]{1,0} %broadcast.3025), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2662 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.1008, f32[4,1024]{1,0} %divide.1008), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.414 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.1009, f32[4,1024]{1,0} %multiply.2662), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.180 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.3027, f32[4,1024]{1,0} %subtract.414), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_13.332 = f32[] parameter(13)
  %broadcast.3024 = f32[4,1024]{1,0} broadcast(f32[] %param_13.332), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.2036 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.180, f32[4,1024]{1,0} %broadcast.3024), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1524 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.2036), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.42 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1524), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %divide.480 = f32[4,1024,1]{1,0,2} divide(f32[4,1024,1]{1,0,2} %rsqrt.42, f32[4,1024,1]{1,0,2} %bitcast.1524), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %constant_623 = f32[] constant(-0.5)
  %broadcast.1386 = f32[4,1024,1]{1,0,2} broadcast(f32[] %constant_623), dimensions={}
  %multiply.1716 = f32[4,1024,1]{1,0,2} multiply(f32[4,1024,1]{1,0,2} %divide.480, f32[4,1024,1]{1,0,2} %broadcast.1386), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.1715 = f32[4,1024,1]{1,0,2} multiply(f32[4,1024,1]{1,0,2} %bitcast.701, f32[4,1024,1]{1,0,2} %multiply.1716), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.700 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %multiply.1715), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reshape[new_sizes=(8, 1024) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=132}
  %compare.10 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %subtract.414, f32[4,1024]{1,0} %maximum.180), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_5.1509 = f32[8,1024]{1,0} parameter(5)
  %param_2.2614 = s32[] parameter(2)
  %constant_625 = s32[] constant(0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %dynamic-slice.21 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_5.1509, s32[] %param_2.2614, s32[] %constant_625), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_4.1488 = f32[8,1024]{1,0} parameter(4)
  %dynamic-slice.20 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_4.1488, s32[] %param_2.2614, s32[] %constant_625), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.44 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.10, f32[4,1024]{1,0} %dynamic-slice.21, f32[4,1024]{1,0} %dynamic-slice.20), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %compare.9 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %broadcast.3027, f32[4,1024]{1,0} %maximum.180), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_3.2054 = f32[8,1024]{1,0} parameter(3)
  %dynamic-slice.19 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_3.2054, s32[] %param_2.2614, s32[] %constant_625), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_1.2803 = f32[8,1024]{1,0} parameter(1)
  %dynamic-slice.18 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_1.2803, s32[] %param_2.2614, s32[] %constant_625), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.43 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.9, f32[4,1024]{1,0} %dynamic-slice.19, f32[4,1024]{1,0} %dynamic-slice.18), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.479 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %select.44, f32[4,1024]{1,0} %select.43), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.1714 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %bitcast.700, f32[4,1024]{1,0} %divide.479), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.478 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %multiply.1714, f32[4,1024]{1,0} %broadcast.3026), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_624 = f32[] constant(2)
  %broadcast.1387 = f32[4,1024]{1,0} broadcast(f32[] %constant_624), dimensions={}
  %multiply.1713 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.478, f32[4,1024]{1,0} %broadcast.1387), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %broadcast.46 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.1713), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.1712 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.436, f32[4,1024,1024]{2,1,0} %broadcast.46), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %negate.49 = f32[4,1024]{1,0} negate(f32[4,1024]{1,0} %multiply.1714), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.1711 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.1008, f32[4,1024]{1,0} %broadcast.1387), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.1710 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %negate.49, f32[4,1024]{1,0} %multiply.1711), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %param_0.2304 = f32[4,1024]{1,0} parameter(0)
  %add.1184 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %multiply.1710, f32[4,1024]{1,0} %param_0.2304), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=131}
  %divide.477 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %add.1184, f32[4,1024]{1,0} %broadcast.3025), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.45 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.477), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/broadcast_in_dim[shape=(8, 1024, 1024) broadcast_dimensions=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %add.1183 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.1712, f32[4,1024,1024]{2,1,0} %broadcast.45), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %convert.435 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.1183), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %param_15.388 = f16[4,1024,1024]{2,1,0} parameter(15)
  %param_16.407 = f16[4096,1024]{1,0} parameter(16)
  %bitcast.1578 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_16.407), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.2127 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %param_15.388, f16[4,1024,1024]{2,1,0} %bitcast.1578), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.940 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.2127), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %bitcast.1576 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.42), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.3221 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1576), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_14.359 = f32[1024]{0} parameter(14)
  %broadcast.3220 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_14.359), dimensions={2}
  %multiply.2759 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.3221, f32[4,1024,1024]{2,1,0} %broadcast.3220), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2758 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.940, f32[4,1024,1024]{2,1,0} %multiply.2759), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %convert.434 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %multiply.2758), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  ROOT %add.1182 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %convert.435, f16[4,1024,1024]{2,1,0} %convert.434), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%region_140.3967 (Arg_0.3968: f32[], Arg_1.3969: f32[]) -> f32[] {
  %Arg_0.3968 = f32[] parameter(0)
  %Arg_1.3969 = f32[] parameter(1)
  ROOT %add.3970 = f32[] add(f32[] %Arg_0.3968, f32[] %Arg_1.3969), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%region_139.3950 (Arg_0.3951: f32[], Arg_1.3952: f32[]) -> f32[] {
  %Arg_0.3951 = f32[] parameter(0)
  %Arg_1.3952 = f32[] parameter(1)
  ROOT %add.3953 = f32[] add(f32[] %Arg_0.3951, f32[] %Arg_1.3952), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%fused_computation.314 (param_0.2302: f32[1024], param_1.2801: f32[], param_2.2612: f32[4,1024], param_3.2052: f32[], param_4.1486: f32[4,1024], param_5.1507: f32[], param_6.1242: f32[], param_7.846: f16[4,1024,1024], param_8.514: f16[4096,1024], param_9.428: f16[4,1024,1024]) -> (f32[4,1024], f32[4,1024]) {
  %param_7.846 = f16[4,1024,1024]{2,1,0} parameter(7)
  %param_8.514 = f16[4096,1024]{1,0} parameter(8)
  %bitcast.1572 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_8.514), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.2123 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %param_7.846, f16[4,1024,1024]{2,1,0} %bitcast.1572), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.938 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.2123), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %param_6.1242 = f32[] parameter(6)
  %broadcast.3213 = f32[4,1024]{1,0} broadcast(f32[] %param_6.1242), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_4.1486 = f32[4,1024]{1,0} parameter(4)
  %param_5.1507 = f32[] parameter(5)
  %broadcast.3212 = f32[4,1024]{1,0} broadcast(f32[] %param_5.1507), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.1069 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_4.1486, f32[4,1024]{1,0} %broadcast.3212), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.2612 = f32[4,1024]{1,0} parameter(2)
  %param_3.2052 = f32[] parameter(3)
  %broadcast.3211 = f32[4,1024]{1,0} broadcast(f32[] %param_3.2052), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.1068 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_2.2612, f32[4,1024]{1,0} %broadcast.3211), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2754 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.1068, f32[4,1024]{1,0} %divide.1068), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.442 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.1069, f32[4,1024]{1,0} %multiply.2754), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.200 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.3213, f32[4,1024]{1,0} %subtract.442), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_1.2801 = f32[] parameter(1)
  %broadcast.3210 = f32[4,1024]{1,0} broadcast(f32[] %param_1.2801), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.2122 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.200, f32[4,1024]{1,0} %broadcast.3210), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1571 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.2122), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.156 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1571), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1570 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.156), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.3209 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1570), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_0.2302 = f32[1024]{0} parameter(0)
  %broadcast.3208 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_0.2302), dimensions={2}
  %multiply.2753 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.3209, f32[4,1024,1024]{2,1,0} %broadcast.3208), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2752 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.938, f32[4,1024,1024]{2,1,0} %multiply.2753), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %negate.50 = f32[4,1024,1024]{2,1,0} negate(f32[4,1024,1024]{2,1,0} %multiply.2752), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %constant_627 = f32[] constant(0)
  %reduce.252 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %negate.50, f32[] %constant_627), dimensions={2}, to_apply=%region_140.3967, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_9.428 = f16[4,1024,1024]{2,1,0} parameter(9)
  %convert.944.clone.1 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_9.428), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %broadcast.3228.clone.1 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.1068), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.446.clone.1 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.944.clone.1, f32[4,1024,1024]{2,1,0} %broadcast.3228.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.2762.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.446.clone.1, f32[4,1024,1024]{2,1,0} %convert.938), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.1718.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %multiply.2762.clone.1, f32[4,1024,1024]{2,1,0} %broadcast.3208), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.253.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1718.clone.1, f32[] %constant_627), dimensions={2}, to_apply=%region_139.3950, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  ROOT %tuple.174 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) tuple(f32[4,1024]{1,0} %reduce.252, f32[4,1024]{1,0} %reduce.253.clone.1)
}

%fused_computation.319 (param_0.2273: f16[4096,512], param_1.2759: f16[], param_2.2559: f16[], param_3.2006: f16[4096,512], param_4.1445: f16[]) -> f16[4,1024,512] {
  %param_3.2006 = f16[4096,512]{1,0} parameter(3)
  %bitcast.982 = f16[4,1024,512]{2,1,0} bitcast(f16[4096,512]{1,0} %param_3.2006), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_0.2273 = f16[4096,512]{1,0} parameter(0)
  %param_1.2759 = f16[] parameter(1)
  %broadcast.47 = f16[4096,512]{1,0} broadcast(f16[] %param_1.2759), dimensions={}
  %divide.482 = f16[4096,512]{1,0} divide(f16[4096,512]{1,0} %param_0.2273, f16[4096,512]{1,0} %broadcast.47), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %bitcast.703 = f16[4,1024,512]{2,1,0} bitcast(f16[4096,512]{1,0} %divide.482), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1724 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %bitcast.982, f16[4,1024,512]{2,1,0} %bitcast.703), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_628 = f16[] constant(1.1279)
  %broadcast.1391 = f16[4,1024,512]{2,1,0} broadcast(f16[] %constant_628), dimensions={}
  %multiply.1723 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.1724, f16[4,1024,512]{2,1,0} %broadcast.1391), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %param_2.2559 = f16[] parameter(2)
  %broadcast.1390 = f16[4,1024,512]{2,1,0} broadcast(f16[] %param_2.2559), dimensions={}
  %divide.590 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %bitcast.982, f16[4,1024,512]{2,1,0} %broadcast.1390), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1722 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %divide.590, f16[4,1024,512]{2,1,0} %divide.590), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %negate.51 = f16[4,1024,512]{2,1,0} negate(f16[4,1024,512]{2,1,0} %multiply.1722), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/neg" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %exponential.21 = f16[4,1024,512]{2,1,0} exponential(f16[4,1024,512]{2,1,0} %negate.51), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/exp" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1721 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.1723, f16[4,1024,512]{2,1,0} %exponential.21), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.481 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %multiply.1721, f16[4,1024,512]{2,1,0} %broadcast.1390), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1690 = f32[] constant(-4)
  %broadcast.3125 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1690), dimensions={}
  %convert.924 = f32[4,1024,512]{2,1,0} convert(f16[4,1024,512]{2,1,0} %divide.590), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1689 = f32[] constant(4)
  %broadcast.3123 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1689), dimensions={}
  %clamp.45 = f32[4,1024,512]{2,1,0} clamp(f32[4,1024,512]{2,1,0} %broadcast.3125, f32[4,1024,512]{2,1,0} %convert.924, f32[4,1024,512]{2,1,0} %broadcast.3123), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2720 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.45, f32[4,1024,512]{2,1,0} %clamp.45), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1688 = f32[] constant(0)
  %broadcast.3122 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1688), dimensions={}
  %multiply.2719 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %multiply.2720, f32[4,1024,512]{2,1,0} %broadcast.3122), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1687 = f32[] constant(-2.72614237e-10)
  %broadcast.3121 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1687), dimensions={}
  %add.2099 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2719, f32[4,1024,512]{2,1,0} %broadcast.3121), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2718 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2099, f32[4,1024,512]{2,1,0} %multiply.2720), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1686 = f32[] constant(2.77068146e-08)
  %broadcast.3120 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1686), dimensions={}
  %add.2097 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2718, f32[4,1024,512]{2,1,0} %broadcast.3120), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2717 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2097, f32[4,1024,512]{2,1,0} %multiply.2720), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1685 = f32[] constant(-2.10102394e-06)
  %broadcast.3119 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1685), dimensions={}
  %add.2096 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2717, f32[4,1024,512]{2,1,0} %broadcast.3119), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2716 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2096, f32[4,1024,512]{2,1,0} %multiply.2720), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1684 = f32[] constant(-5.69250624e-05)
  %broadcast.3118 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1684), dimensions={}
  %add.2095 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2716, f32[4,1024,512]{2,1,0} %broadcast.3118), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2715 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2095, f32[4,1024,512]{2,1,0} %multiply.2720), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1683 = f32[] constant(-0.000734990637)
  %broadcast.3117 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1683), dimensions={}
  %add.2094 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2715, f32[4,1024,512]{2,1,0} %broadcast.3117), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2714 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2094, f32[4,1024,512]{2,1,0} %multiply.2720), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1682 = f32[] constant(-0.0029546)
  %broadcast.3116 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1682), dimensions={}
  %add.2093 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2714, f32[4,1024,512]{2,1,0} %broadcast.3116), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2713 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2093, f32[4,1024,512]{2,1,0} %multiply.2720), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1681 = f32[] constant(-0.0160960332)
  %broadcast.3115 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1681), dimensions={}
  %add.2092 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2713, f32[4,1024,512]{2,1,0} %broadcast.3115), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2712 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.45, f32[4,1024,512]{2,1,0} %add.2092), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1680 = f32[] constant(-1.45660715e-05)
  %broadcast.3114 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1680), dimensions={}
  %add.2091 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2719, f32[4,1024,512]{2,1,0} %broadcast.3114), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2711 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2091, f32[4,1024,512]{2,1,0} %multiply.2720), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1679 = f32[] constant(-0.000213374049)
  %broadcast.3113 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1679), dimensions={}
  %add.2090 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2711, f32[4,1024,512]{2,1,0} %broadcast.3113), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2710 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2090, f32[4,1024,512]{2,1,0} %multiply.2720), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1678 = f32[] constant(-0.00168282702)
  %broadcast.3112 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1678), dimensions={}
  %add.2089 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2710, f32[4,1024,512]{2,1,0} %broadcast.3112), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2709 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2089, f32[4,1024,512]{2,1,0} %multiply.2720), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1677 = f32[] constant(-0.00737332925)
  %broadcast.3111 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1677), dimensions={}
  %add.2088 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2709, f32[4,1024,512]{2,1,0} %broadcast.3111), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2708 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2088, f32[4,1024,512]{2,1,0} %multiply.2720), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1676 = f32[] constant(-0.0142647391)
  %broadcast.3110 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1676), dimensions={}
  %add.2087 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2708, f32[4,1024,512]{2,1,0} %broadcast.3110), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.1028 = f32[4,1024,512]{2,1,0} divide(f32[4,1024,512]{2,1,0} %multiply.2712, f32[4,1024,512]{2,1,0} %add.2087), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.923 = f16[4,1024,512]{2,1,0} convert(f32[4,1024,512]{2,1,0} %divide.1028), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %param_4.1445 = f16[] parameter(4)
  %broadcast.3109 = f16[4,1024,512]{2,1,0} broadcast(f16[] %param_4.1445), dimensions={}
  %add.2085 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %convert.923, f16[4,1024,512]{2,1,0} %broadcast.3109), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1720 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %bitcast.703, f16[4,1024,512]{2,1,0} %add.2085), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  ROOT %add.1186 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %divide.481, f16[4,1024,512]{2,1,0} %multiply.1720), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
}

%fused_computation.320 (param_0.2289: f32[4,1024], param_1.2784: f32[8,1024], param_2.2593: s32[], param_3.2037: f32[8,1024], param_4.1473: f32[8,1024], param_5.1498: f32[8,1024], param_6.1233: f32[4,1024], param_7.835: f16[4,1024,1024], param_8.506: f32[], param_9.378: f32[4,1024], param_10.313: f32[], param_11.338: f32[], param_12.333: f32[4,1024], param_13.331: f32[], param_14.357: f32[1024], param_15.383: f16[4,1024,1024]) -> f16[4,1024,1024] {
  %param_7.835 = f16[4,1024,1024]{2,1,0} parameter(7)
  %convert.441 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_7.835), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %param_6.1233 = f32[4,1024]{1,0} parameter(6)
  %bitcast.705 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %param_6.1233), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reshape[new_sizes=(8, 1024, 1) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_11.338 = f32[] parameter(11)
  %broadcast.3149 = f32[4,1024]{1,0} broadcast(f32[] %param_11.338), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_12.333 = f32[4,1024]{1,0} parameter(12)
  %param_8.506 = f32[] parameter(8)
  %broadcast.3148 = f32[4,1024]{1,0} broadcast(f32[] %param_8.506), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.1045 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_12.333, f32[4,1024]{1,0} %broadcast.3148), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_9.378 = f32[4,1024]{1,0} parameter(9)
  %param_10.313 = f32[] parameter(10)
  %broadcast.3147 = f32[4,1024]{1,0} broadcast(f32[] %param_10.313), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.1044 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_9.378, f32[4,1024]{1,0} %broadcast.3147), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2728 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.1044, f32[4,1024]{1,0} %divide.1044), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.428 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.1045, f32[4,1024]{1,0} %multiply.2728), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.190 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.3149, f32[4,1024]{1,0} %subtract.428), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_13.331 = f32[] parameter(13)
  %broadcast.3146 = f32[4,1024]{1,0} broadcast(f32[] %param_13.331), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.2103 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.190, f32[4,1024]{1,0} %broadcast.3146), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1544 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.2103), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.43 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1544), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %divide.486 = f32[4,1024,1]{1,0,2} divide(f32[4,1024,1]{1,0,2} %rsqrt.43, f32[4,1024,1]{1,0,2} %bitcast.1544), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %constant_629 = f32[] constant(-0.5)
  %broadcast.1395 = f32[4,1024,1]{1,0,2} broadcast(f32[] %constant_629), dimensions={}
  %multiply.1731 = f32[4,1024,1]{1,0,2} multiply(f32[4,1024,1]{1,0,2} %divide.486, f32[4,1024,1]{1,0,2} %broadcast.1395), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.1730 = f32[4,1024,1]{1,0,2} multiply(f32[4,1024,1]{1,0,2} %bitcast.705, f32[4,1024,1]{1,0,2} %multiply.1731), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.704 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %multiply.1730), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reshape[new_sizes=(8, 1024) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=132}
  %compare.52 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %subtract.428, f32[4,1024]{1,0} %maximum.190), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_5.1498 = f32[8,1024]{1,0} parameter(5)
  %param_2.2593 = s32[] parameter(2)
  %constant_631 = s32[] constant(0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %dynamic-slice.25 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_5.1498, s32[] %param_2.2593, s32[] %constant_631), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_4.1473 = f32[8,1024]{1,0} parameter(4)
  %dynamic-slice.24 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_4.1473, s32[] %param_2.2593, s32[] %constant_631), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.46 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.52, f32[4,1024]{1,0} %dynamic-slice.25, f32[4,1024]{1,0} %dynamic-slice.24), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %compare.11 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %broadcast.3149, f32[4,1024]{1,0} %maximum.190), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_3.2037 = f32[8,1024]{1,0} parameter(3)
  %dynamic-slice.23 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_3.2037, s32[] %param_2.2593, s32[] %constant_631), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_1.2784 = f32[8,1024]{1,0} parameter(1)
  %dynamic-slice.22 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_1.2784, s32[] %param_2.2593, s32[] %constant_631), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.45 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.11, f32[4,1024]{1,0} %dynamic-slice.23, f32[4,1024]{1,0} %dynamic-slice.22), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.485 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %select.46, f32[4,1024]{1,0} %select.45), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.1729 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %bitcast.704, f32[4,1024]{1,0} %divide.485), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.484 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %multiply.1729, f32[4,1024]{1,0} %broadcast.3148), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_630 = f32[] constant(2)
  %broadcast.1396 = f32[4,1024]{1,0} broadcast(f32[] %constant_630), dimensions={}
  %multiply.1728 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.484, f32[4,1024]{1,0} %broadcast.1396), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %broadcast.49 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.1728), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.1727 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.441, f32[4,1024,1024]{2,1,0} %broadcast.49), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %negate.52 = f32[4,1024]{1,0} negate(f32[4,1024]{1,0} %multiply.1729), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.1726 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.1044, f32[4,1024]{1,0} %broadcast.1396), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.1725 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %negate.52, f32[4,1024]{1,0} %multiply.1726), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %param_0.2289 = f32[4,1024]{1,0} parameter(0)
  %add.1189 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %multiply.1725, f32[4,1024]{1,0} %param_0.2289), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=131}
  %divide.483 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %add.1189, f32[4,1024]{1,0} %broadcast.3147), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.48 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.483), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/broadcast_in_dim[shape=(8, 1024, 1024) broadcast_dimensions=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %add.1188 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.1727, f32[4,1024,1024]{2,1,0} %broadcast.48), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %convert.440 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.1188), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %param_15.383 = f16[4,1024,1024]{2,1,0} parameter(15)
  %convert.928 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_15.383), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %bitcast.1559 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.43), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.3189 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1559), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_14.357 = f32[1024]{0} parameter(14)
  %broadcast.3188 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_14.357), dimensions={2}
  %multiply.2743 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.3189, f32[4,1024,1024]{2,1,0} %broadcast.3188), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2742 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.928, f32[4,1024,1024]{2,1,0} %multiply.2743), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %convert.439 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %multiply.2742), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  ROOT %add.1187 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %convert.440, f16[4,1024,1024]{2,1,0} %convert.439), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%region_134.3887 (Arg_0.3888: f32[], Arg_1.3889: f32[]) -> f32[] {
  %Arg_0.3888 = f32[] parameter(0)
  %Arg_1.3889 = f32[] parameter(1)
  ROOT %add.3890 = f32[] add(f32[] %Arg_0.3888, f32[] %Arg_1.3889), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%region_133.3870 (Arg_0.3871: f32[], Arg_1.3872: f32[]) -> f32[] {
  %Arg_0.3871 = f32[] parameter(0)
  %Arg_1.3872 = f32[] parameter(1)
  ROOT %add.3873 = f32[] add(f32[] %Arg_0.3871, f32[] %Arg_1.3872), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%fused_computation.321 (param_0.2287: f32[1024], param_1.2782: f32[], param_2.2591: f32[4,1024], param_3.2035: f32[], param_4.1471: f32[4,1024], param_5.1496: f32[], param_6.1231: f32[], param_7.833: f16[4,1024,1024], param_8.594: f16[4,1024,1024]) -> (f32[4,1024], f32[4,1024]) {
  %param_7.833 = f16[4,1024,1024]{2,1,0} parameter(7)
  %convert.926 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_7.833), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %param_6.1231 = f32[] parameter(6)
  %broadcast.3181 = f32[4,1024]{1,0} broadcast(f32[] %param_6.1231), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_4.1471 = f32[4,1024]{1,0} parameter(4)
  %param_5.1496 = f32[] parameter(5)
  %broadcast.3180 = f32[4,1024]{1,0} broadcast(f32[] %param_5.1496), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.1057 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_4.1471, f32[4,1024]{1,0} %broadcast.3180), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.2591 = f32[4,1024]{1,0} parameter(2)
  %param_3.2035 = f32[] parameter(3)
  %broadcast.3179 = f32[4,1024]{1,0} broadcast(f32[] %param_3.2035), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.1056 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_2.2591, f32[4,1024]{1,0} %broadcast.3179), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2738 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.1056, f32[4,1024]{1,0} %divide.1056), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.434 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.1057, f32[4,1024]{1,0} %multiply.2738), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.196 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.3181, f32[4,1024]{1,0} %subtract.434), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_1.2782 = f32[] parameter(1)
  %broadcast.3178 = f32[4,1024]{1,0} broadcast(f32[] %param_1.2782), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.2110 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.196, f32[4,1024]{1,0} %broadcast.3178), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1556 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.2110), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.152 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1556), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1555 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.152), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.3177 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1555), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_0.2287 = f32[1024]{0} parameter(0)
  %broadcast.3176 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_0.2287), dimensions={2}
  %multiply.2737 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.3177, f32[4,1024,1024]{2,1,0} %broadcast.3176), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2736 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.926, f32[4,1024,1024]{2,1,0} %multiply.2737), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %negate.53 = f32[4,1024,1024]{2,1,0} negate(f32[4,1024,1024]{2,1,0} %multiply.2736), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %constant_633 = f32[] constant(0)
  %reduce.254 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %negate.53, f32[] %constant_633), dimensions={2}, to_apply=%region_134.3887, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_8.594 = f16[4,1024,1024]{2,1,0} parameter(8)
  %convert.932.clone.1 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_8.594), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %broadcast.3196.clone.1 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.1056), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.438.clone.1 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.932.clone.1, f32[4,1024,1024]{2,1,0} %broadcast.3196.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.2746.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.438.clone.1, f32[4,1024,1024]{2,1,0} %convert.926), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.1736.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %multiply.2746.clone.1, f32[4,1024,1024]{2,1,0} %broadcast.3176), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.256.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1736.clone.1, f32[] %constant_633), dimensions={2}, to_apply=%region_133.3870, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  ROOT %tuple.176 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) tuple(f32[4,1024]{1,0} %reduce.254, f32[4,1024]{1,0} %reduce.256.clone.1)
}

%region_129.3789 (Arg_0.3790: f32[], Arg_1.3791: f32[]) -> f32[] {
  %Arg_0.3790 = f32[] parameter(0)
  %Arg_1.3791 = f32[] parameter(1)
  ROOT %add.3792 = f32[] add(f32[] %Arg_0.3790, f32[] %Arg_1.3791), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_130.3802 (Arg_0.3803: f32[], Arg_1.3804: f32[]) -> f32[] {
  %Arg_0.3803 = f32[] parameter(0)
  %Arg_1.3804 = f32[] parameter(1)
  ROOT %add.3805 = f32[] add(f32[] %Arg_0.3803, f32[] %Arg_1.3804), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%fused_computation.329 (param_0.2405: f16[4,1024,1024], param_1.3024: f32[1024], param_2.2930: f16[4096,1024]) -> (f32[4,1024], f32[4,1024], f16[4,1024,1024]) {
  %param_2.2930 = f16[4096,1024]{1,0} parameter(2)
  %bitcast.708.clone.1 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_2.2930), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_1.3024 = f32[1024]{0} parameter(1)
  %convert.447.clone.1 = f16[1024]{0} convert(f32[1024]{0} %param_1.3024), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.53.clone.1 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.447.clone.1), dimensions={2}
  %add.1192.clone.1 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.708.clone.1, f16[4,1024,1024]{2,1,0} %broadcast.53.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_0.2405 = f16[4,1024,1024]{2,1,0} parameter(0)
  %add.1191.clone.1 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1192.clone.1, f16[4,1024,1024]{2,1,0} %param_0.2405), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=310}
  %convert.446 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1191.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %constant_635 = f32[] constant(0)
  %reduce.257 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.446, f32[] %constant_635), dimensions={2}, to_apply=%region_129.3789, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.1735.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.446, f32[4,1024,1024]{2,1,0} %convert.446), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.255.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1735.clone.1, f32[] %constant_635), dimensions={2}, to_apply=%region_130.3802, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  ROOT %tuple.178 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) tuple(f32[4,1024]{1,0} %reduce.257, f32[4,1024]{1,0} %reduce.255.clone.1, f16[4,1024,1024]{2,1,0} %add.1191.clone.1)
}

%fused_computation.331 (param_0.598: f16[], param_1.2757: f16[4096,512], param_2.2557: f16[], param_3.2005: f16[]) -> f16[4096,512] {
  %param_1.2757 = f16[4096,512]{1,0} parameter(1)
  %bitcast.983 = f16[4,1024,512]{2,1,0} bitcast(f16[4096,512]{1,0} %param_1.2757), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %constant_1659 = f32[] constant(-4)
  %broadcast.3091 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1659), dimensions={}
  %param_3.2005 = f16[] parameter(3)
  %broadcast.3090 = f16[4,1024,512]{2,1,0} broadcast(f16[] %param_3.2005), dimensions={}
  %divide.1025 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %bitcast.983, f16[4,1024,512]{2,1,0} %broadcast.3090), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.920 = f32[4,1024,512]{2,1,0} convert(f16[4,1024,512]{2,1,0} %divide.1025), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1658 = f32[] constant(4)
  %broadcast.3089 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1658), dimensions={}
  %clamp.43 = f32[4,1024,512]{2,1,0} clamp(f32[4,1024,512]{2,1,0} %broadcast.3091, f32[4,1024,512]{2,1,0} %convert.920, f32[4,1024,512]{2,1,0} %broadcast.3089), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2694 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.43, f32[4,1024,512]{2,1,0} %clamp.43), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1657 = f32[] constant(0)
  %broadcast.3088 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1657), dimensions={}
  %multiply.2693 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %multiply.2694, f32[4,1024,512]{2,1,0} %broadcast.3088), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1656 = f32[] constant(-2.72614237e-10)
  %broadcast.3087 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1656), dimensions={}
  %add.2071 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2693, f32[4,1024,512]{2,1,0} %broadcast.3087), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2692 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2071, f32[4,1024,512]{2,1,0} %multiply.2694), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1655 = f32[] constant(2.77068146e-08)
  %broadcast.3086 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1655), dimensions={}
  %add.2070 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2692, f32[4,1024,512]{2,1,0} %broadcast.3086), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2691 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2070, f32[4,1024,512]{2,1,0} %multiply.2694), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1654 = f32[] constant(-2.10102394e-06)
  %broadcast.3085 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1654), dimensions={}
  %add.2069 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2691, f32[4,1024,512]{2,1,0} %broadcast.3085), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2690 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2069, f32[4,1024,512]{2,1,0} %multiply.2694), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1653 = f32[] constant(-5.69250624e-05)
  %broadcast.3084 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1653), dimensions={}
  %add.2068 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2690, f32[4,1024,512]{2,1,0} %broadcast.3084), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2689 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2068, f32[4,1024,512]{2,1,0} %multiply.2694), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1652 = f32[] constant(-0.000734990637)
  %broadcast.3083 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1652), dimensions={}
  %add.2066 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2689, f32[4,1024,512]{2,1,0} %broadcast.3083), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2688 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2066, f32[4,1024,512]{2,1,0} %multiply.2694), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1651 = f32[] constant(-0.0029546)
  %broadcast.3082 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1651), dimensions={}
  %add.2065 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2688, f32[4,1024,512]{2,1,0} %broadcast.3082), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2687 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2065, f32[4,1024,512]{2,1,0} %multiply.2694), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1650 = f32[] constant(-0.0160960332)
  %broadcast.3081 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1650), dimensions={}
  %add.2064 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2687, f32[4,1024,512]{2,1,0} %broadcast.3081), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2686 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.43, f32[4,1024,512]{2,1,0} %add.2064), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1649 = f32[] constant(-1.45660715e-05)
  %broadcast.3080 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1649), dimensions={}
  %add.2063 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2693, f32[4,1024,512]{2,1,0} %broadcast.3080), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2685 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2063, f32[4,1024,512]{2,1,0} %multiply.2694), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1648 = f32[] constant(-0.000213374049)
  %broadcast.3079 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1648), dimensions={}
  %add.2062 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2685, f32[4,1024,512]{2,1,0} %broadcast.3079), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2684 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2062, f32[4,1024,512]{2,1,0} %multiply.2694), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1647 = f32[] constant(-0.00168282702)
  %broadcast.3078 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1647), dimensions={}
  %add.2061 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2684, f32[4,1024,512]{2,1,0} %broadcast.3078), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2683 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2061, f32[4,1024,512]{2,1,0} %multiply.2694), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1646 = f32[] constant(-0.00737332925)
  %broadcast.3077 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1646), dimensions={}
  %add.2060 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2683, f32[4,1024,512]{2,1,0} %broadcast.3077), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2682 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2060, f32[4,1024,512]{2,1,0} %multiply.2694), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1645 = f32[] constant(-0.0142647391)
  %broadcast.3076 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1645), dimensions={}
  %add.2059 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2682, f32[4,1024,512]{2,1,0} %broadcast.3076), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.1024 = f32[4,1024,512]{2,1,0} divide(f32[4,1024,512]{2,1,0} %multiply.2686, f32[4,1024,512]{2,1,0} %add.2059), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.919 = f16[4,1024,512]{2,1,0} convert(f32[4,1024,512]{2,1,0} %divide.1024), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %param_2.2557 = f16[] parameter(2)
  %broadcast.3075 = f16[4,1024,512]{2,1,0} broadcast(f16[] %param_2.2557), dimensions={}
  %add.2058 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %convert.919, f16[4,1024,512]{2,1,0} %broadcast.3075), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1738 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %bitcast.983, f16[4,1024,512]{2,1,0} %add.2058), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %param_0.598 = f16[] parameter(0)
  %broadcast.54 = f16[4,1024,512]{2,1,0} broadcast(f16[] %param_0.598), dimensions={}
  %divide.488 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %multiply.1738, f16[4,1024,512]{2,1,0} %broadcast.54), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  ROOT %bitcast.709 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %divide.488)
}

%fused_computation.333 (param_0.603: f32[512]) -> f16[4096,512] {
  %param_0.603 = f32[512]{0} parameter(0)
  %convert.450 = f16[512]{0} convert(f32[512]{0} %param_0.603), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.56 = f16[4,1024,512]{2,1,0} broadcast(f16[512]{0} %convert.450), dimensions={2}
  ROOT %bitcast.710 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %broadcast.56)
}

%fused_computation.334 (param_0.606: f32[1024], param_1.2746: f32[1024], param_2.2542: f32[4,1024], param_3.1987: f32[], param_4.1421: f16[4,1024,1024], param_5.1435: f32[], param_6.1179: f32[4,1024], param_7.792: f32[], param_8.480: f32[]) -> f16[4,1024,1024] {
  %param_4.1421 = f16[4,1024,1024]{2,1,0} parameter(4)
  %convert.914 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_4.1421), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %param_2.2542 = f32[4,1024]{1,0} parameter(2)
  %param_3.1987 = f32[] parameter(3)
  %broadcast.2999 = f32[4,1024]{1,0} broadcast(f32[] %param_3.1987), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.991 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_2.2542, f32[4,1024]{1,0} %broadcast.2999), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.2998 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.991), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.404 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.914, f32[4,1024,1024]{2,1,0} %broadcast.2998), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_8.480 = f32[] parameter(8)
  %broadcast.3037 = f32[4,1024]{1,0} broadcast(f32[] %param_8.480), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_6.1179 = f32[4,1024]{1,0} parameter(6)
  %param_7.792 = f32[] parameter(7)
  %broadcast.3036 = f32[4,1024]{1,0} broadcast(f32[] %param_7.792), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.1013 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_6.1179, f32[4,1024]{1,0} %broadcast.3036), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.2664 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.991, f32[4,1024]{1,0} %divide.991), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.416 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.1013, f32[4,1024]{1,0} %multiply.2664), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.182 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.3037, f32[4,1024]{1,0} %subtract.416), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_5.1435 = f32[] parameter(5)
  %broadcast.3034 = f32[4,1024]{1,0} broadcast(f32[] %param_5.1435), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.2039 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.182, f32[4,1024]{1,0} %broadcast.3034), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1528 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.2039), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.142 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1528), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1527 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.142), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.3033 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1527), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_1.2746 = f32[1024]{0} parameter(1)
  %broadcast.1419 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_1.2746), dimensions={2}
  %multiply.2145 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.3033, f32[4,1024,1024]{2,1,0} %broadcast.1419), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.1752 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.404, f32[4,1024,1024]{2,1,0} %multiply.2145), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.606 = f32[1024]{0} parameter(0)
  %broadcast.57 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_0.606), dimensions={2}
  %add.1206 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.1752, f32[4,1024,1024]{2,1,0} %broadcast.57), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  ROOT %convert.451 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.1206), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
}

%region_127.3690 (Arg_0.3691: f32[], Arg_1.3692: f32[]) -> f32[] {
  %Arg_0.3691 = f32[] parameter(0)
  %Arg_1.3692 = f32[] parameter(1)
  ROOT %add.3693 = f32[] add(f32[] %Arg_0.3691, f32[] %Arg_1.3692), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_128.3703 (Arg_0.3704: f32[], Arg_1.3705: f32[]) -> f32[] {
  %Arg_0.3704 = f32[] parameter(0)
  %Arg_1.3705 = f32[] parameter(1)
  ROOT %add.3706 = f32[] add(f32[] %Arg_0.3704, f32[] %Arg_1.3705), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%fused_computation.340 (param_0.2407: f16[4,1024,1024], param_1.3029: f32[1024], param_2.2935: f16[4096,1024]) -> (f32[4,1024], f32[4,1024], f16[4,1024,1024]) {
  %param_2.2935 = f16[4096,1024]{1,0} parameter(2)
  %bitcast.713.clone.1 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_2.2935), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_1.3029 = f32[1024]{0} parameter(1)
  %convert.455.clone.1 = f16[1024]{0} convert(f32[1024]{0} %param_1.3029), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.61.clone.1 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.455.clone.1), dimensions={2}
  %add.1209.clone.1 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.713.clone.1, f16[4,1024,1024]{2,1,0} %broadcast.61.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_0.2407 = f16[4,1024,1024]{2,1,0} parameter(0)
  %add.1208.clone.1 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1209.clone.1, f16[4,1024,1024]{2,1,0} %param_0.2407), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=233}
  %convert.454 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1208.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %constant_652 = f32[] constant(0)
  %reduce.259 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.454, f32[] %constant_652), dimensions={2}, to_apply=%region_127.3690, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.1754.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.454, f32[4,1024,1024]{2,1,0} %convert.454), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.258.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1754.clone.1, f32[] %constant_652), dimensions={2}, to_apply=%region_128.3703, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  ROOT %tuple.180 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) tuple(f32[4,1024]{1,0} %reduce.259, f32[4,1024]{1,0} %reduce.258.clone.1, f16[4,1024,1024]{2,1,0} %add.1208.clone.1)
}

%fused_computation.342 (param_0.623: f16[4,4,32,1024]) -> f16[4096,128] {
  %param_0.623 = f16[4,4,32,1024]{3,2,1,0} parameter(0)
  %transpose.19 = f16[4,1024,4,32]{1,3,2,0} transpose(f16[4,4,32,1024]{3,2,1,0} %param_0.623), dimensions={0,3,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %copy.99 = f16[4,1024,4,32]{3,2,1,0} copy(f16[4,1024,4,32]{1,3,2,0} %transpose.19), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  ROOT %bitcast.714 = f16[4096,128]{1,0} bitcast(f16[4,1024,4,32]{3,2,1,0} %copy.99)
}

%region_126.3665 (Arg_0.3666: f32[], Arg_1.3667: f32[]) -> f32[] {
  %Arg_0.3666 = f32[] parameter(0)
  %Arg_1.3667 = f32[] parameter(1)
  ROOT %add.3668 = f32[] add(f32[] %Arg_0.3666, f32[] %Arg_1.3667), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.343 (param_0.2250: f16[4,4,1024,1024], param_1.2723: f16[4,4,1024]) -> f32[4,4,1024] {
  %param_0.2250 = f16[4,4,1024,1024]{3,2,1,0} parameter(0)
  %param_1.2723 = f16[4,4,1024]{2,1,0} parameter(1)
  %broadcast.2989 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_1.2723), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.396 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_0.2250, f16[4,4,1024,1024]{3,2,1,0} %broadcast.2989), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.99 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.396), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.456 = f32[4,4,1024,1024]{3,2,1,0} convert(f16[4,4,1024,1024]{3,2,1,0} %exponential.99), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %constant_653 = f32[] constant(0)
  ROOT %reduce.260 = f32[4,4,1024]{2,1,0} reduce(f32[4,4,1024,1024]{3,2,1,0} %convert.456, f32[] %constant_653), dimensions={3}, to_apply=%region_126.3665, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.345 (param_0.631: f16[8,1,1,1024], param_1.972: s32[], param_2.1698: f16[8,1,1,1024], param_3.1360: s32[4,1024], param_4.854: s32[]) -> f16[4,4,1024,1024] {
  %param_3.1360 = s32[4,1024]{1,0} parameter(3)
  %param_4.854 = s32[] parameter(4)
  %broadcast.64 = s32[4,1024]{1,0} broadcast(s32[] %param_4.854), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %compare.53 = pred[4,1024]{1,0} compare(s32[4,1024]{1,0} %param_3.1360, s32[4,1024]{1,0} %broadcast.64), direction=GT, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %bitcast.717 = pred[4,1,1,1024]{3,0,2,1} bitcast(pred[4,1024]{1,0} %compare.53), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %param_2.1698 = f16[8,1,1,1024]{3,0,2,1} parameter(2)
  %param_1.972 = s32[] parameter(1)
  %constant_654 = s32[] constant(0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %dynamic-slice.27 = f16[4,1,1,1024]{3,0,2,1} dynamic-slice(f16[8,1,1,1024]{3,0,2,1} %param_2.1698, s32[] %param_1.972, s32[] %constant_654, s32[] %constant_654, s32[] %constant_654), dynamic_slice_sizes={4,1,1,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %param_0.631 = f16[8,1,1,1024]{3,0,2,1} parameter(0)
  %dynamic-slice.26 = f16[4,1,1,1024]{3,0,2,1} dynamic-slice(f16[8,1,1,1024]{3,0,2,1} %param_0.631, s32[] %param_1.972, s32[] %constant_654, s32[] %constant_654, s32[] %constant_654), dynamic_slice_sizes={4,1,1,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %select.47 = f16[4,1,1,1024]{3,0,2,1} select(pred[4,1,1,1024]{3,0,2,1} %bitcast.717, f16[4,1,1,1024]{3,0,2,1} %dynamic-slice.27, f16[4,1,1,1024]{3,0,2,1} %dynamic-slice.26), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %bitcast.716 = f16[4,1024]{1,0} bitcast(f16[4,1,1,1024]{3,0,2,1} %select.47), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=100}
  ROOT %broadcast.63 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,1024]{1,0} %bitcast.716), dimensions={0,3}
}

%fused_computation.347 (param_0.2238: f16[4096,384]) -> (f16[4,4,32,1024], f16[4,4,32,1024]) {
  %param_0.2238 = f16[4096,384]{1,0} parameter(0)
  %bitcast.1508 = f16[4,1024,384]{2,1,0} bitcast(f16[4096,384]{1,0} %param_0.2238), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %bitcast.1507 = f16[4,1024,128,3]{3,2,1,0} bitcast(f16[4,1024,384]{2,1,0} %bitcast.1508), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %copy.225 = f16[4,1024,128,3]{1,2,0,3} copy(f16[4,1024,128,3]{3,2,1,0} %bitcast.1507), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %slice.39 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.225), slice={[0:4], [0:1024], [0:128], [1:2]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/slice[start_indices=(0, 0, 0, 1) limit_indices=(8, 1024, 1024, 2) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.20 = f16[1,4,128,1024]{3,2,1,0} transpose(f16[4,1024,128,1]{1,2,0,3} %slice.39), dimensions={3,0,2,1}
  %bitcast.720 = f16[4,4,32,1024]{3,2,1,0} bitcast(f16[1,4,128,1024]{3,2,1,0} %transpose.20)
  %slice.69.clone.1 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.225), slice={[0:4], [0:1024], [0:128], [2:3]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/slice[start_indices=(0, 0, 0, 2) limit_indices=(8, 1024, 1024, 3) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.21 = f16[1,4,128,1024]{3,2,1,0} transpose(f16[4,1024,128,1]{1,2,0,3} %slice.69.clone.1), dimensions={3,0,2,1}
  %bitcast.985.clone.1 = f16[4,4,32,1024]{3,2,1,0} bitcast(f16[1,4,128,1024]{3,2,1,0} %transpose.21)
  ROOT %tuple.182 = (f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) tuple(f16[4,4,32,1024]{3,2,1,0} %bitcast.720, f16[4,4,32,1024]{3,2,1,0} %bitcast.985.clone.1)
}

%fused_computation.349 (param_0.644: f32[384]) -> f16[4096,384] {
  %param_0.644 = f32[384]{0} parameter(0)
  %convert.457 = f16[384]{0} convert(f32[384]{0} %param_0.644), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.66 = f16[4,1024,384]{2,1,0} broadcast(f16[384]{0} %convert.457), dimensions={2}
  ROOT %bitcast.723 = f16[4096,384]{1,0} bitcast(f16[4,1024,384]{2,1,0} %broadcast.66)
}

%fused_computation.350 (param_0.645: f16[4,1024,1024], param_1.977: f16[4096,1024]) -> f16[4,1024,1024] {
  %param_0.645 = f16[4,1024,1024]{2,1,0} parameter(0)
  %param_1.977 = f16[4096,1024]{1,0} parameter(1)
  %bitcast.724 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_1.977), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  ROOT %add.1210 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %param_0.645, f16[4,1024,1024]{2,1,0} %bitcast.724), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
}

%fused_computation.351 (param_0.1534: f16[4,4,32,1024], param_1.1983: f16[4,4,1024,32], param_2.1699: f16[], param_3.1361: f16[4,4,1024,32]) -> f16[4,1024,128,3] {
  %param_3.1361 = f16[4,4,1024,32]{3,2,1,0} parameter(3)
  %transpose.22 = f16[4,1024,4,32]{3,1,2,0} transpose(f16[4,4,1024,32]{3,2,1,0} %param_3.1361), dimensions={0,2,1,3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/transpose[permutation=(0, 2, 1, 3)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %copy.104 = f16[4,1024,4,32]{1,3,2,0} copy(f16[4,1024,4,32]{3,1,2,0} %transpose.22), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/transpose[permutation=(0, 2, 1, 3)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %bitcast.728 = f16[4,1024,128,1]{1,2,0,3} bitcast(f16[4,1024,4,32]{1,3,2,0} %copy.104), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=173}
  %constant_655 = f16[] constant(0)
  %pad.26 = f16[4,1024,128,3]{1,2,0,3} pad(f16[4,1024,128,1]{1,2,0,3} %bitcast.728, f16[] %constant_655), padding=0_0x0_0x0_0x2_0, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (2, 0, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %param_1.1983 = f16[4,4,1024,32]{3,2,1,0} parameter(1)
  %param_2.1699 = f16[] parameter(2)
  %broadcast.67 = f16[4,4,1024,32]{3,2,1,0} broadcast(f16[] %param_2.1699), dimensions={}
  %divide.492 = f16[4,4,1024,32]{3,2,1,0} divide(f16[4,4,1024,32]{3,2,1,0} %param_1.1983, f16[4,4,1024,32]{3,2,1,0} %broadcast.67), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %transpose.23 = f16[4,1024,4,32]{3,1,2,0} transpose(f16[4,4,1024,32]{3,2,1,0} %divide.492), dimensions={0,2,1,3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %copy.103 = f16[4,1024,4,32]{1,3,2,0} copy(f16[4,1024,4,32]{3,1,2,0} %transpose.23), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %bitcast.726 = f16[4,1024,128,1]{1,2,0,3} bitcast(f16[4,1024,4,32]{1,3,2,0} %copy.103), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=167}
  %pad.25 = f16[4,1024,128,3]{1,2,0,3} pad(f16[4,1024,128,1]{1,2,0,3} %bitcast.726, f16[] %constant_655), padding=0_0x0_0x0_0x0_2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 2, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %add.1212 = f16[4,1024,128,3]{1,2,0,3} add(f16[4,1024,128,3]{1,2,0,3} %pad.26, f16[4,1024,128,3]{1,2,0,3} %pad.25), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %param_0.1534 = f16[4,4,32,1024]{3,2,1,0} parameter(0)
  %bitcast.1865 = f16[1,4,128,1024]{3,2,1,0} bitcast(f16[4,4,32,1024]{3,2,1,0} %param_0.1534)
  %transpose.24 = f16[4,1024,128,1]{1,2,0,3} transpose(f16[1,4,128,1024]{3,2,1,0} %bitcast.1865), dimensions={1,3,2,0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=170}
  %pad.24 = f16[4,1024,128,3]{1,2,0,3} pad(f16[4,1024,128,1]{1,2,0,3} %transpose.24, f16[] %constant_655), padding=0_0x0_0x0_0x1_1, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (1, 1, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %add.1211 = f16[4,1024,128,3]{1,2,0,3} add(f16[4,1024,128,3]{1,2,0,3} %add.1212, f16[4,1024,128,3]{1,2,0,3} %pad.24), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  ROOT %copy.102 = f16[4,1024,128,3]{3,2,1,0} copy(f16[4,1024,128,3]{1,2,0,3} %add.1211), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
}

%fused_computation.352 (param_0.650: f16[4096,128]) -> f16[4,4,32,1024] {
  %param_0.650 = f16[4096,128]{1,0} parameter(0)
  %bitcast.1866 = f16[4,1024,4,32]{3,2,1,0} bitcast(f16[4096,128]{1,0} %param_0.650)
  %transpose.25 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %bitcast.1866), dimensions={0,2,3,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/transpose[permutation=(0, 2, 3, 1)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  ROOT %copy.105 = f16[4,4,32,1024]{3,2,1,0} copy(f16[4,4,32,1024]{2,1,3,0} %transpose.25), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/transpose[permutation=(0, 2, 3, 1)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
}

%fused_computation.353 (param_0.2173: f16[4096,384], param_1.3053: f16[]) -> (f16[4,4,1024,32], f16[4,4,1024,32]) {
  %param_0.2173 = f16[4096,384]{1,0} parameter(0)
  %bitcast.1442 = f16[4,1024,384]{2,1,0} bitcast(f16[4096,384]{1,0} %param_0.2173), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %bitcast.1441 = f16[4,1024,128,3]{3,2,1,0} bitcast(f16[4,1024,384]{2,1,0} %bitcast.1442), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %copy.223 = f16[4,1024,128,3]{1,2,0,3} copy(f16[4,1024,128,3]{3,2,1,0} %bitcast.1441), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %slice.70 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.223), slice={[0:4], [0:1024], [0:128], [2:3]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/slice[start_indices=(0, 0, 0, 2) limit_indices=(8, 1024, 1024, 3) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.26 = f16[1,4,128,1024]{3,2,1,0} transpose(f16[4,1024,128,1]{1,2,0,3} %slice.70), dimensions={3,0,2,1}
  %bitcast.1867 = f16[4,4,32,1024]{3,2,1,0} bitcast(f16[1,4,128,1024]{3,2,1,0} %transpose.26)
  %transpose.27 = f16[4,4,1024,32]{2,3,1,0} transpose(f16[4,4,32,1024]{3,2,1,0} %bitcast.1867), dimensions={0,1,3,2}
  %copy.106 = f16[4,4,1024,32]{3,2,1,0} copy(f16[4,4,1024,32]{2,3,1,0} %transpose.27)
  %slice.40.clone.1 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.223), slice={[0:4], [0:1024], [0:128], [0:1]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/slice[start_indices=(0, 0, 0, 0) limit_indices=(8, 1024, 1024, 1) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %param_1.3053 = f16[] parameter(1)
  %broadcast.90.clone.1 = f16[4,1024,128,1]{1,2,0,3} broadcast(f16[] %param_1.3053), dimensions={}
  %divide.509.clone.1 = f16[4,1024,128,1]{1,2,0,3} divide(f16[4,1024,128,1]{1,2,0,3} %slice.40.clone.1, f16[4,1024,128,1]{1,2,0,3} %broadcast.90.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %bitcast.753.clone.1 = f16[4,1024,4,32]{1,3,2,0} bitcast(f16[4,1024,128,1]{1,2,0,3} %divide.509.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %copy.109.clone.1 = f16[4,1024,4,32]{3,1,2,0} copy(f16[4,1024,4,32]{1,3,2,0} %bitcast.753.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %transpose.28 = f16[4,4,1024,32]{3,2,1,0} transpose(f16[4,1024,4,32]{3,1,2,0} %copy.109.clone.1), dimensions={0,2,1,3}
  ROOT %tuple.191 = (f16[4,4,1024,32]{3,2,1,0}, f16[4,4,1024,32]{3,2,1,0}) tuple(f16[4,4,1024,32]{3,2,1,0} %copy.106, f16[4,4,1024,32]{3,2,1,0} %transpose.28)
}

%fused_computation.354 (param_0.2182: f16[4,4,1024,1024], param_1.2639: f16[4,4,1024], param_2.2420: f32[4,4,1024], param_3.1869: f16[4,4,1024,1024], param_4.1315: f16[4,4,1024]) -> f16[4,4,1024,1024] {
  %param_1.2639 = f16[4,4,1024]{2,1,0} parameter(1)
  %negate.54 = f16[4,4,1024]{2,1,0} negate(f16[4,4,1024]{2,1,0} %param_1.2639), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.68 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %negate.54), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/broadcast_in_dim[shape=(8, 32, 1024, 1024) broadcast_dimensions=(0, 1, 2)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %param_0.2182 = f16[4,4,1024,1024]{3,2,1,0} parameter(0)
  %param_2.2420 = f32[4,4,1024]{2,1,0} parameter(2)
  %convert.667 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %param_2.2420), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.1425 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %convert.667), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.493 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %param_0.2182, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1425), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %add.1213 = f16[4,4,1024,1024]{3,2,1,0} add(f16[4,4,1024,1024]{3,2,1,0} %broadcast.68, f16[4,4,1024,1024]{3,2,1,0} %divide.493), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %param_3.1869 = f16[4,4,1024,1024]{3,2,1,0} parameter(3)
  %param_4.1315 = f16[4,4,1024]{2,1,0} parameter(4)
  %broadcast.2749 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_4.1315), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.348 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_3.1869, f16[4,4,1024,1024]{3,2,1,0} %broadcast.2749), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.97 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.348), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  ROOT %multiply.1755 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %add.1213, f16[4,4,1024,1024]{3,2,1,0} %exponential.97), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%region_122.3539 (Arg_0.3540: f16[], Arg_1.3541: f16[]) -> f16[] {
  %Arg_0.3540 = f16[] parameter(0)
  %Arg_1.3541 = f16[] parameter(1)
  ROOT %add.3542 = f16[] add(f16[] %Arg_0.3540, f16[] %Arg_1.3541), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.355 (param_0.2180: f16[4,4,1024,1024], param_1.2637: f32[4,4,1024], param_2.2419: f16[4,4,1024,1024], param_3.1867: f16[4,4,1024]) -> f16[4,4,1024] {
  %param_0.2180 = f16[4,4,1024,1024]{3,2,1,0} parameter(0)
  %constant_656 = f16[] constant(1)
  %broadcast.1426 = f16[4,4,1024]{2,1,0} broadcast(f16[] %constant_656), dimensions={}
  %param_1.2637 = f32[4,4,1024]{2,1,0} parameter(1)
  %convert.668 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %param_1.2637), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.1758 = f16[4,4,1024]{2,1,0} multiply(f16[4,4,1024]{2,1,0} %convert.668, f16[4,4,1024]{2,1,0} %convert.668), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.494 = f16[4,4,1024]{2,1,0} divide(f16[4,4,1024]{2,1,0} %broadcast.1426, f16[4,4,1024]{2,1,0} %multiply.1758), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.69 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %divide.494), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.1757 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %param_0.2180, f16[4,4,1024,1024]{3,2,1,0} %broadcast.69), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %param_2.2419 = f16[4,4,1024,1024]{3,2,1,0} parameter(2)
  %param_3.1867 = f16[4,4,1024]{2,1,0} parameter(3)
  %broadcast.2747 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_3.1867), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.346 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_2.2419, f16[4,4,1024,1024]{3,2,1,0} %broadcast.2747), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.95 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.346), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.1756 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %multiply.1757, f16[4,4,1024,1024]{3,2,1,0} %exponential.95), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %constant_657 = f16[] constant(0)
  ROOT %reduce.261 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %multiply.1756, f16[] %constant_657), dimensions={3}, to_apply=%region_122.3539, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.356 (param_0.659: f16[4096,128]) -> f16[4,4,1024,32] {
  %param_0.659 = f16[4096,128]{1,0} parameter(0)
  %bitcast.733 = f16[4,1024,4,32]{3,2,1,0} bitcast(f16[4096,128]{1,0} %param_0.659), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %copy.107 = f16[4,1024,4,32]{3,1,2,0} copy(f16[4,1024,4,32]{3,2,1,0} %bitcast.733), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  ROOT %transpose.29 = f16[4,4,1024,32]{3,2,1,0} transpose(f16[4,1024,4,32]{3,1,2,0} %copy.107), dimensions={0,2,1,3}
}

%fused_computation.357 (param_0.2230: f32[4,1024], param_1.2712: f32[8,1024], param_2.2511: s32[], param_3.1956: f32[8,1024], param_4.1397: f32[8,1024], param_5.1414: f32[8,1024], param_6.1173: f32[4,1024], param_7.782: f16[4,1024,1024], param_8.468: f32[], param_9.358: f32[4,1024], param_10.308: f32[], param_11.333: f32[], param_12.328: f32[4,1024], param_13.326: f32[], param_14.345: f32[1024], param_15.365: f16[4,1024,1024], param_16.383: f16[4096,1024]) -> f16[4,1024,1024] {
  %param_7.782 = f16[4,1024,1024]{2,1,0} parameter(7)
  %convert.460 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_7.782), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %param_6.1173 = f32[4,1024]{1,0} parameter(6)
  %bitcast.735 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %param_6.1173), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reshape[new_sizes=(8, 1024, 1) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_11.333 = f32[] parameter(11)
  %broadcast.2781 = f32[4,1024]{1,0} broadcast(f32[] %param_11.333), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_12.328 = f32[4,1024]{1,0} parameter(12)
  %param_8.468 = f32[] parameter(8)
  %broadcast.2780 = f32[4,1024]{1,0} broadcast(f32[] %param_8.468), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.921 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_12.328, f32[4,1024]{1,0} %broadcast.2780), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_9.358 = f32[4,1024]{1,0} parameter(9)
  %param_10.308 = f32[] parameter(10)
  %broadcast.2779 = f32[4,1024]{1,0} broadcast(f32[] %param_10.308), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.920 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_9.358, f32[4,1024]{1,0} %broadcast.2779), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2552 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.920, f32[4,1024]{1,0} %divide.920), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.360 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.921, f32[4,1024]{1,0} %multiply.2552), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.154 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.2781, f32[4,1024]{1,0} %subtract.360), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_13.326 = f32[] parameter(13)
  %broadcast.2778 = f32[4,1024]{1,0} broadcast(f32[] %param_13.326), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.1940 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.154, f32[4,1024]{1,0} %broadcast.2778), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1446 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.1940), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.46 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1446), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %divide.498 = f32[4,1024,1]{1,0,2} divide(f32[4,1024,1]{1,0,2} %rsqrt.46, f32[4,1024,1]{1,0,2} %bitcast.1446), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %constant_658 = f32[] constant(-0.5)
  %broadcast.1430 = f32[4,1024,1]{1,0,2} broadcast(f32[] %constant_658), dimensions={}
  %multiply.1765 = f32[4,1024,1]{1,0,2} multiply(f32[4,1024,1]{1,0,2} %divide.498, f32[4,1024,1]{1,0,2} %broadcast.1430), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.1764 = f32[4,1024,1]{1,0,2} multiply(f32[4,1024,1]{1,0,2} %bitcast.735, f32[4,1024,1]{1,0,2} %multiply.1765), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.734 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %multiply.1764), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reshape[new_sizes=(8, 1024) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=132}
  %compare.55 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %subtract.360, f32[4,1024]{1,0} %maximum.154), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_5.1414 = f32[8,1024]{1,0} parameter(5)
  %param_2.2511 = s32[] parameter(2)
  %constant_660 = s32[] constant(0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %dynamic-slice.31 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_5.1414, s32[] %param_2.2511, s32[] %constant_660), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_4.1397 = f32[8,1024]{1,0} parameter(4)
  %dynamic-slice.30 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_4.1397, s32[] %param_2.2511, s32[] %constant_660), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.49 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.55, f32[4,1024]{1,0} %dynamic-slice.31, f32[4,1024]{1,0} %dynamic-slice.30), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %compare.54 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %broadcast.2781, f32[4,1024]{1,0} %maximum.154), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_3.1956 = f32[8,1024]{1,0} parameter(3)
  %dynamic-slice.29 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_3.1956, s32[] %param_2.2511, s32[] %constant_660), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_1.2712 = f32[8,1024]{1,0} parameter(1)
  %dynamic-slice.28 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_1.2712, s32[] %param_2.2511, s32[] %constant_660), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.48 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.54, f32[4,1024]{1,0} %dynamic-slice.29, f32[4,1024]{1,0} %dynamic-slice.28), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.497 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %select.49, f32[4,1024]{1,0} %select.48), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.1763 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %bitcast.734, f32[4,1024]{1,0} %divide.497), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.496 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %multiply.1763, f32[4,1024]{1,0} %broadcast.2780), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_659 = f32[] constant(2)
  %broadcast.1431 = f32[4,1024]{1,0} broadcast(f32[] %constant_659), dimensions={}
  %multiply.1762 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.496, f32[4,1024]{1,0} %broadcast.1431), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %broadcast.71 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.1762), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.1761 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.460, f32[4,1024,1024]{2,1,0} %broadcast.71), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %negate.55 = f32[4,1024]{1,0} negate(f32[4,1024]{1,0} %multiply.1763), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.1760 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.920, f32[4,1024]{1,0} %broadcast.1431), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.1759 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %negate.55, f32[4,1024]{1,0} %multiply.1760), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %param_0.2230 = f32[4,1024]{1,0} parameter(0)
  %add.1216 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %multiply.1759, f32[4,1024]{1,0} %param_0.2230), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=131}
  %divide.495 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %add.1216, f32[4,1024]{1,0} %broadcast.2779), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.70 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.495), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/broadcast_in_dim[shape=(8, 1024, 1024) broadcast_dimensions=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %add.1215 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.1761, f32[4,1024,1024]{2,1,0} %broadcast.70), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %convert.459 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.1215), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %param_15.365 = f16[4,1024,1024]{2,1,0} parameter(15)
  %param_16.383 = f16[4096,1024]{1,0} parameter(16)
  %bitcast.1500 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_16.383), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.2027 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %param_15.365, f16[4,1024,1024]{2,1,0} %bitcast.1500), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.904 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.2027), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %bitcast.1498 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.46), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.2975 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1498), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_14.345 = f32[1024]{0} parameter(14)
  %broadcast.2974 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_14.345), dimensions={2}
  %multiply.2649 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.2975, f32[4,1024,1024]{2,1,0} %broadcast.2974), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2648 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.904, f32[4,1024,1024]{2,1,0} %multiply.2649), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %convert.458 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %multiply.2648), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  ROOT %add.1214 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %convert.459, f16[4,1024,1024]{2,1,0} %convert.458), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%region_120.3508 (Arg_0.3509: f32[], Arg_1.3510: f32[]) -> f32[] {
  %Arg_0.3509 = f32[] parameter(0)
  %Arg_1.3510 = f32[] parameter(1)
  ROOT %add.3511 = f32[] add(f32[] %Arg_0.3509, f32[] %Arg_1.3510), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%region_119.3491 (Arg_0.3492: f32[], Arg_1.3493: f32[]) -> f32[] {
  %Arg_0.3492 = f32[] parameter(0)
  %Arg_1.3493 = f32[] parameter(1)
  ROOT %add.3494 = f32[] add(f32[] %Arg_0.3492, f32[] %Arg_1.3493), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%fused_computation.358 (param_0.2228: f32[1024], param_1.2710: f32[], param_2.2509: f32[4,1024], param_3.1954: f32[], param_4.1395: f32[4,1024], param_5.1412: f32[], param_6.1171: f32[], param_7.780: f16[4,1024,1024], param_8.466: f16[4096,1024], param_9.444: f16[4,1024,1024]) -> (f32[4,1024], f32[4,1024]) {
  %param_7.780 = f16[4,1024,1024]{2,1,0} parameter(7)
  %param_8.466 = f16[4096,1024]{1,0} parameter(8)
  %bitcast.1494 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_8.466), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.2023 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %param_7.780, f16[4,1024,1024]{2,1,0} %bitcast.1494), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.902 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.2023), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %param_6.1171 = f32[] parameter(6)
  %broadcast.2967 = f32[4,1024]{1,0} broadcast(f32[] %param_6.1171), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_4.1395 = f32[4,1024]{1,0} parameter(4)
  %param_5.1412 = f32[] parameter(5)
  %broadcast.2966 = f32[4,1024]{1,0} broadcast(f32[] %param_5.1412), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.981 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_4.1395, f32[4,1024]{1,0} %broadcast.2966), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.2509 = f32[4,1024]{1,0} parameter(2)
  %param_3.1954 = f32[] parameter(3)
  %broadcast.2965 = f32[4,1024]{1,0} broadcast(f32[] %param_3.1954), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.980 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_2.2509, f32[4,1024]{1,0} %broadcast.2965), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2644 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.980, f32[4,1024]{1,0} %divide.980), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.388 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.981, f32[4,1024]{1,0} %multiply.2644), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.174 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.2967, f32[4,1024]{1,0} %subtract.388), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_1.2710 = f32[] parameter(1)
  %broadcast.2964 = f32[4,1024]{1,0} broadcast(f32[] %param_1.2710), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.2022 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.174, f32[4,1024]{1,0} %broadcast.2964), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1493 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.2022), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.138 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1493), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1492 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.138), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.2963 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1492), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_0.2228 = f32[1024]{0} parameter(0)
  %broadcast.2962 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_0.2228), dimensions={2}
  %multiply.2643 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.2963, f32[4,1024,1024]{2,1,0} %broadcast.2962), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2642 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.902, f32[4,1024,1024]{2,1,0} %multiply.2643), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %negate.56 = f32[4,1024,1024]{2,1,0} negate(f32[4,1024,1024]{2,1,0} %multiply.2642), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %constant_662 = f32[] constant(0)
  %reduce.262 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %negate.56, f32[] %constant_662), dimensions={2}, to_apply=%region_120.3508, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_9.444 = f16[4,1024,1024]{2,1,0} parameter(9)
  %convert.908.clone.1 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_9.444), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %broadcast.2982.clone.1 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.980), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.392.clone.1 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.908.clone.1, f32[4,1024,1024]{2,1,0} %broadcast.2982.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.2652.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.392.clone.1, f32[4,1024,1024]{2,1,0} %convert.902), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.1767.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %multiply.2652.clone.1, f32[4,1024,1024]{2,1,0} %broadcast.2962), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.263.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1767.clone.1, f32[] %constant_662), dimensions={2}, to_apply=%region_119.3491, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  ROOT %tuple.184 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) tuple(f32[4,1024]{1,0} %reduce.262, f32[4,1024]{1,0} %reduce.263.clone.1)
}

%fused_computation.363 (param_0.2199: f16[4096,512], param_1.2668: f16[], param_2.2456: f16[], param_3.1908: f16[4096,512], param_4.1354: f16[]) -> f16[4,1024,512] {
  %param_3.1908 = f16[4096,512]{1,0} parameter(3)
  %bitcast.986 = f16[4,1024,512]{2,1,0} bitcast(f16[4096,512]{1,0} %param_3.1908), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_0.2199 = f16[4096,512]{1,0} parameter(0)
  %param_1.2668 = f16[] parameter(1)
  %broadcast.72 = f16[4096,512]{1,0} broadcast(f16[] %param_1.2668), dimensions={}
  %divide.500 = f16[4096,512]{1,0} divide(f16[4096,512]{1,0} %param_0.2199, f16[4096,512]{1,0} %broadcast.72), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %bitcast.737 = f16[4,1024,512]{2,1,0} bitcast(f16[4096,512]{1,0} %divide.500), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1773 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %bitcast.986, f16[4,1024,512]{2,1,0} %bitcast.737), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_663 = f16[] constant(1.1279)
  %broadcast.1435 = f16[4,1024,512]{2,1,0} broadcast(f16[] %constant_663), dimensions={}
  %multiply.1772 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.1773, f16[4,1024,512]{2,1,0} %broadcast.1435), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %param_2.2456 = f16[] parameter(2)
  %broadcast.1434 = f16[4,1024,512]{2,1,0} broadcast(f16[] %param_2.2456), dimensions={}
  %divide.599 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %bitcast.986, f16[4,1024,512]{2,1,0} %broadcast.1434), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1771 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %divide.599, f16[4,1024,512]{2,1,0} %divide.599), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %negate.57 = f16[4,1024,512]{2,1,0} negate(f16[4,1024,512]{2,1,0} %multiply.1771), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/neg" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %exponential.23 = f16[4,1024,512]{2,1,0} exponential(f16[4,1024,512]{2,1,0} %negate.57), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/exp" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1770 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.1772, f16[4,1024,512]{2,1,0} %exponential.23), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.499 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %multiply.1770, f16[4,1024,512]{2,1,0} %broadcast.1434), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1594 = f32[] constant(-4)
  %broadcast.2879 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1594), dimensions={}
  %convert.888 = f32[4,1024,512]{2,1,0} convert(f16[4,1024,512]{2,1,0} %divide.599), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1593 = f32[] constant(4)
  %broadcast.2877 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1593), dimensions={}
  %clamp.41 = f32[4,1024,512]{2,1,0} clamp(f32[4,1024,512]{2,1,0} %broadcast.2879, f32[4,1024,512]{2,1,0} %convert.888, f32[4,1024,512]{2,1,0} %broadcast.2877), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2610 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.41, f32[4,1024,512]{2,1,0} %clamp.41), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1592 = f32[] constant(0)
  %broadcast.2876 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1592), dimensions={}
  %multiply.2609 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %multiply.2610, f32[4,1024,512]{2,1,0} %broadcast.2876), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1591 = f32[] constant(-2.72614237e-10)
  %broadcast.2875 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1591), dimensions={}
  %add.2000 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2609, f32[4,1024,512]{2,1,0} %broadcast.2875), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2608 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.2000, f32[4,1024,512]{2,1,0} %multiply.2610), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1590 = f32[] constant(2.77068146e-08)
  %broadcast.2874 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1590), dimensions={}
  %add.1999 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2608, f32[4,1024,512]{2,1,0} %broadcast.2874), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2607 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1999, f32[4,1024,512]{2,1,0} %multiply.2610), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1589 = f32[] constant(-2.10102394e-06)
  %broadcast.2873 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1589), dimensions={}
  %add.1998 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2607, f32[4,1024,512]{2,1,0} %broadcast.2873), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2606 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1998, f32[4,1024,512]{2,1,0} %multiply.2610), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1588 = f32[] constant(-5.69250624e-05)
  %broadcast.2872 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1588), dimensions={}
  %add.1997 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2606, f32[4,1024,512]{2,1,0} %broadcast.2872), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2605 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1997, f32[4,1024,512]{2,1,0} %multiply.2610), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1587 = f32[] constant(-0.000734990637)
  %broadcast.2871 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1587), dimensions={}
  %add.1996 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2605, f32[4,1024,512]{2,1,0} %broadcast.2871), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2604 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1996, f32[4,1024,512]{2,1,0} %multiply.2610), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1586 = f32[] constant(-0.0029546)
  %broadcast.2870 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1586), dimensions={}
  %add.1995 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2604, f32[4,1024,512]{2,1,0} %broadcast.2870), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2603 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1995, f32[4,1024,512]{2,1,0} %multiply.2610), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1585 = f32[] constant(-0.0160960332)
  %broadcast.2869 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1585), dimensions={}
  %add.1994 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2603, f32[4,1024,512]{2,1,0} %broadcast.2869), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2602 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.41, f32[4,1024,512]{2,1,0} %add.1994), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1584 = f32[] constant(-1.45660715e-05)
  %broadcast.2868 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1584), dimensions={}
  %add.1993 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2609, f32[4,1024,512]{2,1,0} %broadcast.2868), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2601 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1993, f32[4,1024,512]{2,1,0} %multiply.2610), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1583 = f32[] constant(-0.000213374049)
  %broadcast.2867 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1583), dimensions={}
  %add.1992 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2601, f32[4,1024,512]{2,1,0} %broadcast.2867), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2600 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1992, f32[4,1024,512]{2,1,0} %multiply.2610), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1582 = f32[] constant(-0.00168282702)
  %broadcast.2866 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1582), dimensions={}
  %add.1991 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2600, f32[4,1024,512]{2,1,0} %broadcast.2866), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2599 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1991, f32[4,1024,512]{2,1,0} %multiply.2610), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1581 = f32[] constant(-0.00737332925)
  %broadcast.2865 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1581), dimensions={}
  %add.1990 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2599, f32[4,1024,512]{2,1,0} %broadcast.2865), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2598 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1990, f32[4,1024,512]{2,1,0} %multiply.2610), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1580 = f32[] constant(-0.0142647391)
  %broadcast.2864 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1580), dimensions={}
  %add.1989 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2598, f32[4,1024,512]{2,1,0} %broadcast.2864), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.940 = f32[4,1024,512]{2,1,0} divide(f32[4,1024,512]{2,1,0} %multiply.2602, f32[4,1024,512]{2,1,0} %add.1989), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.887 = f16[4,1024,512]{2,1,0} convert(f32[4,1024,512]{2,1,0} %divide.940), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %param_4.1354 = f16[] parameter(4)
  %broadcast.2863 = f16[4,1024,512]{2,1,0} broadcast(f16[] %param_4.1354), dimensions={}
  %add.1988 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %convert.887, f16[4,1024,512]{2,1,0} %broadcast.2863), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1769 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %bitcast.737, f16[4,1024,512]{2,1,0} %add.1988), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  ROOT %add.1218 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %divide.499, f16[4,1024,512]{2,1,0} %multiply.1769), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
}

%fused_computation.364 (param_0.2215: f32[4,1024], param_1.2693: f32[8,1024], param_2.2490: s32[], param_3.1939: f32[8,1024], param_4.1382: f32[8,1024], param_5.1403: f32[8,1024], param_6.1162: f32[4,1024], param_7.769: f16[4,1024,1024], param_8.458: f32[], param_9.357: f32[4,1024], param_10.307: f32[], param_11.332: f32[], param_12.327: f32[4,1024], param_13.325: f32[], param_14.343: f32[1024], param_15.360: f16[4,1024,1024]) -> f16[4,1024,1024] {
  %param_7.769 = f16[4,1024,1024]{2,1,0} parameter(7)
  %convert.465 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_7.769), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %param_6.1162 = f32[4,1024]{1,0} parameter(6)
  %bitcast.739 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %param_6.1162), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reshape[new_sizes=(8, 1024, 1) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_11.332 = f32[] parameter(11)
  %broadcast.2903 = f32[4,1024]{1,0} broadcast(f32[] %param_11.332), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_12.327 = f32[4,1024]{1,0} parameter(12)
  %param_8.458 = f32[] parameter(8)
  %broadcast.2902 = f32[4,1024]{1,0} broadcast(f32[] %param_8.458), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.957 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_12.327, f32[4,1024]{1,0} %broadcast.2902), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_9.357 = f32[4,1024]{1,0} parameter(9)
  %param_10.307 = f32[] parameter(10)
  %broadcast.2901 = f32[4,1024]{1,0} broadcast(f32[] %param_10.307), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.956 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_9.357, f32[4,1024]{1,0} %broadcast.2901), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2618 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.956, f32[4,1024]{1,0} %divide.956), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.374 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.957, f32[4,1024]{1,0} %multiply.2618), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.164 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.2903, f32[4,1024]{1,0} %subtract.374), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_13.325 = f32[] parameter(13)
  %broadcast.2900 = f32[4,1024]{1,0} broadcast(f32[] %param_13.325), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.2004 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.164, f32[4,1024]{1,0} %broadcast.2900), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1466 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.2004), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.47 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1466), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %divide.504 = f32[4,1024,1]{1,0,2} divide(f32[4,1024,1]{1,0,2} %rsqrt.47, f32[4,1024,1]{1,0,2} %bitcast.1466), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %constant_664 = f32[] constant(-0.5)
  %broadcast.1439 = f32[4,1024,1]{1,0,2} broadcast(f32[] %constant_664), dimensions={}
  %multiply.1780 = f32[4,1024,1]{1,0,2} multiply(f32[4,1024,1]{1,0,2} %divide.504, f32[4,1024,1]{1,0,2} %broadcast.1439), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.1779 = f32[4,1024,1]{1,0,2} multiply(f32[4,1024,1]{1,0,2} %bitcast.739, f32[4,1024,1]{1,0,2} %multiply.1780), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.738 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %multiply.1779), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reshape[new_sizes=(8, 1024) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=132}
  %compare.57 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %subtract.374, f32[4,1024]{1,0} %maximum.164), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_5.1403 = f32[8,1024]{1,0} parameter(5)
  %param_2.2490 = s32[] parameter(2)
  %constant_666 = s32[] constant(0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %dynamic-slice.36 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_5.1403, s32[] %param_2.2490, s32[] %constant_666), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_4.1382 = f32[8,1024]{1,0} parameter(4)
  %dynamic-slice.35 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_4.1382, s32[] %param_2.2490, s32[] %constant_666), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.51 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.57, f32[4,1024]{1,0} %dynamic-slice.36, f32[4,1024]{1,0} %dynamic-slice.35), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %compare.56 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %broadcast.2903, f32[4,1024]{1,0} %maximum.164), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_3.1939 = f32[8,1024]{1,0} parameter(3)
  %dynamic-slice.33 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_3.1939, s32[] %param_2.2490, s32[] %constant_666), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_1.2693 = f32[8,1024]{1,0} parameter(1)
  %dynamic-slice.32 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_1.2693, s32[] %param_2.2490, s32[] %constant_666), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.50 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.56, f32[4,1024]{1,0} %dynamic-slice.33, f32[4,1024]{1,0} %dynamic-slice.32), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.503 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %select.51, f32[4,1024]{1,0} %select.50), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.1778 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %bitcast.738, f32[4,1024]{1,0} %divide.503), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.502 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %multiply.1778, f32[4,1024]{1,0} %broadcast.2902), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_665 = f32[] constant(2)
  %broadcast.1440 = f32[4,1024]{1,0} broadcast(f32[] %constant_665), dimensions={}
  %multiply.1777 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.502, f32[4,1024]{1,0} %broadcast.1440), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %broadcast.74 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.1777), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.1776 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.465, f32[4,1024,1024]{2,1,0} %broadcast.74), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %negate.58 = f32[4,1024]{1,0} negate(f32[4,1024]{1,0} %multiply.1778), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.1775 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.956, f32[4,1024]{1,0} %broadcast.1440), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.1774 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %negate.58, f32[4,1024]{1,0} %multiply.1775), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %param_0.2215 = f32[4,1024]{1,0} parameter(0)
  %add.1221 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %multiply.1774, f32[4,1024]{1,0} %param_0.2215), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=131}
  %divide.501 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %add.1221, f32[4,1024]{1,0} %broadcast.2901), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.73 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.501), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/broadcast_in_dim[shape=(8, 1024, 1024) broadcast_dimensions=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %add.1220 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.1776, f32[4,1024,1024]{2,1,0} %broadcast.73), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %convert.464 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.1220), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %param_15.360 = f16[4,1024,1024]{2,1,0} parameter(15)
  %convert.892 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_15.360), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %bitcast.1481 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.47), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.2943 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1481), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_14.343 = f32[1024]{0} parameter(14)
  %broadcast.2942 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_14.343), dimensions={2}
  %multiply.2633 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.2943, f32[4,1024,1024]{2,1,0} %broadcast.2942), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2632 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.892, f32[4,1024,1024]{2,1,0} %multiply.2633), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %convert.463 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %multiply.2632), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  ROOT %add.1219 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %convert.464, f16[4,1024,1024]{2,1,0} %convert.463), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%region_114.3428 (Arg_0.3429: f32[], Arg_1.3430: f32[]) -> f32[] {
  %Arg_0.3429 = f32[] parameter(0)
  %Arg_1.3430 = f32[] parameter(1)
  ROOT %add.3431 = f32[] add(f32[] %Arg_0.3429, f32[] %Arg_1.3430), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%region_113.3411 (Arg_0.3412: f32[], Arg_1.3413: f32[]) -> f32[] {
  %Arg_0.3412 = f32[] parameter(0)
  %Arg_1.3413 = f32[] parameter(1)
  ROOT %add.3414 = f32[] add(f32[] %Arg_0.3412, f32[] %Arg_1.3413), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%fused_computation.365 (param_0.2213: f32[1024], param_1.2691: f32[], param_2.2488: f32[4,1024], param_3.1937: f32[], param_4.1380: f32[4,1024], param_5.1401: f32[], param_6.1160: f32[], param_7.767: f16[4,1024,1024], param_8.601: f16[4,1024,1024]) -> (f32[4,1024], f32[4,1024]) {
  %param_7.767 = f16[4,1024,1024]{2,1,0} parameter(7)
  %convert.890 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_7.767), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %param_6.1160 = f32[] parameter(6)
  %broadcast.2935 = f32[4,1024]{1,0} broadcast(f32[] %param_6.1160), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_4.1380 = f32[4,1024]{1,0} parameter(4)
  %param_5.1401 = f32[] parameter(5)
  %broadcast.2934 = f32[4,1024]{1,0} broadcast(f32[] %param_5.1401), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.969 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_4.1380, f32[4,1024]{1,0} %broadcast.2934), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.2488 = f32[4,1024]{1,0} parameter(2)
  %param_3.1937 = f32[] parameter(3)
  %broadcast.2933 = f32[4,1024]{1,0} broadcast(f32[] %param_3.1937), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.968 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_2.2488, f32[4,1024]{1,0} %broadcast.2933), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2628 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.968, f32[4,1024]{1,0} %divide.968), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.380 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.969, f32[4,1024]{1,0} %multiply.2628), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.170 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.2935, f32[4,1024]{1,0} %subtract.380), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_1.2691 = f32[] parameter(1)
  %broadcast.2932 = f32[4,1024]{1,0} broadcast(f32[] %param_1.2691), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.2010 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.170, f32[4,1024]{1,0} %broadcast.2932), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1478 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.2010), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.134 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1478), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1477 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.134), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.2931 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1477), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_0.2213 = f32[1024]{0} parameter(0)
  %broadcast.2930 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_0.2213), dimensions={2}
  %multiply.2627 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.2931, f32[4,1024,1024]{2,1,0} %broadcast.2930), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2626 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.890, f32[4,1024,1024]{2,1,0} %multiply.2627), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %negate.59 = f32[4,1024,1024]{2,1,0} negate(f32[4,1024,1024]{2,1,0} %multiply.2626), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %constant_668 = f32[] constant(0)
  %reduce.264 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %negate.59, f32[] %constant_668), dimensions={2}, to_apply=%region_114.3428, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_8.601 = f16[4,1024,1024]{2,1,0} parameter(8)
  %convert.896.clone.1 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_8.601), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %broadcast.2950.clone.1 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.968), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.384.clone.1 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.896.clone.1, f32[4,1024,1024]{2,1,0} %broadcast.2950.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.2636.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.384.clone.1, f32[4,1024,1024]{2,1,0} %convert.890), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.1785.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %multiply.2636.clone.1, f32[4,1024,1024]{2,1,0} %broadcast.2930), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.266.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1785.clone.1, f32[] %constant_668), dimensions={2}, to_apply=%region_113.3411, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  ROOT %tuple.186 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) tuple(f32[4,1024]{1,0} %reduce.264, f32[4,1024]{1,0} %reduce.266.clone.1)
}

%region_109.3330 (Arg_0.3331: f32[], Arg_1.3332: f32[]) -> f32[] {
  %Arg_0.3331 = f32[] parameter(0)
  %Arg_1.3332 = f32[] parameter(1)
  ROOT %add.3333 = f32[] add(f32[] %Arg_0.3331, f32[] %Arg_1.3332), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_110.3343 (Arg_0.3344: f32[], Arg_1.3345: f32[]) -> f32[] {
  %Arg_0.3344 = f32[] parameter(0)
  %Arg_1.3345 = f32[] parameter(1)
  ROOT %add.3346 = f32[] add(f32[] %Arg_0.3344, f32[] %Arg_1.3345), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%fused_computation.373 (param_0.2409: f16[4,1024,1024], param_1.3043: f32[1024], param_2.2945: f16[4096,1024]) -> (f32[4,1024], f32[4,1024], f16[4,1024,1024]) {
  %param_2.2945 = f16[4096,1024]{1,0} parameter(2)
  %bitcast.742.clone.1 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_2.2945), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_1.3043 = f32[1024]{0} parameter(1)
  %convert.471.clone.1 = f16[1024]{0} convert(f32[1024]{0} %param_1.3043), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.78.clone.1 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.471.clone.1), dimensions={2}
  %add.1224.clone.1 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.742.clone.1, f16[4,1024,1024]{2,1,0} %broadcast.78.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_0.2409 = f16[4,1024,1024]{2,1,0} parameter(0)
  %add.1223.clone.1 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1224.clone.1, f16[4,1024,1024]{2,1,0} %param_0.2409), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=310}
  %convert.470 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1223.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %constant_670 = f32[] constant(0)
  %reduce.267 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.470, f32[] %constant_670), dimensions={2}, to_apply=%region_109.3330, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.1784.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.470, f32[4,1024,1024]{2,1,0} %convert.470), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.265.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1784.clone.1, f32[] %constant_670), dimensions={2}, to_apply=%region_110.3343, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  ROOT %tuple.188 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) tuple(f32[4,1024]{1,0} %reduce.267, f32[4,1024]{1,0} %reduce.265.clone.1, f16[4,1024,1024]{2,1,0} %add.1223.clone.1)
}

%fused_computation.375 (param_0.697: f16[], param_1.2666: f16[4096,512], param_2.2454: f16[], param_3.1907: f16[]) -> f16[4096,512] {
  %param_1.2666 = f16[4096,512]{1,0} parameter(1)
  %bitcast.987 = f16[4,1024,512]{2,1,0} bitcast(f16[4096,512]{1,0} %param_1.2666), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %constant_1563 = f32[] constant(-4)
  %broadcast.2845 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1563), dimensions={}
  %param_3.1907 = f16[] parameter(3)
  %broadcast.2844 = f16[4,1024,512]{2,1,0} broadcast(f16[] %param_3.1907), dimensions={}
  %divide.937 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %bitcast.987, f16[4,1024,512]{2,1,0} %broadcast.2844), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.884 = f32[4,1024,512]{2,1,0} convert(f16[4,1024,512]{2,1,0} %divide.937), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1562 = f32[] constant(4)
  %broadcast.2843 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1562), dimensions={}
  %clamp.39 = f32[4,1024,512]{2,1,0} clamp(f32[4,1024,512]{2,1,0} %broadcast.2845, f32[4,1024,512]{2,1,0} %convert.884, f32[4,1024,512]{2,1,0} %broadcast.2843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2584 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.39, f32[4,1024,512]{2,1,0} %clamp.39), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1561 = f32[] constant(0)
  %broadcast.2842 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1561), dimensions={}
  %multiply.2583 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %multiply.2584, f32[4,1024,512]{2,1,0} %broadcast.2842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1560 = f32[] constant(-2.72614237e-10)
  %broadcast.2841 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1560), dimensions={}
  %add.1974 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2583, f32[4,1024,512]{2,1,0} %broadcast.2841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2582 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1974, f32[4,1024,512]{2,1,0} %multiply.2584), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1559 = f32[] constant(2.77068146e-08)
  %broadcast.2840 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1559), dimensions={}
  %add.1973 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2582, f32[4,1024,512]{2,1,0} %broadcast.2840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2581 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1973, f32[4,1024,512]{2,1,0} %multiply.2584), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1558 = f32[] constant(-2.10102394e-06)
  %broadcast.2839 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1558), dimensions={}
  %add.1972 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2581, f32[4,1024,512]{2,1,0} %broadcast.2839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2580 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1972, f32[4,1024,512]{2,1,0} %multiply.2584), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1557 = f32[] constant(-5.69250624e-05)
  %broadcast.2838 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1557), dimensions={}
  %add.1971 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2580, f32[4,1024,512]{2,1,0} %broadcast.2838), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2579 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1971, f32[4,1024,512]{2,1,0} %multiply.2584), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1556 = f32[] constant(-0.000734990637)
  %broadcast.2837 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1556), dimensions={}
  %add.1970 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2579, f32[4,1024,512]{2,1,0} %broadcast.2837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2578 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1970, f32[4,1024,512]{2,1,0} %multiply.2584), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1555 = f32[] constant(-0.0029546)
  %broadcast.2836 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1555), dimensions={}
  %add.1968 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2578, f32[4,1024,512]{2,1,0} %broadcast.2836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2577 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1968, f32[4,1024,512]{2,1,0} %multiply.2584), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1554 = f32[] constant(-0.0160960332)
  %broadcast.2835 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1554), dimensions={}
  %add.1967 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2577, f32[4,1024,512]{2,1,0} %broadcast.2835), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2576 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.39, f32[4,1024,512]{2,1,0} %add.1967), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1553 = f32[] constant(-1.45660715e-05)
  %broadcast.2834 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1553), dimensions={}
  %add.1966 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2583, f32[4,1024,512]{2,1,0} %broadcast.2834), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2575 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1966, f32[4,1024,512]{2,1,0} %multiply.2584), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1552 = f32[] constant(-0.000213374049)
  %broadcast.2833 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1552), dimensions={}
  %add.1965 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2575, f32[4,1024,512]{2,1,0} %broadcast.2833), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2574 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1965, f32[4,1024,512]{2,1,0} %multiply.2584), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1551 = f32[] constant(-0.00168282702)
  %broadcast.2832 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1551), dimensions={}
  %add.1964 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2574, f32[4,1024,512]{2,1,0} %broadcast.2832), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2573 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1964, f32[4,1024,512]{2,1,0} %multiply.2584), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1550 = f32[] constant(-0.00737332925)
  %broadcast.2831 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1550), dimensions={}
  %add.1963 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2573, f32[4,1024,512]{2,1,0} %broadcast.2831), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2572 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1963, f32[4,1024,512]{2,1,0} %multiply.2584), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1549 = f32[] constant(-0.0142647391)
  %broadcast.2830 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1549), dimensions={}
  %add.1962 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2572, f32[4,1024,512]{2,1,0} %broadcast.2830), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.936 = f32[4,1024,512]{2,1,0} divide(f32[4,1024,512]{2,1,0} %multiply.2576, f32[4,1024,512]{2,1,0} %add.1962), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.883 = f16[4,1024,512]{2,1,0} convert(f32[4,1024,512]{2,1,0} %divide.936), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %param_2.2454 = f16[] parameter(2)
  %broadcast.2829 = f16[4,1024,512]{2,1,0} broadcast(f16[] %param_2.2454), dimensions={}
  %add.1961 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %convert.883, f16[4,1024,512]{2,1,0} %broadcast.2829), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1787 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %bitcast.987, f16[4,1024,512]{2,1,0} %add.1961), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %param_0.697 = f16[] parameter(0)
  %broadcast.79 = f16[4,1024,512]{2,1,0} broadcast(f16[] %param_0.697), dimensions={}
  %divide.506 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %multiply.1787, f16[4,1024,512]{2,1,0} %broadcast.79), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  ROOT %bitcast.743 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %divide.506)
}

%fused_computation.377 (param_0.702: f32[512]) -> f16[4096,512] {
  %param_0.702 = f32[512]{0} parameter(0)
  %convert.474 = f16[512]{0} convert(f32[512]{0} %param_0.702), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.81 = f16[4,1024,512]{2,1,0} broadcast(f16[512]{0} %convert.474), dimensions={2}
  ROOT %bitcast.744 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %broadcast.81)
}

%fused_computation.378 (param_0.705: f32[1024], param_1.2655: f32[1024], param_2.2439: f32[4,1024], param_3.1889: f32[], param_4.1330: f16[4,1024,1024], param_5.1340: f32[], param_6.1108: f32[4,1024], param_7.726: f32[], param_8.432: f32[]) -> f16[4,1024,1024] {
  %param_4.1330 = f16[4,1024,1024]{2,1,0} parameter(4)
  %convert.878 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_4.1330), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %param_2.2439 = f32[4,1024]{1,0} parameter(2)
  %param_3.1889 = f32[] parameter(3)
  %broadcast.2753 = f32[4,1024]{1,0} broadcast(f32[] %param_3.1889), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.903 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_2.2439, f32[4,1024]{1,0} %broadcast.2753), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.2752 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.903), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.350 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.878, f32[4,1024,1024]{2,1,0} %broadcast.2752), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_8.432 = f32[] parameter(8)
  %broadcast.2791 = f32[4,1024]{1,0} broadcast(f32[] %param_8.432), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_6.1108 = f32[4,1024]{1,0} parameter(6)
  %param_7.726 = f32[] parameter(7)
  %broadcast.2790 = f32[4,1024]{1,0} broadcast(f32[] %param_7.726), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.925 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_6.1108, f32[4,1024]{1,0} %broadcast.2790), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.2554 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.903, f32[4,1024]{1,0} %divide.903), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.362 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.925, f32[4,1024]{1,0} %multiply.2554), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.156 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.2791, f32[4,1024]{1,0} %subtract.362), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_5.1340 = f32[] parameter(5)
  %broadcast.2788 = f32[4,1024]{1,0} broadcast(f32[] %param_5.1340), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.1942 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.156, f32[4,1024]{1,0} %broadcast.2788), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1450 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.1942), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.124 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1450), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1449 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.124), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.2787 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1449), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_1.2655 = f32[1024]{0} parameter(1)
  %broadcast.1463 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_1.2655), dimensions={2}
  %multiply.2147 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.2787, f32[4,1024,1024]{2,1,0} %broadcast.1463), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.1801 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.350, f32[4,1024,1024]{2,1,0} %multiply.2147), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.705 = f32[1024]{0} parameter(0)
  %broadcast.82 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_0.705), dimensions={2}
  %add.1238 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.1801, f32[4,1024,1024]{2,1,0} %broadcast.82), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  ROOT %convert.475 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.1238), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
}

%region_107.3231 (Arg_0.3232: f32[], Arg_1.3233: f32[]) -> f32[] {
  %Arg_0.3232 = f32[] parameter(0)
  %Arg_1.3233 = f32[] parameter(1)
  ROOT %add.3234 = f32[] add(f32[] %Arg_0.3232, f32[] %Arg_1.3233), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_108.3244 (Arg_0.3245: f32[], Arg_1.3246: f32[]) -> f32[] {
  %Arg_0.3245 = f32[] parameter(0)
  %Arg_1.3246 = f32[] parameter(1)
  ROOT %add.3247 = f32[] add(f32[] %Arg_0.3245, f32[] %Arg_1.3246), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%fused_computation.384 (param_0.2411: f16[4,1024,1024], param_1.3048: f32[1024], param_2.2950: f16[4096,1024]) -> (f32[4,1024], f32[4,1024], f16[4,1024,1024]) {
  %param_2.2950 = f16[4096,1024]{1,0} parameter(2)
  %bitcast.747.clone.1 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_2.2950), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_1.3048 = f32[1024]{0} parameter(1)
  %convert.479.clone.1 = f16[1024]{0} convert(f32[1024]{0} %param_1.3048), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.86.clone.1 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.479.clone.1), dimensions={2}
  %add.1241.clone.1 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.747.clone.1, f16[4,1024,1024]{2,1,0} %broadcast.86.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_0.2411 = f16[4,1024,1024]{2,1,0} parameter(0)
  %add.1240.clone.1 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1241.clone.1, f16[4,1024,1024]{2,1,0} %param_0.2411), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=233}
  %convert.478 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1240.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %constant_687 = f32[] constant(0)
  %reduce.269 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.478, f32[] %constant_687), dimensions={2}, to_apply=%region_107.3231, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.1803.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.478, f32[4,1024,1024]{2,1,0} %convert.478), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.268.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1803.clone.1, f32[] %constant_687), dimensions={2}, to_apply=%region_108.3244, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  ROOT %tuple.190 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) tuple(f32[4,1024]{1,0} %reduce.269, f32[4,1024]{1,0} %reduce.268.clone.1, f16[4,1024,1024]{2,1,0} %add.1240.clone.1)
}

%fused_computation.386 (param_0.722: f16[4,4,32,1024]) -> f16[4096,128] {
  %param_0.722 = f16[4,4,32,1024]{3,2,1,0} parameter(0)
  %transpose.30 = f16[4,1024,4,32]{1,3,2,0} transpose(f16[4,4,32,1024]{3,2,1,0} %param_0.722), dimensions={0,3,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %copy.108 = f16[4,1024,4,32]{3,2,1,0} copy(f16[4,1024,4,32]{1,3,2,0} %transpose.30), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  ROOT %bitcast.748 = f16[4096,128]{1,0} bitcast(f16[4,1024,4,32]{3,2,1,0} %copy.108)
}

%region_106.3206 (Arg_0.3207: f32[], Arg_1.3208: f32[]) -> f32[] {
  %Arg_0.3207 = f32[] parameter(0)
  %Arg_1.3208 = f32[] parameter(1)
  ROOT %add.3209 = f32[] add(f32[] %Arg_0.3207, f32[] %Arg_1.3208), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.387 (param_0.2176: f16[4,4,1024,1024], param_1.2632: f16[4,4,1024]) -> f32[4,4,1024] {
  %param_0.2176 = f16[4,4,1024,1024]{3,2,1,0} parameter(0)
  %param_1.2632 = f16[4,4,1024]{2,1,0} parameter(1)
  %broadcast.2743 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_1.2632), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.342 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_0.2176, f16[4,4,1024,1024]{3,2,1,0} %broadcast.2743), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.91 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.342), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.480 = f32[4,4,1024,1024]{3,2,1,0} convert(f16[4,4,1024,1024]{3,2,1,0} %exponential.91), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %constant_688 = f32[] constant(0)
  ROOT %reduce.270 = f32[4,4,1024]{2,1,0} reduce(f32[4,4,1024,1024]{3,2,1,0} %convert.480, f32[] %constant_688), dimensions={3}, to_apply=%region_106.3206, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.389 (param_0.730: f16[8,1,1,1024], param_1.1069: s32[], param_2.1730: f16[8,1,1,1024], param_3.1386: s32[4,1024], param_4.873: s32[]) -> f16[4,4,1024,1024] {
  %param_3.1386 = s32[4,1024]{1,0} parameter(3)
  %param_4.873 = s32[] parameter(4)
  %broadcast.89 = s32[4,1024]{1,0} broadcast(s32[] %param_4.873), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %compare.58 = pred[4,1024]{1,0} compare(s32[4,1024]{1,0} %param_3.1386, s32[4,1024]{1,0} %broadcast.89), direction=GT, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %bitcast.751 = pred[4,1,1,1024]{3,0,2,1} bitcast(pred[4,1024]{1,0} %compare.58), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %param_2.1730 = f16[8,1,1,1024]{3,0,2,1} parameter(2)
  %param_1.1069 = s32[] parameter(1)
  %constant_689 = s32[] constant(0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %dynamic-slice.38 = f16[4,1,1,1024]{3,0,2,1} dynamic-slice(f16[8,1,1,1024]{3,0,2,1} %param_2.1730, s32[] %param_1.1069, s32[] %constant_689, s32[] %constant_689, s32[] %constant_689), dynamic_slice_sizes={4,1,1,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %param_0.730 = f16[8,1,1,1024]{3,0,2,1} parameter(0)
  %dynamic-slice.37 = f16[4,1,1,1024]{3,0,2,1} dynamic-slice(f16[8,1,1,1024]{3,0,2,1} %param_0.730, s32[] %param_1.1069, s32[] %constant_689, s32[] %constant_689, s32[] %constant_689), dynamic_slice_sizes={4,1,1,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %select.52 = f16[4,1,1,1024]{3,0,2,1} select(pred[4,1,1,1024]{3,0,2,1} %bitcast.751, f16[4,1,1,1024]{3,0,2,1} %dynamic-slice.38, f16[4,1,1,1024]{3,0,2,1} %dynamic-slice.37), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %bitcast.750 = f16[4,1024]{1,0} bitcast(f16[4,1,1,1024]{3,0,2,1} %select.52), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=100}
  ROOT %broadcast.88 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,1024]{1,0} %bitcast.750), dimensions={0,3}
}

%fused_computation.391 (param_0.2164: f16[4096,384]) -> (f16[4,4,32,1024], f16[4,4,32,1024]) {
  %param_0.2164 = f16[4096,384]{1,0} parameter(0)
  %bitcast.1430 = f16[4,1024,384]{2,1,0} bitcast(f16[4096,384]{1,0} %param_0.2164), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %bitcast.1429 = f16[4,1024,128,3]{3,2,1,0} bitcast(f16[4,1024,384]{2,1,0} %bitcast.1430), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %copy.217 = f16[4,1024,128,3]{1,2,0,3} copy(f16[4,1024,128,3]{3,2,1,0} %bitcast.1429), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %slice.41 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.217), slice={[0:4], [0:1024], [0:128], [1:2]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/slice[start_indices=(0, 0, 0, 1) limit_indices=(8, 1024, 1024, 2) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.31 = f16[1,4,128,1024]{3,2,1,0} transpose(f16[4,1024,128,1]{1,2,0,3} %slice.41), dimensions={3,0,2,1}
  %bitcast.754 = f16[4,4,32,1024]{3,2,1,0} bitcast(f16[1,4,128,1024]{3,2,1,0} %transpose.31)
  %slice.71.clone.1 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.217), slice={[0:4], [0:1024], [0:128], [2:3]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/slice[start_indices=(0, 0, 0, 2) limit_indices=(8, 1024, 1024, 3) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.32 = f16[1,4,128,1024]{3,2,1,0} transpose(f16[4,1024,128,1]{1,2,0,3} %slice.71.clone.1), dimensions={3,0,2,1}
  %bitcast.989.clone.1 = f16[4,4,32,1024]{3,2,1,0} bitcast(f16[1,4,128,1024]{3,2,1,0} %transpose.32)
  ROOT %tuple.192 = (f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) tuple(f16[4,4,32,1024]{3,2,1,0} %bitcast.754, f16[4,4,32,1024]{3,2,1,0} %bitcast.989.clone.1)
}

%fused_computation.393 (param_0.743: f32[384]) -> f16[4096,384] {
  %param_0.743 = f32[384]{0} parameter(0)
  %convert.481 = f16[384]{0} convert(f32[384]{0} %param_0.743), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.91 = f16[4,1024,384]{2,1,0} broadcast(f16[384]{0} %convert.481), dimensions={2}
  ROOT %bitcast.757 = f16[4096,384]{1,0} bitcast(f16[4,1024,384]{2,1,0} %broadcast.91)
}

%fused_computation.394 (param_0.744: f16[4,1024,1024], param_1.1074: f16[4096,1024]) -> f16[4,1024,1024] {
  %param_0.744 = f16[4,1024,1024]{2,1,0} parameter(0)
  %param_1.1074 = f16[4096,1024]{1,0} parameter(1)
  %bitcast.758 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_1.1074), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  ROOT %add.1242 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %param_0.744, f16[4,1024,1024]{2,1,0} %bitcast.758), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
}

%fused_computation.395 (param_0.1557: f16[4,4,32,1024], param_1.2006: f16[4,4,1024,32], param_2.1731: f16[], param_3.1387: f16[4,4,1024,32]) -> f16[4,1024,128,3] {
  %param_3.1387 = f16[4,4,1024,32]{3,2,1,0} parameter(3)
  %transpose.33 = f16[4,1024,4,32]{3,1,2,0} transpose(f16[4,4,1024,32]{3,2,1,0} %param_3.1387), dimensions={0,2,1,3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/transpose[permutation=(0, 2, 1, 3)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %copy.113 = f16[4,1024,4,32]{1,3,2,0} copy(f16[4,1024,4,32]{3,1,2,0} %transpose.33), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/transpose[permutation=(0, 2, 1, 3)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %bitcast.762 = f16[4,1024,128,1]{1,2,0,3} bitcast(f16[4,1024,4,32]{1,3,2,0} %copy.113), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=173}
  %constant_690 = f16[] constant(0)
  %pad.29 = f16[4,1024,128,3]{1,2,0,3} pad(f16[4,1024,128,1]{1,2,0,3} %bitcast.762, f16[] %constant_690), padding=0_0x0_0x0_0x2_0, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (2, 0, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %param_1.2006 = f16[4,4,1024,32]{3,2,1,0} parameter(1)
  %param_2.1731 = f16[] parameter(2)
  %broadcast.92 = f16[4,4,1024,32]{3,2,1,0} broadcast(f16[] %param_2.1731), dimensions={}
  %divide.510 = f16[4,4,1024,32]{3,2,1,0} divide(f16[4,4,1024,32]{3,2,1,0} %param_1.2006, f16[4,4,1024,32]{3,2,1,0} %broadcast.92), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %transpose.34 = f16[4,1024,4,32]{3,1,2,0} transpose(f16[4,4,1024,32]{3,2,1,0} %divide.510), dimensions={0,2,1,3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %copy.112 = f16[4,1024,4,32]{1,3,2,0} copy(f16[4,1024,4,32]{3,1,2,0} %transpose.34), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %bitcast.760 = f16[4,1024,128,1]{1,2,0,3} bitcast(f16[4,1024,4,32]{1,3,2,0} %copy.112), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=167}
  %pad.28 = f16[4,1024,128,3]{1,2,0,3} pad(f16[4,1024,128,1]{1,2,0,3} %bitcast.760, f16[] %constant_690), padding=0_0x0_0x0_0x0_2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 2, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %add.1244 = f16[4,1024,128,3]{1,2,0,3} add(f16[4,1024,128,3]{1,2,0,3} %pad.29, f16[4,1024,128,3]{1,2,0,3} %pad.28), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %param_0.1557 = f16[4,4,32,1024]{3,2,1,0} parameter(0)
  %bitcast.1870 = f16[1,4,128,1024]{3,2,1,0} bitcast(f16[4,4,32,1024]{3,2,1,0} %param_0.1557)
  %transpose.35 = f16[4,1024,128,1]{1,2,0,3} transpose(f16[1,4,128,1024]{3,2,1,0} %bitcast.1870), dimensions={1,3,2,0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=170}
  %pad.27 = f16[4,1024,128,3]{1,2,0,3} pad(f16[4,1024,128,1]{1,2,0,3} %transpose.35, f16[] %constant_690), padding=0_0x0_0x0_0x1_1, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (1, 1, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %add.1243 = f16[4,1024,128,3]{1,2,0,3} add(f16[4,1024,128,3]{1,2,0,3} %add.1244, f16[4,1024,128,3]{1,2,0,3} %pad.27), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  ROOT %copy.111 = f16[4,1024,128,3]{3,2,1,0} copy(f16[4,1024,128,3]{1,2,0,3} %add.1243), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
}

%fused_computation.396 (param_0.749: f16[4096,128]) -> f16[4,4,32,1024] {
  %param_0.749 = f16[4096,128]{1,0} parameter(0)
  %bitcast.1871 = f16[4,1024,4,32]{3,2,1,0} bitcast(f16[4096,128]{1,0} %param_0.749)
  %transpose.36 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %bitcast.1871), dimensions={0,2,3,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/transpose[permutation=(0, 2, 3, 1)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  ROOT %copy.114 = f16[4,4,32,1024]{3,2,1,0} copy(f16[4,4,32,1024]{2,1,3,0} %transpose.36), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/transpose[permutation=(0, 2, 3, 1)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
}

%fused_computation.397 (param_0.2099: f16[4096,384], param_1.3072: f16[]) -> (f16[4,4,1024,32], f16[4,4,1024,32]) {
  %param_0.2099 = f16[4096,384]{1,0} parameter(0)
  %bitcast.1364 = f16[4,1024,384]{2,1,0} bitcast(f16[4096,384]{1,0} %param_0.2099), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %bitcast.1363 = f16[4,1024,128,3]{3,2,1,0} bitcast(f16[4,1024,384]{2,1,0} %bitcast.1364), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %copy.215 = f16[4,1024,128,3]{1,2,0,3} copy(f16[4,1024,128,3]{3,2,1,0} %bitcast.1363), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %slice.72 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.215), slice={[0:4], [0:1024], [0:128], [2:3]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/slice[start_indices=(0, 0, 0, 2) limit_indices=(8, 1024, 1024, 3) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.37 = f16[1,4,128,1024]{3,2,1,0} transpose(f16[4,1024,128,1]{1,2,0,3} %slice.72), dimensions={3,0,2,1}
  %bitcast.1872 = f16[4,4,32,1024]{3,2,1,0} bitcast(f16[1,4,128,1024]{3,2,1,0} %transpose.37)
  %transpose.38 = f16[4,4,1024,32]{2,3,1,0} transpose(f16[4,4,32,1024]{3,2,1,0} %bitcast.1872), dimensions={0,1,3,2}
  %copy.115 = f16[4,4,1024,32]{3,2,1,0} copy(f16[4,4,1024,32]{2,3,1,0} %transpose.38)
  %slice.42.clone.1 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.215), slice={[0:4], [0:1024], [0:128], [0:1]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/slice[start_indices=(0, 0, 0, 0) limit_indices=(8, 1024, 1024, 1) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %param_1.3072 = f16[] parameter(1)
  %broadcast.115.clone.1 = f16[4,1024,128,1]{1,2,0,3} broadcast(f16[] %param_1.3072), dimensions={}
  %divide.527.clone.1 = f16[4,1024,128,1]{1,2,0,3} divide(f16[4,1024,128,1]{1,2,0,3} %slice.42.clone.1, f16[4,1024,128,1]{1,2,0,3} %broadcast.115.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %bitcast.787.clone.1 = f16[4,1024,4,32]{1,3,2,0} bitcast(f16[4,1024,128,1]{1,2,0,3} %divide.527.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %copy.118.clone.1 = f16[4,1024,4,32]{3,1,2,0} copy(f16[4,1024,4,32]{1,3,2,0} %bitcast.787.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %transpose.39 = f16[4,4,1024,32]{3,2,1,0} transpose(f16[4,1024,4,32]{3,1,2,0} %copy.118.clone.1), dimensions={0,2,1,3}
  ROOT %tuple.201 = (f16[4,4,1024,32]{3,2,1,0}, f16[4,4,1024,32]{3,2,1,0}) tuple(f16[4,4,1024,32]{3,2,1,0} %copy.115, f16[4,4,1024,32]{3,2,1,0} %transpose.39)
}

%fused_computation.398 (param_0.2108: f16[4,4,1024,1024], param_1.2548: f16[4,4,1024], param_2.2317: f32[4,4,1024], param_3.1771: f16[4,4,1024,1024], param_4.1224: f16[4,4,1024]) -> f16[4,4,1024,1024] {
  %param_1.2548 = f16[4,4,1024]{2,1,0} parameter(1)
  %negate.60 = f16[4,4,1024]{2,1,0} negate(f16[4,4,1024]{2,1,0} %param_1.2548), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.93 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %negate.60), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/broadcast_in_dim[shape=(8, 32, 1024, 1024) broadcast_dimensions=(0, 1, 2)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %param_0.2108 = f16[4,4,1024,1024]{3,2,1,0} parameter(0)
  %param_2.2317 = f32[4,4,1024]{2,1,0} parameter(2)
  %convert.670 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %param_2.2317), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.1469 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %convert.670), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.511 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %param_0.2108, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1469), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %add.1245 = f16[4,4,1024,1024]{3,2,1,0} add(f16[4,4,1024,1024]{3,2,1,0} %broadcast.93, f16[4,4,1024,1024]{3,2,1,0} %divide.511), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %param_3.1771 = f16[4,4,1024,1024]{3,2,1,0} parameter(3)
  %param_4.1224 = f16[4,4,1024]{2,1,0} parameter(4)
  %broadcast.2503 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_4.1224), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.294 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_3.1771, f16[4,4,1024,1024]{3,2,1,0} %broadcast.2503), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.89 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.294), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  ROOT %multiply.1804 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %add.1245, f16[4,4,1024,1024]{3,2,1,0} %exponential.89), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%region_102.3080 (Arg_0.3081: f16[], Arg_1.3082: f16[]) -> f16[] {
  %Arg_0.3081 = f16[] parameter(0)
  %Arg_1.3082 = f16[] parameter(1)
  ROOT %add.3083 = f16[] add(f16[] %Arg_0.3081, f16[] %Arg_1.3082), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.399 (param_0.2106: f16[4,4,1024,1024], param_1.2546: f32[4,4,1024], param_2.2316: f16[4,4,1024,1024], param_3.1769: f16[4,4,1024]) -> f16[4,4,1024] {
  %param_0.2106 = f16[4,4,1024,1024]{3,2,1,0} parameter(0)
  %constant_691 = f16[] constant(1)
  %broadcast.1470 = f16[4,4,1024]{2,1,0} broadcast(f16[] %constant_691), dimensions={}
  %param_1.2546 = f32[4,4,1024]{2,1,0} parameter(1)
  %convert.671 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %param_1.2546), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.1807 = f16[4,4,1024]{2,1,0} multiply(f16[4,4,1024]{2,1,0} %convert.671, f16[4,4,1024]{2,1,0} %convert.671), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.512 = f16[4,4,1024]{2,1,0} divide(f16[4,4,1024]{2,1,0} %broadcast.1470, f16[4,4,1024]{2,1,0} %multiply.1807), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.94 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %divide.512), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.1806 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %param_0.2106, f16[4,4,1024,1024]{3,2,1,0} %broadcast.94), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %param_2.2316 = f16[4,4,1024,1024]{3,2,1,0} parameter(2)
  %param_3.1769 = f16[4,4,1024]{2,1,0} parameter(3)
  %broadcast.2501 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_3.1769), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.290 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_2.2316, f16[4,4,1024,1024]{3,2,1,0} %broadcast.2501), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.87 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.290), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.1805 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %multiply.1806, f16[4,4,1024,1024]{3,2,1,0} %exponential.87), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %constant_692 = f16[] constant(0)
  ROOT %reduce.271 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %multiply.1805, f16[] %constant_692), dimensions={3}, to_apply=%region_102.3080, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.400 (param_0.758: f16[4096,128]) -> f16[4,4,1024,32] {
  %param_0.758 = f16[4096,128]{1,0} parameter(0)
  %bitcast.767 = f16[4,1024,4,32]{3,2,1,0} bitcast(f16[4096,128]{1,0} %param_0.758), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %copy.116 = f16[4,1024,4,32]{3,1,2,0} copy(f16[4,1024,4,32]{3,2,1,0} %bitcast.767), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  ROOT %transpose.40 = f16[4,4,1024,32]{3,2,1,0} transpose(f16[4,1024,4,32]{3,1,2,0} %copy.116), dimensions={0,2,1,3}
}

%fused_computation.401 (param_0.2156: f32[4,1024], param_1.2621: f32[8,1024], param_2.2408: s32[], param_3.1858: f32[8,1024], param_4.1306: f32[8,1024], param_5.1319: f32[8,1024], param_6.1102: f32[4,1024], param_7.716: f16[4,1024,1024], param_8.420: f32[], param_9.337: f32[4,1024], param_10.302: f32[], param_11.327: f32[], param_12.322: f32[4,1024], param_13.320: f32[], param_14.331: f32[1024], param_15.342: f16[4,1024,1024], param_16.359: f16[4096,1024]) -> f16[4,1024,1024] {
  %param_7.716 = f16[4,1024,1024]{2,1,0} parameter(7)
  %convert.484 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_7.716), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %param_6.1102 = f32[4,1024]{1,0} parameter(6)
  %bitcast.769 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %param_6.1102), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reshape[new_sizes=(8, 1024, 1) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_11.327 = f32[] parameter(11)
  %broadcast.2535 = f32[4,1024]{1,0} broadcast(f32[] %param_11.327), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_12.322 = f32[4,1024]{1,0} parameter(12)
  %param_8.420 = f32[] parameter(8)
  %broadcast.2534 = f32[4,1024]{1,0} broadcast(f32[] %param_8.420), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.833 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_12.322, f32[4,1024]{1,0} %broadcast.2534), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_9.337 = f32[4,1024]{1,0} parameter(9)
  %param_10.302 = f32[] parameter(10)
  %broadcast.2533 = f32[4,1024]{1,0} broadcast(f32[] %param_10.302), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.832 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_9.337, f32[4,1024]{1,0} %broadcast.2533), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2442 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.832, f32[4,1024]{1,0} %divide.832), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.306 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.833, f32[4,1024]{1,0} %multiply.2442), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.128 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.2535, f32[4,1024]{1,0} %subtract.306), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_13.320 = f32[] parameter(13)
  %broadcast.2532 = f32[4,1024]{1,0} broadcast(f32[] %param_13.320), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.1846 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.128, f32[4,1024]{1,0} %broadcast.2532), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1368 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.1846), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.50 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1368), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %divide.516 = f32[4,1024,1]{1,0,2} divide(f32[4,1024,1]{1,0,2} %rsqrt.50, f32[4,1024,1]{1,0,2} %bitcast.1368), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %constant_693 = f32[] constant(-0.5)
  %broadcast.1474 = f32[4,1024,1]{1,0,2} broadcast(f32[] %constant_693), dimensions={}
  %multiply.1814 = f32[4,1024,1]{1,0,2} multiply(f32[4,1024,1]{1,0,2} %divide.516, f32[4,1024,1]{1,0,2} %broadcast.1474), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.1813 = f32[4,1024,1]{1,0,2} multiply(f32[4,1024,1]{1,0,2} %bitcast.769, f32[4,1024,1]{1,0,2} %multiply.1814), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.768 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %multiply.1813), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reshape[new_sizes=(8, 1024) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=132}
  %compare.60 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %subtract.306, f32[4,1024]{1,0} %maximum.128), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_5.1319 = f32[8,1024]{1,0} parameter(5)
  %param_2.2408 = s32[] parameter(2)
  %constant_695 = s32[] constant(0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %dynamic-slice.42 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_5.1319, s32[] %param_2.2408, s32[] %constant_695), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_4.1306 = f32[8,1024]{1,0} parameter(4)
  %dynamic-slice.41 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_4.1306, s32[] %param_2.2408, s32[] %constant_695), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.54 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.60, f32[4,1024]{1,0} %dynamic-slice.42, f32[4,1024]{1,0} %dynamic-slice.41), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %compare.59 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %broadcast.2535, f32[4,1024]{1,0} %maximum.128), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_3.1858 = f32[8,1024]{1,0} parameter(3)
  %dynamic-slice.40 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_3.1858, s32[] %param_2.2408, s32[] %constant_695), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_1.2621 = f32[8,1024]{1,0} parameter(1)
  %dynamic-slice.39 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_1.2621, s32[] %param_2.2408, s32[] %constant_695), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.53 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.59, f32[4,1024]{1,0} %dynamic-slice.40, f32[4,1024]{1,0} %dynamic-slice.39), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.515 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %select.54, f32[4,1024]{1,0} %select.53), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.1812 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %bitcast.768, f32[4,1024]{1,0} %divide.515), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.514 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %multiply.1812, f32[4,1024]{1,0} %broadcast.2534), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_694 = f32[] constant(2)
  %broadcast.1475 = f32[4,1024]{1,0} broadcast(f32[] %constant_694), dimensions={}
  %multiply.1811 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.514, f32[4,1024]{1,0} %broadcast.1475), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %broadcast.96 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.1811), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.1810 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.484, f32[4,1024,1024]{2,1,0} %broadcast.96), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %negate.61 = f32[4,1024]{1,0} negate(f32[4,1024]{1,0} %multiply.1812), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.1809 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.832, f32[4,1024]{1,0} %broadcast.1475), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.1808 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %negate.61, f32[4,1024]{1,0} %multiply.1809), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %param_0.2156 = f32[4,1024]{1,0} parameter(0)
  %add.1249 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %multiply.1808, f32[4,1024]{1,0} %param_0.2156), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=131}
  %divide.513 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %add.1249, f32[4,1024]{1,0} %broadcast.2533), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.95 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.513), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/broadcast_in_dim[shape=(8, 1024, 1024) broadcast_dimensions=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %add.1248 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.1810, f32[4,1024,1024]{2,1,0} %broadcast.95), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %convert.483 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.1248), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %param_15.342 = f16[4,1024,1024]{2,1,0} parameter(15)
  %param_16.359 = f16[4096,1024]{1,0} parameter(16)
  %bitcast.1422 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_16.359), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.1932 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %param_15.342, f16[4,1024,1024]{2,1,0} %bitcast.1422), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.868 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1932), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %bitcast.1420 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.50), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.2729 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1420), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_14.331 = f32[1024]{0} parameter(14)
  %broadcast.2728 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_14.331), dimensions={2}
  %multiply.2539 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.2729, f32[4,1024,1024]{2,1,0} %broadcast.2728), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2538 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.868, f32[4,1024,1024]{2,1,0} %multiply.2539), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %convert.482 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %multiply.2538), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  ROOT %add.1246 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %convert.483, f16[4,1024,1024]{2,1,0} %convert.482), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%region_100.3049 (Arg_0.3050: f32[], Arg_1.3051: f32[]) -> f32[] {
  %Arg_0.3050 = f32[] parameter(0)
  %Arg_1.3051 = f32[] parameter(1)
  ROOT %add.3052 = f32[] add(f32[] %Arg_0.3050, f32[] %Arg_1.3051), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%region_99.3032 (Arg_0.3033: f32[], Arg_1.3034: f32[]) -> f32[] {
  %Arg_0.3033 = f32[] parameter(0)
  %Arg_1.3034 = f32[] parameter(1)
  ROOT %add.3035 = f32[] add(f32[] %Arg_0.3033, f32[] %Arg_1.3034), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%fused_computation.402 (param_0.2154: f32[1024], param_1.2619: f32[], param_2.2406: f32[4,1024], param_3.1856: f32[], param_4.1304: f32[4,1024], param_5.1317: f32[], param_6.1100: f32[], param_7.714: f16[4,1024,1024], param_8.418: f16[4096,1024], param_9.460: f16[4,1024,1024]) -> (f32[4,1024], f32[4,1024]) {
  %param_7.714 = f16[4,1024,1024]{2,1,0} parameter(7)
  %param_8.418 = f16[4096,1024]{1,0} parameter(8)
  %bitcast.1416 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_8.418), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.1928 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %param_7.714, f16[4,1024,1024]{2,1,0} %bitcast.1416), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.866 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1928), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %param_6.1100 = f32[] parameter(6)
  %broadcast.2721 = f32[4,1024]{1,0} broadcast(f32[] %param_6.1100), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_4.1304 = f32[4,1024]{1,0} parameter(4)
  %param_5.1317 = f32[] parameter(5)
  %broadcast.2720 = f32[4,1024]{1,0} broadcast(f32[] %param_5.1317), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.893 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_4.1304, f32[4,1024]{1,0} %broadcast.2720), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.2406 = f32[4,1024]{1,0} parameter(2)
  %param_3.1856 = f32[] parameter(3)
  %broadcast.2719 = f32[4,1024]{1,0} broadcast(f32[] %param_3.1856), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.892 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_2.2406, f32[4,1024]{1,0} %broadcast.2719), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2534 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.892, f32[4,1024]{1,0} %divide.892), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.334 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.893, f32[4,1024]{1,0} %multiply.2534), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.148 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.2721, f32[4,1024]{1,0} %subtract.334), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_1.2619 = f32[] parameter(1)
  %broadcast.2718 = f32[4,1024]{1,0} broadcast(f32[] %param_1.2619), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.1927 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.148, f32[4,1024]{1,0} %broadcast.2718), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1415 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.1927), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.120 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1415), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1414 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.120), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.2717 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1414), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_0.2154 = f32[1024]{0} parameter(0)
  %broadcast.2716 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_0.2154), dimensions={2}
  %multiply.2533 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.2717, f32[4,1024,1024]{2,1,0} %broadcast.2716), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2532 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.866, f32[4,1024,1024]{2,1,0} %multiply.2533), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %negate.62 = f32[4,1024,1024]{2,1,0} negate(f32[4,1024,1024]{2,1,0} %multiply.2532), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %constant_697 = f32[] constant(0)
  %reduce.272 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %negate.62, f32[] %constant_697), dimensions={2}, to_apply=%region_100.3049, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_9.460 = f16[4,1024,1024]{2,1,0} parameter(9)
  %convert.872.clone.1 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_9.460), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %broadcast.2736.clone.1 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.892), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.338.clone.1 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.872.clone.1, f32[4,1024,1024]{2,1,0} %broadcast.2736.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.2542.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.338.clone.1, f32[4,1024,1024]{2,1,0} %convert.866), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.1816.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %multiply.2542.clone.1, f32[4,1024,1024]{2,1,0} %broadcast.2716), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.273.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1816.clone.1, f32[] %constant_697), dimensions={2}, to_apply=%region_99.3032, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  ROOT %tuple.194 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) tuple(f32[4,1024]{1,0} %reduce.272, f32[4,1024]{1,0} %reduce.273.clone.1)
}

%fused_computation.407 (param_0.2125: f16[4096,512], param_1.2577: f16[], param_2.2353: f16[], param_3.1810: f16[4096,512], param_4.1263: f16[]) -> f16[4,1024,512] {
  %param_3.1810 = f16[4096,512]{1,0} parameter(3)
  %bitcast.990 = f16[4,1024,512]{2,1,0} bitcast(f16[4096,512]{1,0} %param_3.1810), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_0.2125 = f16[4096,512]{1,0} parameter(0)
  %param_1.2577 = f16[] parameter(1)
  %broadcast.97 = f16[4096,512]{1,0} broadcast(f16[] %param_1.2577), dimensions={}
  %divide.518 = f16[4096,512]{1,0} divide(f16[4096,512]{1,0} %param_0.2125, f16[4096,512]{1,0} %broadcast.97), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %bitcast.771 = f16[4,1024,512]{2,1,0} bitcast(f16[4096,512]{1,0} %divide.518), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1822 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %bitcast.990, f16[4,1024,512]{2,1,0} %bitcast.771), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_698 = f16[] constant(1.1279)
  %broadcast.1479 = f16[4,1024,512]{2,1,0} broadcast(f16[] %constant_698), dimensions={}
  %multiply.1821 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.1822, f16[4,1024,512]{2,1,0} %broadcast.1479), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %param_2.2353 = f16[] parameter(2)
  %broadcast.1478 = f16[4,1024,512]{2,1,0} broadcast(f16[] %param_2.2353), dimensions={}
  %divide.608 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %bitcast.990, f16[4,1024,512]{2,1,0} %broadcast.1478), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1820 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %divide.608, f16[4,1024,512]{2,1,0} %divide.608), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %negate.63 = f16[4,1024,512]{2,1,0} negate(f16[4,1024,512]{2,1,0} %multiply.1820), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/neg" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %exponential.25 = f16[4,1024,512]{2,1,0} exponential(f16[4,1024,512]{2,1,0} %negate.63), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/exp" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1819 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.1821, f16[4,1024,512]{2,1,0} %exponential.25), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.517 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %multiply.1819, f16[4,1024,512]{2,1,0} %broadcast.1478), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1498 = f32[] constant(-4)
  %broadcast.2633 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1498), dimensions={}
  %convert.852 = f32[4,1024,512]{2,1,0} convert(f16[4,1024,512]{2,1,0} %divide.608), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1497 = f32[] constant(4)
  %broadcast.2631 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1497), dimensions={}
  %clamp.37 = f32[4,1024,512]{2,1,0} clamp(f32[4,1024,512]{2,1,0} %broadcast.2633, f32[4,1024,512]{2,1,0} %convert.852, f32[4,1024,512]{2,1,0} %broadcast.2631), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2500 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.37, f32[4,1024,512]{2,1,0} %clamp.37), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1496 = f32[] constant(0)
  %broadcast.2630 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1496), dimensions={}
  %multiply.2499 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %multiply.2500, f32[4,1024,512]{2,1,0} %broadcast.2630), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1495 = f32[] constant(-2.72614237e-10)
  %broadcast.2629 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1495), dimensions={}
  %add.1906 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2499, f32[4,1024,512]{2,1,0} %broadcast.2629), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2498 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1906, f32[4,1024,512]{2,1,0} %multiply.2500), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1494 = f32[] constant(2.77068146e-08)
  %broadcast.2628 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1494), dimensions={}
  %add.1905 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2498, f32[4,1024,512]{2,1,0} %broadcast.2628), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2497 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1905, f32[4,1024,512]{2,1,0} %multiply.2500), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1493 = f32[] constant(-2.10102394e-06)
  %broadcast.2627 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1493), dimensions={}
  %add.1904 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2497, f32[4,1024,512]{2,1,0} %broadcast.2627), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2496 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1904, f32[4,1024,512]{2,1,0} %multiply.2500), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1492 = f32[] constant(-5.69250624e-05)
  %broadcast.2626 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1492), dimensions={}
  %add.1903 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2496, f32[4,1024,512]{2,1,0} %broadcast.2626), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2495 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1903, f32[4,1024,512]{2,1,0} %multiply.2500), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1491 = f32[] constant(-0.000734990637)
  %broadcast.2625 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1491), dimensions={}
  %add.1902 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2495, f32[4,1024,512]{2,1,0} %broadcast.2625), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2494 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1902, f32[4,1024,512]{2,1,0} %multiply.2500), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1490 = f32[] constant(-0.0029546)
  %broadcast.2624 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1490), dimensions={}
  %add.1901 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2494, f32[4,1024,512]{2,1,0} %broadcast.2624), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2493 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1901, f32[4,1024,512]{2,1,0} %multiply.2500), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1489 = f32[] constant(-0.0160960332)
  %broadcast.2623 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1489), dimensions={}
  %add.1900 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2493, f32[4,1024,512]{2,1,0} %broadcast.2623), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2492 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.37, f32[4,1024,512]{2,1,0} %add.1900), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1488 = f32[] constant(-1.45660715e-05)
  %broadcast.2622 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1488), dimensions={}
  %add.1899 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2499, f32[4,1024,512]{2,1,0} %broadcast.2622), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2491 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1899, f32[4,1024,512]{2,1,0} %multiply.2500), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1487 = f32[] constant(-0.000213374049)
  %broadcast.2621 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1487), dimensions={}
  %add.1898 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2491, f32[4,1024,512]{2,1,0} %broadcast.2621), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2490 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1898, f32[4,1024,512]{2,1,0} %multiply.2500), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1486 = f32[] constant(-0.00168282702)
  %broadcast.2620 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1486), dimensions={}
  %add.1897 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2490, f32[4,1024,512]{2,1,0} %broadcast.2620), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2489 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1897, f32[4,1024,512]{2,1,0} %multiply.2500), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1485 = f32[] constant(-0.00737332925)
  %broadcast.2619 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1485), dimensions={}
  %add.1896 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2489, f32[4,1024,512]{2,1,0} %broadcast.2619), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2488 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1896, f32[4,1024,512]{2,1,0} %multiply.2500), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1484 = f32[] constant(-0.0142647391)
  %broadcast.2618 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1484), dimensions={}
  %add.1895 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2488, f32[4,1024,512]{2,1,0} %broadcast.2618), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.852 = f32[4,1024,512]{2,1,0} divide(f32[4,1024,512]{2,1,0} %multiply.2492, f32[4,1024,512]{2,1,0} %add.1895), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.851 = f16[4,1024,512]{2,1,0} convert(f32[4,1024,512]{2,1,0} %divide.852), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %param_4.1263 = f16[] parameter(4)
  %broadcast.2617 = f16[4,1024,512]{2,1,0} broadcast(f16[] %param_4.1263), dimensions={}
  %add.1894 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %convert.851, f16[4,1024,512]{2,1,0} %broadcast.2617), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1818 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %bitcast.771, f16[4,1024,512]{2,1,0} %add.1894), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  ROOT %add.1251 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %divide.517, f16[4,1024,512]{2,1,0} %multiply.1818), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
}

%fused_computation.408 (param_0.2141: f32[4,1024], param_1.2602: f32[8,1024], param_2.2387: s32[], param_3.1841: f32[8,1024], param_4.1291: f32[8,1024], param_5.1308: f32[8,1024], param_6.1091: f32[4,1024], param_7.703: f16[4,1024,1024], param_8.410: f32[], param_9.336: f32[4,1024], param_10.301: f32[], param_11.326: f32[], param_12.321: f32[4,1024], param_13.319: f32[], param_14.329: f32[1024], param_15.337: f16[4,1024,1024]) -> f16[4,1024,1024] {
  %param_7.703 = f16[4,1024,1024]{2,1,0} parameter(7)
  %convert.489 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_7.703), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %param_6.1091 = f32[4,1024]{1,0} parameter(6)
  %bitcast.773 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %param_6.1091), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reshape[new_sizes=(8, 1024, 1) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_11.326 = f32[] parameter(11)
  %broadcast.2657 = f32[4,1024]{1,0} broadcast(f32[] %param_11.326), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_12.321 = f32[4,1024]{1,0} parameter(12)
  %param_8.410 = f32[] parameter(8)
  %broadcast.2656 = f32[4,1024]{1,0} broadcast(f32[] %param_8.410), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.869 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_12.321, f32[4,1024]{1,0} %broadcast.2656), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_9.336 = f32[4,1024]{1,0} parameter(9)
  %param_10.301 = f32[] parameter(10)
  %broadcast.2655 = f32[4,1024]{1,0} broadcast(f32[] %param_10.301), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.868 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_9.336, f32[4,1024]{1,0} %broadcast.2655), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2508 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.868, f32[4,1024]{1,0} %divide.868), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.320 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.869, f32[4,1024]{1,0} %multiply.2508), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.138 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.2657, f32[4,1024]{1,0} %subtract.320), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_13.319 = f32[] parameter(13)
  %broadcast.2654 = f32[4,1024]{1,0} broadcast(f32[] %param_13.319), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.1910 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.138, f32[4,1024]{1,0} %broadcast.2654), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1388 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.1910), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.51 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1388), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %divide.522 = f32[4,1024,1]{1,0,2} divide(f32[4,1024,1]{1,0,2} %rsqrt.51, f32[4,1024,1]{1,0,2} %bitcast.1388), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %constant_699 = f32[] constant(-0.5)
  %broadcast.1483 = f32[4,1024,1]{1,0,2} broadcast(f32[] %constant_699), dimensions={}
  %multiply.1829 = f32[4,1024,1]{1,0,2} multiply(f32[4,1024,1]{1,0,2} %divide.522, f32[4,1024,1]{1,0,2} %broadcast.1483), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.1828 = f32[4,1024,1]{1,0,2} multiply(f32[4,1024,1]{1,0,2} %bitcast.773, f32[4,1024,1]{1,0,2} %multiply.1829), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.772 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %multiply.1828), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reshape[new_sizes=(8, 1024) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=132}
  %compare.62 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %subtract.320, f32[4,1024]{1,0} %maximum.138), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_5.1308 = f32[8,1024]{1,0} parameter(5)
  %param_2.2387 = s32[] parameter(2)
  %constant_701 = s32[] constant(0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %dynamic-slice.46 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_5.1308, s32[] %param_2.2387, s32[] %constant_701), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_4.1291 = f32[8,1024]{1,0} parameter(4)
  %dynamic-slice.45 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_4.1291, s32[] %param_2.2387, s32[] %constant_701), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.56 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.62, f32[4,1024]{1,0} %dynamic-slice.46, f32[4,1024]{1,0} %dynamic-slice.45), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %compare.61 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %broadcast.2657, f32[4,1024]{1,0} %maximum.138), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_3.1841 = f32[8,1024]{1,0} parameter(3)
  %dynamic-slice.44 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_3.1841, s32[] %param_2.2387, s32[] %constant_701), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_1.2602 = f32[8,1024]{1,0} parameter(1)
  %dynamic-slice.43 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_1.2602, s32[] %param_2.2387, s32[] %constant_701), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.55 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.61, f32[4,1024]{1,0} %dynamic-slice.44, f32[4,1024]{1,0} %dynamic-slice.43), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.521 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %select.56, f32[4,1024]{1,0} %select.55), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.1827 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %bitcast.772, f32[4,1024]{1,0} %divide.521), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.520 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %multiply.1827, f32[4,1024]{1,0} %broadcast.2656), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_700 = f32[] constant(2)
  %broadcast.1484 = f32[4,1024]{1,0} broadcast(f32[] %constant_700), dimensions={}
  %multiply.1826 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.520, f32[4,1024]{1,0} %broadcast.1484), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %broadcast.99 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.1826), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.1825 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.489, f32[4,1024,1024]{2,1,0} %broadcast.99), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %negate.64 = f32[4,1024]{1,0} negate(f32[4,1024]{1,0} %multiply.1827), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.1824 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.868, f32[4,1024]{1,0} %broadcast.1484), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.1823 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %negate.64, f32[4,1024]{1,0} %multiply.1824), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %param_0.2141 = f32[4,1024]{1,0} parameter(0)
  %add.1254 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %multiply.1823, f32[4,1024]{1,0} %param_0.2141), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=131}
  %divide.519 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %add.1254, f32[4,1024]{1,0} %broadcast.2655), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.98 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.519), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/broadcast_in_dim[shape=(8, 1024, 1024) broadcast_dimensions=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %add.1253 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.1825, f32[4,1024,1024]{2,1,0} %broadcast.98), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %convert.488 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.1253), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %param_15.337 = f16[4,1024,1024]{2,1,0} parameter(15)
  %convert.856 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_15.337), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %bitcast.1403 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.51), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.2697 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1403), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_14.329 = f32[1024]{0} parameter(14)
  %broadcast.2696 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_14.329), dimensions={2}
  %multiply.2523 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.2697, f32[4,1024,1024]{2,1,0} %broadcast.2696), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2522 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.856, f32[4,1024,1024]{2,1,0} %multiply.2523), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %convert.487 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %multiply.2522), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  ROOT %add.1252 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %convert.488, f16[4,1024,1024]{2,1,0} %convert.487), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%region_94.2969 (Arg_0.2970: f32[], Arg_1.2971: f32[]) -> f32[] {
  %Arg_0.2970 = f32[] parameter(0)
  %Arg_1.2971 = f32[] parameter(1)
  ROOT %add.2972 = f32[] add(f32[] %Arg_0.2970, f32[] %Arg_1.2971), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%region_93.2952 (Arg_0.2953: f32[], Arg_1.2954: f32[]) -> f32[] {
  %Arg_0.2953 = f32[] parameter(0)
  %Arg_1.2954 = f32[] parameter(1)
  ROOT %add.2955 = f32[] add(f32[] %Arg_0.2953, f32[] %Arg_1.2954), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%fused_computation.409 (param_0.2139: f32[1024], param_1.2600: f32[], param_2.2385: f32[4,1024], param_3.1839: f32[], param_4.1289: f32[4,1024], param_5.1306: f32[], param_6.1089: f32[], param_7.701: f16[4,1024,1024], param_8.608: f16[4,1024,1024]) -> (f32[4,1024], f32[4,1024]) {
  %param_7.701 = f16[4,1024,1024]{2,1,0} parameter(7)
  %convert.854 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_7.701), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %param_6.1089 = f32[] parameter(6)
  %broadcast.2689 = f32[4,1024]{1,0} broadcast(f32[] %param_6.1089), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_4.1289 = f32[4,1024]{1,0} parameter(4)
  %param_5.1306 = f32[] parameter(5)
  %broadcast.2688 = f32[4,1024]{1,0} broadcast(f32[] %param_5.1306), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.881 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_4.1289, f32[4,1024]{1,0} %broadcast.2688), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.2385 = f32[4,1024]{1,0} parameter(2)
  %param_3.1839 = f32[] parameter(3)
  %broadcast.2687 = f32[4,1024]{1,0} broadcast(f32[] %param_3.1839), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.880 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_2.2385, f32[4,1024]{1,0} %broadcast.2687), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2518 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.880, f32[4,1024]{1,0} %divide.880), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.326 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.881, f32[4,1024]{1,0} %multiply.2518), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.144 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.2689, f32[4,1024]{1,0} %subtract.326), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_1.2600 = f32[] parameter(1)
  %broadcast.2686 = f32[4,1024]{1,0} broadcast(f32[] %param_1.2600), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.1916 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.144, f32[4,1024]{1,0} %broadcast.2686), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1400 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.1916), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.116 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1400), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1399 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.116), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.2685 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1399), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_0.2139 = f32[1024]{0} parameter(0)
  %broadcast.2684 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_0.2139), dimensions={2}
  %multiply.2517 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.2685, f32[4,1024,1024]{2,1,0} %broadcast.2684), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2516 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.854, f32[4,1024,1024]{2,1,0} %multiply.2517), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %negate.65 = f32[4,1024,1024]{2,1,0} negate(f32[4,1024,1024]{2,1,0} %multiply.2516), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %constant_703 = f32[] constant(0)
  %reduce.274 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %negate.65, f32[] %constant_703), dimensions={2}, to_apply=%region_94.2969, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_8.608 = f16[4,1024,1024]{2,1,0} parameter(8)
  %convert.860.clone.1 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_8.608), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %broadcast.2704.clone.1 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.880), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.330.clone.1 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.860.clone.1, f32[4,1024,1024]{2,1,0} %broadcast.2704.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.2526.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.330.clone.1, f32[4,1024,1024]{2,1,0} %convert.854), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.1834.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %multiply.2526.clone.1, f32[4,1024,1024]{2,1,0} %broadcast.2684), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.276.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1834.clone.1, f32[] %constant_703), dimensions={2}, to_apply=%region_93.2952, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  ROOT %tuple.196 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) tuple(f32[4,1024]{1,0} %reduce.274, f32[4,1024]{1,0} %reduce.276.clone.1)
}

%region_89.2871 (Arg_0.2872: f32[], Arg_1.2873: f32[]) -> f32[] {
  %Arg_0.2872 = f32[] parameter(0)
  %Arg_1.2873 = f32[] parameter(1)
  ROOT %add.2874 = f32[] add(f32[] %Arg_0.2872, f32[] %Arg_1.2873), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_90.2884 (Arg_0.2885: f32[], Arg_1.2886: f32[]) -> f32[] {
  %Arg_0.2885 = f32[] parameter(0)
  %Arg_1.2886 = f32[] parameter(1)
  ROOT %add.2887 = f32[] add(f32[] %Arg_0.2885, f32[] %Arg_1.2886), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%fused_computation.417 (param_0.2413: f16[4,1024,1024], param_1.3062: f32[1024], param_2.2960: f16[4096,1024]) -> (f32[4,1024], f32[4,1024], f16[4,1024,1024]) {
  %param_2.2960 = f16[4096,1024]{1,0} parameter(2)
  %bitcast.776.clone.1 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_2.2960), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_1.3062 = f32[1024]{0} parameter(1)
  %convert.495.clone.1 = f16[1024]{0} convert(f32[1024]{0} %param_1.3062), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.103.clone.1 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.495.clone.1), dimensions={2}
  %add.1257.clone.1 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.776.clone.1, f16[4,1024,1024]{2,1,0} %broadcast.103.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_0.2413 = f16[4,1024,1024]{2,1,0} parameter(0)
  %add.1256.clone.1 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1257.clone.1, f16[4,1024,1024]{2,1,0} %param_0.2413), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=310}
  %convert.494 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1256.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %constant_705 = f32[] constant(0)
  %reduce.277 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.494, f32[] %constant_705), dimensions={2}, to_apply=%region_89.2871, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.1833.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.494, f32[4,1024,1024]{2,1,0} %convert.494), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.275.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1833.clone.1, f32[] %constant_705), dimensions={2}, to_apply=%region_90.2884, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  ROOT %tuple.198 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) tuple(f32[4,1024]{1,0} %reduce.277, f32[4,1024]{1,0} %reduce.275.clone.1, f16[4,1024,1024]{2,1,0} %add.1256.clone.1)
}

%fused_computation.419 (param_0.796: f16[], param_1.2575: f16[4096,512], param_2.2351: f16[], param_3.1809: f16[]) -> f16[4096,512] {
  %param_1.2575 = f16[4096,512]{1,0} parameter(1)
  %bitcast.991 = f16[4,1024,512]{2,1,0} bitcast(f16[4096,512]{1,0} %param_1.2575), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %constant_1467 = f32[] constant(-4)
  %broadcast.2599 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1467), dimensions={}
  %param_3.1809 = f16[] parameter(3)
  %broadcast.2598 = f16[4,1024,512]{2,1,0} broadcast(f16[] %param_3.1809), dimensions={}
  %divide.849 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %bitcast.991, f16[4,1024,512]{2,1,0} %broadcast.2598), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.848 = f32[4,1024,512]{2,1,0} convert(f16[4,1024,512]{2,1,0} %divide.849), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1466 = f32[] constant(4)
  %broadcast.2597 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1466), dimensions={}
  %clamp.35 = f32[4,1024,512]{2,1,0} clamp(f32[4,1024,512]{2,1,0} %broadcast.2599, f32[4,1024,512]{2,1,0} %convert.848, f32[4,1024,512]{2,1,0} %broadcast.2597), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2474 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.35, f32[4,1024,512]{2,1,0} %clamp.35), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1465 = f32[] constant(0)
  %broadcast.2596 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1465), dimensions={}
  %multiply.2473 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %multiply.2474, f32[4,1024,512]{2,1,0} %broadcast.2596), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1464 = f32[] constant(-2.72614237e-10)
  %broadcast.2595 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1464), dimensions={}
  %add.1880 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2473, f32[4,1024,512]{2,1,0} %broadcast.2595), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2472 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1880, f32[4,1024,512]{2,1,0} %multiply.2474), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1463 = f32[] constant(2.77068146e-08)
  %broadcast.2594 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1463), dimensions={}
  %add.1879 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2472, f32[4,1024,512]{2,1,0} %broadcast.2594), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2471 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1879, f32[4,1024,512]{2,1,0} %multiply.2474), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1462 = f32[] constant(-2.10102394e-06)
  %broadcast.2593 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1462), dimensions={}
  %add.1878 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2471, f32[4,1024,512]{2,1,0} %broadcast.2593), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2470 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1878, f32[4,1024,512]{2,1,0} %multiply.2474), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1461 = f32[] constant(-5.69250624e-05)
  %broadcast.2592 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1461), dimensions={}
  %add.1877 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2470, f32[4,1024,512]{2,1,0} %broadcast.2592), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2469 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1877, f32[4,1024,512]{2,1,0} %multiply.2474), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1460 = f32[] constant(-0.000734990637)
  %broadcast.2591 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1460), dimensions={}
  %add.1876 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2469, f32[4,1024,512]{2,1,0} %broadcast.2591), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2468 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1876, f32[4,1024,512]{2,1,0} %multiply.2474), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1459 = f32[] constant(-0.0029546)
  %broadcast.2590 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1459), dimensions={}
  %add.1875 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2468, f32[4,1024,512]{2,1,0} %broadcast.2590), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2467 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1875, f32[4,1024,512]{2,1,0} %multiply.2474), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1458 = f32[] constant(-0.0160960332)
  %broadcast.2589 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1458), dimensions={}
  %add.1874 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2467, f32[4,1024,512]{2,1,0} %broadcast.2589), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2466 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.35, f32[4,1024,512]{2,1,0} %add.1874), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1457 = f32[] constant(-1.45660715e-05)
  %broadcast.2588 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1457), dimensions={}
  %add.1873 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2473, f32[4,1024,512]{2,1,0} %broadcast.2588), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2465 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1873, f32[4,1024,512]{2,1,0} %multiply.2474), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1456 = f32[] constant(-0.000213374049)
  %broadcast.2587 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1456), dimensions={}
  %add.1872 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2465, f32[4,1024,512]{2,1,0} %broadcast.2587), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2464 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1872, f32[4,1024,512]{2,1,0} %multiply.2474), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1455 = f32[] constant(-0.00168282702)
  %broadcast.2586 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1455), dimensions={}
  %add.1871 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2464, f32[4,1024,512]{2,1,0} %broadcast.2586), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2463 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1871, f32[4,1024,512]{2,1,0} %multiply.2474), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1454 = f32[] constant(-0.00737332925)
  %broadcast.2585 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1454), dimensions={}
  %add.1869 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2463, f32[4,1024,512]{2,1,0} %broadcast.2585), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2462 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1869, f32[4,1024,512]{2,1,0} %multiply.2474), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1453 = f32[] constant(-0.0142647391)
  %broadcast.2584 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1453), dimensions={}
  %add.1868 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2462, f32[4,1024,512]{2,1,0} %broadcast.2584), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.848 = f32[4,1024,512]{2,1,0} divide(f32[4,1024,512]{2,1,0} %multiply.2466, f32[4,1024,512]{2,1,0} %add.1868), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.847 = f16[4,1024,512]{2,1,0} convert(f32[4,1024,512]{2,1,0} %divide.848), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %param_2.2351 = f16[] parameter(2)
  %broadcast.2583 = f16[4,1024,512]{2,1,0} broadcast(f16[] %param_2.2351), dimensions={}
  %add.1867 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %convert.847, f16[4,1024,512]{2,1,0} %broadcast.2583), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1836 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %bitcast.991, f16[4,1024,512]{2,1,0} %add.1867), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %param_0.796 = f16[] parameter(0)
  %broadcast.104 = f16[4,1024,512]{2,1,0} broadcast(f16[] %param_0.796), dimensions={}
  %divide.524 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %multiply.1836, f16[4,1024,512]{2,1,0} %broadcast.104), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  ROOT %bitcast.777 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %divide.524)
}

%fused_computation.421 (param_0.801: f32[512]) -> f16[4096,512] {
  %param_0.801 = f32[512]{0} parameter(0)
  %convert.498 = f16[512]{0} convert(f32[512]{0} %param_0.801), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.106 = f16[4,1024,512]{2,1,0} broadcast(f16[512]{0} %convert.498), dimensions={2}
  ROOT %bitcast.778 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %broadcast.106)
}

%fused_computation.422 (param_0.804: f32[1024], param_1.2564: f32[1024], param_2.2336: f32[4,1024], param_3.1791: f32[], param_4.1239: f16[4,1024,1024], param_5.1245: f32[], param_6.1037: f32[4,1024], param_7.660: f32[], param_8.384: f32[]) -> f16[4,1024,1024] {
  %param_4.1239 = f16[4,1024,1024]{2,1,0} parameter(4)
  %convert.842 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_4.1239), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %param_2.2336 = f32[4,1024]{1,0} parameter(2)
  %param_3.1791 = f32[] parameter(3)
  %broadcast.2507 = f32[4,1024]{1,0} broadcast(f32[] %param_3.1791), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.815 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_2.2336, f32[4,1024]{1,0} %broadcast.2507), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.2506 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.815), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.296 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.842, f32[4,1024,1024]{2,1,0} %broadcast.2506), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_8.384 = f32[] parameter(8)
  %broadcast.2545 = f32[4,1024]{1,0} broadcast(f32[] %param_8.384), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_6.1037 = f32[4,1024]{1,0} parameter(6)
  %param_7.660 = f32[] parameter(7)
  %broadcast.2544 = f32[4,1024]{1,0} broadcast(f32[] %param_7.660), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.837 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_6.1037, f32[4,1024]{1,0} %broadcast.2544), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.2444 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.815, f32[4,1024]{1,0} %divide.815), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.308 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.837, f32[4,1024]{1,0} %multiply.2444), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.130 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.2545, f32[4,1024]{1,0} %subtract.308), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_5.1245 = f32[] parameter(5)
  %broadcast.2542 = f32[4,1024]{1,0} broadcast(f32[] %param_5.1245), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.1848 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.130, f32[4,1024]{1,0} %broadcast.2542), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1372 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.1848), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.106 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1372), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1371 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.106), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.2541 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1371), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_1.2564 = f32[1024]{0} parameter(1)
  %broadcast.1507 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_1.2564), dimensions={2}
  %multiply.2149 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.2541, f32[4,1024,1024]{2,1,0} %broadcast.1507), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.1850 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.296, f32[4,1024,1024]{2,1,0} %multiply.2149), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.804 = f32[1024]{0} parameter(0)
  %broadcast.107 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_0.804), dimensions={2}
  %add.1272 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.1850, f32[4,1024,1024]{2,1,0} %broadcast.107), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  ROOT %convert.499 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.1272), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
}

%region_87.2772 (Arg_0.2773: f32[], Arg_1.2774: f32[]) -> f32[] {
  %Arg_0.2773 = f32[] parameter(0)
  %Arg_1.2774 = f32[] parameter(1)
  ROOT %add.2775 = f32[] add(f32[] %Arg_0.2773, f32[] %Arg_1.2774), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_88.2785 (Arg_0.2786: f32[], Arg_1.2787: f32[]) -> f32[] {
  %Arg_0.2786 = f32[] parameter(0)
  %Arg_1.2787 = f32[] parameter(1)
  ROOT %add.2788 = f32[] add(f32[] %Arg_0.2786, f32[] %Arg_1.2787), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%fused_computation.428 (param_0.2415: f16[4,1024,1024], param_1.3067: f32[1024], param_2.2965: f16[4096,1024]) -> (f32[4,1024], f32[4,1024], f16[4,1024,1024]) {
  %param_2.2965 = f16[4096,1024]{1,0} parameter(2)
  %bitcast.781.clone.1 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_2.2965), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_1.3067 = f32[1024]{0} parameter(1)
  %convert.503.clone.1 = f16[1024]{0} convert(f32[1024]{0} %param_1.3067), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.111.clone.1 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.503.clone.1), dimensions={2}
  %add.1275.clone.1 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.781.clone.1, f16[4,1024,1024]{2,1,0} %broadcast.111.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_0.2415 = f16[4,1024,1024]{2,1,0} parameter(0)
  %add.1274.clone.1 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1275.clone.1, f16[4,1024,1024]{2,1,0} %param_0.2415), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=233}
  %convert.502 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1274.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %constant_722 = f32[] constant(0)
  %reduce.279 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.502, f32[] %constant_722), dimensions={2}, to_apply=%region_87.2772, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.1852.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.502, f32[4,1024,1024]{2,1,0} %convert.502), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.278.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1852.clone.1, f32[] %constant_722), dimensions={2}, to_apply=%region_88.2785, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  ROOT %tuple.200 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) tuple(f32[4,1024]{1,0} %reduce.279, f32[4,1024]{1,0} %reduce.278.clone.1, f16[4,1024,1024]{2,1,0} %add.1274.clone.1)
}

%fused_computation.430 (param_0.821: f16[4,4,32,1024]) -> f16[4096,128] {
  %param_0.821 = f16[4,4,32,1024]{3,2,1,0} parameter(0)
  %transpose.41 = f16[4,1024,4,32]{1,3,2,0} transpose(f16[4,4,32,1024]{3,2,1,0} %param_0.821), dimensions={0,3,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %copy.117 = f16[4,1024,4,32]{3,2,1,0} copy(f16[4,1024,4,32]{1,3,2,0} %transpose.41), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  ROOT %bitcast.782 = f16[4096,128]{1,0} bitcast(f16[4,1024,4,32]{3,2,1,0} %copy.117)
}

%region_86.2747 (Arg_0.2748: f32[], Arg_1.2749: f32[]) -> f32[] {
  %Arg_0.2748 = f32[] parameter(0)
  %Arg_1.2749 = f32[] parameter(1)
  ROOT %add.2750 = f32[] add(f32[] %Arg_0.2748, f32[] %Arg_1.2749), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.431 (param_0.2102: f16[4,4,1024,1024], param_1.2541: f16[4,4,1024]) -> f32[4,4,1024] {
  %param_0.2102 = f16[4,4,1024,1024]{3,2,1,0} parameter(0)
  %param_1.2541 = f16[4,4,1024]{2,1,0} parameter(1)
  %broadcast.2497 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_1.2541), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.286 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_0.2102, f16[4,4,1024,1024]{3,2,1,0} %broadcast.2497), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.83 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.286), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.504 = f32[4,4,1024,1024]{3,2,1,0} convert(f16[4,4,1024,1024]{3,2,1,0} %exponential.83), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %constant_723 = f32[] constant(0)
  ROOT %reduce.280 = f32[4,4,1024]{2,1,0} reduce(f32[4,4,1024,1024]{3,2,1,0} %convert.504, f32[] %constant_723), dimensions={3}, to_apply=%region_86.2747, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.433 (param_0.829: f16[8,1,1,1024], param_1.1166: s32[], param_2.1762: f16[8,1,1,1024], param_3.1412: s32[4,1024], param_4.892: s32[]) -> f16[4,4,1024,1024] {
  %param_3.1412 = s32[4,1024]{1,0} parameter(3)
  %param_4.892 = s32[] parameter(4)
  %broadcast.114 = s32[4,1024]{1,0} broadcast(s32[] %param_4.892), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %compare.63 = pred[4,1024]{1,0} compare(s32[4,1024]{1,0} %param_3.1412, s32[4,1024]{1,0} %broadcast.114), direction=GT, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %bitcast.785 = pred[4,1,1,1024]{3,0,2,1} bitcast(pred[4,1024]{1,0} %compare.63), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %param_2.1762 = f16[8,1,1,1024]{3,0,2,1} parameter(2)
  %param_1.1166 = s32[] parameter(1)
  %constant_724 = s32[] constant(0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %dynamic-slice.48 = f16[4,1,1,1024]{3,0,2,1} dynamic-slice(f16[8,1,1,1024]{3,0,2,1} %param_2.1762, s32[] %param_1.1166, s32[] %constant_724, s32[] %constant_724, s32[] %constant_724), dynamic_slice_sizes={4,1,1,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %param_0.829 = f16[8,1,1,1024]{3,0,2,1} parameter(0)
  %dynamic-slice.47 = f16[4,1,1,1024]{3,0,2,1} dynamic-slice(f16[8,1,1,1024]{3,0,2,1} %param_0.829, s32[] %param_1.1166, s32[] %constant_724, s32[] %constant_724, s32[] %constant_724), dynamic_slice_sizes={4,1,1,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %select.57 = f16[4,1,1,1024]{3,0,2,1} select(pred[4,1,1,1024]{3,0,2,1} %bitcast.785, f16[4,1,1,1024]{3,0,2,1} %dynamic-slice.48, f16[4,1,1,1024]{3,0,2,1} %dynamic-slice.47), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %bitcast.784 = f16[4,1024]{1,0} bitcast(f16[4,1,1,1024]{3,0,2,1} %select.57), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=100}
  ROOT %broadcast.113 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,1024]{1,0} %bitcast.784), dimensions={0,3}
}

%fused_computation.435 (param_0.2090: f16[4096,384]) -> (f16[4,4,32,1024], f16[4,4,32,1024]) {
  %param_0.2090 = f16[4096,384]{1,0} parameter(0)
  %bitcast.1352 = f16[4,1024,384]{2,1,0} bitcast(f16[4096,384]{1,0} %param_0.2090), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %bitcast.1351 = f16[4,1024,128,3]{3,2,1,0} bitcast(f16[4,1024,384]{2,1,0} %bitcast.1352), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %copy.209 = f16[4,1024,128,3]{1,2,0,3} copy(f16[4,1024,128,3]{3,2,1,0} %bitcast.1351), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %slice.43 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.209), slice={[0:4], [0:1024], [0:128], [1:2]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/slice[start_indices=(0, 0, 0, 1) limit_indices=(8, 1024, 1024, 2) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.42 = f16[1,4,128,1024]{3,2,1,0} transpose(f16[4,1024,128,1]{1,2,0,3} %slice.43), dimensions={3,0,2,1}
  %bitcast.788 = f16[4,4,32,1024]{3,2,1,0} bitcast(f16[1,4,128,1024]{3,2,1,0} %transpose.42)
  %slice.73.clone.1 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.209), slice={[0:4], [0:1024], [0:128], [2:3]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/slice[start_indices=(0, 0, 0, 2) limit_indices=(8, 1024, 1024, 3) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.43 = f16[1,4,128,1024]{3,2,1,0} transpose(f16[4,1024,128,1]{1,2,0,3} %slice.73.clone.1), dimensions={3,0,2,1}
  %bitcast.993.clone.1 = f16[4,4,32,1024]{3,2,1,0} bitcast(f16[1,4,128,1024]{3,2,1,0} %transpose.43)
  ROOT %tuple.202 = (f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) tuple(f16[4,4,32,1024]{3,2,1,0} %bitcast.788, f16[4,4,32,1024]{3,2,1,0} %bitcast.993.clone.1)
}

%fused_computation.437 (param_0.842: f32[384]) -> f16[4096,384] {
  %param_0.842 = f32[384]{0} parameter(0)
  %convert.505 = f16[384]{0} convert(f32[384]{0} %param_0.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.116 = f16[4,1024,384]{2,1,0} broadcast(f16[384]{0} %convert.505), dimensions={2}
  ROOT %bitcast.791 = f16[4096,384]{1,0} bitcast(f16[4,1024,384]{2,1,0} %broadcast.116)
}

%fused_computation.438 (param_0.843: f16[4,1024,1024], param_1.1171: f16[4096,1024]) -> f16[4,1024,1024] {
  %param_0.843 = f16[4,1024,1024]{2,1,0} parameter(0)
  %param_1.1171 = f16[4096,1024]{1,0} parameter(1)
  %bitcast.792 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_1.1171), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  ROOT %add.1276 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %param_0.843, f16[4,1024,1024]{2,1,0} %bitcast.792), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
}

%fused_computation.439 (param_0.1580: f16[4,4,32,1024], param_1.2029: f16[4,4,1024,32], param_2.1763: f16[], param_3.1413: f16[4,4,1024,32]) -> f16[4,1024,128,3] {
  %param_3.1413 = f16[4,4,1024,32]{3,2,1,0} parameter(3)
  %transpose.44 = f16[4,1024,4,32]{3,1,2,0} transpose(f16[4,4,1024,32]{3,2,1,0} %param_3.1413), dimensions={0,2,1,3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/transpose[permutation=(0, 2, 1, 3)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %copy.122 = f16[4,1024,4,32]{1,3,2,0} copy(f16[4,1024,4,32]{3,1,2,0} %transpose.44), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/transpose[permutation=(0, 2, 1, 3)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %bitcast.796 = f16[4,1024,128,1]{1,2,0,3} bitcast(f16[4,1024,4,32]{1,3,2,0} %copy.122), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=173}
  %constant_725 = f16[] constant(0)
  %pad.32 = f16[4,1024,128,3]{1,2,0,3} pad(f16[4,1024,128,1]{1,2,0,3} %bitcast.796, f16[] %constant_725), padding=0_0x0_0x0_0x2_0, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (2, 0, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %param_1.2029 = f16[4,4,1024,32]{3,2,1,0} parameter(1)
  %param_2.1763 = f16[] parameter(2)
  %broadcast.117 = f16[4,4,1024,32]{3,2,1,0} broadcast(f16[] %param_2.1763), dimensions={}
  %divide.528 = f16[4,4,1024,32]{3,2,1,0} divide(f16[4,4,1024,32]{3,2,1,0} %param_1.2029, f16[4,4,1024,32]{3,2,1,0} %broadcast.117), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %transpose.45 = f16[4,1024,4,32]{3,1,2,0} transpose(f16[4,4,1024,32]{3,2,1,0} %divide.528), dimensions={0,2,1,3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %copy.121 = f16[4,1024,4,32]{1,3,2,0} copy(f16[4,1024,4,32]{3,1,2,0} %transpose.45), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %bitcast.794 = f16[4,1024,128,1]{1,2,0,3} bitcast(f16[4,1024,4,32]{1,3,2,0} %copy.121), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=167}
  %pad.31 = f16[4,1024,128,3]{1,2,0,3} pad(f16[4,1024,128,1]{1,2,0,3} %bitcast.794, f16[] %constant_725), padding=0_0x0_0x0_0x0_2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 2, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %add.1278 = f16[4,1024,128,3]{1,2,0,3} add(f16[4,1024,128,3]{1,2,0,3} %pad.32, f16[4,1024,128,3]{1,2,0,3} %pad.31), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %param_0.1580 = f16[4,4,32,1024]{3,2,1,0} parameter(0)
  %bitcast.1875 = f16[1,4,128,1024]{3,2,1,0} bitcast(f16[4,4,32,1024]{3,2,1,0} %param_0.1580)
  %transpose.46 = f16[4,1024,128,1]{1,2,0,3} transpose(f16[1,4,128,1024]{3,2,1,0} %bitcast.1875), dimensions={1,3,2,0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=170}
  %pad.30 = f16[4,1024,128,3]{1,2,0,3} pad(f16[4,1024,128,1]{1,2,0,3} %transpose.46, f16[] %constant_725), padding=0_0x0_0x0_0x1_1, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (1, 1, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %add.1277 = f16[4,1024,128,3]{1,2,0,3} add(f16[4,1024,128,3]{1,2,0,3} %add.1278, f16[4,1024,128,3]{1,2,0,3} %pad.30), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  ROOT %copy.120 = f16[4,1024,128,3]{3,2,1,0} copy(f16[4,1024,128,3]{1,2,0,3} %add.1277), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
}

%fused_computation.440 (param_0.848: f16[4096,128]) -> f16[4,4,32,1024] {
  %param_0.848 = f16[4096,128]{1,0} parameter(0)
  %bitcast.1876 = f16[4,1024,4,32]{3,2,1,0} bitcast(f16[4096,128]{1,0} %param_0.848)
  %transpose.47 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %bitcast.1876), dimensions={0,2,3,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/transpose[permutation=(0, 2, 3, 1)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  ROOT %copy.123 = f16[4,4,32,1024]{3,2,1,0} copy(f16[4,4,32,1024]{2,1,3,0} %transpose.47), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/transpose[permutation=(0, 2, 3, 1)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
}

%fused_computation.441 (param_0.2025: f16[4096,384], param_1.3091: f16[]) -> (f16[4,4,1024,32], f16[4,4,1024,32]) {
  %param_0.2025 = f16[4096,384]{1,0} parameter(0)
  %bitcast.1286 = f16[4,1024,384]{2,1,0} bitcast(f16[4096,384]{1,0} %param_0.2025), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %bitcast.1285 = f16[4,1024,128,3]{3,2,1,0} bitcast(f16[4,1024,384]{2,1,0} %bitcast.1286), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %copy.207 = f16[4,1024,128,3]{1,2,0,3} copy(f16[4,1024,128,3]{3,2,1,0} %bitcast.1285), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %slice.74 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.207), slice={[0:4], [0:1024], [0:128], [2:3]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/slice[start_indices=(0, 0, 0, 2) limit_indices=(8, 1024, 1024, 3) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.48 = f16[1,4,128,1024]{3,2,1,0} transpose(f16[4,1024,128,1]{1,2,0,3} %slice.74), dimensions={3,0,2,1}
  %bitcast.1877 = f16[4,4,32,1024]{3,2,1,0} bitcast(f16[1,4,128,1024]{3,2,1,0} %transpose.48)
  %transpose.49 = f16[4,4,1024,32]{2,3,1,0} transpose(f16[4,4,32,1024]{3,2,1,0} %bitcast.1877), dimensions={0,1,3,2}
  %copy.124 = f16[4,4,1024,32]{3,2,1,0} copy(f16[4,4,1024,32]{2,3,1,0} %transpose.49)
  %slice.44.clone.1 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.207), slice={[0:4], [0:1024], [0:128], [0:1]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/slice[start_indices=(0, 0, 0, 0) limit_indices=(8, 1024, 1024, 1) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %param_1.3091 = f16[] parameter(1)
  %broadcast.140.clone.1 = f16[4,1024,128,1]{1,2,0,3} broadcast(f16[] %param_1.3091), dimensions={}
  %divide.545.clone.1 = f16[4,1024,128,1]{1,2,0,3} divide(f16[4,1024,128,1]{1,2,0,3} %slice.44.clone.1, f16[4,1024,128,1]{1,2,0,3} %broadcast.140.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %bitcast.821.clone.1 = f16[4,1024,4,32]{1,3,2,0} bitcast(f16[4,1024,128,1]{1,2,0,3} %divide.545.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %copy.127.clone.1 = f16[4,1024,4,32]{3,1,2,0} copy(f16[4,1024,4,32]{1,3,2,0} %bitcast.821.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %transpose.50 = f16[4,4,1024,32]{3,2,1,0} transpose(f16[4,1024,4,32]{3,1,2,0} %copy.127.clone.1), dimensions={0,2,1,3}
  ROOT %tuple.211 = (f16[4,4,1024,32]{3,2,1,0}, f16[4,4,1024,32]{3,2,1,0}) tuple(f16[4,4,1024,32]{3,2,1,0} %copy.124, f16[4,4,1024,32]{3,2,1,0} %transpose.50)
}

%fused_computation.442 (param_0.2034: f16[4,4,1024,1024], param_1.2457: f16[4,4,1024], param_2.2214: f32[4,4,1024], param_3.1673: f16[4,4,1024,1024], param_4.1133: f16[4,4,1024]) -> f16[4,4,1024,1024] {
  %param_1.2457 = f16[4,4,1024]{2,1,0} parameter(1)
  %negate.66 = f16[4,4,1024]{2,1,0} negate(f16[4,4,1024]{2,1,0} %param_1.2457), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.118 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %negate.66), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/broadcast_in_dim[shape=(8, 32, 1024, 1024) broadcast_dimensions=(0, 1, 2)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %param_0.2034 = f16[4,4,1024,1024]{3,2,1,0} parameter(0)
  %param_2.2214 = f32[4,4,1024]{2,1,0} parameter(2)
  %convert.673 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %param_2.2214), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.1513 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %convert.673), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.529 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %param_0.2034, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1513), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %add.1279 = f16[4,4,1024,1024]{3,2,1,0} add(f16[4,4,1024,1024]{3,2,1,0} %broadcast.118, f16[4,4,1024,1024]{3,2,1,0} %divide.529), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %param_3.1673 = f16[4,4,1024,1024]{3,2,1,0} parameter(3)
  %param_4.1133 = f16[4,4,1024]{2,1,0} parameter(4)
  %broadcast.2257 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_4.1133), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.230 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_3.1673, f16[4,4,1024,1024]{3,2,1,0} %broadcast.2257), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.81 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.230), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  ROOT %multiply.1853 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %add.1279, f16[4,4,1024,1024]{3,2,1,0} %exponential.81), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%region_82.2621 (Arg_0.2622: f16[], Arg_1.2623: f16[]) -> f16[] {
  %Arg_0.2622 = f16[] parameter(0)
  %Arg_1.2623 = f16[] parameter(1)
  ROOT %add.2624 = f16[] add(f16[] %Arg_0.2622, f16[] %Arg_1.2623), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.443 (param_0.2032: f16[4,4,1024,1024], param_1.2455: f32[4,4,1024], param_2.2213: f16[4,4,1024,1024], param_3.1671: f16[4,4,1024]) -> f16[4,4,1024] {
  %param_0.2032 = f16[4,4,1024,1024]{3,2,1,0} parameter(0)
  %constant_726 = f16[] constant(1)
  %broadcast.1514 = f16[4,4,1024]{2,1,0} broadcast(f16[] %constant_726), dimensions={}
  %param_1.2455 = f32[4,4,1024]{2,1,0} parameter(1)
  %convert.674 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %param_1.2455), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.1856 = f16[4,4,1024]{2,1,0} multiply(f16[4,4,1024]{2,1,0} %convert.674, f16[4,4,1024]{2,1,0} %convert.674), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.530 = f16[4,4,1024]{2,1,0} divide(f16[4,4,1024]{2,1,0} %broadcast.1514, f16[4,4,1024]{2,1,0} %multiply.1856), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.119 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %divide.530), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.1855 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %param_0.2032, f16[4,4,1024,1024]{3,2,1,0} %broadcast.119), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %param_2.2213 = f16[4,4,1024,1024]{3,2,1,0} parameter(2)
  %param_3.1671 = f16[4,4,1024]{2,1,0} parameter(3)
  %broadcast.2255 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_3.1671), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.228 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_2.2213, f16[4,4,1024,1024]{3,2,1,0} %broadcast.2255), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.79 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.228), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.1854 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %multiply.1855, f16[4,4,1024,1024]{3,2,1,0} %exponential.79), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %constant_727 = f16[] constant(0)
  ROOT %reduce.281 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %multiply.1854, f16[] %constant_727), dimensions={3}, to_apply=%region_82.2621, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.444 (param_0.857: f16[4096,128]) -> f16[4,4,1024,32] {
  %param_0.857 = f16[4096,128]{1,0} parameter(0)
  %bitcast.801 = f16[4,1024,4,32]{3,2,1,0} bitcast(f16[4096,128]{1,0} %param_0.857), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %copy.125 = f16[4,1024,4,32]{3,1,2,0} copy(f16[4,1024,4,32]{3,2,1,0} %bitcast.801), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  ROOT %transpose.51 = f16[4,4,1024,32]{3,2,1,0} transpose(f16[4,1024,4,32]{3,1,2,0} %copy.125), dimensions={0,2,1,3}
}

%fused_computation.445 (param_0.2082: f32[4,1024], param_1.2530: f32[8,1024], param_2.2305: s32[], param_3.1760: f32[8,1024], param_4.1215: f32[8,1024], param_5.1224: f32[8,1024], param_6.1031: f32[4,1024], param_7.650: f16[4,1024,1024], param_8.372: f32[], param_9.316: f32[4,1024], param_10.296: f32[], param_11.321: f32[], param_12.316: f32[4,1024], param_13.314: f32[], param_14.317: f32[1024], param_15.319: f16[4,1024,1024], param_16.335: f16[4096,1024]) -> f16[4,1024,1024] {
  %param_7.650 = f16[4,1024,1024]{2,1,0} parameter(7)
  %convert.508 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_7.650), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %param_6.1031 = f32[4,1024]{1,0} parameter(6)
  %bitcast.803 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %param_6.1031), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reshape[new_sizes=(8, 1024, 1) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_11.321 = f32[] parameter(11)
  %broadcast.2289 = f32[4,1024]{1,0} broadcast(f32[] %param_11.321), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_12.316 = f32[4,1024]{1,0} parameter(12)
  %param_8.372 = f32[] parameter(8)
  %broadcast.2288 = f32[4,1024]{1,0} broadcast(f32[] %param_8.372), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.745 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_12.316, f32[4,1024]{1,0} %broadcast.2288), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_9.316 = f32[4,1024]{1,0} parameter(9)
  %param_10.296 = f32[] parameter(10)
  %broadcast.2287 = f32[4,1024]{1,0} broadcast(f32[] %param_10.296), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.744 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_9.316, f32[4,1024]{1,0} %broadcast.2287), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2332 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.744, f32[4,1024]{1,0} %divide.744), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.242 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.745, f32[4,1024]{1,0} %multiply.2332), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.102 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.2289, f32[4,1024]{1,0} %subtract.242), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_13.314 = f32[] parameter(13)
  %broadcast.2286 = f32[4,1024]{1,0} broadcast(f32[] %param_13.314), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.1753 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.102, f32[4,1024]{1,0} %broadcast.2286), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1290 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.1753), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.54 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1290), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %divide.534 = f32[4,1024,1]{1,0,2} divide(f32[4,1024,1]{1,0,2} %rsqrt.54, f32[4,1024,1]{1,0,2} %bitcast.1290), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %constant_728 = f32[] constant(-0.5)
  %broadcast.1518 = f32[4,1024,1]{1,0,2} broadcast(f32[] %constant_728), dimensions={}
  %multiply.1863 = f32[4,1024,1]{1,0,2} multiply(f32[4,1024,1]{1,0,2} %divide.534, f32[4,1024,1]{1,0,2} %broadcast.1518), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.1862 = f32[4,1024,1]{1,0,2} multiply(f32[4,1024,1]{1,0,2} %bitcast.803, f32[4,1024,1]{1,0,2} %multiply.1863), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.802 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %multiply.1862), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reshape[new_sizes=(8, 1024) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=132}
  %compare.65 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %subtract.242, f32[4,1024]{1,0} %maximum.102), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_5.1224 = f32[8,1024]{1,0} parameter(5)
  %param_2.2305 = s32[] parameter(2)
  %constant_730 = s32[] constant(0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %dynamic-slice.52 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_5.1224, s32[] %param_2.2305, s32[] %constant_730), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_4.1215 = f32[8,1024]{1,0} parameter(4)
  %dynamic-slice.51 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_4.1215, s32[] %param_2.2305, s32[] %constant_730), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.59 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.65, f32[4,1024]{1,0} %dynamic-slice.52, f32[4,1024]{1,0} %dynamic-slice.51), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %compare.64 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %broadcast.2289, f32[4,1024]{1,0} %maximum.102), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_3.1760 = f32[8,1024]{1,0} parameter(3)
  %dynamic-slice.50 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_3.1760, s32[] %param_2.2305, s32[] %constant_730), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_1.2530 = f32[8,1024]{1,0} parameter(1)
  %dynamic-slice.49 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_1.2530, s32[] %param_2.2305, s32[] %constant_730), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.58 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.64, f32[4,1024]{1,0} %dynamic-slice.50, f32[4,1024]{1,0} %dynamic-slice.49), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.533 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %select.59, f32[4,1024]{1,0} %select.58), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.1861 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %bitcast.802, f32[4,1024]{1,0} %divide.533), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.532 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %multiply.1861, f32[4,1024]{1,0} %broadcast.2288), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_729 = f32[] constant(2)
  %broadcast.1519 = f32[4,1024]{1,0} broadcast(f32[] %constant_729), dimensions={}
  %multiply.1860 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.532, f32[4,1024]{1,0} %broadcast.1519), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %broadcast.121 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.1860), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.1859 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.508, f32[4,1024,1024]{2,1,0} %broadcast.121), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %negate.67 = f32[4,1024]{1,0} negate(f32[4,1024]{1,0} %multiply.1861), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.1858 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.744, f32[4,1024]{1,0} %broadcast.1519), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.1857 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %negate.67, f32[4,1024]{1,0} %multiply.1858), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %param_0.2082 = f32[4,1024]{1,0} parameter(0)
  %add.1282 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %multiply.1857, f32[4,1024]{1,0} %param_0.2082), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=131}
  %divide.531 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %add.1282, f32[4,1024]{1,0} %broadcast.2287), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.120 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.531), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/broadcast_in_dim[shape=(8, 1024, 1024) broadcast_dimensions=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %add.1281 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.1859, f32[4,1024,1024]{2,1,0} %broadcast.120), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %convert.507 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.1281), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %param_15.319 = f16[4,1024,1024]{2,1,0} parameter(15)
  %param_16.335 = f16[4096,1024]{1,0} parameter(16)
  %bitcast.1344 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_16.335), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.1838 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %param_15.319, f16[4,1024,1024]{2,1,0} %bitcast.1344), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.832 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1838), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %bitcast.1342 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.54), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.2483 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1342), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_14.317 = f32[1024]{0} parameter(14)
  %broadcast.2482 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_14.317), dimensions={2}
  %multiply.2429 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.2483, f32[4,1024,1024]{2,1,0} %broadcast.2482), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2428 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.832, f32[4,1024,1024]{2,1,0} %multiply.2429), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %convert.506 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %multiply.2428), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  ROOT %add.1280 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %convert.507, f16[4,1024,1024]{2,1,0} %convert.506), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%region_80.2590 (Arg_0.2591: f32[], Arg_1.2592: f32[]) -> f32[] {
  %Arg_0.2591 = f32[] parameter(0)
  %Arg_1.2592 = f32[] parameter(1)
  ROOT %add.2593 = f32[] add(f32[] %Arg_0.2591, f32[] %Arg_1.2592), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%region_79.2573 (Arg_0.2574: f32[], Arg_1.2575: f32[]) -> f32[] {
  %Arg_0.2574 = f32[] parameter(0)
  %Arg_1.2575 = f32[] parameter(1)
  ROOT %add.2576 = f32[] add(f32[] %Arg_0.2574, f32[] %Arg_1.2575), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%fused_computation.446 (param_0.2080: f32[1024], param_1.2528: f32[], param_2.2303: f32[4,1024], param_3.1758: f32[], param_4.1213: f32[4,1024], param_5.1222: f32[], param_6.1029: f32[], param_7.648: f16[4,1024,1024], param_8.370: f16[4096,1024], param_9.476: f16[4,1024,1024]) -> (f32[4,1024], f32[4,1024]) {
  %param_7.648 = f16[4,1024,1024]{2,1,0} parameter(7)
  %param_8.370 = f16[4096,1024]{1,0} parameter(8)
  %bitcast.1338 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_8.370), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.1834 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %param_7.648, f16[4,1024,1024]{2,1,0} %bitcast.1338), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.830 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1834), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %param_6.1029 = f32[] parameter(6)
  %broadcast.2475 = f32[4,1024]{1,0} broadcast(f32[] %param_6.1029), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_4.1213 = f32[4,1024]{1,0} parameter(4)
  %param_5.1222 = f32[] parameter(5)
  %broadcast.2474 = f32[4,1024]{1,0} broadcast(f32[] %param_5.1222), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.805 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_4.1213, f32[4,1024]{1,0} %broadcast.2474), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.2303 = f32[4,1024]{1,0} parameter(2)
  %param_3.1758 = f32[] parameter(3)
  %broadcast.2473 = f32[4,1024]{1,0} broadcast(f32[] %param_3.1758), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.804 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_2.2303, f32[4,1024]{1,0} %broadcast.2473), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2424 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.804, f32[4,1024]{1,0} %divide.804), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.275 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.805, f32[4,1024]{1,0} %multiply.2424), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.122 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.2475, f32[4,1024]{1,0} %subtract.275), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_1.2528 = f32[] parameter(1)
  %broadcast.2472 = f32[4,1024]{1,0} broadcast(f32[] %param_1.2528), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.1833 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.122, f32[4,1024]{1,0} %broadcast.2472), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1337 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.1833), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.102 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1337), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1336 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.102), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.2471 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1336), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_0.2080 = f32[1024]{0} parameter(0)
  %broadcast.2470 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_0.2080), dimensions={2}
  %multiply.2423 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.2471, f32[4,1024,1024]{2,1,0} %broadcast.2470), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2422 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.830, f32[4,1024,1024]{2,1,0} %multiply.2423), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %negate.68 = f32[4,1024,1024]{2,1,0} negate(f32[4,1024,1024]{2,1,0} %multiply.2422), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %constant_732 = f32[] constant(0)
  %reduce.282 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %negate.68, f32[] %constant_732), dimensions={2}, to_apply=%region_80.2590, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_9.476 = f16[4,1024,1024]{2,1,0} parameter(9)
  %convert.836.clone.1 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_9.476), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %broadcast.2490.clone.1 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.804), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.282.clone.1 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.836.clone.1, f32[4,1024,1024]{2,1,0} %broadcast.2490.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.2432.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.282.clone.1, f32[4,1024,1024]{2,1,0} %convert.830), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.1865.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %multiply.2432.clone.1, f32[4,1024,1024]{2,1,0} %broadcast.2470), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.283.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1865.clone.1, f32[] %constant_732), dimensions={2}, to_apply=%region_79.2573, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  ROOT %tuple.204 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) tuple(f32[4,1024]{1,0} %reduce.282, f32[4,1024]{1,0} %reduce.283.clone.1)
}

%fused_computation.451 (param_0.2051: f16[4096,512], param_1.2486: f16[], param_2.2250: f16[], param_3.1712: f16[4096,512], param_4.1172: f16[]) -> f16[4,1024,512] {
  %param_3.1712 = f16[4096,512]{1,0} parameter(3)
  %bitcast.994 = f16[4,1024,512]{2,1,0} bitcast(f16[4096,512]{1,0} %param_3.1712), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_0.2051 = f16[4096,512]{1,0} parameter(0)
  %param_1.2486 = f16[] parameter(1)
  %broadcast.122 = f16[4096,512]{1,0} broadcast(f16[] %param_1.2486), dimensions={}
  %divide.536 = f16[4096,512]{1,0} divide(f16[4096,512]{1,0} %param_0.2051, f16[4096,512]{1,0} %broadcast.122), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %bitcast.805 = f16[4,1024,512]{2,1,0} bitcast(f16[4096,512]{1,0} %divide.536), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1871 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %bitcast.994, f16[4,1024,512]{2,1,0} %bitcast.805), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_733 = f16[] constant(1.1279)
  %broadcast.1523 = f16[4,1024,512]{2,1,0} broadcast(f16[] %constant_733), dimensions={}
  %multiply.1870 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.1871, f16[4,1024,512]{2,1,0} %broadcast.1523), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %param_2.2250 = f16[] parameter(2)
  %broadcast.1522 = f16[4,1024,512]{2,1,0} broadcast(f16[] %param_2.2250), dimensions={}
  %divide.617 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %bitcast.994, f16[4,1024,512]{2,1,0} %broadcast.1522), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1869 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %divide.617, f16[4,1024,512]{2,1,0} %divide.617), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %negate.69 = f16[4,1024,512]{2,1,0} negate(f16[4,1024,512]{2,1,0} %multiply.1869), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/neg" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %exponential.27 = f16[4,1024,512]{2,1,0} exponential(f16[4,1024,512]{2,1,0} %negate.69), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/exp" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1868 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.1870, f16[4,1024,512]{2,1,0} %exponential.27), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.535 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %multiply.1868, f16[4,1024,512]{2,1,0} %broadcast.1522), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1402 = f32[] constant(-4)
  %broadcast.2387 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1402), dimensions={}
  %convert.816 = f32[4,1024,512]{2,1,0} convert(f16[4,1024,512]{2,1,0} %divide.617), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1401 = f32[] constant(4)
  %broadcast.2385 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1401), dimensions={}
  %clamp.33 = f32[4,1024,512]{2,1,0} clamp(f32[4,1024,512]{2,1,0} %broadcast.2387, f32[4,1024,512]{2,1,0} %convert.816, f32[4,1024,512]{2,1,0} %broadcast.2385), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2390 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.33, f32[4,1024,512]{2,1,0} %clamp.33), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1400 = f32[] constant(0)
  %broadcast.2384 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1400), dimensions={}
  %multiply.2389 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %multiply.2390, f32[4,1024,512]{2,1,0} %broadcast.2384), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1399 = f32[] constant(-2.72614237e-10)
  %broadcast.2383 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1399), dimensions={}
  %add.1811 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2389, f32[4,1024,512]{2,1,0} %broadcast.2383), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2388 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1811, f32[4,1024,512]{2,1,0} %multiply.2390), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1398 = f32[] constant(2.77068146e-08)
  %broadcast.2382 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1398), dimensions={}
  %add.1810 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2388, f32[4,1024,512]{2,1,0} %broadcast.2382), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2387 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1810, f32[4,1024,512]{2,1,0} %multiply.2390), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1397 = f32[] constant(-2.10102394e-06)
  %broadcast.2381 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1397), dimensions={}
  %add.1809 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2387, f32[4,1024,512]{2,1,0} %broadcast.2381), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2386 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1809, f32[4,1024,512]{2,1,0} %multiply.2390), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1396 = f32[] constant(-5.69250624e-05)
  %broadcast.2380 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1396), dimensions={}
  %add.1808 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2386, f32[4,1024,512]{2,1,0} %broadcast.2380), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2385 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1808, f32[4,1024,512]{2,1,0} %multiply.2390), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1395 = f32[] constant(-0.000734990637)
  %broadcast.2379 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1395), dimensions={}
  %add.1807 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2385, f32[4,1024,512]{2,1,0} %broadcast.2379), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2384 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1807, f32[4,1024,512]{2,1,0} %multiply.2390), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1394 = f32[] constant(-0.0029546)
  %broadcast.2378 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1394), dimensions={}
  %add.1806 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2384, f32[4,1024,512]{2,1,0} %broadcast.2378), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2383 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1806, f32[4,1024,512]{2,1,0} %multiply.2390), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1393 = f32[] constant(-0.0160960332)
  %broadcast.2377 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1393), dimensions={}
  %add.1805 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2383, f32[4,1024,512]{2,1,0} %broadcast.2377), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2382 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.33, f32[4,1024,512]{2,1,0} %add.1805), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1392 = f32[] constant(-1.45660715e-05)
  %broadcast.2376 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1392), dimensions={}
  %add.1804 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2389, f32[4,1024,512]{2,1,0} %broadcast.2376), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2381 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1804, f32[4,1024,512]{2,1,0} %multiply.2390), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1391 = f32[] constant(-0.000213374049)
  %broadcast.2375 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1391), dimensions={}
  %add.1803 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2381, f32[4,1024,512]{2,1,0} %broadcast.2375), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2380 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1803, f32[4,1024,512]{2,1,0} %multiply.2390), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1390 = f32[] constant(-0.00168282702)
  %broadcast.2374 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1390), dimensions={}
  %add.1802 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2380, f32[4,1024,512]{2,1,0} %broadcast.2374), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2379 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1802, f32[4,1024,512]{2,1,0} %multiply.2390), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1389 = f32[] constant(-0.00737332925)
  %broadcast.2373 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1389), dimensions={}
  %add.1801 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2379, f32[4,1024,512]{2,1,0} %broadcast.2373), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2378 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1801, f32[4,1024,512]{2,1,0} %multiply.2390), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1388 = f32[] constant(-0.0142647391)
  %broadcast.2372 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1388), dimensions={}
  %add.1800 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2378, f32[4,1024,512]{2,1,0} %broadcast.2372), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.764 = f32[4,1024,512]{2,1,0} divide(f32[4,1024,512]{2,1,0} %multiply.2382, f32[4,1024,512]{2,1,0} %add.1800), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.815 = f16[4,1024,512]{2,1,0} convert(f32[4,1024,512]{2,1,0} %divide.764), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %param_4.1172 = f16[] parameter(4)
  %broadcast.2371 = f16[4,1024,512]{2,1,0} broadcast(f16[] %param_4.1172), dimensions={}
  %add.1799 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %convert.815, f16[4,1024,512]{2,1,0} %broadcast.2371), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1867 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %bitcast.805, f16[4,1024,512]{2,1,0} %add.1799), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  ROOT %add.1284 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %divide.535, f16[4,1024,512]{2,1,0} %multiply.1867), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
}

%fused_computation.452 (param_0.2067: f32[4,1024], param_1.2511: f32[8,1024], param_2.2284: s32[], param_3.1743: f32[8,1024], param_4.1200: f32[8,1024], param_5.1213: f32[8,1024], param_6.1020: f32[4,1024], param_7.637: f16[4,1024,1024], param_8.362: f32[], param_9.315: f32[4,1024], param_10.295: f32[], param_11.320: f32[], param_12.315: f32[4,1024], param_13.313: f32[], param_14.315: f32[1024], param_15.314: f16[4,1024,1024]) -> f16[4,1024,1024] {
  %param_7.637 = f16[4,1024,1024]{2,1,0} parameter(7)
  %convert.513 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_7.637), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %param_6.1020 = f32[4,1024]{1,0} parameter(6)
  %bitcast.807 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %param_6.1020), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reshape[new_sizes=(8, 1024, 1) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_11.320 = f32[] parameter(11)
  %broadcast.2411 = f32[4,1024]{1,0} broadcast(f32[] %param_11.320), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_12.315 = f32[4,1024]{1,0} parameter(12)
  %param_8.362 = f32[] parameter(8)
  %broadcast.2410 = f32[4,1024]{1,0} broadcast(f32[] %param_8.362), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.781 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_12.315, f32[4,1024]{1,0} %broadcast.2410), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_9.315 = f32[4,1024]{1,0} parameter(9)
  %param_10.295 = f32[] parameter(10)
  %broadcast.2409 = f32[4,1024]{1,0} broadcast(f32[] %param_10.295), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.780 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_9.315, f32[4,1024]{1,0} %broadcast.2409), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2398 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.780, f32[4,1024]{1,0} %divide.780), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.261 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.781, f32[4,1024]{1,0} %multiply.2398), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.112 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.2411, f32[4,1024]{1,0} %subtract.261), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_13.313 = f32[] parameter(13)
  %broadcast.2408 = f32[4,1024]{1,0} broadcast(f32[] %param_13.313), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.1815 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.112, f32[4,1024]{1,0} %broadcast.2408), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1310 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.1815), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.55 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1310), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %divide.540 = f32[4,1024,1]{1,0,2} divide(f32[4,1024,1]{1,0,2} %rsqrt.55, f32[4,1024,1]{1,0,2} %bitcast.1310), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %constant_734 = f32[] constant(-0.5)
  %broadcast.1527 = f32[4,1024,1]{1,0,2} broadcast(f32[] %constant_734), dimensions={}
  %multiply.1878 = f32[4,1024,1]{1,0,2} multiply(f32[4,1024,1]{1,0,2} %divide.540, f32[4,1024,1]{1,0,2} %broadcast.1527), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.1877 = f32[4,1024,1]{1,0,2} multiply(f32[4,1024,1]{1,0,2} %bitcast.807, f32[4,1024,1]{1,0,2} %multiply.1878), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.806 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %multiply.1877), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reshape[new_sizes=(8, 1024) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=132}
  %compare.67 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %subtract.261, f32[4,1024]{1,0} %maximum.112), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_5.1213 = f32[8,1024]{1,0} parameter(5)
  %param_2.2284 = s32[] parameter(2)
  %constant_736 = s32[] constant(0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %dynamic-slice.56 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_5.1213, s32[] %param_2.2284, s32[] %constant_736), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_4.1200 = f32[8,1024]{1,0} parameter(4)
  %dynamic-slice.55 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_4.1200, s32[] %param_2.2284, s32[] %constant_736), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.61 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.67, f32[4,1024]{1,0} %dynamic-slice.56, f32[4,1024]{1,0} %dynamic-slice.55), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %compare.66 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %broadcast.2411, f32[4,1024]{1,0} %maximum.112), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_3.1743 = f32[8,1024]{1,0} parameter(3)
  %dynamic-slice.54 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_3.1743, s32[] %param_2.2284, s32[] %constant_736), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_1.2511 = f32[8,1024]{1,0} parameter(1)
  %dynamic-slice.53 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_1.2511, s32[] %param_2.2284, s32[] %constant_736), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.60 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.66, f32[4,1024]{1,0} %dynamic-slice.54, f32[4,1024]{1,0} %dynamic-slice.53), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.539 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %select.61, f32[4,1024]{1,0} %select.60), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.1876 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %bitcast.806, f32[4,1024]{1,0} %divide.539), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.538 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %multiply.1876, f32[4,1024]{1,0} %broadcast.2410), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_735 = f32[] constant(2)
  %broadcast.1528 = f32[4,1024]{1,0} broadcast(f32[] %constant_735), dimensions={}
  %multiply.1875 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.538, f32[4,1024]{1,0} %broadcast.1528), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %broadcast.124 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.1875), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.1874 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.513, f32[4,1024,1024]{2,1,0} %broadcast.124), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %negate.70 = f32[4,1024]{1,0} negate(f32[4,1024]{1,0} %multiply.1876), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.1873 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.780, f32[4,1024]{1,0} %broadcast.1528), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.1872 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %negate.70, f32[4,1024]{1,0} %multiply.1873), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %param_0.2067 = f32[4,1024]{1,0} parameter(0)
  %add.1287 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %multiply.1872, f32[4,1024]{1,0} %param_0.2067), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=131}
  %divide.537 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %add.1287, f32[4,1024]{1,0} %broadcast.2409), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.123 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.537), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/broadcast_in_dim[shape=(8, 1024, 1024) broadcast_dimensions=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %add.1286 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.1874, f32[4,1024,1024]{2,1,0} %broadcast.123), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %convert.512 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.1286), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %param_15.314 = f16[4,1024,1024]{2,1,0} parameter(15)
  %convert.820 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_15.314), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %bitcast.1325 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.55), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.2451 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1325), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_14.315 = f32[1024]{0} parameter(14)
  %broadcast.2450 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_14.315), dimensions={2}
  %multiply.2413 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.2451, f32[4,1024,1024]{2,1,0} %broadcast.2450), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2412 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.820, f32[4,1024,1024]{2,1,0} %multiply.2413), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %convert.511 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %multiply.2412), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  ROOT %add.1285 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %convert.512, f16[4,1024,1024]{2,1,0} %convert.511), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%region_74.2510 (Arg_0.2511: f32[], Arg_1.2512: f32[]) -> f32[] {
  %Arg_0.2511 = f32[] parameter(0)
  %Arg_1.2512 = f32[] parameter(1)
  ROOT %add.2513 = f32[] add(f32[] %Arg_0.2511, f32[] %Arg_1.2512), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%region_73.2493 (Arg_0.2494: f32[], Arg_1.2495: f32[]) -> f32[] {
  %Arg_0.2494 = f32[] parameter(0)
  %Arg_1.2495 = f32[] parameter(1)
  ROOT %add.2496 = f32[] add(f32[] %Arg_0.2494, f32[] %Arg_1.2495), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%fused_computation.453 (param_0.2065: f32[1024], param_1.2509: f32[], param_2.2282: f32[4,1024], param_3.1741: f32[], param_4.1198: f32[4,1024], param_5.1211: f32[], param_6.1018: f32[], param_7.635: f16[4,1024,1024], param_8.615: f16[4,1024,1024]) -> (f32[4,1024], f32[4,1024]) {
  %param_7.635 = f16[4,1024,1024]{2,1,0} parameter(7)
  %convert.818 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_7.635), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %param_6.1018 = f32[] parameter(6)
  %broadcast.2443 = f32[4,1024]{1,0} broadcast(f32[] %param_6.1018), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_4.1198 = f32[4,1024]{1,0} parameter(4)
  %param_5.1211 = f32[] parameter(5)
  %broadcast.2442 = f32[4,1024]{1,0} broadcast(f32[] %param_5.1211), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.793 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_4.1198, f32[4,1024]{1,0} %broadcast.2442), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.2282 = f32[4,1024]{1,0} parameter(2)
  %param_3.1741 = f32[] parameter(3)
  %broadcast.2441 = f32[4,1024]{1,0} broadcast(f32[] %param_3.1741), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.792 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_2.2282, f32[4,1024]{1,0} %broadcast.2441), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2408 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.792, f32[4,1024]{1,0} %divide.792), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.267 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.793, f32[4,1024]{1,0} %multiply.2408), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.118 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.2443, f32[4,1024]{1,0} %subtract.267), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_1.2509 = f32[] parameter(1)
  %broadcast.2440 = f32[4,1024]{1,0} broadcast(f32[] %param_1.2509), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.1821 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.118, f32[4,1024]{1,0} %broadcast.2440), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1322 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.1821), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.98 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1322), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1321 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.98), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.2439 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1321), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_0.2065 = f32[1024]{0} parameter(0)
  %broadcast.2438 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_0.2065), dimensions={2}
  %multiply.2407 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.2439, f32[4,1024,1024]{2,1,0} %broadcast.2438), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2406 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.818, f32[4,1024,1024]{2,1,0} %multiply.2407), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %negate.71 = f32[4,1024,1024]{2,1,0} negate(f32[4,1024,1024]{2,1,0} %multiply.2406), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %constant_738 = f32[] constant(0)
  %reduce.284 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %negate.71, f32[] %constant_738), dimensions={2}, to_apply=%region_74.2510, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_8.615 = f16[4,1024,1024]{2,1,0} parameter(8)
  %convert.824.clone.1 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_8.615), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %broadcast.2458.clone.1 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.792), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.271.clone.1 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.824.clone.1, f32[4,1024,1024]{2,1,0} %broadcast.2458.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.2416.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.271.clone.1, f32[4,1024,1024]{2,1,0} %convert.818), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.1883.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %multiply.2416.clone.1, f32[4,1024,1024]{2,1,0} %broadcast.2438), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.286.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1883.clone.1, f32[] %constant_738), dimensions={2}, to_apply=%region_73.2493, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  ROOT %tuple.206 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) tuple(f32[4,1024]{1,0} %reduce.284, f32[4,1024]{1,0} %reduce.286.clone.1)
}

%region_69.2412 (Arg_0.2413: f32[], Arg_1.2414: f32[]) -> f32[] {
  %Arg_0.2413 = f32[] parameter(0)
  %Arg_1.2414 = f32[] parameter(1)
  ROOT %add.2415 = f32[] add(f32[] %Arg_0.2413, f32[] %Arg_1.2414), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_70.2425 (Arg_0.2426: f32[], Arg_1.2427: f32[]) -> f32[] {
  %Arg_0.2426 = f32[] parameter(0)
  %Arg_1.2427 = f32[] parameter(1)
  ROOT %add.2428 = f32[] add(f32[] %Arg_0.2426, f32[] %Arg_1.2427), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%fused_computation.461 (param_0.2417: f16[4,1024,1024], param_1.3081: f32[1024], param_2.2975: f16[4096,1024]) -> (f32[4,1024], f32[4,1024], f16[4,1024,1024]) {
  %param_2.2975 = f16[4096,1024]{1,0} parameter(2)
  %bitcast.810.clone.1 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_2.2975), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_1.3081 = f32[1024]{0} parameter(1)
  %convert.519.clone.1 = f16[1024]{0} convert(f32[1024]{0} %param_1.3081), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.128.clone.1 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.519.clone.1), dimensions={2}
  %add.1290.clone.1 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.810.clone.1, f16[4,1024,1024]{2,1,0} %broadcast.128.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_0.2417 = f16[4,1024,1024]{2,1,0} parameter(0)
  %add.1289.clone.1 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1290.clone.1, f16[4,1024,1024]{2,1,0} %param_0.2417), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=310}
  %convert.518 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1289.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %constant_740 = f32[] constant(0)
  %reduce.287 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.518, f32[] %constant_740), dimensions={2}, to_apply=%region_69.2412, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.1882.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.518, f32[4,1024,1024]{2,1,0} %convert.518), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.285.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1882.clone.1, f32[] %constant_740), dimensions={2}, to_apply=%region_70.2425, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  ROOT %tuple.208 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) tuple(f32[4,1024]{1,0} %reduce.287, f32[4,1024]{1,0} %reduce.285.clone.1, f16[4,1024,1024]{2,1,0} %add.1289.clone.1)
}

%fused_computation.463 (param_0.895: f16[], param_1.2484: f16[4096,512], param_2.2248: f16[], param_3.1711: f16[]) -> f16[4096,512] {
  %param_1.2484 = f16[4096,512]{1,0} parameter(1)
  %bitcast.995 = f16[4,1024,512]{2,1,0} bitcast(f16[4096,512]{1,0} %param_1.2484), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %constant_1371 = f32[] constant(-4)
  %broadcast.2353 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1371), dimensions={}
  %param_3.1711 = f16[] parameter(3)
  %broadcast.2352 = f16[4,1024,512]{2,1,0} broadcast(f16[] %param_3.1711), dimensions={}
  %divide.761 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %bitcast.995, f16[4,1024,512]{2,1,0} %broadcast.2352), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.812 = f32[4,1024,512]{2,1,0} convert(f16[4,1024,512]{2,1,0} %divide.761), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1370 = f32[] constant(4)
  %broadcast.2351 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1370), dimensions={}
  %clamp.31 = f32[4,1024,512]{2,1,0} clamp(f32[4,1024,512]{2,1,0} %broadcast.2353, f32[4,1024,512]{2,1,0} %convert.812, f32[4,1024,512]{2,1,0} %broadcast.2351), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2364 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.31, f32[4,1024,512]{2,1,0} %clamp.31), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1369 = f32[] constant(0)
  %broadcast.2350 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1369), dimensions={}
  %multiply.2363 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %multiply.2364, f32[4,1024,512]{2,1,0} %broadcast.2350), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1368 = f32[] constant(-2.72614237e-10)
  %broadcast.2349 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1368), dimensions={}
  %add.1785 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2363, f32[4,1024,512]{2,1,0} %broadcast.2349), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2362 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1785, f32[4,1024,512]{2,1,0} %multiply.2364), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1367 = f32[] constant(2.77068146e-08)
  %broadcast.2348 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1367), dimensions={}
  %add.1784 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2362, f32[4,1024,512]{2,1,0} %broadcast.2348), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2361 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1784, f32[4,1024,512]{2,1,0} %multiply.2364), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1366 = f32[] constant(-2.10102394e-06)
  %broadcast.2347 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1366), dimensions={}
  %add.1783 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2361, f32[4,1024,512]{2,1,0} %broadcast.2347), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2360 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1783, f32[4,1024,512]{2,1,0} %multiply.2364), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1365 = f32[] constant(-5.69250624e-05)
  %broadcast.2346 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1365), dimensions={}
  %add.1782 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2360, f32[4,1024,512]{2,1,0} %broadcast.2346), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2359 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1782, f32[4,1024,512]{2,1,0} %multiply.2364), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1364 = f32[] constant(-0.000734990637)
  %broadcast.2345 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1364), dimensions={}
  %add.1781 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2359, f32[4,1024,512]{2,1,0} %broadcast.2345), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2358 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1781, f32[4,1024,512]{2,1,0} %multiply.2364), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1363 = f32[] constant(-0.0029546)
  %broadcast.2344 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1363), dimensions={}
  %add.1780 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2358, f32[4,1024,512]{2,1,0} %broadcast.2344), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2357 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1780, f32[4,1024,512]{2,1,0} %multiply.2364), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1362 = f32[] constant(-0.0160960332)
  %broadcast.2343 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1362), dimensions={}
  %add.1779 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2357, f32[4,1024,512]{2,1,0} %broadcast.2343), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2356 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.31, f32[4,1024,512]{2,1,0} %add.1779), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1361 = f32[] constant(-1.45660715e-05)
  %broadcast.2342 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1361), dimensions={}
  %add.1778 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2363, f32[4,1024,512]{2,1,0} %broadcast.2342), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2355 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1778, f32[4,1024,512]{2,1,0} %multiply.2364), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1360 = f32[] constant(-0.000213374049)
  %broadcast.2341 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1360), dimensions={}
  %add.1777 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2355, f32[4,1024,512]{2,1,0} %broadcast.2341), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2354 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1777, f32[4,1024,512]{2,1,0} %multiply.2364), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1359 = f32[] constant(-0.00168282702)
  %broadcast.2340 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1359), dimensions={}
  %add.1776 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2354, f32[4,1024,512]{2,1,0} %broadcast.2340), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2353 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1776, f32[4,1024,512]{2,1,0} %multiply.2364), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1358 = f32[] constant(-0.00737332925)
  %broadcast.2339 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1358), dimensions={}
  %add.1775 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2353, f32[4,1024,512]{2,1,0} %broadcast.2339), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2352 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1775, f32[4,1024,512]{2,1,0} %multiply.2364), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1357 = f32[] constant(-0.0142647391)
  %broadcast.2338 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1357), dimensions={}
  %add.1774 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2352, f32[4,1024,512]{2,1,0} %broadcast.2338), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.760 = f32[4,1024,512]{2,1,0} divide(f32[4,1024,512]{2,1,0} %multiply.2356, f32[4,1024,512]{2,1,0} %add.1774), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.811 = f16[4,1024,512]{2,1,0} convert(f32[4,1024,512]{2,1,0} %divide.760), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %param_2.2248 = f16[] parameter(2)
  %broadcast.2337 = f16[4,1024,512]{2,1,0} broadcast(f16[] %param_2.2248), dimensions={}
  %add.1773 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %convert.811, f16[4,1024,512]{2,1,0} %broadcast.2337), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1885 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %bitcast.995, f16[4,1024,512]{2,1,0} %add.1773), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %param_0.895 = f16[] parameter(0)
  %broadcast.129 = f16[4,1024,512]{2,1,0} broadcast(f16[] %param_0.895), dimensions={}
  %divide.542 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %multiply.1885, f16[4,1024,512]{2,1,0} %broadcast.129), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  ROOT %bitcast.811 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %divide.542)
}

%fused_computation.465 (param_0.900: f32[512]) -> f16[4096,512] {
  %param_0.900 = f32[512]{0} parameter(0)
  %convert.522 = f16[512]{0} convert(f32[512]{0} %param_0.900), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.131 = f16[4,1024,512]{2,1,0} broadcast(f16[512]{0} %convert.522), dimensions={2}
  ROOT %bitcast.812 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %broadcast.131)
}

%fused_computation.466 (param_0.903: f32[1024], param_1.2473: f32[1024], param_2.2233: f32[4,1024], param_3.1693: f32[], param_4.1148: f16[4,1024,1024], param_5.1150: f32[], param_6.966: f32[4,1024], param_7.594: f32[], param_8.336: f32[]) -> f16[4,1024,1024] {
  %param_4.1148 = f16[4,1024,1024]{2,1,0} parameter(4)
  %convert.806 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_4.1148), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %param_2.2233 = f32[4,1024]{1,0} parameter(2)
  %param_3.1693 = f32[] parameter(3)
  %broadcast.2261 = f32[4,1024]{1,0} broadcast(f32[] %param_3.1693), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.727 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_2.2233, f32[4,1024]{1,0} %broadcast.2261), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.2260 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.727), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.232 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.806, f32[4,1024,1024]{2,1,0} %broadcast.2260), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_8.336 = f32[] parameter(8)
  %broadcast.2299 = f32[4,1024]{1,0} broadcast(f32[] %param_8.336), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_6.966 = f32[4,1024]{1,0} parameter(6)
  %param_7.594 = f32[] parameter(7)
  %broadcast.2298 = f32[4,1024]{1,0} broadcast(f32[] %param_7.594), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.749 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_6.966, f32[4,1024]{1,0} %broadcast.2298), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.2334 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.727, f32[4,1024]{1,0} %divide.727), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.247 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.749, f32[4,1024]{1,0} %multiply.2334), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.104 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.2299, f32[4,1024]{1,0} %subtract.247), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_5.1150 = f32[] parameter(5)
  %broadcast.2296 = f32[4,1024]{1,0} broadcast(f32[] %param_5.1150), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.1755 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.104, f32[4,1024]{1,0} %broadcast.2296), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1294 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.1755), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.88 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1294), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1293 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.88), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.2295 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1293), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_1.2473 = f32[1024]{0} parameter(1)
  %broadcast.1551 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_1.2473), dimensions={2}
  %multiply.2151 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.2295, f32[4,1024,1024]{2,1,0} %broadcast.1551), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.1899 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.232, f32[4,1024,1024]{2,1,0} %multiply.2151), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.903 = f32[1024]{0} parameter(0)
  %broadcast.132 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_0.903), dimensions={2}
  %add.1304 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.1899, f32[4,1024,1024]{2,1,0} %broadcast.132), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  ROOT %convert.523 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.1304), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
}

%region_67.2313 (Arg_0.2314: f32[], Arg_1.2315: f32[]) -> f32[] {
  %Arg_0.2314 = f32[] parameter(0)
  %Arg_1.2315 = f32[] parameter(1)
  ROOT %add.2316 = f32[] add(f32[] %Arg_0.2314, f32[] %Arg_1.2315), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_68.2326 (Arg_0.2327: f32[], Arg_1.2328: f32[]) -> f32[] {
  %Arg_0.2327 = f32[] parameter(0)
  %Arg_1.2328 = f32[] parameter(1)
  ROOT %add.2329 = f32[] add(f32[] %Arg_0.2327, f32[] %Arg_1.2328), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%fused_computation.472 (param_0.2419: f16[4,1024,1024], param_1.3086: f32[1024], param_2.2980: f16[4096,1024]) -> (f32[4,1024], f32[4,1024], f16[4,1024,1024]) {
  %param_2.2980 = f16[4096,1024]{1,0} parameter(2)
  %bitcast.815.clone.1 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_2.2980), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_1.3086 = f32[1024]{0} parameter(1)
  %convert.527.clone.1 = f16[1024]{0} convert(f32[1024]{0} %param_1.3086), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.136.clone.1 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.527.clone.1), dimensions={2}
  %add.1307.clone.1 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.815.clone.1, f16[4,1024,1024]{2,1,0} %broadcast.136.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_0.2419 = f16[4,1024,1024]{2,1,0} parameter(0)
  %add.1306.clone.1 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1307.clone.1, f16[4,1024,1024]{2,1,0} %param_0.2419), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=233}
  %convert.526 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1306.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %constant_757 = f32[] constant(0)
  %reduce.289 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.526, f32[] %constant_757), dimensions={2}, to_apply=%region_67.2313, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.1901.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.526, f32[4,1024,1024]{2,1,0} %convert.526), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.288.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1901.clone.1, f32[] %constant_757), dimensions={2}, to_apply=%region_68.2326, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  ROOT %tuple.210 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) tuple(f32[4,1024]{1,0} %reduce.289, f32[4,1024]{1,0} %reduce.288.clone.1, f16[4,1024,1024]{2,1,0} %add.1306.clone.1)
}

%fused_computation.474 (param_0.920: f16[4,4,32,1024]) -> f16[4096,128] {
  %param_0.920 = f16[4,4,32,1024]{3,2,1,0} parameter(0)
  %transpose.52 = f16[4,1024,4,32]{1,3,2,0} transpose(f16[4,4,32,1024]{3,2,1,0} %param_0.920), dimensions={0,3,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %copy.126 = f16[4,1024,4,32]{3,2,1,0} copy(f16[4,1024,4,32]{1,3,2,0} %transpose.52), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  ROOT %bitcast.816 = f16[4096,128]{1,0} bitcast(f16[4,1024,4,32]{3,2,1,0} %copy.126)
}

%region_66.2288 (Arg_0.2289: f32[], Arg_1.2290: f32[]) -> f32[] {
  %Arg_0.2289 = f32[] parameter(0)
  %Arg_1.2290 = f32[] parameter(1)
  ROOT %add.2291 = f32[] add(f32[] %Arg_0.2289, f32[] %Arg_1.2290), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.475 (param_0.2028: f16[4,4,1024,1024], param_1.2450: f16[4,4,1024]) -> f32[4,4,1024] {
  %param_0.2028 = f16[4,4,1024,1024]{3,2,1,0} parameter(0)
  %param_1.2450 = f16[4,4,1024]{2,1,0} parameter(1)
  %broadcast.2251 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_1.2450), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.222 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_0.2028, f16[4,4,1024,1024]{3,2,1,0} %broadcast.2251), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.75 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.222), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.528 = f32[4,4,1024,1024]{3,2,1,0} convert(f16[4,4,1024,1024]{3,2,1,0} %exponential.75), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %constant_758 = f32[] constant(0)
  ROOT %reduce.290 = f32[4,4,1024]{2,1,0} reduce(f32[4,4,1024,1024]{3,2,1,0} %convert.528, f32[] %constant_758), dimensions={3}, to_apply=%region_66.2288, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.477 (param_0.928: f16[8,1,1,1024], param_1.1263: s32[], param_2.1794: f16[8,1,1,1024], param_3.1438: s32[4,1024], param_4.911: s32[]) -> f16[4,4,1024,1024] {
  %param_3.1438 = s32[4,1024]{1,0} parameter(3)
  %param_4.911 = s32[] parameter(4)
  %broadcast.139 = s32[4,1024]{1,0} broadcast(s32[] %param_4.911), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %compare.68 = pred[4,1024]{1,0} compare(s32[4,1024]{1,0} %param_3.1438, s32[4,1024]{1,0} %broadcast.139), direction=GT, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %bitcast.819 = pred[4,1,1,1024]{3,0,2,1} bitcast(pred[4,1024]{1,0} %compare.68), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %param_2.1794 = f16[8,1,1,1024]{3,0,2,1} parameter(2)
  %param_1.1263 = s32[] parameter(1)
  %constant_759 = s32[] constant(0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %dynamic-slice.58 = f16[4,1,1,1024]{3,0,2,1} dynamic-slice(f16[8,1,1,1024]{3,0,2,1} %param_2.1794, s32[] %param_1.1263, s32[] %constant_759, s32[] %constant_759, s32[] %constant_759), dynamic_slice_sizes={4,1,1,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %param_0.928 = f16[8,1,1,1024]{3,0,2,1} parameter(0)
  %dynamic-slice.57 = f16[4,1,1,1024]{3,0,2,1} dynamic-slice(f16[8,1,1,1024]{3,0,2,1} %param_0.928, s32[] %param_1.1263, s32[] %constant_759, s32[] %constant_759, s32[] %constant_759), dynamic_slice_sizes={4,1,1,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %select.62 = f16[4,1,1,1024]{3,0,2,1} select(pred[4,1,1,1024]{3,0,2,1} %bitcast.819, f16[4,1,1,1024]{3,0,2,1} %dynamic-slice.58, f16[4,1,1,1024]{3,0,2,1} %dynamic-slice.57), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %bitcast.818 = f16[4,1024]{1,0} bitcast(f16[4,1,1,1024]{3,0,2,1} %select.62), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=100}
  ROOT %broadcast.138 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,1024]{1,0} %bitcast.818), dimensions={0,3}
}

%fused_computation.479 (param_0.2016: f16[4096,384]) -> (f16[4,4,32,1024], f16[4,4,32,1024]) {
  %param_0.2016 = f16[4096,384]{1,0} parameter(0)
  %bitcast.1274 = f16[4,1024,384]{2,1,0} bitcast(f16[4096,384]{1,0} %param_0.2016), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %bitcast.1273 = f16[4,1024,128,3]{3,2,1,0} bitcast(f16[4,1024,384]{2,1,0} %bitcast.1274), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %copy.201 = f16[4,1024,128,3]{1,2,0,3} copy(f16[4,1024,128,3]{3,2,1,0} %bitcast.1273), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %slice.45 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.201), slice={[0:4], [0:1024], [0:128], [1:2]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/slice[start_indices=(0, 0, 0, 1) limit_indices=(8, 1024, 1024, 2) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.53 = f16[1,4,128,1024]{3,2,1,0} transpose(f16[4,1024,128,1]{1,2,0,3} %slice.45), dimensions={3,0,2,1}
  %bitcast.822 = f16[4,4,32,1024]{3,2,1,0} bitcast(f16[1,4,128,1024]{3,2,1,0} %transpose.53)
  %slice.75.clone.1 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.201), slice={[0:4], [0:1024], [0:128], [2:3]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/slice[start_indices=(0, 0, 0, 2) limit_indices=(8, 1024, 1024, 3) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.54 = f16[1,4,128,1024]{3,2,1,0} transpose(f16[4,1024,128,1]{1,2,0,3} %slice.75.clone.1), dimensions={3,0,2,1}
  %bitcast.997.clone.1 = f16[4,4,32,1024]{3,2,1,0} bitcast(f16[1,4,128,1024]{3,2,1,0} %transpose.54)
  ROOT %tuple.212 = (f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) tuple(f16[4,4,32,1024]{3,2,1,0} %bitcast.822, f16[4,4,32,1024]{3,2,1,0} %bitcast.997.clone.1)
}

%fused_computation.481 (param_0.941: f32[384]) -> f16[4096,384] {
  %param_0.941 = f32[384]{0} parameter(0)
  %convert.529 = f16[384]{0} convert(f32[384]{0} %param_0.941), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.141 = f16[4,1024,384]{2,1,0} broadcast(f16[384]{0} %convert.529), dimensions={2}
  ROOT %bitcast.825 = f16[4096,384]{1,0} bitcast(f16[4,1024,384]{2,1,0} %broadcast.141)
}

%fused_computation.482 (param_0.942: f16[4,1024,1024], param_1.1268: f16[4096,1024]) -> f16[4,1024,1024] {
  %param_0.942 = f16[4,1024,1024]{2,1,0} parameter(0)
  %param_1.1268 = f16[4096,1024]{1,0} parameter(1)
  %bitcast.826 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_1.1268), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  ROOT %add.1308 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %param_0.942, f16[4,1024,1024]{2,1,0} %bitcast.826), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
}

%fused_computation.483 (param_0.1603: f16[4,4,32,1024], param_1.2052: f16[4,4,1024,32], param_2.1795: f16[], param_3.1439: f16[4,4,1024,32]) -> f16[4,1024,128,3] {
  %param_3.1439 = f16[4,4,1024,32]{3,2,1,0} parameter(3)
  %transpose.55 = f16[4,1024,4,32]{3,1,2,0} transpose(f16[4,4,1024,32]{3,2,1,0} %param_3.1439), dimensions={0,2,1,3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/transpose[permutation=(0, 2, 1, 3)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %copy.131 = f16[4,1024,4,32]{1,3,2,0} copy(f16[4,1024,4,32]{3,1,2,0} %transpose.55), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/transpose[permutation=(0, 2, 1, 3)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %bitcast.830 = f16[4,1024,128,1]{1,2,0,3} bitcast(f16[4,1024,4,32]{1,3,2,0} %copy.131), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=173}
  %constant_760 = f16[] constant(0)
  %pad.35 = f16[4,1024,128,3]{1,2,0,3} pad(f16[4,1024,128,1]{1,2,0,3} %bitcast.830, f16[] %constant_760), padding=0_0x0_0x0_0x2_0, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (2, 0, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %param_1.2052 = f16[4,4,1024,32]{3,2,1,0} parameter(1)
  %param_2.1795 = f16[] parameter(2)
  %broadcast.142 = f16[4,4,1024,32]{3,2,1,0} broadcast(f16[] %param_2.1795), dimensions={}
  %divide.546 = f16[4,4,1024,32]{3,2,1,0} divide(f16[4,4,1024,32]{3,2,1,0} %param_1.2052, f16[4,4,1024,32]{3,2,1,0} %broadcast.142), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %transpose.56 = f16[4,1024,4,32]{3,1,2,0} transpose(f16[4,4,1024,32]{3,2,1,0} %divide.546), dimensions={0,2,1,3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %copy.130 = f16[4,1024,4,32]{1,3,2,0} copy(f16[4,1024,4,32]{3,1,2,0} %transpose.56), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %bitcast.828 = f16[4,1024,128,1]{1,2,0,3} bitcast(f16[4,1024,4,32]{1,3,2,0} %copy.130), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=167}
  %pad.34 = f16[4,1024,128,3]{1,2,0,3} pad(f16[4,1024,128,1]{1,2,0,3} %bitcast.828, f16[] %constant_760), padding=0_0x0_0x0_0x0_2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 2, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %add.1310 = f16[4,1024,128,3]{1,2,0,3} add(f16[4,1024,128,3]{1,2,0,3} %pad.35, f16[4,1024,128,3]{1,2,0,3} %pad.34), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %param_0.1603 = f16[4,4,32,1024]{3,2,1,0} parameter(0)
  %bitcast.1880 = f16[1,4,128,1024]{3,2,1,0} bitcast(f16[4,4,32,1024]{3,2,1,0} %param_0.1603)
  %transpose.57 = f16[4,1024,128,1]{1,2,0,3} transpose(f16[1,4,128,1024]{3,2,1,0} %bitcast.1880), dimensions={1,3,2,0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=170}
  %pad.33 = f16[4,1024,128,3]{1,2,0,3} pad(f16[4,1024,128,1]{1,2,0,3} %transpose.57, f16[] %constant_760), padding=0_0x0_0x0_0x1_1, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (1, 1, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %add.1309 = f16[4,1024,128,3]{1,2,0,3} add(f16[4,1024,128,3]{1,2,0,3} %add.1310, f16[4,1024,128,3]{1,2,0,3} %pad.33), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  ROOT %copy.129 = f16[4,1024,128,3]{3,2,1,0} copy(f16[4,1024,128,3]{1,2,0,3} %add.1309), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
}

%fused_computation.484 (param_0.947: f16[4096,128]) -> f16[4,4,32,1024] {
  %param_0.947 = f16[4096,128]{1,0} parameter(0)
  %bitcast.1881 = f16[4,1024,4,32]{3,2,1,0} bitcast(f16[4096,128]{1,0} %param_0.947)
  %transpose.58 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %bitcast.1881), dimensions={0,2,3,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/transpose[permutation=(0, 2, 3, 1)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  ROOT %copy.132 = f16[4,4,32,1024]{3,2,1,0} copy(f16[4,4,32,1024]{2,1,3,0} %transpose.58), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/transpose[permutation=(0, 2, 3, 1)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
}

%fused_computation.485 (param_0.1951: f16[4096,384], param_1.3110: f16[]) -> (f16[4,4,1024,32], f16[4,4,1024,32]) {
  %param_0.1951 = f16[4096,384]{1,0} parameter(0)
  %bitcast.1208 = f16[4,1024,384]{2,1,0} bitcast(f16[4096,384]{1,0} %param_0.1951), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %bitcast.1207 = f16[4,1024,128,3]{3,2,1,0} bitcast(f16[4,1024,384]{2,1,0} %bitcast.1208), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %copy.199 = f16[4,1024,128,3]{1,2,0,3} copy(f16[4,1024,128,3]{3,2,1,0} %bitcast.1207), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %slice.76 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.199), slice={[0:4], [0:1024], [0:128], [2:3]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/slice[start_indices=(0, 0, 0, 2) limit_indices=(8, 1024, 1024, 3) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.59 = f16[1,4,128,1024]{3,2,1,0} transpose(f16[4,1024,128,1]{1,2,0,3} %slice.76), dimensions={3,0,2,1}
  %bitcast.1882 = f16[4,4,32,1024]{3,2,1,0} bitcast(f16[1,4,128,1024]{3,2,1,0} %transpose.59)
  %transpose.60 = f16[4,4,1024,32]{2,3,1,0} transpose(f16[4,4,32,1024]{3,2,1,0} %bitcast.1882), dimensions={0,1,3,2}
  %copy.133 = f16[4,4,1024,32]{3,2,1,0} copy(f16[4,4,1024,32]{2,3,1,0} %transpose.60)
  %slice.46.clone.1 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.199), slice={[0:4], [0:1024], [0:128], [0:1]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/slice[start_indices=(0, 0, 0, 0) limit_indices=(8, 1024, 1024, 1) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %param_1.3110 = f16[] parameter(1)
  %broadcast.165.clone.1 = f16[4,1024,128,1]{1,2,0,3} broadcast(f16[] %param_1.3110), dimensions={}
  %divide.563.clone.1 = f16[4,1024,128,1]{1,2,0,3} divide(f16[4,1024,128,1]{1,2,0,3} %slice.46.clone.1, f16[4,1024,128,1]{1,2,0,3} %broadcast.165.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %bitcast.856.clone.1 = f16[4,1024,4,32]{1,3,2,0} bitcast(f16[4,1024,128,1]{1,2,0,3} %divide.563.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %copy.136.clone.1 = f16[4,1024,4,32]{3,1,2,0} copy(f16[4,1024,4,32]{1,3,2,0} %bitcast.856.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %transpose.61 = f16[4,4,1024,32]{3,2,1,0} transpose(f16[4,1024,4,32]{3,1,2,0} %copy.136.clone.1), dimensions={0,2,1,3}
  ROOT %tuple.221 = (f16[4,4,1024,32]{3,2,1,0}, f16[4,4,1024,32]{3,2,1,0}) tuple(f16[4,4,1024,32]{3,2,1,0} %copy.133, f16[4,4,1024,32]{3,2,1,0} %transpose.61)
}

%fused_computation.486 (param_0.1960: f16[4,4,1024,1024], param_1.2366: f16[4,4,1024], param_2.2111: f32[4,4,1024], param_3.1575: f16[4,4,1024,1024], param_4.1042: f16[4,4,1024]) -> f16[4,4,1024,1024] {
  %param_1.2366 = f16[4,4,1024]{2,1,0} parameter(1)
  %negate.72 = f16[4,4,1024]{2,1,0} negate(f16[4,4,1024]{2,1,0} %param_1.2366), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.143 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %negate.72), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/broadcast_in_dim[shape=(8, 32, 1024, 1024) broadcast_dimensions=(0, 1, 2)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %param_0.1960 = f16[4,4,1024,1024]{3,2,1,0} parameter(0)
  %param_2.2111 = f32[4,4,1024]{2,1,0} parameter(2)
  %convert.676 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %param_2.2111), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.1557 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %convert.676), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.547 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %param_0.1960, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1557), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %add.1311 = f16[4,4,1024,1024]{3,2,1,0} add(f16[4,4,1024,1024]{3,2,1,0} %broadcast.143, f16[4,4,1024,1024]{3,2,1,0} %divide.547), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %param_3.1575 = f16[4,4,1024,1024]{3,2,1,0} parameter(3)
  %param_4.1042 = f16[4,4,1024]{2,1,0} parameter(4)
  %broadcast.2011 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_4.1042), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.166 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_3.1575, f16[4,4,1024,1024]{3,2,1,0} %broadcast.2011), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.73 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.166), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  ROOT %multiply.1902 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %add.1311, f16[4,4,1024,1024]{3,2,1,0} %exponential.73), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%region_62.2162 (Arg_0.2163: f16[], Arg_1.2164: f16[]) -> f16[] {
  %Arg_0.2163 = f16[] parameter(0)
  %Arg_1.2164 = f16[] parameter(1)
  ROOT %add.2165 = f16[] add(f16[] %Arg_0.2163, f16[] %Arg_1.2164), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.487 (param_0.1958: f16[4,4,1024,1024], param_1.2364: f32[4,4,1024], param_2.2110: f16[4,4,1024,1024], param_3.1573: f16[4,4,1024]) -> f16[4,4,1024] {
  %param_0.1958 = f16[4,4,1024,1024]{3,2,1,0} parameter(0)
  %constant_761 = f16[] constant(1)
  %broadcast.1558 = f16[4,4,1024]{2,1,0} broadcast(f16[] %constant_761), dimensions={}
  %param_1.2364 = f32[4,4,1024]{2,1,0} parameter(1)
  %convert.677 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %param_1.2364), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.1905 = f16[4,4,1024]{2,1,0} multiply(f16[4,4,1024]{2,1,0} %convert.677, f16[4,4,1024]{2,1,0} %convert.677), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.548 = f16[4,4,1024]{2,1,0} divide(f16[4,4,1024]{2,1,0} %broadcast.1558, f16[4,4,1024]{2,1,0} %multiply.1905), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.144 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %divide.548), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.1904 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %param_0.1958, f16[4,4,1024,1024]{3,2,1,0} %broadcast.144), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %param_2.2110 = f16[4,4,1024,1024]{3,2,1,0} parameter(2)
  %param_3.1573 = f16[4,4,1024]{2,1,0} parameter(3)
  %broadcast.2009 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_3.1573), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.164 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_2.2110, f16[4,4,1024,1024]{3,2,1,0} %broadcast.2009), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.71 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.164), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.1903 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %multiply.1904, f16[4,4,1024,1024]{3,2,1,0} %exponential.71), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %constant_762 = f16[] constant(0)
  ROOT %reduce.291 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %multiply.1903, f16[] %constant_762), dimensions={3}, to_apply=%region_62.2162, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.488 (param_0.956: f16[4096,128]) -> f16[4,4,1024,32] {
  %param_0.956 = f16[4096,128]{1,0} parameter(0)
  %bitcast.835 = f16[4,1024,4,32]{3,2,1,0} bitcast(f16[4096,128]{1,0} %param_0.956), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %copy.134 = f16[4,1024,4,32]{3,1,2,0} copy(f16[4,1024,4,32]{3,2,1,0} %bitcast.835), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  ROOT %transpose.62 = f16[4,4,1024,32]{3,2,1,0} transpose(f16[4,1024,4,32]{3,1,2,0} %copy.134), dimensions={0,2,1,3}
}

%fused_computation.489 (param_0.2008: f32[4,1024], param_1.2439: f32[8,1024], param_2.2202: s32[], param_3.1662: f32[8,1024], param_4.1124: f32[8,1024], param_5.1129: f32[8,1024], param_6.960: f32[4,1024], param_7.584: f16[4,1024,1024], param_8.324: f32[], param_9.295: f32[4,1024], param_10.290: f32[], param_11.315: f32[], param_12.310: f32[4,1024], param_13.308: f32[], param_14.303: f32[1024], param_15.296: f16[4,1024,1024], param_16.311: f16[4096,1024]) -> f16[4,1024,1024] {
  %param_7.584 = f16[4,1024,1024]{2,1,0} parameter(7)
  %convert.532 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_7.584), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %param_6.960 = f32[4,1024]{1,0} parameter(6)
  %bitcast.837 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %param_6.960), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reshape[new_sizes=(8, 1024, 1) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_11.315 = f32[] parameter(11)
  %broadcast.2043 = f32[4,1024]{1,0} broadcast(f32[] %param_11.315), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_12.310 = f32[4,1024]{1,0} parameter(12)
  %param_8.324 = f32[] parameter(8)
  %broadcast.2042 = f32[4,1024]{1,0} broadcast(f32[] %param_8.324), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.657 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_12.310, f32[4,1024]{1,0} %broadcast.2042), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_9.295 = f32[4,1024]{1,0} parameter(9)
  %param_10.290 = f32[] parameter(10)
  %broadcast.2041 = f32[4,1024]{1,0} broadcast(f32[] %param_10.290), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.656 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_9.295, f32[4,1024]{1,0} %broadcast.2041), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2222 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.656, f32[4,1024]{1,0} %divide.656), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.181 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.657, f32[4,1024]{1,0} %multiply.2222), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.76 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.2043, f32[4,1024]{1,0} %subtract.181), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_13.308 = f32[] parameter(13)
  %broadcast.2040 = f32[4,1024]{1,0} broadcast(f32[] %param_13.308), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.1657 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.76, f32[4,1024]{1,0} %broadcast.2040), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1212 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.1657), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.58 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1212), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %divide.552 = f32[4,1024,1]{1,0,2} divide(f32[4,1024,1]{1,0,2} %rsqrt.58, f32[4,1024,1]{1,0,2} %bitcast.1212), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %constant_763 = f32[] constant(-0.5)
  %broadcast.1559 = f32[4,1024,1]{1,0,2} broadcast(f32[] %constant_763), dimensions={}
  %multiply.1912 = f32[4,1024,1]{1,0,2} multiply(f32[4,1024,1]{1,0,2} %divide.552, f32[4,1024,1]{1,0,2} %broadcast.1559), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.1911 = f32[4,1024,1]{1,0,2} multiply(f32[4,1024,1]{1,0,2} %bitcast.837, f32[4,1024,1]{1,0,2} %multiply.1912), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.836 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %multiply.1911), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reshape[new_sizes=(8, 1024) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=132}
  %compare.70 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %subtract.181, f32[4,1024]{1,0} %maximum.76), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_5.1129 = f32[8,1024]{1,0} parameter(5)
  %param_2.2202 = s32[] parameter(2)
  %constant_765 = s32[] constant(0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %dynamic-slice.62 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_5.1129, s32[] %param_2.2202, s32[] %constant_765), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_4.1124 = f32[8,1024]{1,0} parameter(4)
  %dynamic-slice.61 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_4.1124, s32[] %param_2.2202, s32[] %constant_765), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.64 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.70, f32[4,1024]{1,0} %dynamic-slice.62, f32[4,1024]{1,0} %dynamic-slice.61), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %compare.69 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %broadcast.2043, f32[4,1024]{1,0} %maximum.76), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_3.1662 = f32[8,1024]{1,0} parameter(3)
  %dynamic-slice.60 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_3.1662, s32[] %param_2.2202, s32[] %constant_765), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_1.2439 = f32[8,1024]{1,0} parameter(1)
  %dynamic-slice.59 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_1.2439, s32[] %param_2.2202, s32[] %constant_765), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.63 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.69, f32[4,1024]{1,0} %dynamic-slice.60, f32[4,1024]{1,0} %dynamic-slice.59), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.551 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %select.64, f32[4,1024]{1,0} %select.63), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.1910 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %bitcast.836, f32[4,1024]{1,0} %divide.551), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.550 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %multiply.1910, f32[4,1024]{1,0} %broadcast.2042), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_764 = f32[] constant(2)
  %broadcast.1560 = f32[4,1024]{1,0} broadcast(f32[] %constant_764), dimensions={}
  %multiply.1909 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.550, f32[4,1024]{1,0} %broadcast.1560), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %broadcast.146 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.1909), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.1908 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.532, f32[4,1024,1024]{2,1,0} %broadcast.146), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %negate.73 = f32[4,1024]{1,0} negate(f32[4,1024]{1,0} %multiply.1910), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.1907 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.656, f32[4,1024]{1,0} %broadcast.1560), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.1906 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %negate.73, f32[4,1024]{1,0} %multiply.1907), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %param_0.2008 = f32[4,1024]{1,0} parameter(0)
  %add.1314 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %multiply.1906, f32[4,1024]{1,0} %param_0.2008), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=131}
  %divide.549 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %add.1314, f32[4,1024]{1,0} %broadcast.2041), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.145 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.549), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/broadcast_in_dim[shape=(8, 1024, 1024) broadcast_dimensions=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %add.1313 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.1908, f32[4,1024,1024]{2,1,0} %broadcast.145), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %convert.531 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.1313), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %param_15.296 = f16[4,1024,1024]{2,1,0} parameter(15)
  %param_16.311 = f16[4096,1024]{1,0} parameter(16)
  %bitcast.1266 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_16.311), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.1745 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %param_15.296, f16[4,1024,1024]{2,1,0} %bitcast.1266), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.796 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1745), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %bitcast.1264 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.58), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.2237 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1264), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_14.303 = f32[1024]{0} parameter(14)
  %broadcast.2236 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_14.303), dimensions={2}
  %multiply.2319 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.2237, f32[4,1024,1024]{2,1,0} %broadcast.2236), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2318 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.796, f32[4,1024,1024]{2,1,0} %multiply.2319), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %convert.530 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %multiply.2318), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  ROOT %add.1312 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %convert.531, f16[4,1024,1024]{2,1,0} %convert.530), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%region_60.2131 (Arg_0.2132: f32[], Arg_1.2133: f32[]) -> f32[] {
  %Arg_0.2132 = f32[] parameter(0)
  %Arg_1.2133 = f32[] parameter(1)
  ROOT %add.2134 = f32[] add(f32[] %Arg_0.2132, f32[] %Arg_1.2133), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%region_59.2114 (Arg_0.2115: f32[], Arg_1.2116: f32[]) -> f32[] {
  %Arg_0.2115 = f32[] parameter(0)
  %Arg_1.2116 = f32[] parameter(1)
  ROOT %add.2117 = f32[] add(f32[] %Arg_0.2115, f32[] %Arg_1.2116), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%fused_computation.490 (param_0.2006: f32[1024], param_1.2437: f32[], param_2.2200: f32[4,1024], param_3.1660: f32[], param_4.1122: f32[4,1024], param_5.1127: f32[], param_6.958: f32[], param_7.582: f16[4,1024,1024], param_8.322: f16[4096,1024], param_9.492: f16[4,1024,1024]) -> (f32[4,1024], f32[4,1024]) {
  %param_7.582 = f16[4,1024,1024]{2,1,0} parameter(7)
  %param_8.322 = f16[4096,1024]{1,0} parameter(8)
  %bitcast.1260 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_8.322), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.1741 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %param_7.582, f16[4,1024,1024]{2,1,0} %bitcast.1260), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.794 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1741), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %param_6.958 = f32[] parameter(6)
  %broadcast.2229 = f32[4,1024]{1,0} broadcast(f32[] %param_6.958), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_4.1122 = f32[4,1024]{1,0} parameter(4)
  %param_5.1127 = f32[] parameter(5)
  %broadcast.2228 = f32[4,1024]{1,0} broadcast(f32[] %param_5.1127), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.717 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_4.1122, f32[4,1024]{1,0} %broadcast.2228), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.2200 = f32[4,1024]{1,0} parameter(2)
  %param_3.1660 = f32[] parameter(3)
  %broadcast.2227 = f32[4,1024]{1,0} broadcast(f32[] %param_3.1660), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.716 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_2.2200, f32[4,1024]{1,0} %broadcast.2227), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2314 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.716, f32[4,1024]{1,0} %divide.716), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.214 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.717, f32[4,1024]{1,0} %multiply.2314), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.96 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.2229, f32[4,1024]{1,0} %subtract.214), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_1.2437 = f32[] parameter(1)
  %broadcast.2226 = f32[4,1024]{1,0} broadcast(f32[] %param_1.2437), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.1739 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.96, f32[4,1024]{1,0} %broadcast.2226), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1259 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.1739), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.84 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1259), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1258 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.84), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.2225 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1258), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_0.2006 = f32[1024]{0} parameter(0)
  %broadcast.2224 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_0.2006), dimensions={2}
  %multiply.2313 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.2225, f32[4,1024,1024]{2,1,0} %broadcast.2224), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2312 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.794, f32[4,1024,1024]{2,1,0} %multiply.2313), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %negate.74 = f32[4,1024,1024]{2,1,0} negate(f32[4,1024,1024]{2,1,0} %multiply.2312), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %constant_767 = f32[] constant(0)
  %reduce.292 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %negate.74, f32[] %constant_767), dimensions={2}, to_apply=%region_60.2131, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_9.492 = f16[4,1024,1024]{2,1,0} parameter(9)
  %convert.800.clone.1 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_9.492), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %broadcast.2244.clone.1 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.716), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.218.clone.1 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.800.clone.1, f32[4,1024,1024]{2,1,0} %broadcast.2244.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.2322.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.218.clone.1, f32[4,1024,1024]{2,1,0} %convert.794), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.1914.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %multiply.2322.clone.1, f32[4,1024,1024]{2,1,0} %broadcast.2224), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.293.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1914.clone.1, f32[] %constant_767), dimensions={2}, to_apply=%region_59.2114, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  ROOT %tuple.214 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) tuple(f32[4,1024]{1,0} %reduce.292, f32[4,1024]{1,0} %reduce.293.clone.1)
}

%fused_computation.495 (param_0.1977: f16[4096,512], param_1.2395: f16[], param_2.2147: f16[], param_3.1614: f16[4096,512], param_4.1081: f16[]) -> f16[4,1024,512] {
  %param_3.1614 = f16[4096,512]{1,0} parameter(3)
  %bitcast.998 = f16[4,1024,512]{2,1,0} bitcast(f16[4096,512]{1,0} %param_3.1614), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_0.1977 = f16[4096,512]{1,0} parameter(0)
  %param_1.2395 = f16[] parameter(1)
  %broadcast.147 = f16[4096,512]{1,0} broadcast(f16[] %param_1.2395), dimensions={}
  %divide.554 = f16[4096,512]{1,0} divide(f16[4096,512]{1,0} %param_0.1977, f16[4096,512]{1,0} %broadcast.147), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %bitcast.839 = f16[4,1024,512]{2,1,0} bitcast(f16[4096,512]{1,0} %divide.554), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1920 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %bitcast.998, f16[4,1024,512]{2,1,0} %bitcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_768 = f16[] constant(1.1279)
  %broadcast.1566 = f16[4,1024,512]{2,1,0} broadcast(f16[] %constant_768), dimensions={}
  %multiply.1919 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.1920, f16[4,1024,512]{2,1,0} %broadcast.1566), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %param_2.2147 = f16[] parameter(2)
  %broadcast.1567 = f16[4,1024,512]{2,1,0} broadcast(f16[] %param_2.2147), dimensions={}
  %divide.626 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %bitcast.998, f16[4,1024,512]{2,1,0} %broadcast.1567), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1918 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %divide.626, f16[4,1024,512]{2,1,0} %divide.626), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %negate.75 = f16[4,1024,512]{2,1,0} negate(f16[4,1024,512]{2,1,0} %multiply.1918), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/neg" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %exponential.29 = f16[4,1024,512]{2,1,0} exponential(f16[4,1024,512]{2,1,0} %negate.75), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/exp" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1917 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.1919, f16[4,1024,512]{2,1,0} %exponential.29), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.553 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %multiply.1917, f16[4,1024,512]{2,1,0} %broadcast.1567), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1306 = f32[] constant(-4)
  %broadcast.2141 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1306), dimensions={}
  %convert.780 = f32[4,1024,512]{2,1,0} convert(f16[4,1024,512]{2,1,0} %divide.626), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1305 = f32[] constant(4)
  %broadcast.2139 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1305), dimensions={}
  %clamp.29 = f32[4,1024,512]{2,1,0} clamp(f32[4,1024,512]{2,1,0} %broadcast.2141, f32[4,1024,512]{2,1,0} %convert.780, f32[4,1024,512]{2,1,0} %broadcast.2139), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2280 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.29, f32[4,1024,512]{2,1,0} %clamp.29), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1304 = f32[] constant(0)
  %broadcast.2138 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1304), dimensions={}
  %multiply.2279 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %multiply.2280, f32[4,1024,512]{2,1,0} %broadcast.2138), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1303 = f32[] constant(-2.72614237e-10)
  %broadcast.2137 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1303), dimensions={}
  %add.1717 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2279, f32[4,1024,512]{2,1,0} %broadcast.2137), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2278 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1717, f32[4,1024,512]{2,1,0} %multiply.2280), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1302 = f32[] constant(2.77068146e-08)
  %broadcast.2136 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1302), dimensions={}
  %add.1716 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2278, f32[4,1024,512]{2,1,0} %broadcast.2136), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2277 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1716, f32[4,1024,512]{2,1,0} %multiply.2280), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1301 = f32[] constant(-2.10102394e-06)
  %broadcast.2135 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1301), dimensions={}
  %add.1715 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2277, f32[4,1024,512]{2,1,0} %broadcast.2135), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2276 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1715, f32[4,1024,512]{2,1,0} %multiply.2280), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1300 = f32[] constant(-5.69250624e-05)
  %broadcast.2134 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1300), dimensions={}
  %add.1714 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2276, f32[4,1024,512]{2,1,0} %broadcast.2134), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2275 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1714, f32[4,1024,512]{2,1,0} %multiply.2280), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1299 = f32[] constant(-0.000734990637)
  %broadcast.2133 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1299), dimensions={}
  %add.1713 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2275, f32[4,1024,512]{2,1,0} %broadcast.2133), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2274 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1713, f32[4,1024,512]{2,1,0} %multiply.2280), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1298 = f32[] constant(-0.0029546)
  %broadcast.2132 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1298), dimensions={}
  %add.1712 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2274, f32[4,1024,512]{2,1,0} %broadcast.2132), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2273 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1712, f32[4,1024,512]{2,1,0} %multiply.2280), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1297 = f32[] constant(-0.0160960332)
  %broadcast.2131 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1297), dimensions={}
  %add.1711 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2273, f32[4,1024,512]{2,1,0} %broadcast.2131), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2272 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.29, f32[4,1024,512]{2,1,0} %add.1711), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1296 = f32[] constant(-1.45660715e-05)
  %broadcast.2130 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1296), dimensions={}
  %add.1709 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2279, f32[4,1024,512]{2,1,0} %broadcast.2130), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2271 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1709, f32[4,1024,512]{2,1,0} %multiply.2280), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1295 = f32[] constant(-0.000213374049)
  %broadcast.2129 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1295), dimensions={}
  %add.1708 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2271, f32[4,1024,512]{2,1,0} %broadcast.2129), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2270 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1708, f32[4,1024,512]{2,1,0} %multiply.2280), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1294 = f32[] constant(-0.00168282702)
  %broadcast.2128 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1294), dimensions={}
  %add.1707 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2270, f32[4,1024,512]{2,1,0} %broadcast.2128), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2269 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1707, f32[4,1024,512]{2,1,0} %multiply.2280), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1293 = f32[] constant(-0.00737332925)
  %broadcast.2127 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1293), dimensions={}
  %add.1706 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2269, f32[4,1024,512]{2,1,0} %broadcast.2127), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2268 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1706, f32[4,1024,512]{2,1,0} %multiply.2280), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1292 = f32[] constant(-0.0142647391)
  %broadcast.2126 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1292), dimensions={}
  %add.1705 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2268, f32[4,1024,512]{2,1,0} %broadcast.2126), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.676 = f32[4,1024,512]{2,1,0} divide(f32[4,1024,512]{2,1,0} %multiply.2272, f32[4,1024,512]{2,1,0} %add.1705), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.779 = f16[4,1024,512]{2,1,0} convert(f32[4,1024,512]{2,1,0} %divide.676), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %param_4.1081 = f16[] parameter(4)
  %broadcast.2125 = f16[4,1024,512]{2,1,0} broadcast(f16[] %param_4.1081), dimensions={}
  %add.1704 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %convert.779, f16[4,1024,512]{2,1,0} %broadcast.2125), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1916 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %bitcast.839, f16[4,1024,512]{2,1,0} %add.1704), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  ROOT %add.1316 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %divide.553, f16[4,1024,512]{2,1,0} %multiply.1916), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
}

%fused_computation.496 (param_0.1993: f32[4,1024], param_1.2420: f32[8,1024], param_2.2181: s32[], param_3.1645: f32[8,1024], param_4.1109: f32[8,1024], param_5.1118: f32[8,1024], param_6.949: f32[4,1024], param_7.571: f16[4,1024,1024], param_8.314: f32[], param_9.294: f32[4,1024], param_10.289: f32[], param_11.314: f32[], param_12.309: f32[4,1024], param_13.307: f32[], param_14.301: f32[1024], param_15.291: f16[4,1024,1024]) -> f16[4,1024,1024] {
  %param_7.571 = f16[4,1024,1024]{2,1,0} parameter(7)
  %convert.537 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_7.571), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %param_6.949 = f32[4,1024]{1,0} parameter(6)
  %bitcast.841 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %param_6.949), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reshape[new_sizes=(8, 1024, 1) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_11.314 = f32[] parameter(11)
  %broadcast.2165 = f32[4,1024]{1,0} broadcast(f32[] %param_11.314), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_12.309 = f32[4,1024]{1,0} parameter(12)
  %param_8.314 = f32[] parameter(8)
  %broadcast.2164 = f32[4,1024]{1,0} broadcast(f32[] %param_8.314), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.693 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_12.309, f32[4,1024]{1,0} %broadcast.2164), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_9.294 = f32[4,1024]{1,0} parameter(9)
  %param_10.289 = f32[] parameter(10)
  %broadcast.2163 = f32[4,1024]{1,0} broadcast(f32[] %param_10.289), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.692 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_9.294, f32[4,1024]{1,0} %broadcast.2163), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2288 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.692, f32[4,1024]{1,0} %divide.692), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.197 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.693, f32[4,1024]{1,0} %multiply.2288), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.86 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.2165, f32[4,1024]{1,0} %subtract.197), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_13.307 = f32[] parameter(13)
  %broadcast.2162 = f32[4,1024]{1,0} broadcast(f32[] %param_13.307), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.1721 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.86, f32[4,1024]{1,0} %broadcast.2162), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1232 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.1721), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.59 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1232), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %divide.558 = f32[4,1024,1]{1,0,2} divide(f32[4,1024,1]{1,0,2} %rsqrt.59, f32[4,1024,1]{1,0,2} %bitcast.1232), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %constant_769 = f32[] constant(-0.5)
  %broadcast.1568 = f32[4,1024,1]{1,0,2} broadcast(f32[] %constant_769), dimensions={}
  %multiply.1927 = f32[4,1024,1]{1,0,2} multiply(f32[4,1024,1]{1,0,2} %divide.558, f32[4,1024,1]{1,0,2} %broadcast.1568), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.1926 = f32[4,1024,1]{1,0,2} multiply(f32[4,1024,1]{1,0,2} %bitcast.841, f32[4,1024,1]{1,0,2} %multiply.1927), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.840 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %multiply.1926), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reshape[new_sizes=(8, 1024) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=132}
  %compare.72 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %subtract.197, f32[4,1024]{1,0} %maximum.86), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_5.1118 = f32[8,1024]{1,0} parameter(5)
  %param_2.2181 = s32[] parameter(2)
  %constant_771 = s32[] constant(0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %dynamic-slice.66 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_5.1118, s32[] %param_2.2181, s32[] %constant_771), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_4.1109 = f32[8,1024]{1,0} parameter(4)
  %dynamic-slice.65 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_4.1109, s32[] %param_2.2181, s32[] %constant_771), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.66 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.72, f32[4,1024]{1,0} %dynamic-slice.66, f32[4,1024]{1,0} %dynamic-slice.65), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %compare.71 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %broadcast.2165, f32[4,1024]{1,0} %maximum.86), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_3.1645 = f32[8,1024]{1,0} parameter(3)
  %dynamic-slice.64 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_3.1645, s32[] %param_2.2181, s32[] %constant_771), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_1.2420 = f32[8,1024]{1,0} parameter(1)
  %dynamic-slice.63 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %param_1.2420, s32[] %param_2.2181, s32[] %constant_771), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.65 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.71, f32[4,1024]{1,0} %dynamic-slice.64, f32[4,1024]{1,0} %dynamic-slice.63), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.557 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %select.66, f32[4,1024]{1,0} %select.65), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.1925 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %bitcast.840, f32[4,1024]{1,0} %divide.557), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.556 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %multiply.1925, f32[4,1024]{1,0} %broadcast.2164), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_770 = f32[] constant(2)
  %broadcast.1569 = f32[4,1024]{1,0} broadcast(f32[] %constant_770), dimensions={}
  %multiply.1924 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.556, f32[4,1024]{1,0} %broadcast.1569), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %broadcast.149 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.1924), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.1923 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.537, f32[4,1024,1024]{2,1,0} %broadcast.149), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %negate.76 = f32[4,1024]{1,0} negate(f32[4,1024]{1,0} %multiply.1925), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.1922 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.692, f32[4,1024]{1,0} %broadcast.1569), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.1921 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %negate.76, f32[4,1024]{1,0} %multiply.1922), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %param_0.1993 = f32[4,1024]{1,0} parameter(0)
  %add.1319 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %multiply.1921, f32[4,1024]{1,0} %param_0.1993), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=131}
  %divide.555 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %add.1319, f32[4,1024]{1,0} %broadcast.2163), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.148 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.555), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/broadcast_in_dim[shape=(8, 1024, 1024) broadcast_dimensions=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %add.1318 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.1923, f32[4,1024,1024]{2,1,0} %broadcast.148), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %convert.536 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.1318), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %param_15.291 = f16[4,1024,1024]{2,1,0} parameter(15)
  %convert.784 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_15.291), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %bitcast.1247 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.59), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.2205 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1247), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_14.301 = f32[1024]{0} parameter(14)
  %broadcast.2204 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_14.301), dimensions={2}
  %multiply.2303 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.2205, f32[4,1024,1024]{2,1,0} %broadcast.2204), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2302 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.784, f32[4,1024,1024]{2,1,0} %multiply.2303), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %convert.535 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %multiply.2302), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  ROOT %add.1317 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %convert.536, f16[4,1024,1024]{2,1,0} %convert.535), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%region_54.2051 (Arg_0.2052: f32[], Arg_1.2053: f32[]) -> f32[] {
  %Arg_0.2052 = f32[] parameter(0)
  %Arg_1.2053 = f32[] parameter(1)
  ROOT %add.2054 = f32[] add(f32[] %Arg_0.2052, f32[] %Arg_1.2053), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%region_53.2034 (Arg_0.2035: f32[], Arg_1.2036: f32[]) -> f32[] {
  %Arg_0.2035 = f32[] parameter(0)
  %Arg_1.2036 = f32[] parameter(1)
  ROOT %add.2037 = f32[] add(f32[] %Arg_0.2035, f32[] %Arg_1.2036), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%fused_computation.497 (param_0.1991: f32[1024], param_1.2418: f32[], param_2.2179: f32[4,1024], param_3.1643: f32[], param_4.1107: f32[4,1024], param_5.1116: f32[], param_6.947: f32[], param_7.569: f16[4,1024,1024], param_8.622: f16[4,1024,1024]) -> (f32[4,1024], f32[4,1024]) {
  %param_7.569 = f16[4,1024,1024]{2,1,0} parameter(7)
  %convert.782 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_7.569), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %param_6.947 = f32[] parameter(6)
  %broadcast.2197 = f32[4,1024]{1,0} broadcast(f32[] %param_6.947), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_4.1107 = f32[4,1024]{1,0} parameter(4)
  %param_5.1116 = f32[] parameter(5)
  %broadcast.2196 = f32[4,1024]{1,0} broadcast(f32[] %param_5.1116), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.705 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_4.1107, f32[4,1024]{1,0} %broadcast.2196), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.2179 = f32[4,1024]{1,0} parameter(2)
  %param_3.1643 = f32[] parameter(3)
  %broadcast.2195 = f32[4,1024]{1,0} broadcast(f32[] %param_3.1643), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.704 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_2.2179, f32[4,1024]{1,0} %broadcast.2195), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2298 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.704, f32[4,1024]{1,0} %divide.704), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.203 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.705, f32[4,1024]{1,0} %multiply.2298), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.92 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.2197, f32[4,1024]{1,0} %subtract.203), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_1.2418 = f32[] parameter(1)
  %broadcast.2194 = f32[4,1024]{1,0} broadcast(f32[] %param_1.2418), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.1728 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.92, f32[4,1024]{1,0} %broadcast.2194), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1244 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.1728), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.80 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1244), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1243 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.80), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.2193 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1243), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_0.1991 = f32[1024]{0} parameter(0)
  %broadcast.2192 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_0.1991), dimensions={2}
  %multiply.2297 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.2193, f32[4,1024,1024]{2,1,0} %broadcast.2192), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2296 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.782, f32[4,1024,1024]{2,1,0} %multiply.2297), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %negate.77 = f32[4,1024,1024]{2,1,0} negate(f32[4,1024,1024]{2,1,0} %multiply.2296), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %constant_773 = f32[] constant(0)
  %reduce.294 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %negate.77, f32[] %constant_773), dimensions={2}, to_apply=%region_54.2051, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_8.622 = f16[4,1024,1024]{2,1,0} parameter(8)
  %convert.788.clone.1 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_8.622), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %broadcast.2212.clone.1 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.704), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.207.clone.1 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.788.clone.1, f32[4,1024,1024]{2,1,0} %broadcast.2212.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.2306.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.207.clone.1, f32[4,1024,1024]{2,1,0} %convert.782), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.1932.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %multiply.2306.clone.1, f32[4,1024,1024]{2,1,0} %broadcast.2192), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.296.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1932.clone.1, f32[] %constant_773), dimensions={2}, to_apply=%region_53.2034, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  ROOT %tuple.216 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) tuple(f32[4,1024]{1,0} %reduce.294, f32[4,1024]{1,0} %reduce.296.clone.1)
}

%region_49.1953 (Arg_0.1954: f32[], Arg_1.1955: f32[]) -> f32[] {
  %Arg_0.1954 = f32[] parameter(0)
  %Arg_1.1955 = f32[] parameter(1)
  ROOT %add.1956 = f32[] add(f32[] %Arg_0.1954, f32[] %Arg_1.1955), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_50.1966 (Arg_0.1967: f32[], Arg_1.1968: f32[]) -> f32[] {
  %Arg_0.1967 = f32[] parameter(0)
  %Arg_1.1968 = f32[] parameter(1)
  ROOT %add.1969 = f32[] add(f32[] %Arg_0.1967, f32[] %Arg_1.1968), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%fused_computation.505 (param_0.2421: f16[4,1024,1024], param_1.3100: f32[1024], param_2.2990: f16[4096,1024]) -> (f32[4,1024], f32[4,1024], f16[4,1024,1024]) {
  %param_2.2990 = f16[4096,1024]{1,0} parameter(2)
  %bitcast.844.clone.1 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_2.2990), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_1.3100 = f32[1024]{0} parameter(1)
  %convert.543.clone.1 = f16[1024]{0} convert(f32[1024]{0} %param_1.3100), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.153.clone.1 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.543.clone.1), dimensions={2}
  %add.1323.clone.1 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.844.clone.1, f16[4,1024,1024]{2,1,0} %broadcast.153.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_0.2421 = f16[4,1024,1024]{2,1,0} parameter(0)
  %add.1321.clone.1 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1323.clone.1, f16[4,1024,1024]{2,1,0} %param_0.2421), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=310}
  %convert.542 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1321.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %constant_775 = f32[] constant(0)
  %reduce.297 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.542, f32[] %constant_775), dimensions={2}, to_apply=%region_49.1953, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.1931.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.542, f32[4,1024,1024]{2,1,0} %convert.542), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.295.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1931.clone.1, f32[] %constant_775), dimensions={2}, to_apply=%region_50.1966, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  ROOT %tuple.218 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) tuple(f32[4,1024]{1,0} %reduce.297, f32[4,1024]{1,0} %reduce.295.clone.1, f16[4,1024,1024]{2,1,0} %add.1321.clone.1)
}

%fused_computation.507 (param_0.994: f16[], param_1.2393: f16[4096,512], param_2.2145: f16[], param_3.1613: f16[]) -> f16[4096,512] {
  %param_1.2393 = f16[4096,512]{1,0} parameter(1)
  %bitcast.999 = f16[4,1024,512]{2,1,0} bitcast(f16[4096,512]{1,0} %param_1.2393), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %constant_1275 = f32[] constant(-4)
  %broadcast.2107 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1275), dimensions={}
  %param_3.1613 = f16[] parameter(3)
  %broadcast.2106 = f16[4,1024,512]{2,1,0} broadcast(f16[] %param_3.1613), dimensions={}
  %divide.673 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %bitcast.999, f16[4,1024,512]{2,1,0} %broadcast.2106), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.776 = f32[4,1024,512]{2,1,0} convert(f16[4,1024,512]{2,1,0} %divide.673), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1274 = f32[] constant(4)
  %broadcast.2105 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1274), dimensions={}
  %clamp.27 = f32[4,1024,512]{2,1,0} clamp(f32[4,1024,512]{2,1,0} %broadcast.2107, f32[4,1024,512]{2,1,0} %convert.776, f32[4,1024,512]{2,1,0} %broadcast.2105), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2254 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.27, f32[4,1024,512]{2,1,0} %clamp.27), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1273 = f32[] constant(0)
  %broadcast.2104 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1273), dimensions={}
  %multiply.2253 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %multiply.2254, f32[4,1024,512]{2,1,0} %broadcast.2104), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1272 = f32[] constant(-2.72614237e-10)
  %broadcast.2103 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1272), dimensions={}
  %add.1689 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2253, f32[4,1024,512]{2,1,0} %broadcast.2103), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2252 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1689, f32[4,1024,512]{2,1,0} %multiply.2254), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1271 = f32[] constant(2.77068146e-08)
  %broadcast.2102 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1271), dimensions={}
  %add.1688 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2252, f32[4,1024,512]{2,1,0} %broadcast.2102), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2251 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1688, f32[4,1024,512]{2,1,0} %multiply.2254), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1270 = f32[] constant(-2.10102394e-06)
  %broadcast.2101 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1270), dimensions={}
  %add.1687 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2251, f32[4,1024,512]{2,1,0} %broadcast.2101), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2250 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1687, f32[4,1024,512]{2,1,0} %multiply.2254), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1269 = f32[] constant(-5.69250624e-05)
  %broadcast.2100 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1269), dimensions={}
  %add.1686 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2250, f32[4,1024,512]{2,1,0} %broadcast.2100), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2249 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1686, f32[4,1024,512]{2,1,0} %multiply.2254), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1268 = f32[] constant(-0.000734990637)
  %broadcast.2099 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1268), dimensions={}
  %add.1685 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2249, f32[4,1024,512]{2,1,0} %broadcast.2099), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2248 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1685, f32[4,1024,512]{2,1,0} %multiply.2254), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1267 = f32[] constant(-0.0029546)
  %broadcast.2098 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1267), dimensions={}
  %add.1684 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2248, f32[4,1024,512]{2,1,0} %broadcast.2098), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2247 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1684, f32[4,1024,512]{2,1,0} %multiply.2254), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1266 = f32[] constant(-0.0160960332)
  %broadcast.2097 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1266), dimensions={}
  %add.1683 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2247, f32[4,1024,512]{2,1,0} %broadcast.2097), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2246 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.27, f32[4,1024,512]{2,1,0} %add.1683), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1265 = f32[] constant(-1.45660715e-05)
  %broadcast.2096 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1265), dimensions={}
  %add.1682 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2253, f32[4,1024,512]{2,1,0} %broadcast.2096), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2245 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1682, f32[4,1024,512]{2,1,0} %multiply.2254), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1264 = f32[] constant(-0.000213374049)
  %broadcast.2095 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1264), dimensions={}
  %add.1681 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2245, f32[4,1024,512]{2,1,0} %broadcast.2095), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2244 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1681, f32[4,1024,512]{2,1,0} %multiply.2254), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1263 = f32[] constant(-0.00168282702)
  %broadcast.2094 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1263), dimensions={}
  %add.1680 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2244, f32[4,1024,512]{2,1,0} %broadcast.2094), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2243 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1680, f32[4,1024,512]{2,1,0} %multiply.2254), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1262 = f32[] constant(-0.00737332925)
  %broadcast.2093 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1262), dimensions={}
  %add.1679 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2243, f32[4,1024,512]{2,1,0} %broadcast.2093), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2242 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1679, f32[4,1024,512]{2,1,0} %multiply.2254), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_1261 = f32[] constant(-0.0142647391)
  %broadcast.2092 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_1261), dimensions={}
  %add.1678 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2242, f32[4,1024,512]{2,1,0} %broadcast.2092), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.672 = f32[4,1024,512]{2,1,0} divide(f32[4,1024,512]{2,1,0} %multiply.2246, f32[4,1024,512]{2,1,0} %add.1678), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.775 = f16[4,1024,512]{2,1,0} convert(f32[4,1024,512]{2,1,0} %divide.672), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %param_2.2145 = f16[] parameter(2)
  %broadcast.2091 = f16[4,1024,512]{2,1,0} broadcast(f16[] %param_2.2145), dimensions={}
  %add.1677 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %convert.775, f16[4,1024,512]{2,1,0} %broadcast.2091), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1934 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %bitcast.999, f16[4,1024,512]{2,1,0} %add.1677), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %param_0.994 = f16[] parameter(0)
  %broadcast.154 = f16[4,1024,512]{2,1,0} broadcast(f16[] %param_0.994), dimensions={}
  %divide.560 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %multiply.1934, f16[4,1024,512]{2,1,0} %broadcast.154), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  ROOT %bitcast.845 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %divide.560)
}

%fused_computation.509 (param_0.999: f32[512]) -> f16[4096,512] {
  %param_0.999 = f32[512]{0} parameter(0)
  %convert.546 = f16[512]{0} convert(f32[512]{0} %param_0.999), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.156 = f16[4,1024,512]{2,1,0} broadcast(f16[512]{0} %convert.546), dimensions={2}
  ROOT %bitcast.846 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %broadcast.156)
}

%fused_computation.510 (param_0.1002: f32[1024], param_1.2382: f32[1024], param_2.2130: f32[4,1024], param_3.1595: f32[], param_4.1057: f16[4,1024,1024], param_5.1055: f32[], param_6.895: f32[4,1024], param_7.528: f32[], param_8.288: f32[]) -> f16[4,1024,1024] {
  %param_4.1057 = f16[4,1024,1024]{2,1,0} parameter(4)
  %convert.770 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %param_4.1057), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %param_2.2130 = f32[4,1024]{1,0} parameter(2)
  %param_3.1595 = f32[] parameter(3)
  %broadcast.2015 = f32[4,1024]{1,0} broadcast(f32[] %param_3.1595), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.639 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_2.2130, f32[4,1024]{1,0} %broadcast.2015), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.2014 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.639), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.168 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.770, f32[4,1024,1024]{2,1,0} %broadcast.2014), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_8.288 = f32[] parameter(8)
  %broadcast.2053 = f32[4,1024]{1,0} broadcast(f32[] %param_8.288), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_6.895 = f32[4,1024]{1,0} parameter(6)
  %param_7.528 = f32[] parameter(7)
  %broadcast.2052 = f32[4,1024]{1,0} broadcast(f32[] %param_7.528), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.661 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %param_6.895, f32[4,1024]{1,0} %broadcast.2052), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.2224 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.639, f32[4,1024]{1,0} %divide.639), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.183 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.661, f32[4,1024]{1,0} %multiply.2224), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.78 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.2053, f32[4,1024]{1,0} %subtract.183), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %param_5.1055 = f32[] parameter(5)
  %broadcast.2050 = f32[4,1024]{1,0} broadcast(f32[] %param_5.1055), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.1659 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.78, f32[4,1024]{1,0} %broadcast.2050), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1216 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.1659), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.70 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1216), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1215 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.70), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.2049 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1215), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_1.2382 = f32[1024]{0} parameter(1)
  %broadcast.1595 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_1.2382), dimensions={2}
  %multiply.2153 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.2049, f32[4,1024,1024]{2,1,0} %broadcast.1595), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.1948 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.168, f32[4,1024,1024]{2,1,0} %multiply.2153), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.1002 = f32[1024]{0} parameter(0)
  %broadcast.157 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_0.1002), dimensions={2}
  %add.1337 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.1948, f32[4,1024,1024]{2,1,0} %broadcast.157), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  ROOT %convert.547 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.1337), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
}

%region_47.1854 (Arg_0.1855: f32[], Arg_1.1856: f32[]) -> f32[] {
  %Arg_0.1855 = f32[] parameter(0)
  %Arg_1.1856 = f32[] parameter(1)
  ROOT %add.1857 = f32[] add(f32[] %Arg_0.1855, f32[] %Arg_1.1856), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_48.1867 (Arg_0.1868: f32[], Arg_1.1869: f32[]) -> f32[] {
  %Arg_0.1868 = f32[] parameter(0)
  %Arg_1.1869 = f32[] parameter(1)
  ROOT %add.1870 = f32[] add(f32[] %Arg_0.1868, f32[] %Arg_1.1869), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%fused_computation.516 (param_0.2423: f16[4,1024,1024], param_1.3105: f32[1024], param_2.2995: f16[4096,1024]) -> (f32[4,1024], f32[4,1024], f16[4,1024,1024]) {
  %param_2.2995 = f16[4096,1024]{1,0} parameter(2)
  %bitcast.849.clone.1 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_2.2995), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_1.3105 = f32[1024]{0} parameter(1)
  %convert.551.clone.1 = f16[1024]{0} convert(f32[1024]{0} %param_1.3105), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.161.clone.1 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.551.clone.1), dimensions={2}
  %add.1340.clone.1 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.849.clone.1, f16[4,1024,1024]{2,1,0} %broadcast.161.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_0.2423 = f16[4,1024,1024]{2,1,0} parameter(0)
  %add.1339.clone.1 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1340.clone.1, f16[4,1024,1024]{2,1,0} %param_0.2423), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=233}
  %convert.550 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1339.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %constant_792 = f32[] constant(0)
  %reduce.299 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.550, f32[] %constant_792), dimensions={2}, to_apply=%region_47.1854, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.1950.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.550, f32[4,1024,1024]{2,1,0} %convert.550), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.298.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1950.clone.1, f32[] %constant_792), dimensions={2}, to_apply=%region_48.1867, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  ROOT %tuple.220 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) tuple(f32[4,1024]{1,0} %reduce.299, f32[4,1024]{1,0} %reduce.298.clone.1, f16[4,1024,1024]{2,1,0} %add.1339.clone.1)
}

%fused_computation.518 (param_0.1019: f16[4,4,32,1024]) -> f16[4096,128] {
  %param_0.1019 = f16[4,4,32,1024]{3,2,1,0} parameter(0)
  %transpose.63 = f16[4,1024,4,32]{1,3,2,0} transpose(f16[4,4,32,1024]{3,2,1,0} %param_0.1019), dimensions={0,3,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %copy.135 = f16[4,1024,4,32]{3,2,1,0} copy(f16[4,1024,4,32]{1,3,2,0} %transpose.63), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  ROOT %bitcast.850 = f16[4096,128]{1,0} bitcast(f16[4,1024,4,32]{3,2,1,0} %copy.135)
}

%region_46.1829 (Arg_0.1830: f32[], Arg_1.1831: f32[]) -> f32[] {
  %Arg_0.1830 = f32[] parameter(0)
  %Arg_1.1831 = f32[] parameter(1)
  ROOT %add.1832 = f32[] add(f32[] %Arg_0.1830, f32[] %Arg_1.1831), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.519 (param_0.1954: f16[4,4,1024,1024], param_1.2359: f16[4,4,1024]) -> f32[4,4,1024] {
  %param_0.1954 = f16[4,4,1024,1024]{3,2,1,0} parameter(0)
  %param_1.2359 = f16[4,4,1024]{2,1,0} parameter(1)
  %broadcast.2005 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_1.2359), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.160 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_0.1954, f16[4,4,1024,1024]{3,2,1,0} %broadcast.2005), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.67 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.160), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.552 = f32[4,4,1024,1024]{3,2,1,0} convert(f16[4,4,1024,1024]{3,2,1,0} %exponential.67), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %constant_793 = f32[] constant(0)
  ROOT %reduce.300 = f32[4,4,1024]{2,1,0} reduce(f32[4,4,1024,1024]{3,2,1,0} %convert.552, f32[] %constant_793), dimensions={3}, to_apply=%region_46.1829, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.521 (param_0.1027: f16[8,1,1,1024], param_1.1360: s32[], param_2.1823: f16[8,1,1,1024], param_3.1464: s32[4,1024], param_4.932: s32[]) -> f16[4,4,1024,1024] {
  %param_3.1464 = s32[4,1024]{1,0} parameter(3)
  %param_4.932 = s32[] parameter(4)
  %broadcast.164 = s32[4,1024]{1,0} broadcast(s32[] %param_4.932), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %compare.73 = pred[4,1024]{1,0} compare(s32[4,1024]{1,0} %param_3.1464, s32[4,1024]{1,0} %broadcast.164), direction=GT, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %bitcast.853 = pred[4,1,1,1024]{3,0,2,1} bitcast(pred[4,1024]{1,0} %compare.73), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %param_2.1823 = f16[8,1,1,1024]{3,0,2,1} parameter(2)
  %param_1.1360 = s32[] parameter(1)
  %constant_794 = s32[] constant(0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %dynamic-slice.68 = f16[4,1,1,1024]{3,0,2,1} dynamic-slice(f16[8,1,1,1024]{3,0,2,1} %param_2.1823, s32[] %param_1.1360, s32[] %constant_794, s32[] %constant_794, s32[] %constant_794), dynamic_slice_sizes={4,1,1,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %param_0.1027 = f16[8,1,1,1024]{3,0,2,1} parameter(0)
  %dynamic-slice.67 = f16[4,1,1,1024]{3,0,2,1} dynamic-slice(f16[8,1,1,1024]{3,0,2,1} %param_0.1027, s32[] %param_1.1360, s32[] %constant_794, s32[] %constant_794, s32[] %constant_794), dynamic_slice_sizes={4,1,1,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %select.67 = f16[4,1,1,1024]{3,0,2,1} select(pred[4,1,1,1024]{3,0,2,1} %bitcast.853, f16[4,1,1,1024]{3,0,2,1} %dynamic-slice.68, f16[4,1,1,1024]{3,0,2,1} %dynamic-slice.67), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %bitcast.852 = f16[4,1024]{1,0} bitcast(f16[4,1,1,1024]{3,0,2,1} %select.67), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=100}
  ROOT %broadcast.163 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,1024]{1,0} %bitcast.852), dimensions={0,3}
}

%fused_computation.522 (param_0.1032: u32[16], param_1.1366: u32[]) -> s32[] {
  %constant_7 = u32[] constant(0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/gt" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=86}
  %param_0.1032 = u32[16]{0} parameter(0)
  %param_1.1366 = u32[] parameter(1)
  %dynamic-slice.69 = u32[1]{0} dynamic-slice(u32[16]{0} %param_0.1032, u32[] %param_1.1366), dynamic_slice_sizes={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/gt" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=86}
  %bitcast.854 = u32[] bitcast(u32[1]{0} %dynamic-slice.69), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/gt" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=86}
  %constant_6 = u32[] constant(1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/gt" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=86}
  %clamp.19 = u32[] clamp(u32[] %constant_7, u32[] %bitcast.854, u32[] %constant_6), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/gt" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=86}
  %convert.553 = s32[] convert(u32[] %clamp.19), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/gt" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=86}
  %constant_5 = s32[] constant(4), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/gt" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=86}
  ROOT %multiply.1951 = s32[] multiply(s32[] %convert.553, s32[] %constant_5), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/gt" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=86}
}

%fused_computation.524 (param_0.1942: f16[4096,384]) -> (f16[4,4,32,1024], f16[4,4,32,1024]) {
  %param_0.1942 = f16[4096,384]{1,0} parameter(0)
  %bitcast.1196 = f16[4,1024,384]{2,1,0} bitcast(f16[4096,384]{1,0} %param_0.1942), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %bitcast.1195 = f16[4,1024,128,3]{3,2,1,0} bitcast(f16[4,1024,384]{2,1,0} %bitcast.1196), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %copy.193 = f16[4,1024,128,3]{1,2,0,3} copy(f16[4,1024,128,3]{3,2,1,0} %bitcast.1195), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %slice.47 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.193), slice={[0:4], [0:1024], [0:128], [1:2]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/slice[start_indices=(0, 0, 0, 1) limit_indices=(8, 1024, 1024, 2) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.64 = f16[1,4,128,1024]{3,2,1,0} transpose(f16[4,1024,128,1]{1,2,0,3} %slice.47), dimensions={3,0,2,1}
  %bitcast.857 = f16[4,4,32,1024]{3,2,1,0} bitcast(f16[1,4,128,1024]{3,2,1,0} %transpose.64)
  %slice.77.clone.1 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.193), slice={[0:4], [0:1024], [0:128], [2:3]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/slice[start_indices=(0, 0, 0, 2) limit_indices=(8, 1024, 1024, 3) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.65 = f16[1,4,128,1024]{3,2,1,0} transpose(f16[4,1024,128,1]{1,2,0,3} %slice.77.clone.1), dimensions={3,0,2,1}
  %bitcast.1001.clone.1 = f16[4,4,32,1024]{3,2,1,0} bitcast(f16[1,4,128,1024]{3,2,1,0} %transpose.65)
  ROOT %tuple.222 = (f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) tuple(f16[4,4,32,1024]{3,2,1,0} %bitcast.857, f16[4,4,32,1024]{3,2,1,0} %bitcast.1001.clone.1)
}

%fused_computation.526 (param_0.1045: f32[384]) -> f16[4096,384] {
  %param_0.1045 = f32[384]{0} parameter(0)
  %convert.554 = f16[384]{0} convert(f32[384]{0} %param_0.1045), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.166 = f16[4,1024,384]{2,1,0} broadcast(f16[384]{0} %convert.554), dimensions={2}
  ROOT %bitcast.860 = f16[4096,384]{1,0} bitcast(f16[4,1024,384]{2,1,0} %broadcast.166)
}

%fused_computation.527 (param_0.1046: f32[6400], param_1.3119: f32[], param_2.3006: f32[], param_3.2348: f32[6400], param_4.1854: f16[6400], param_5.2144: f32[6400]) -> (f32[6400], f32[6400], f32[6400]) {
  %param_0.1046 = f32[6400]{0} parameter(0)
  %param_4.1854 = f16[6400]{0} parameter(4)
  %convert.556.clone.1 = f32[6400]{0} convert(f16[6400]{0} %param_4.1854), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/code/alpa/alpa/model/gpt_model.py" source_line=75}
  %constant_804_clone_1 = f32[] constant(0.1)
  %broadcast.174.clone.1 = f32[6400]{0} broadcast(f32[] %constant_804_clone_1), dimensions={}
  %multiply.1958.clone.1 = f32[6400]{0} multiply(f32[6400]{0} %convert.556.clone.1, f32[6400]{0} %broadcast.174.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_5.2144 = f32[6400]{0} parameter(5)
  %constant_805_clone_1 = f32[] constant(0.9)
  %broadcast.173.clone.1 = f32[6400]{0} broadcast(f32[] %constant_805_clone_1), dimensions={}
  %multiply.1957.clone.1 = f32[6400]{0} multiply(f32[6400]{0} %param_5.2144, f32[6400]{0} %broadcast.173.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.1345.clone.1 = f32[6400]{0} add(f32[6400]{0} %multiply.1958.clone.1, f32[6400]{0} %multiply.1957.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param_2.3006 = f32[] parameter(2)
  %broadcast.170 = f32[6400]{0} broadcast(f32[] %param_2.3006), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %multiply.1956.clone.1 = f32[6400]{0} multiply(f32[6400]{0} %convert.556.clone.1, f32[6400]{0} %convert.556.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %constant_802_clone_1 = f32[] constant(0.001)
  %broadcast.172.clone.1 = f32[6400]{0} broadcast(f32[] %constant_802_clone_1), dimensions={}
  %multiply.1955.clone.1 = f32[6400]{0} multiply(f32[6400]{0} %multiply.1956.clone.1, f32[6400]{0} %broadcast.172.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_3.2348 = f32[6400]{0} parameter(3)
  %constant_803_clone_1 = f32[] constant(0.999)
  %broadcast.171.clone.1 = f32[6400]{0} broadcast(f32[] %constant_803_clone_1), dimensions={}
  %multiply.1954.clone.1 = f32[6400]{0} multiply(f32[6400]{0} %param_3.2348, f32[6400]{0} %broadcast.171.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.1343.clone.1 = f32[6400]{0} add(f32[6400]{0} %multiply.1955.clone.1, f32[6400]{0} %multiply.1954.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param_1.3119 = f32[] parameter(1)
  %broadcast.169 = f32[6400]{0} broadcast(f32[] %param_1.3119), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %divide.565 = f32[6400]{0} divide(f32[6400]{0} %add.1343.clone.1, f32[6400]{0} %broadcast.169), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.153 = f32[6400]{0} sqrt(f32[6400]{0} %divide.565), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %constant_795 = f32[] constant(1e-08)
  %broadcast.168 = f32[6400]{0} broadcast(f32[] %constant_795), dimensions={}
  %add.1342 = f32[6400]{0} add(f32[6400]{0} %sqrt.153, f32[6400]{0} %broadcast.168), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1953 = f32[6400]{0} multiply(f32[6400]{0} %broadcast.170, f32[6400]{0} %add.1342)
  %divide.564 = f32[6400]{0} divide(f32[6400]{0} %add.1345.clone.1, f32[6400]{0} %multiply.1953), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %constant_796 = f32[] constant(-0.01)
  %broadcast.167 = f32[6400]{0} broadcast(f32[] %constant_796), dimensions={}
  %multiply.1952 = f32[6400]{0} multiply(f32[6400]{0} %divide.564, f32[6400]{0} %broadcast.167), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.1341 = f32[6400]{0} add(f32[6400]{0} %param_0.1046, f32[6400]{0} %multiply.1952), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  ROOT %tuple.290 = (f32[6400]{0}, f32[6400]{0}, f32[6400]{0}) tuple(f32[6400]{0} %add.1341, f32[6400]{0} %add.1343.clone.1, f32[6400]{0} %add.1345.clone.1)
}

%fused_computation.528 (param_0.2426: s32[]) -> (f32[], f32[], s32[]) {
  %constant_800 = f32[] constant(1)
  %constant_799 = f32[] constant(0.999)
  %param_0.2426 = s32[] parameter(0)
  %constant_8_clone_1 = s32[] constant(2147483647)
  %compare.74.clone.1 = pred[] compare(s32[] %param_0.2426, s32[] %constant_8_clone_1), direction=LT, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/lt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/numerics.py" source_line=118}
  %constant_801_clone_1 = s32[] constant(1)
  %add.1344.clone.1 = s32[] add(s32[] %param_0.2426, s32[] %constant_801_clone_1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/numerics.py" source_line=118}
  %select.68.clone.1 = s32[] select(pred[] %compare.74.clone.1, s32[] %add.1344.clone.1, s32[] %constant_8_clone_1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/numerics.py" source_line=118}
  %convert.680 = f32[] convert(s32[] %select.68.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/convert_element_type[new_dtype=float32 weak_type=True]" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=112}
  %power.2 = f32[] power(f32[] %constant_799, f32[] %convert.680), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/pow" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=112}
  %subtract.47 = f32[] subtract(f32[] %constant_800, f32[] %power.2), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sub" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=112}
  %constant_798_clone_1 = f32[] constant(0.9)
  %power.3.clone.1 = f32[] power(f32[] %constant_798_clone_1, f32[] %convert.680), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/pow" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=112}
  %subtract.48.clone.1 = f32[] subtract(f32[] %constant_800, f32[] %power.3.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sub" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=112}
  ROOT %tuple.288 = (f32[], f32[], s32[]) tuple(f32[] %subtract.47, f32[] %subtract.48.clone.1, s32[] %select.68.clone.1)
}

%fused_computation.533 (param_0.1938: f16[4,1024], param_1.2353: f32[4,1024], param_2.2106: f32[], param_3.1571: s32[4,1024], param_4.1040: s32[16], param_5.1038: u32[], param_6.891: f16[4,1024], param_7.522: f16[4096,6400]) -> f16[4,1024,6400] {
  %param_3.1571 = s32[4,1024]{1,0} parameter(3)
  %broadcast.1807 = s32[4,1024,6400]{2,1,0} broadcast(s32[4,1024]{1,0} %param_3.1571), dimensions={0,1}
  %iota.14 = s32[4,1024,6400]{2,1,0} iota(), iota_dimension=2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %param_4.1040 = s32[16]{0} parameter(4)
  %param_5.1038 = u32[] parameter(5)
  %dynamic-slice.82 = s32[1]{0} dynamic-slice(s32[16]{0} %param_4.1040, u32[] %param_5.1038), dynamic_slice_sizes={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %bitcast.1016 = s32[] bitcast(s32[1]{0} %dynamic-slice.82), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %constant_1066 = s32[] constant(6400), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %multiply.2168 = s32[] multiply(s32[] %bitcast.1016, s32[] %constant_1066), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %broadcast.1806 = s32[4,1024,6400]{2,1,0} broadcast(s32[] %multiply.2168), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %add.1488 = s32[4,1024,6400]{2,1,0} add(s32[4,1024,6400]{2,1,0} %iota.14, s32[4,1024,6400]{2,1,0} %broadcast.1806), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %compare.91 = pred[4,1024,6400]{2,1,0} compare(s32[4,1024,6400]{2,1,0} %broadcast.1807, s32[4,1024,6400]{2,1,0} %add.1488), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %constant_1065 = s32[] constant(0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %broadcast.1805 = s32[4,1024]{1,0} broadcast(s32[] %constant_1065), dimensions={}
  %compare.90 = pred[4,1024]{1,0} compare(s32[4,1024]{1,0} %param_3.1571, s32[4,1024]{1,0} %broadcast.1805), direction=GT, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/gt" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=86}
  %constant_1062 = f32[] constant(1)
  %broadcast.1804 = f32[4,1024]{1,0} broadcast(f32[] %constant_1062), dimensions={}
  %constant_1061 = f32[] constant(0)
  %broadcast.1803 = f32[4,1024]{1,0} broadcast(f32[] %constant_1061), dimensions={}
  %select.83 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.90, f32[4,1024]{1,0} %broadcast.1804, f32[4,1024]{1,0} %broadcast.1803), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/select_n" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=86}
  %param_2.2106 = f32[] parameter(2)
  %divide.637 = f32[] divide(f32[] %constant_1062, f32[] %param_2.2106), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=90}
  %broadcast.1802 = f32[4,1024]{1,0} broadcast(f32[] %divide.637), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/broadcast_in_dim[shape=(8, 1024) broadcast_dimensions=()]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=90}
  %multiply.2167 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %select.83, f32[4,1024]{1,0} %broadcast.1802), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=90}
  %negate.83 = f32[4,1024]{1,0} negate(f32[4,1024]{1,0} %multiply.2167), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/neg" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %broadcast.1801 = f32[4,1024,6400]{2,1,0} broadcast(f32[4,1024]{1,0} %negate.83), dimensions={0,1}
  %broadcast.1800 = f32[4,1024,6400]{2,1,0} broadcast(f32[] %constant_1061), dimensions={}
  %select.82 = f32[4,1024,6400]{2,1,0} select(pred[4,1024,6400]{2,1,0} %compare.91, f32[4,1024,6400]{2,1,0} %broadcast.1801, f32[4,1024,6400]{2,1,0} %broadcast.1800), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %convert.684 = f16[4,1024,6400]{2,1,0} convert(f32[4,1024,6400]{2,1,0} %select.82), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %param_0.1938 = f16[4,1024]{1,0} parameter(0)
  %param_1.2353 = f32[4,1024]{1,0} parameter(1)
  %convert.557 = f16[4,1024]{1,0} convert(f32[4,1024]{1,0} %param_1.2353), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %divide.566 = f16[4,1024]{1,0} divide(f16[4,1024]{1,0} %param_0.1938, f16[4,1024]{1,0} %convert.557), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %broadcast.175 = f16[4,1024,6400]{2,1,0} broadcast(f16[4,1024]{1,0} %divide.566), dimensions={0,1}
  %param_7.522 = f16[4096,6400]{1,0} parameter(7)
  %bitcast.1192 = f16[4,1024,6400]{2,1,0} bitcast(f16[4096,6400]{1,0} %param_7.522), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/add" source_file="/code/alpa/alpa/model/gpt_model.py" source_line=75}
  %param_6.891 = f16[4,1024]{1,0} parameter(6)
  %broadcast.2003 = f16[4,1024,6400]{2,1,0} broadcast(f16[4,1024]{1,0} %param_6.891), dimensions={0,1}
  %subtract.158 = f16[4,1024,6400]{2,1,0} subtract(f16[4,1024,6400]{2,1,0} %bitcast.1192, f16[4,1024,6400]{2,1,0} %broadcast.2003), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sub" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %exponential.65 = f16[4,1024,6400]{2,1,0} exponential(f16[4,1024,6400]{2,1,0} %subtract.158), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/exp" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %multiply.1959 = f16[4,1024,6400]{2,1,0} multiply(f16[4,1024,6400]{2,1,0} %broadcast.175, f16[4,1024,6400]{2,1,0} %exponential.65), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  ROOT %add.1346 = f16[4,1024,6400]{2,1,0} add(f16[4,1024,6400]{2,1,0} %convert.684, f16[4,1024,6400]{2,1,0} %multiply.1959), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add_any" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
}

%fused_computation.534 (param_0.1936: f16[4,1024], param_1.2351: f16[4096,6400]) -> f32[4,1024] {
  %param_1.2351 = f16[4096,6400]{1,0} parameter(1)
  %bitcast.1190 = f16[4,1024,6400]{2,1,0} bitcast(f16[4096,6400]{1,0} %param_1.2351), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/add" source_file="/code/alpa/alpa/model/gpt_model.py" source_line=75}
  %param_0.1936 = f16[4,1024]{1,0} parameter(0)
  %broadcast.2001 = f16[4,1024,6400]{2,1,0} broadcast(f16[4,1024]{1,0} %param_0.1936), dimensions={0,1}
  %subtract.154 = f16[4,1024,6400]{2,1,0} subtract(f16[4,1024,6400]{2,1,0} %bitcast.1190, f16[4,1024,6400]{2,1,0} %broadcast.2001), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sub" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %exponential.63 = f16[4,1024,6400]{2,1,0} exponential(f16[4,1024,6400]{2,1,0} %subtract.154), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/exp" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %convert.558 = f32[4,1024,6400]{2,1,0} convert(f16[4,1024,6400]{2,1,0} %exponential.63), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %constant_807 = f32[] constant(0)
  ROOT %reduce.301 = f32[4,1024]{1,0} reduce(f32[4,1024,6400]{2,1,0} %convert.558, f32[] %constant_807), dimensions={2}, to_apply=%region_39.1700, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(2,)]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
}

%fused_computation.536 (param_0.1063: f32[6400]) -> f16[4096,6400] {
  %param_0.1063 = f32[6400]{0} parameter(0)
  %convert.559 = f16[6400]{0} convert(f32[6400]{0} %param_0.1063), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/code/alpa/alpa/model/gpt_model.py" source_line=75}
  %broadcast.177 = f16[4,1024,6400]{2,1,0} broadcast(f16[6400]{0} %convert.559), dimensions={2}
  ROOT %bitcast.861 = f16[4096,6400]{1,0} bitcast(f16[4,1024,6400]{2,1,0} %broadcast.177)
}

%fused_computation.537 (param_0.1067: f32[1024], param_1.1397: f32[1024], param_2.1830: f32[4,1024], param_3.1468: f32[4,1024], param_4.1039: f16[4,1024,1024], param_5.1037: f32[1024], param_6.888: f16[4096,1024]) -> f16[4096,1024] {
  %param_6.888 = f16[4096,1024]{1,0} parameter(6)
  %bitcast.1186 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_6.888), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_5.1037 = f32[1024]{0} parameter(5)
  %convert.766 = f16[1024]{0} convert(f32[1024]{0} %param_5.1037), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1997 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.766), dimensions={2}
  %add.1649 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.1186, f16[4,1024,1024]{2,1,0} %broadcast.1997), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_4.1039 = f16[4,1024,1024]{2,1,0} parameter(4)
  %add.1648 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1649, f16[4,1024,1024]{2,1,0} %param_4.1039), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=310}
  %convert.561 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1648), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_3.1468 = f32[4,1024]{1,0} parameter(3)
  %constant_810 = f32[] constant(0.0009765625)
  %broadcast.1601 = f32[4,1024]{1,0} broadcast(f32[] %constant_810), dimensions={}
  %multiply.1964 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_3.1468, f32[4,1024]{1,0} %broadcast.1601), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.181 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.1964), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.53 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.561, f32[4,1024,1024]{2,1,0} %broadcast.181), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.1830 = f32[4,1024]{1,0} parameter(2)
  %multiply.1963 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_2.1830, f32[4,1024]{1,0} %broadcast.1601), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.1962 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %multiply.1964, f32[4,1024]{1,0} %multiply.1964), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.52 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %multiply.1963, f32[4,1024]{1,0} %multiply.1962), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_812 = f32[] constant(0)
  %broadcast.1603 = f32[4,1024]{1,0} broadcast(f32[] %constant_812), dimensions={}
  %maximum.25 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %subtract.52, f32[4,1024]{1,0} %broadcast.1603), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_811 = f32[] constant(1e-12)
  %broadcast.1602 = f32[4,1024]{1,0} broadcast(f32[] %constant_811), dimensions={}
  %add.1349 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.25, f32[4,1024]{1,0} %broadcast.1602), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.0 = f32[4,1024]{1,0} rsqrt(f32[4,1024]{1,0} %add.1349), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %broadcast.180 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %rsqrt.0), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_1.1397 = f32[1024]{0} parameter(1)
  %broadcast.179 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_1.1397), dimensions={2}
  %multiply.1961 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.180, f32[4,1024,1024]{2,1,0} %broadcast.179), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.1960 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.53, f32[4,1024,1024]{2,1,0} %multiply.1961), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.1067 = f32[1024]{0} parameter(0)
  %broadcast.178 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_0.1067), dimensions={2}
  %add.1348 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.1960, f32[4,1024,1024]{2,1,0} %broadcast.178), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %convert.560 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.1348), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  ROOT %bitcast.862 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %convert.560)
}

%region_37.1640 (Arg_0.1641: f32[], Arg_1.1642: f32[]) -> f32[] {
  %Arg_0.1641 = f32[] parameter(0)
  %Arg_1.1642 = f32[] parameter(1)
  ROOT %add.1643 = f32[] add(f32[] %Arg_0.1641, f32[] %Arg_1.1642), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%region_36.1628 (Arg_0.1629: f32[], Arg_1.1630: f32[]) -> f32[] {
  %Arg_0.1629 = f32[] parameter(0)
  %Arg_1.1630 = f32[] parameter(1)
  ROOT %add.1631 = f32[] add(f32[] %Arg_0.1629, f32[] %Arg_1.1630), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%fused_computation.538 (param_0.1932: f16[4,1024,1024], param_1.2347: f32[1024], param_2.2105: f16[4096,1024]) -> (f32[4,1024], f32[4,1024]) {
  %param_2.2105 = f16[4096,1024]{1,0} parameter(2)
  %bitcast.1188 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_2.2105), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_1.2347 = f32[1024]{0} parameter(1)
  %convert.768 = f16[1024]{0} convert(f32[1024]{0} %param_1.2347), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1999 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.768), dimensions={2}
  %add.1653 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.1188, f16[4,1024,1024]{2,1,0} %broadcast.1999), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_0.1932 = f16[4,1024,1024]{2,1,0} parameter(0)
  %add.1652 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1653, f16[4,1024,1024]{2,1,0} %param_0.1932), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=310}
  %convert.562 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1652), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.1965 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.562, f32[4,1024,1024]{2,1,0} %convert.562), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %constant_814 = f32[] constant(0)
  %reduce.302 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1965, f32[] %constant_814), dimensions={2}, to_apply=%region_37.1640, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %reduce.303.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.562, f32[] %constant_814), dimensions={2}, to_apply=%region_36.1628, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  ROOT %tuple.291 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) tuple(f32[4,1024]{1,0} %reduce.302, f32[4,1024]{1,0} %reduce.303.clone.1)
}

%fused_computation.541 (param_0.1640: f16[4096,512]) -> f16[4096,512] {
  %param_0.1640 = f16[4096,512]{1,0} parameter(0)
  %bitcast.865 = f16[4,1024,512]{2,1,0} bitcast(f16[4096,512]{1,0} %param_0.1640), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %constant_817 = f32[] constant(-4)
  %broadcast.1606 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_817), dimensions={}
  %constant_815 = f16[] constant(0.70703)
  %broadcast.1604 = f16[4,1024,512]{2,1,0} broadcast(f16[] %constant_815), dimensions={}
  %multiply.1981 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %bitcast.865, f16[4,1024,512]{2,1,0} %broadcast.1604), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.566 = f32[4,1024,512]{2,1,0} convert(f16[4,1024,512]{2,1,0} %multiply.1981), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_816 = f32[] constant(4)
  %broadcast.1605 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_816), dimensions={}
  %clamp.20 = f32[4,1024,512]{2,1,0} clamp(f32[4,1024,512]{2,1,0} %broadcast.1606, f32[4,1024,512]{2,1,0} %convert.566, f32[4,1024,512]{2,1,0} %broadcast.1605), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1980 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.20, f32[4,1024,512]{2,1,0} %clamp.20), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_832 = f32[] constant(0)
  %broadcast.1607 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_832), dimensions={}
  %multiply.1979 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %multiply.1980, f32[4,1024,512]{2,1,0} %broadcast.1607), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_818 = f32[] constant(-2.72614237e-10)
  %broadcast.1608 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_818), dimensions={}
  %add.1365 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.1979, f32[4,1024,512]{2,1,0} %broadcast.1608), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1978 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1365, f32[4,1024,512]{2,1,0} %multiply.1980), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_819 = f32[] constant(2.77068146e-08)
  %broadcast.1609 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_819), dimensions={}
  %add.1364 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.1978, f32[4,1024,512]{2,1,0} %broadcast.1609), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1977 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1364, f32[4,1024,512]{2,1,0} %multiply.1980), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_820 = f32[] constant(-2.10102394e-06)
  %broadcast.1610 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_820), dimensions={}
  %add.1363 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.1977, f32[4,1024,512]{2,1,0} %broadcast.1610), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1976 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1363, f32[4,1024,512]{2,1,0} %multiply.1980), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_821 = f32[] constant(-5.69250624e-05)
  %broadcast.1611 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_821), dimensions={}
  %add.1362 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.1976, f32[4,1024,512]{2,1,0} %broadcast.1611), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1975 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1362, f32[4,1024,512]{2,1,0} %multiply.1980), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_822 = f32[] constant(-0.000734990637)
  %broadcast.1612 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_822), dimensions={}
  %add.1361 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.1975, f32[4,1024,512]{2,1,0} %broadcast.1612), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1974 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1361, f32[4,1024,512]{2,1,0} %multiply.1980), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_823 = f32[] constant(-0.0029546)
  %broadcast.1613 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_823), dimensions={}
  %add.1360 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.1974, f32[4,1024,512]{2,1,0} %broadcast.1613), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1973 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1360, f32[4,1024,512]{2,1,0} %multiply.1980), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_824 = f32[] constant(-0.0160960332)
  %broadcast.1614 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_824), dimensions={}
  %add.1358 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.1973, f32[4,1024,512]{2,1,0} %broadcast.1614), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1972 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.20, f32[4,1024,512]{2,1,0} %add.1358), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_825 = f32[] constant(-1.45660715e-05)
  %broadcast.1615 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_825), dimensions={}
  %add.1357 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.1979, f32[4,1024,512]{2,1,0} %broadcast.1615), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1971 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1357, f32[4,1024,512]{2,1,0} %multiply.1980), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_826 = f32[] constant(-0.000213374049)
  %broadcast.1616 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_826), dimensions={}
  %add.1356 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.1971, f32[4,1024,512]{2,1,0} %broadcast.1616), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1970 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1356, f32[4,1024,512]{2,1,0} %multiply.1980), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_827 = f32[] constant(-0.00168282702)
  %broadcast.1617 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_827), dimensions={}
  %add.1355 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.1970, f32[4,1024,512]{2,1,0} %broadcast.1617), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1969 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1355, f32[4,1024,512]{2,1,0} %multiply.1980), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_828 = f32[] constant(-0.00737332925)
  %broadcast.1618 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_828), dimensions={}
  %add.1354 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.1969, f32[4,1024,512]{2,1,0} %broadcast.1618), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1968 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1354, f32[4,1024,512]{2,1,0} %multiply.1980), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_829 = f32[] constant(-0.0142647391)
  %broadcast.1619 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_829), dimensions={}
  %add.1353 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.1968, f32[4,1024,512]{2,1,0} %broadcast.1619), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.567 = f32[4,1024,512]{2,1,0} divide(f32[4,1024,512]{2,1,0} %multiply.1972, f32[4,1024,512]{2,1,0} %add.1353), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.565 = f16[4,1024,512]{2,1,0} convert(f32[4,1024,512]{2,1,0} %divide.567), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_830 = f16[] constant(1)
  %broadcast.1620 = f16[4,1024,512]{2,1,0} broadcast(f16[] %constant_830), dimensions={}
  %add.1352 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %convert.565, f16[4,1024,512]{2,1,0} %broadcast.1620), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1967 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %bitcast.865, f16[4,1024,512]{2,1,0} %add.1352), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_831 = f16[] constant(0.5)
  %broadcast.1621 = f16[4,1024,512]{2,1,0} broadcast(f16[] %constant_831), dimensions={}
  %multiply.1966 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.1967, f16[4,1024,512]{2,1,0} %broadcast.1621), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  ROOT %bitcast.864 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %multiply.1966)
}

%fused_computation.542 (param_0.1079: f32[512]) -> f16[4096,512] {
  %param_0.1079 = f32[512]{0} parameter(0)
  %convert.567 = f16[512]{0} convert(f32[512]{0} %param_0.1079), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.183 = f16[4,1024,512]{2,1,0} broadcast(f16[512]{0} %convert.567), dimensions={2}
  ROOT %bitcast.866 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %broadcast.183)
}

%fused_computation.543 (param_0.1082: f32[1024], param_1.1412: f32[1024], param_2.1835: f32[4,1024], param_3.1473: f32[4,1024], param_4.1037: f16[4,1024,1024], param_5.1034: f32[1024], param_6.884: f16[4096,1024]) -> f16[4,1024,1024] {
  %param_6.884 = f16[4096,1024]{1,0} parameter(6)
  %bitcast.1180 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_6.884), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_5.1034 = f32[1024]{0} parameter(5)
  %convert.760 = f16[1024]{0} convert(f32[1024]{0} %param_5.1034), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1991 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.760), dimensions={2}
  %add.1636 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.1180, f16[4,1024,1024]{2,1,0} %broadcast.1991), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_4.1037 = f16[4,1024,1024]{2,1,0} parameter(4)
  %add.1635 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1636, f16[4,1024,1024]{2,1,0} %param_4.1037), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=233}
  %convert.569 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1635), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_3.1473 = f32[4,1024]{1,0} parameter(3)
  %constant_833 = f32[] constant(0.0009765625)
  %broadcast.1622 = f32[4,1024]{1,0} broadcast(f32[] %constant_833), dimensions={}
  %multiply.1986 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_3.1473, f32[4,1024]{1,0} %broadcast.1622), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.187 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.1986), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.58 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.569, f32[4,1024,1024]{2,1,0} %broadcast.187), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.1835 = f32[4,1024]{1,0} parameter(2)
  %multiply.1985 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_2.1835, f32[4,1024]{1,0} %broadcast.1622), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.1984 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %multiply.1986, f32[4,1024]{1,0} %multiply.1986), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.57 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %multiply.1985, f32[4,1024]{1,0} %multiply.1984), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_835 = f32[] constant(0)
  %broadcast.1624 = f32[4,1024]{1,0} broadcast(f32[] %constant_835), dimensions={}
  %maximum.26 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %subtract.57, f32[4,1024]{1,0} %broadcast.1624), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_834 = f32[] constant(1e-12)
  %broadcast.1623 = f32[4,1024]{1,0} broadcast(f32[] %constant_834), dimensions={}
  %add.1367 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.26, f32[4,1024]{1,0} %broadcast.1623), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.1 = f32[4,1024]{1,0} rsqrt(f32[4,1024]{1,0} %add.1367), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %broadcast.186 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %rsqrt.1), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_1.1412 = f32[1024]{0} parameter(1)
  %broadcast.185 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_1.1412), dimensions={2}
  %multiply.1983 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.186, f32[4,1024,1024]{2,1,0} %broadcast.185), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.1982 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.58, f32[4,1024,1024]{2,1,0} %multiply.1983), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.1082 = f32[1024]{0} parameter(0)
  %broadcast.184 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_0.1082), dimensions={2}
  %add.1366 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.1982, f32[4,1024,1024]{2,1,0} %broadcast.184), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  ROOT %convert.568 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.1366), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
}

%region_35.1548 (Arg_0.1549: f32[], Arg_1.1550: f32[]) -> f32[] {
  %Arg_0.1549 = f32[] parameter(0)
  %Arg_1.1550 = f32[] parameter(1)
  ROOT %add.1551 = f32[] add(f32[] %Arg_0.1549, f32[] %Arg_1.1550), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%region_34.1536 (Arg_0.1537: f32[], Arg_1.1538: f32[]) -> f32[] {
  %Arg_0.1537 = f32[] parameter(0)
  %Arg_1.1538 = f32[] parameter(1)
  ROOT %add.1539 = f32[] add(f32[] %Arg_0.1537, f32[] %Arg_1.1538), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%fused_computation.544 (param_0.1925: f16[4,1024,1024], param_1.2338: f32[1024], param_2.2094: f16[4096,1024]) -> (f32[4,1024], f32[4,1024]) {
  %param_2.2094 = f16[4096,1024]{1,0} parameter(2)
  %bitcast.1182 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_2.2094), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_1.2338 = f32[1024]{0} parameter(1)
  %convert.762 = f16[1024]{0} convert(f32[1024]{0} %param_1.2338), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1993 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.762), dimensions={2}
  %add.1640 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.1182, f16[4,1024,1024]{2,1,0} %broadcast.1993), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_0.1925 = f16[4,1024,1024]{2,1,0} parameter(0)
  %add.1639 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1640, f16[4,1024,1024]{2,1,0} %param_0.1925), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=233}
  %convert.570 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1639), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.1987 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.570, f32[4,1024,1024]{2,1,0} %convert.570), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %constant_837 = f32[] constant(0)
  %reduce.304 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1987, f32[] %constant_837), dimensions={2}, to_apply=%region_35.1548, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %reduce.305.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.570, f32[] %constant_837), dimensions={2}, to_apply=%region_34.1536, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  ROOT %tuple.292 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) tuple(f32[4,1024]{1,0} %reduce.304, f32[4,1024]{1,0} %reduce.305.clone.1)
}

%fused_computation.547 (param_0.1091: f16[4,4,32,1024]) -> f16[4096,128] {
  %param_0.1091 = f16[4,4,32,1024]{3,2,1,0} parameter(0)
  %transpose.66 = f16[4,1024,4,32]{1,3,2,0} transpose(f16[4,4,32,1024]{3,2,1,0} %param_0.1091), dimensions={0,3,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %copy.138 = f16[4,1024,4,32]{3,2,1,0} copy(f16[4,1024,4,32]{1,3,2,0} %transpose.66), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  ROOT %bitcast.868 = f16[4096,128]{1,0} bitcast(f16[4,1024,4,32]{3,2,1,0} %copy.138)
}

%fused_computation.548 (param_0.1918: f32[4,4,1024], param_1.2329: f16[4,4,1024,1024], param_2.2083: f16[4,4,1024]) -> f16[4,4,1024,1024] {
  %param_1.2329 = f16[4,4,1024,1024]{3,2,1,0} parameter(1)
  %param_2.2083 = f16[4,4,1024]{2,1,0} parameter(2)
  %broadcast.1987 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_2.2083), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.152 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_1.2329, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1987), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.61 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.152), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %param_0.1918 = f32[4,4,1024]{2,1,0} parameter(0)
  %convert.573 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %param_0.1918), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.189 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %convert.573), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  ROOT %divide.568 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %exponential.61, f16[4,4,1024,1024]{3,2,1,0} %broadcast.189), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%region_33.1511 (Arg_0.1512: f32[], Arg_1.1513: f32[]) -> f32[] {
  %Arg_0.1512 = f32[] parameter(0)
  %Arg_1.1513 = f32[] parameter(1)
  ROOT %add.1514 = f32[] add(f32[] %Arg_0.1512, f32[] %Arg_1.1513), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.549 (param_0.1916: f16[4,4,1024,1024], param_1.2326: f16[4,4,1024]) -> f32[4,4,1024] {
  %param_0.1916 = f16[4,4,1024,1024]{3,2,1,0} parameter(0)
  %param_1.2326 = f16[4,4,1024]{2,1,0} parameter(1)
  %broadcast.1985 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_1.2326), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.150 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_0.1916, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1985), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.59 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.150), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.574 = f32[4,4,1024,1024]{3,2,1,0} convert(f16[4,4,1024,1024]{3,2,1,0} %exponential.59), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %constant_838 = f32[] constant(0)
  ROOT %reduce.306 = f32[4,4,1024]{2,1,0} reduce(f32[4,4,1024,1024]{3,2,1,0} %convert.574, f32[] %constant_838), dimensions={3}, to_apply=%region_33.1511, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.551 (param_0.1913: f16[4096,384]) -> (f16[4,4,32,1024], f16[4,4,32,1024]) {
  %param_0.1913 = f16[4096,384]{1,0} parameter(0)
  %bitcast.1176 = f16[4,1024,384]{2,1,0} bitcast(f16[4096,384]{1,0} %param_0.1913), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %bitcast.1175 = f16[4,1024,128,3]{3,2,1,0} bitcast(f16[4,1024,384]{2,1,0} %bitcast.1176), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %copy.191 = f16[4,1024,128,3]{1,2,0,3} copy(f16[4,1024,128,3]{3,2,1,0} %bitcast.1175), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %slice.48 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.191), slice={[0:4], [0:1024], [0:128], [2:3]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/slice[start_indices=(0, 0, 0, 2) limit_indices=(8, 1024, 1024, 3) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.67 = f16[1,4,128,1024]{3,2,1,0} transpose(f16[4,1024,128,1]{1,2,0,3} %slice.48), dimensions={3,0,2,1}
  %bitcast.870 = f16[4,4,32,1024]{3,2,1,0} bitcast(f16[1,4,128,1024]{3,2,1,0} %transpose.67)
  %slice.50.clone.1 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.191), slice={[0:4], [0:1024], [0:128], [1:2]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/slice[start_indices=(0, 0, 0, 1) limit_indices=(8, 1024, 1024, 2) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.68 = f16[1,4,128,1024]{3,2,1,0} transpose(f16[4,1024,128,1]{1,2,0,3} %slice.50.clone.1), dimensions={3,0,2,1}
  %bitcast.873.clone.1 = f16[4,4,32,1024]{3,2,1,0} bitcast(f16[1,4,128,1024]{3,2,1,0} %transpose.68)
  ROOT %tuple.294 = (f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) tuple(f16[4,4,32,1024]{3,2,1,0} %bitcast.870, f16[4,4,32,1024]{3,2,1,0} %bitcast.873.clone.1)
}

%fused_computation.552 (param_0.1909: f16[4096,384]) -> f16[4,4,1024,32] {
  %param_0.1909 = f16[4096,384]{1,0} parameter(0)
  %bitcast.1172 = f16[4,1024,384]{2,1,0} bitcast(f16[4096,384]{1,0} %param_0.1909), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %bitcast.1171 = f16[4,1024,128,3]{3,2,1,0} bitcast(f16[4,1024,384]{2,1,0} %bitcast.1172), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %copy.189 = f16[4,1024,128,3]{1,2,0,3} copy(f16[4,1024,128,3]{3,2,1,0} %bitcast.1171), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %slice.49 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.189), slice={[0:4], [0:1024], [0:128], [0:1]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/slice[start_indices=(0, 0, 0, 0) limit_indices=(8, 1024, 1024, 1) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %constant_839 = f16[] constant(0.17676)
  %broadcast.1625 = f16[4,1024,128,1]{1,2,0,3} broadcast(f16[] %constant_839), dimensions={}
  %multiply.1988 = f16[4,1024,128,1]{1,2,0,3} multiply(f16[4,1024,128,1]{1,2,0,3} %slice.49, f16[4,1024,128,1]{1,2,0,3} %broadcast.1625), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %bitcast.872 = f16[4,1024,4,32]{1,3,2,0} bitcast(f16[4,1024,128,1]{1,2,0,3} %multiply.1988), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %copy.139 = f16[4,1024,4,32]{3,1,2,0} copy(f16[4,1024,4,32]{1,3,2,0} %bitcast.872), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  ROOT %transpose.69 = f16[4,4,1024,32]{3,2,1,0} transpose(f16[4,1024,4,32]{3,1,2,0} %copy.139), dimensions={0,2,1,3}
}

%fused_computation.555 (param_0.1111: f32[384]) -> f16[4096,384] {
  %param_0.1111 = f32[384]{0} parameter(0)
  %convert.575 = f16[384]{0} convert(f32[384]{0} %param_0.1111), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.191 = f16[4,1024,384]{2,1,0} broadcast(f16[384]{0} %convert.575), dimensions={2}
  ROOT %bitcast.876 = f16[4096,384]{1,0} bitcast(f16[4,1024,384]{2,1,0} %broadcast.191)
}

%fused_computation.556 (param_0.1114: f32[1024], param_1.1433: f32[1024], param_2.1837: f32[4,1024], param_3.1475: f32[4,1024], param_4.1035: f16[4,1024,1024], param_5.1031: f32[1024], param_6.880: f16[4096,1024]) -> f16[4,1024,1024] {
  %param_6.880 = f16[4096,1024]{1,0} parameter(6)
  %bitcast.1162 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_6.880), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_5.1031 = f32[1024]{0} parameter(5)
  %convert.754 = f16[1024]{0} convert(f32[1024]{0} %param_5.1031), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1981 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.754), dimensions={2}
  %add.1623 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.1162, f16[4,1024,1024]{2,1,0} %broadcast.1981), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_4.1035 = f16[4,1024,1024]{2,1,0} parameter(4)
  %add.1622 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1623, f16[4,1024,1024]{2,1,0} %param_4.1035), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=310}
  %convert.577 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1622), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_3.1475 = f32[4,1024]{1,0} parameter(3)
  %constant_840 = f32[] constant(0.0009765625)
  %broadcast.1626 = f32[4,1024]{1,0} broadcast(f32[] %constant_840), dimensions={}
  %multiply.1993 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_3.1475, f32[4,1024]{1,0} %broadcast.1626), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.195 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.1993), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.63 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.577, f32[4,1024,1024]{2,1,0} %broadcast.195), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.1837 = f32[4,1024]{1,0} parameter(2)
  %multiply.1992 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_2.1837, f32[4,1024]{1,0} %broadcast.1626), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.1991 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %multiply.1993, f32[4,1024]{1,0} %multiply.1993), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.62 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %multiply.1992, f32[4,1024]{1,0} %multiply.1991), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_842 = f32[] constant(0)
  %broadcast.1628 = f32[4,1024]{1,0} broadcast(f32[] %constant_842), dimensions={}
  %maximum.27 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %subtract.62, f32[4,1024]{1,0} %broadcast.1628), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_841 = f32[] constant(1e-12)
  %broadcast.1627 = f32[4,1024]{1,0} broadcast(f32[] %constant_841), dimensions={}
  %add.1371 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.27, f32[4,1024]{1,0} %broadcast.1627), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.2 = f32[4,1024]{1,0} rsqrt(f32[4,1024]{1,0} %add.1371), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %broadcast.194 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %rsqrt.2), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_1.1433 = f32[1024]{0} parameter(1)
  %broadcast.193 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_1.1433), dimensions={2}
  %multiply.1990 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.194, f32[4,1024,1024]{2,1,0} %broadcast.193), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.1989 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.63, f32[4,1024,1024]{2,1,0} %multiply.1990), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.1114 = f32[1024]{0} parameter(0)
  %broadcast.192 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_0.1114), dimensions={2}
  %add.1370 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.1989, f32[4,1024,1024]{2,1,0} %broadcast.192), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  ROOT %convert.576 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.1370), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
}

%region_31.1448 (Arg_0.1449: f32[], Arg_1.1450: f32[]) -> f32[] {
  %Arg_0.1449 = f32[] parameter(0)
  %Arg_1.1450 = f32[] parameter(1)
  ROOT %add.1451 = f32[] add(f32[] %Arg_0.1449, f32[] %Arg_1.1450), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%region_30.1436 (Arg_0.1437: f32[], Arg_1.1438: f32[]) -> f32[] {
  %Arg_0.1437 = f32[] parameter(0)
  %Arg_1.1438 = f32[] parameter(1)
  ROOT %add.1439 = f32[] add(f32[] %Arg_0.1437, f32[] %Arg_1.1438), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%fused_computation.557 (param_0.1901: f16[4,1024,1024], param_1.2323: f32[1024], param_2.2081: f16[4096,1024]) -> (f32[4,1024], f32[4,1024]) {
  %param_2.2081 = f16[4096,1024]{1,0} parameter(2)
  %bitcast.1164 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_2.2081), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_1.2323 = f32[1024]{0} parameter(1)
  %convert.756 = f16[1024]{0} convert(f32[1024]{0} %param_1.2323), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1983 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.756), dimensions={2}
  %add.1627 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.1164, f16[4,1024,1024]{2,1,0} %broadcast.1983), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_0.1901 = f16[4,1024,1024]{2,1,0} parameter(0)
  %add.1626 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1627, f16[4,1024,1024]{2,1,0} %param_0.1901), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=310}
  %convert.578 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1626), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.1994 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.578, f32[4,1024,1024]{2,1,0} %convert.578), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %constant_845 = f32[] constant(0)
  %reduce.307 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1994, f32[] %constant_845), dimensions={2}, to_apply=%region_31.1448, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %reduce.308.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.578, f32[] %constant_845), dimensions={2}, to_apply=%region_30.1436, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  ROOT %tuple.295 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) tuple(f32[4,1024]{1,0} %reduce.307, f32[4,1024]{1,0} %reduce.308.clone.1)
}

%fused_computation.560 (param_0.1647: f16[4096,512]) -> f16[4096,512] {
  %param_0.1647 = f16[4096,512]{1,0} parameter(0)
  %bitcast.879 = f16[4,1024,512]{2,1,0} bitcast(f16[4096,512]{1,0} %param_0.1647), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %constant_851 = f32[] constant(-4)
  %broadcast.1631 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_851), dimensions={}
  %constant_847 = f16[] constant(0.70703)
  %broadcast.1629 = f16[4,1024,512]{2,1,0} broadcast(f16[] %constant_847), dimensions={}
  %multiply.2010 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %bitcast.879, f16[4,1024,512]{2,1,0} %broadcast.1629), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.582 = f32[4,1024,512]{2,1,0} convert(f16[4,1024,512]{2,1,0} %multiply.2010), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_848 = f32[] constant(4)
  %broadcast.1630 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_848), dimensions={}
  %clamp.21 = f32[4,1024,512]{2,1,0} clamp(f32[4,1024,512]{2,1,0} %broadcast.1631, f32[4,1024,512]{2,1,0} %convert.582, f32[4,1024,512]{2,1,0} %broadcast.1630), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2009 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.21, f32[4,1024,512]{2,1,0} %clamp.21), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_874 = f32[] constant(0)
  %broadcast.1632 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_874), dimensions={}
  %multiply.2008 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %multiply.2009, f32[4,1024,512]{2,1,0} %broadcast.1632), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_852 = f32[] constant(-2.72614237e-10)
  %broadcast.1633 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_852), dimensions={}
  %add.1386 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2008, f32[4,1024,512]{2,1,0} %broadcast.1633), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2007 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1386, f32[4,1024,512]{2,1,0} %multiply.2009), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_854 = f32[] constant(2.77068146e-08)
  %broadcast.1634 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_854), dimensions={}
  %add.1385 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2007, f32[4,1024,512]{2,1,0} %broadcast.1634), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2006 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1385, f32[4,1024,512]{2,1,0} %multiply.2009), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_855 = f32[] constant(-2.10102394e-06)
  %broadcast.1635 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_855), dimensions={}
  %add.1384 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2006, f32[4,1024,512]{2,1,0} %broadcast.1635), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2005 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1384, f32[4,1024,512]{2,1,0} %multiply.2009), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_857 = f32[] constant(-5.69250624e-05)
  %broadcast.1636 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_857), dimensions={}
  %add.1383 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2005, f32[4,1024,512]{2,1,0} %broadcast.1636), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2004 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1383, f32[4,1024,512]{2,1,0} %multiply.2009), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_858 = f32[] constant(-0.000734990637)
  %broadcast.1637 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_858), dimensions={}
  %add.1382 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2004, f32[4,1024,512]{2,1,0} %broadcast.1637), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2003 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1382, f32[4,1024,512]{2,1,0} %multiply.2009), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_860 = f32[] constant(-0.0029546)
  %broadcast.1638 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_860), dimensions={}
  %add.1381 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2003, f32[4,1024,512]{2,1,0} %broadcast.1638), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2002 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1381, f32[4,1024,512]{2,1,0} %multiply.2009), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_861 = f32[] constant(-0.0160960332)
  %broadcast.1639 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_861), dimensions={}
  %add.1380 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2002, f32[4,1024,512]{2,1,0} %broadcast.1639), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2001 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.21, f32[4,1024,512]{2,1,0} %add.1380), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_867 = f32[] constant(-1.45660715e-05)
  %broadcast.1640 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_867), dimensions={}
  %add.1379 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2008, f32[4,1024,512]{2,1,0} %broadcast.1640), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2000 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1379, f32[4,1024,512]{2,1,0} %multiply.2009), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_868 = f32[] constant(-0.000213374049)
  %broadcast.1641 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_868), dimensions={}
  %add.1378 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2000, f32[4,1024,512]{2,1,0} %broadcast.1641), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1999 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1378, f32[4,1024,512]{2,1,0} %multiply.2009), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_869 = f32[] constant(-0.00168282702)
  %broadcast.1642 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_869), dimensions={}
  %add.1377 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.1999, f32[4,1024,512]{2,1,0} %broadcast.1642), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1998 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1377, f32[4,1024,512]{2,1,0} %multiply.2009), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_870 = f32[] constant(-0.00737332925)
  %broadcast.1643 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_870), dimensions={}
  %add.1376 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.1998, f32[4,1024,512]{2,1,0} %broadcast.1643), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1997 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1376, f32[4,1024,512]{2,1,0} %multiply.2009), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_871 = f32[] constant(-0.0142647391)
  %broadcast.1644 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_871), dimensions={}
  %add.1375 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.1997, f32[4,1024,512]{2,1,0} %broadcast.1644), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.569 = f32[4,1024,512]{2,1,0} divide(f32[4,1024,512]{2,1,0} %multiply.2001, f32[4,1024,512]{2,1,0} %add.1375), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.581 = f16[4,1024,512]{2,1,0} convert(f32[4,1024,512]{2,1,0} %divide.569), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_872 = f16[] constant(1)
  %broadcast.1645 = f16[4,1024,512]{2,1,0} broadcast(f16[] %constant_872), dimensions={}
  %add.1374 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %convert.581, f16[4,1024,512]{2,1,0} %broadcast.1645), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.1996 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %bitcast.879, f16[4,1024,512]{2,1,0} %add.1374), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_873 = f16[] constant(0.5)
  %broadcast.1646 = f16[4,1024,512]{2,1,0} broadcast(f16[] %constant_873), dimensions={}
  %multiply.1995 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.1996, f16[4,1024,512]{2,1,0} %broadcast.1646), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  ROOT %bitcast.878 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %multiply.1995)
}

%fused_computation.561 (param_0.1126: f32[512]) -> f16[4096,512] {
  %param_0.1126 = f32[512]{0} parameter(0)
  %convert.583 = f16[512]{0} convert(f32[512]{0} %param_0.1126), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.197 = f16[4,1024,512]{2,1,0} broadcast(f16[512]{0} %convert.583), dimensions={2}
  ROOT %bitcast.880 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %broadcast.197)
}

%fused_computation.562 (param_0.1129: f32[1024], param_1.1448: f32[1024], param_2.1842: f32[4,1024], param_3.1480: f32[4,1024], param_4.1033: f16[4,1024,1024], param_5.1028: f32[1024], param_6.876: f16[4096,1024]) -> f16[4,1024,1024] {
  %param_6.876 = f16[4096,1024]{1,0} parameter(6)
  %bitcast.1156 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_6.876), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_5.1028 = f32[1024]{0} parameter(5)
  %convert.748 = f16[1024]{0} convert(f32[1024]{0} %param_5.1028), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1975 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.748), dimensions={2}
  %add.1611 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.1156, f16[4,1024,1024]{2,1,0} %broadcast.1975), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_4.1033 = f16[4,1024,1024]{2,1,0} parameter(4)
  %add.1610 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1611, f16[4,1024,1024]{2,1,0} %param_4.1033), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=233}
  %convert.585 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1610), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_3.1480 = f32[4,1024]{1,0} parameter(3)
  %constant_875 = f32[] constant(0.0009765625)
  %broadcast.1647 = f32[4,1024]{1,0} broadcast(f32[] %constant_875), dimensions={}
  %multiply.2015 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_3.1480, f32[4,1024]{1,0} %broadcast.1647), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.201 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.2015), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.68 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.585, f32[4,1024,1024]{2,1,0} %broadcast.201), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.1842 = f32[4,1024]{1,0} parameter(2)
  %multiply.2014 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_2.1842, f32[4,1024]{1,0} %broadcast.1647), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.2013 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %multiply.2015, f32[4,1024]{1,0} %multiply.2015), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.64 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %multiply.2014, f32[4,1024]{1,0} %multiply.2013), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_877 = f32[] constant(0)
  %broadcast.1649 = f32[4,1024]{1,0} broadcast(f32[] %constant_877), dimensions={}
  %maximum.28 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %subtract.64, f32[4,1024]{1,0} %broadcast.1649), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_876 = f32[] constant(1e-12)
  %broadcast.1648 = f32[4,1024]{1,0} broadcast(f32[] %constant_876), dimensions={}
  %add.1388 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.28, f32[4,1024]{1,0} %broadcast.1648), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.3 = f32[4,1024]{1,0} rsqrt(f32[4,1024]{1,0} %add.1388), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %broadcast.200 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %rsqrt.3), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_1.1448 = f32[1024]{0} parameter(1)
  %broadcast.199 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_1.1448), dimensions={2}
  %multiply.2012 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.200, f32[4,1024,1024]{2,1,0} %broadcast.199), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2011 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.68, f32[4,1024,1024]{2,1,0} %multiply.2012), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.1129 = f32[1024]{0} parameter(0)
  %broadcast.198 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_0.1129), dimensions={2}
  %add.1387 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.2011, f32[4,1024,1024]{2,1,0} %broadcast.198), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  ROOT %convert.584 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.1387), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
}

%region_29.1356 (Arg_0.1357: f32[], Arg_1.1358: f32[]) -> f32[] {
  %Arg_0.1357 = f32[] parameter(0)
  %Arg_1.1358 = f32[] parameter(1)
  ROOT %add.1359 = f32[] add(f32[] %Arg_0.1357, f32[] %Arg_1.1358), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%region_28.1344 (Arg_0.1345: f32[], Arg_1.1346: f32[]) -> f32[] {
  %Arg_0.1345 = f32[] parameter(0)
  %Arg_1.1346 = f32[] parameter(1)
  ROOT %add.1347 = f32[] add(f32[] %Arg_0.1345, f32[] %Arg_1.1346), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%fused_computation.563 (param_0.1894: f16[4,1024,1024], param_1.2314: f32[1024], param_2.2070: f16[4096,1024]) -> (f32[4,1024], f32[4,1024]) {
  %param_2.2070 = f16[4096,1024]{1,0} parameter(2)
  %bitcast.1158 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_2.2070), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_1.2314 = f32[1024]{0} parameter(1)
  %convert.750 = f16[1024]{0} convert(f32[1024]{0} %param_1.2314), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1977 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.750), dimensions={2}
  %add.1615 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.1158, f16[4,1024,1024]{2,1,0} %broadcast.1977), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_0.1894 = f16[4,1024,1024]{2,1,0} parameter(0)
  %add.1614 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1615, f16[4,1024,1024]{2,1,0} %param_0.1894), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=233}
  %convert.586 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1614), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.2016 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.586, f32[4,1024,1024]{2,1,0} %convert.586), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %constant_879 = f32[] constant(0)
  %reduce.309 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.2016, f32[] %constant_879), dimensions={2}, to_apply=%region_29.1356, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %reduce.310.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.586, f32[] %constant_879), dimensions={2}, to_apply=%region_28.1344, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  ROOT %tuple.296 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) tuple(f32[4,1024]{1,0} %reduce.309, f32[4,1024]{1,0} %reduce.310.clone.1)
}

%fused_computation.566 (param_0.1138: f16[4,4,32,1024]) -> f16[4096,128] {
  %param_0.1138 = f16[4,4,32,1024]{3,2,1,0} parameter(0)
  %transpose.70 = f16[4,1024,4,32]{1,3,2,0} transpose(f16[4,4,32,1024]{3,2,1,0} %param_0.1138), dimensions={0,3,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %copy.141 = f16[4,1024,4,32]{3,2,1,0} copy(f16[4,1024,4,32]{1,3,2,0} %transpose.70), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  ROOT %bitcast.882 = f16[4096,128]{1,0} bitcast(f16[4,1024,4,32]{3,2,1,0} %copy.141)
}

%fused_computation.567 (param_0.1887: f32[4,4,1024], param_1.2305: f16[4,4,1024,1024], param_2.2059: f16[4,4,1024]) -> f16[4,4,1024,1024] {
  %param_1.2305 = f16[4,4,1024,1024]{3,2,1,0} parameter(1)
  %param_2.2059 = f16[4,4,1024]{2,1,0} parameter(2)
  %broadcast.1971 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_2.2059), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.148 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_1.2305, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1971), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.57 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.148), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %param_0.1887 = f32[4,4,1024]{2,1,0} parameter(0)
  %convert.589 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %param_0.1887), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.203 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %convert.589), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  ROOT %divide.570 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %exponential.57, f16[4,4,1024,1024]{3,2,1,0} %broadcast.203), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%region_27.1319 (Arg_0.1320: f32[], Arg_1.1321: f32[]) -> f32[] {
  %Arg_0.1320 = f32[] parameter(0)
  %Arg_1.1321 = f32[] parameter(1)
  ROOT %add.1322 = f32[] add(f32[] %Arg_0.1320, f32[] %Arg_1.1321), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.568 (param_0.1885: f16[4,4,1024,1024], param_1.2302: f16[4,4,1024]) -> f32[4,4,1024] {
  %param_0.1885 = f16[4,4,1024,1024]{3,2,1,0} parameter(0)
  %param_1.2302 = f16[4,4,1024]{2,1,0} parameter(1)
  %broadcast.1969 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_1.2302), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.146 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_0.1885, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1969), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.55 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.146), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.590 = f32[4,4,1024,1024]{3,2,1,0} convert(f16[4,4,1024,1024]{3,2,1,0} %exponential.55), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %constant_880 = f32[] constant(0)
  ROOT %reduce.311 = f32[4,4,1024]{2,1,0} reduce(f32[4,4,1024,1024]{3,2,1,0} %convert.590, f32[] %constant_880), dimensions={3}, to_apply=%region_27.1319, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.570 (param_0.1882: f16[4096,384]) -> (f16[4,4,32,1024], f16[4,4,32,1024]) {
  %param_0.1882 = f16[4096,384]{1,0} parameter(0)
  %bitcast.1152 = f16[4,1024,384]{2,1,0} bitcast(f16[4096,384]{1,0} %param_0.1882), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %bitcast.1151 = f16[4,1024,128,3]{3,2,1,0} bitcast(f16[4,1024,384]{2,1,0} %bitcast.1152), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %copy.185 = f16[4,1024,128,3]{1,2,0,3} copy(f16[4,1024,128,3]{3,2,1,0} %bitcast.1151), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %slice.51 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.185), slice={[0:4], [0:1024], [0:128], [2:3]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/slice[start_indices=(0, 0, 0, 2) limit_indices=(8, 1024, 1024, 3) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.71 = f16[1,4,128,1024]{3,2,1,0} transpose(f16[4,1024,128,1]{1,2,0,3} %slice.51), dimensions={3,0,2,1}
  %bitcast.884 = f16[4,4,32,1024]{3,2,1,0} bitcast(f16[1,4,128,1024]{3,2,1,0} %transpose.71)
  %slice.53.clone.1 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.185), slice={[0:4], [0:1024], [0:128], [1:2]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/slice[start_indices=(0, 0, 0, 1) limit_indices=(8, 1024, 1024, 2) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.72 = f16[1,4,128,1024]{3,2,1,0} transpose(f16[4,1024,128,1]{1,2,0,3} %slice.53.clone.1), dimensions={3,0,2,1}
  %bitcast.887.clone.1 = f16[4,4,32,1024]{3,2,1,0} bitcast(f16[1,4,128,1024]{3,2,1,0} %transpose.72)
  ROOT %tuple.298 = (f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) tuple(f16[4,4,32,1024]{3,2,1,0} %bitcast.884, f16[4,4,32,1024]{3,2,1,0} %bitcast.887.clone.1)
}

%fused_computation.571 (param_0.1878: f16[4096,384]) -> f16[4,4,1024,32] {
  %param_0.1878 = f16[4096,384]{1,0} parameter(0)
  %bitcast.1148 = f16[4,1024,384]{2,1,0} bitcast(f16[4096,384]{1,0} %param_0.1878), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %bitcast.1147 = f16[4,1024,128,3]{3,2,1,0} bitcast(f16[4,1024,384]{2,1,0} %bitcast.1148), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %copy.183 = f16[4,1024,128,3]{1,2,0,3} copy(f16[4,1024,128,3]{3,2,1,0} %bitcast.1147), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %slice.52 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.183), slice={[0:4], [0:1024], [0:128], [0:1]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/slice[start_indices=(0, 0, 0, 0) limit_indices=(8, 1024, 1024, 1) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %constant_881 = f16[] constant(0.17676)
  %broadcast.1650 = f16[4,1024,128,1]{1,2,0,3} broadcast(f16[] %constant_881), dimensions={}
  %multiply.2017 = f16[4,1024,128,1]{1,2,0,3} multiply(f16[4,1024,128,1]{1,2,0,3} %slice.52, f16[4,1024,128,1]{1,2,0,3} %broadcast.1650), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %bitcast.886 = f16[4,1024,4,32]{1,3,2,0} bitcast(f16[4,1024,128,1]{1,2,0,3} %multiply.2017), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %copy.142 = f16[4,1024,4,32]{3,1,2,0} copy(f16[4,1024,4,32]{1,3,2,0} %bitcast.886), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  ROOT %transpose.73 = f16[4,4,1024,32]{3,2,1,0} transpose(f16[4,1024,4,32]{3,1,2,0} %copy.142), dimensions={0,2,1,3}
}

%fused_computation.574 (param_0.1158: f32[384]) -> f16[4096,384] {
  %param_0.1158 = f32[384]{0} parameter(0)
  %convert.591 = f16[384]{0} convert(f32[384]{0} %param_0.1158), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.205 = f16[4,1024,384]{2,1,0} broadcast(f16[384]{0} %convert.591), dimensions={2}
  ROOT %bitcast.890 = f16[4096,384]{1,0} bitcast(f16[4,1024,384]{2,1,0} %broadcast.205)
}

%fused_computation.575 (param_0.1161: f32[1024], param_1.1469: f32[1024], param_2.1844: f32[4,1024], param_3.1482: f32[4,1024], param_4.1031: f16[4,1024,1024], param_5.1025: f32[1024], param_6.872: f16[4096,1024]) -> f16[4,1024,1024] {
  %param_6.872 = f16[4096,1024]{1,0} parameter(6)
  %bitcast.1138 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_6.872), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_5.1025 = f32[1024]{0} parameter(5)
  %convert.742 = f16[1024]{0} convert(f32[1024]{0} %param_5.1025), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1965 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.742), dimensions={2}
  %add.1599 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.1138, f16[4,1024,1024]{2,1,0} %broadcast.1965), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_4.1031 = f16[4,1024,1024]{2,1,0} parameter(4)
  %add.1598 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1599, f16[4,1024,1024]{2,1,0} %param_4.1031), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=310}
  %convert.593 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1598), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_3.1482 = f32[4,1024]{1,0} parameter(3)
  %constant_882 = f32[] constant(0.0009765625)
  %broadcast.1651 = f32[4,1024]{1,0} broadcast(f32[] %constant_882), dimensions={}
  %multiply.2022 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_3.1482, f32[4,1024]{1,0} %broadcast.1651), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.209 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.2022), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.73 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.593, f32[4,1024,1024]{2,1,0} %broadcast.209), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.1844 = f32[4,1024]{1,0} parameter(2)
  %multiply.2021 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_2.1844, f32[4,1024]{1,0} %broadcast.1651), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.2020 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %multiply.2022, f32[4,1024]{1,0} %multiply.2022), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.70 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %multiply.2021, f32[4,1024]{1,0} %multiply.2020), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_884 = f32[] constant(0)
  %broadcast.1653 = f32[4,1024]{1,0} broadcast(f32[] %constant_884), dimensions={}
  %maximum.29 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %subtract.70, f32[4,1024]{1,0} %broadcast.1653), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_883 = f32[] constant(1e-12)
  %broadcast.1652 = f32[4,1024]{1,0} broadcast(f32[] %constant_883), dimensions={}
  %add.1392 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.29, f32[4,1024]{1,0} %broadcast.1652), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.4 = f32[4,1024]{1,0} rsqrt(f32[4,1024]{1,0} %add.1392), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %broadcast.208 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %rsqrt.4), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_1.1469 = f32[1024]{0} parameter(1)
  %broadcast.207 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_1.1469), dimensions={2}
  %multiply.2019 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.208, f32[4,1024,1024]{2,1,0} %broadcast.207), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2018 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.73, f32[4,1024,1024]{2,1,0} %multiply.2019), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.1161 = f32[1024]{0} parameter(0)
  %broadcast.206 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_0.1161), dimensions={2}
  %add.1391 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.2018, f32[4,1024,1024]{2,1,0} %broadcast.206), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  ROOT %convert.592 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.1391), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
}

%region_25.1256 (Arg_0.1257: f32[], Arg_1.1258: f32[]) -> f32[] {
  %Arg_0.1257 = f32[] parameter(0)
  %Arg_1.1258 = f32[] parameter(1)
  ROOT %add.1259 = f32[] add(f32[] %Arg_0.1257, f32[] %Arg_1.1258), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%region_24.1244 (Arg_0.1245: f32[], Arg_1.1246: f32[]) -> f32[] {
  %Arg_0.1245 = f32[] parameter(0)
  %Arg_1.1246 = f32[] parameter(1)
  ROOT %add.1247 = f32[] add(f32[] %Arg_0.1245, f32[] %Arg_1.1246), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%fused_computation.576 (param_0.1870: f16[4,1024,1024], param_1.2299: f32[1024], param_2.2057: f16[4096,1024]) -> (f32[4,1024], f32[4,1024]) {
  %param_2.2057 = f16[4096,1024]{1,0} parameter(2)
  %bitcast.1140 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_2.2057), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_1.2299 = f32[1024]{0} parameter(1)
  %convert.744 = f16[1024]{0} convert(f32[1024]{0} %param_1.2299), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1967 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.744), dimensions={2}
  %add.1603 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.1140, f16[4,1024,1024]{2,1,0} %broadcast.1967), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_0.1870 = f16[4,1024,1024]{2,1,0} parameter(0)
  %add.1602 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1603, f16[4,1024,1024]{2,1,0} %param_0.1870), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=310}
  %convert.594 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1602), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.2023 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.594, f32[4,1024,1024]{2,1,0} %convert.594), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %constant_886 = f32[] constant(0)
  %reduce.312 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.2023, f32[] %constant_886), dimensions={2}, to_apply=%region_25.1256, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %reduce.313.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.594, f32[] %constant_886), dimensions={2}, to_apply=%region_24.1244, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  ROOT %tuple.299 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) tuple(f32[4,1024]{1,0} %reduce.312, f32[4,1024]{1,0} %reduce.313.clone.1)
}

%fused_computation.579 (param_0.1654: f16[4096,512]) -> f16[4096,512] {
  %param_0.1654 = f16[4096,512]{1,0} parameter(0)
  %bitcast.893 = f16[4,1024,512]{2,1,0} bitcast(f16[4096,512]{1,0} %param_0.1654), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %constant_889 = f32[] constant(-4)
  %broadcast.1656 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_889), dimensions={}
  %constant_887 = f16[] constant(0.70703)
  %broadcast.1654 = f16[4,1024,512]{2,1,0} broadcast(f16[] %constant_887), dimensions={}
  %multiply.2039 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %bitcast.893, f16[4,1024,512]{2,1,0} %broadcast.1654), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.598 = f32[4,1024,512]{2,1,0} convert(f16[4,1024,512]{2,1,0} %multiply.2039), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_888 = f32[] constant(4)
  %broadcast.1655 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_888), dimensions={}
  %clamp.22 = f32[4,1024,512]{2,1,0} clamp(f32[4,1024,512]{2,1,0} %broadcast.1656, f32[4,1024,512]{2,1,0} %convert.598, f32[4,1024,512]{2,1,0} %broadcast.1655), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2038 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.22, f32[4,1024,512]{2,1,0} %clamp.22), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_904 = f32[] constant(0)
  %broadcast.1657 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_904), dimensions={}
  %multiply.2037 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %multiply.2038, f32[4,1024,512]{2,1,0} %broadcast.1657), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_890 = f32[] constant(-2.72614237e-10)
  %broadcast.1658 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_890), dimensions={}
  %add.1407 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2037, f32[4,1024,512]{2,1,0} %broadcast.1658), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2036 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1407, f32[4,1024,512]{2,1,0} %multiply.2038), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_891 = f32[] constant(2.77068146e-08)
  %broadcast.1659 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_891), dimensions={}
  %add.1406 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2036, f32[4,1024,512]{2,1,0} %broadcast.1659), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2035 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1406, f32[4,1024,512]{2,1,0} %multiply.2038), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_892 = f32[] constant(-2.10102394e-06)
  %broadcast.1660 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_892), dimensions={}
  %add.1405 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2035, f32[4,1024,512]{2,1,0} %broadcast.1660), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2034 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1405, f32[4,1024,512]{2,1,0} %multiply.2038), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_893 = f32[] constant(-5.69250624e-05)
  %broadcast.1661 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_893), dimensions={}
  %add.1404 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2034, f32[4,1024,512]{2,1,0} %broadcast.1661), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2033 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1404, f32[4,1024,512]{2,1,0} %multiply.2038), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_894 = f32[] constant(-0.000734990637)
  %broadcast.1662 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_894), dimensions={}
  %add.1403 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2033, f32[4,1024,512]{2,1,0} %broadcast.1662), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2032 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1403, f32[4,1024,512]{2,1,0} %multiply.2038), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_895 = f32[] constant(-0.0029546)
  %broadcast.1663 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_895), dimensions={}
  %add.1402 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2032, f32[4,1024,512]{2,1,0} %broadcast.1663), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2031 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1402, f32[4,1024,512]{2,1,0} %multiply.2038), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_896 = f32[] constant(-0.0160960332)
  %broadcast.1664 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_896), dimensions={}
  %add.1401 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2031, f32[4,1024,512]{2,1,0} %broadcast.1664), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2030 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.22, f32[4,1024,512]{2,1,0} %add.1401), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_897 = f32[] constant(-1.45660715e-05)
  %broadcast.1665 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_897), dimensions={}
  %add.1400 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2037, f32[4,1024,512]{2,1,0} %broadcast.1665), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2029 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1400, f32[4,1024,512]{2,1,0} %multiply.2038), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_898 = f32[] constant(-0.000213374049)
  %broadcast.1666 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_898), dimensions={}
  %add.1399 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2029, f32[4,1024,512]{2,1,0} %broadcast.1666), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2028 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1399, f32[4,1024,512]{2,1,0} %multiply.2038), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_899 = f32[] constant(-0.00168282702)
  %broadcast.1667 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_899), dimensions={}
  %add.1398 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2028, f32[4,1024,512]{2,1,0} %broadcast.1667), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2027 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1398, f32[4,1024,512]{2,1,0} %multiply.2038), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_900 = f32[] constant(-0.00737332925)
  %broadcast.1668 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_900), dimensions={}
  %add.1397 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2027, f32[4,1024,512]{2,1,0} %broadcast.1668), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2026 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1397, f32[4,1024,512]{2,1,0} %multiply.2038), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_901 = f32[] constant(-0.0142647391)
  %broadcast.1669 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_901), dimensions={}
  %add.1396 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2026, f32[4,1024,512]{2,1,0} %broadcast.1669), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.571 = f32[4,1024,512]{2,1,0} divide(f32[4,1024,512]{2,1,0} %multiply.2030, f32[4,1024,512]{2,1,0} %add.1396), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.597 = f16[4,1024,512]{2,1,0} convert(f32[4,1024,512]{2,1,0} %divide.571), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_902 = f16[] constant(1)
  %broadcast.1670 = f16[4,1024,512]{2,1,0} broadcast(f16[] %constant_902), dimensions={}
  %add.1395 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %convert.597, f16[4,1024,512]{2,1,0} %broadcast.1670), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2025 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %bitcast.893, f16[4,1024,512]{2,1,0} %add.1395), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_903 = f16[] constant(0.5)
  %broadcast.1671 = f16[4,1024,512]{2,1,0} broadcast(f16[] %constant_903), dimensions={}
  %multiply.2024 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.2025, f16[4,1024,512]{2,1,0} %broadcast.1671), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  ROOT %bitcast.892 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %multiply.2024)
}

%fused_computation.580 (param_0.1173: f32[512]) -> f16[4096,512] {
  %param_0.1173 = f32[512]{0} parameter(0)
  %convert.599 = f16[512]{0} convert(f32[512]{0} %param_0.1173), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.211 = f16[4,1024,512]{2,1,0} broadcast(f16[512]{0} %convert.599), dimensions={2}
  ROOT %bitcast.894 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %broadcast.211)
}

%fused_computation.581 (param_0.1176: f32[1024], param_1.1484: f32[1024], param_2.1849: f32[4,1024], param_3.1487: f32[4,1024], param_4.1029: f16[4,1024,1024], param_5.1022: f32[1024], param_6.868: f16[4096,1024]) -> f16[4,1024,1024] {
  %param_6.868 = f16[4096,1024]{1,0} parameter(6)
  %bitcast.1132 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_6.868), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_5.1022 = f32[1024]{0} parameter(5)
  %convert.736 = f16[1024]{0} convert(f32[1024]{0} %param_5.1022), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1959 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.736), dimensions={2}
  %add.1587 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.1132, f16[4,1024,1024]{2,1,0} %broadcast.1959), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_4.1029 = f16[4,1024,1024]{2,1,0} parameter(4)
  %add.1586 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1587, f16[4,1024,1024]{2,1,0} %param_4.1029), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=233}
  %convert.601 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1586), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_3.1487 = f32[4,1024]{1,0} parameter(3)
  %constant_905 = f32[] constant(0.0009765625)
  %broadcast.1672 = f32[4,1024]{1,0} broadcast(f32[] %constant_905), dimensions={}
  %multiply.2044 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_3.1487, f32[4,1024]{1,0} %broadcast.1672), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.215 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.2044), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.75 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.601, f32[4,1024,1024]{2,1,0} %broadcast.215), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.1849 = f32[4,1024]{1,0} parameter(2)
  %multiply.2043 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_2.1849, f32[4,1024]{1,0} %broadcast.1672), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.2042 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %multiply.2044, f32[4,1024]{1,0} %multiply.2044), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.74 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %multiply.2043, f32[4,1024]{1,0} %multiply.2042), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_907 = f32[] constant(0)
  %broadcast.1674 = f32[4,1024]{1,0} broadcast(f32[] %constant_907), dimensions={}
  %maximum.30 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %subtract.74, f32[4,1024]{1,0} %broadcast.1674), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_906 = f32[] constant(1e-12)
  %broadcast.1673 = f32[4,1024]{1,0} broadcast(f32[] %constant_906), dimensions={}
  %add.1409 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.30, f32[4,1024]{1,0} %broadcast.1673), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.5 = f32[4,1024]{1,0} rsqrt(f32[4,1024]{1,0} %add.1409), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %broadcast.214 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %rsqrt.5), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_1.1484 = f32[1024]{0} parameter(1)
  %broadcast.213 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_1.1484), dimensions={2}
  %multiply.2041 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.214, f32[4,1024,1024]{2,1,0} %broadcast.213), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2040 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.75, f32[4,1024,1024]{2,1,0} %multiply.2041), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.1176 = f32[1024]{0} parameter(0)
  %broadcast.212 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_0.1176), dimensions={2}
  %add.1408 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.2040, f32[4,1024,1024]{2,1,0} %broadcast.212), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  ROOT %convert.600 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.1408), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
}

%region_23.1164 (Arg_0.1165: f32[], Arg_1.1166: f32[]) -> f32[] {
  %Arg_0.1165 = f32[] parameter(0)
  %Arg_1.1166 = f32[] parameter(1)
  ROOT %add.1167 = f32[] add(f32[] %Arg_0.1165, f32[] %Arg_1.1166), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%region_22.1152 (Arg_0.1153: f32[], Arg_1.1154: f32[]) -> f32[] {
  %Arg_0.1153 = f32[] parameter(0)
  %Arg_1.1154 = f32[] parameter(1)
  ROOT %add.1155 = f32[] add(f32[] %Arg_0.1153, f32[] %Arg_1.1154), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%fused_computation.582 (param_0.1863: f16[4,1024,1024], param_1.2290: f32[1024], param_2.2046: f16[4096,1024]) -> (f32[4,1024], f32[4,1024]) {
  %param_2.2046 = f16[4096,1024]{1,0} parameter(2)
  %bitcast.1134 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_2.2046), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_1.2290 = f32[1024]{0} parameter(1)
  %convert.738 = f16[1024]{0} convert(f32[1024]{0} %param_1.2290), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1961 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.738), dimensions={2}
  %add.1591 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.1134, f16[4,1024,1024]{2,1,0} %broadcast.1961), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_0.1863 = f16[4,1024,1024]{2,1,0} parameter(0)
  %add.1590 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1591, f16[4,1024,1024]{2,1,0} %param_0.1863), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=233}
  %convert.602 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1590), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.2045 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.602, f32[4,1024,1024]{2,1,0} %convert.602), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %constant_909 = f32[] constant(0)
  %reduce.314 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.2045, f32[] %constant_909), dimensions={2}, to_apply=%region_23.1164, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %reduce.315.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.602, f32[] %constant_909), dimensions={2}, to_apply=%region_22.1152, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  ROOT %tuple.300 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) tuple(f32[4,1024]{1,0} %reduce.314, f32[4,1024]{1,0} %reduce.315.clone.1)
}

%fused_computation.585 (param_0.1185: f16[4,4,32,1024]) -> f16[4096,128] {
  %param_0.1185 = f16[4,4,32,1024]{3,2,1,0} parameter(0)
  %transpose.74 = f16[4,1024,4,32]{1,3,2,0} transpose(f16[4,4,32,1024]{3,2,1,0} %param_0.1185), dimensions={0,3,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %copy.144 = f16[4,1024,4,32]{3,2,1,0} copy(f16[4,1024,4,32]{1,3,2,0} %transpose.74), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  ROOT %bitcast.896 = f16[4096,128]{1,0} bitcast(f16[4,1024,4,32]{3,2,1,0} %copy.144)
}

%fused_computation.586 (param_0.1856: f32[4,4,1024], param_1.2281: f16[4,4,1024,1024], param_2.2035: f16[4,4,1024]) -> f16[4,4,1024,1024] {
  %param_1.2281 = f16[4,4,1024,1024]{3,2,1,0} parameter(1)
  %param_2.2035 = f16[4,4,1024]{2,1,0} parameter(2)
  %broadcast.1955 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_2.2035), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.144 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_1.2281, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1955), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.53 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.144), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %param_0.1856 = f32[4,4,1024]{2,1,0} parameter(0)
  %convert.605 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %param_0.1856), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.217 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %convert.605), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  ROOT %divide.572 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %exponential.53, f16[4,4,1024,1024]{3,2,1,0} %broadcast.217), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%region_21.1127 (Arg_0.1128: f32[], Arg_1.1129: f32[]) -> f32[] {
  %Arg_0.1128 = f32[] parameter(0)
  %Arg_1.1129 = f32[] parameter(1)
  ROOT %add.1130 = f32[] add(f32[] %Arg_0.1128, f32[] %Arg_1.1129), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.587 (param_0.1854: f16[4,4,1024,1024], param_1.2278: f16[4,4,1024]) -> f32[4,4,1024] {
  %param_0.1854 = f16[4,4,1024,1024]{3,2,1,0} parameter(0)
  %param_1.2278 = f16[4,4,1024]{2,1,0} parameter(1)
  %broadcast.1953 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_1.2278), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.139 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_0.1854, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1953), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.51 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.139), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.606 = f32[4,4,1024,1024]{3,2,1,0} convert(f16[4,4,1024,1024]{3,2,1,0} %exponential.51), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %constant_910 = f32[] constant(0)
  ROOT %reduce.316 = f32[4,4,1024]{2,1,0} reduce(f32[4,4,1024,1024]{3,2,1,0} %convert.606, f32[] %constant_910), dimensions={3}, to_apply=%region_21.1127, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.589 (param_0.1851: f16[4096,384]) -> (f16[4,4,32,1024], f16[4,4,32,1024]) {
  %param_0.1851 = f16[4096,384]{1,0} parameter(0)
  %bitcast.1128 = f16[4,1024,384]{2,1,0} bitcast(f16[4096,384]{1,0} %param_0.1851), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %bitcast.1127 = f16[4,1024,128,3]{3,2,1,0} bitcast(f16[4,1024,384]{2,1,0} %bitcast.1128), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %copy.179 = f16[4,1024,128,3]{1,2,0,3} copy(f16[4,1024,128,3]{3,2,1,0} %bitcast.1127), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %slice.54 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.179), slice={[0:4], [0:1024], [0:128], [2:3]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/slice[start_indices=(0, 0, 0, 2) limit_indices=(8, 1024, 1024, 3) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.75 = f16[1,4,128,1024]{3,2,1,0} transpose(f16[4,1024,128,1]{1,2,0,3} %slice.54), dimensions={3,0,2,1}
  %bitcast.898 = f16[4,4,32,1024]{3,2,1,0} bitcast(f16[1,4,128,1024]{3,2,1,0} %transpose.75)
  %slice.56.clone.1 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.179), slice={[0:4], [0:1024], [0:128], [1:2]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/slice[start_indices=(0, 0, 0, 1) limit_indices=(8, 1024, 1024, 2) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.76 = f16[1,4,128,1024]{3,2,1,0} transpose(f16[4,1024,128,1]{1,2,0,3} %slice.56.clone.1), dimensions={3,0,2,1}
  %bitcast.901.clone.1 = f16[4,4,32,1024]{3,2,1,0} bitcast(f16[1,4,128,1024]{3,2,1,0} %transpose.76)
  ROOT %tuple.302 = (f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) tuple(f16[4,4,32,1024]{3,2,1,0} %bitcast.898, f16[4,4,32,1024]{3,2,1,0} %bitcast.901.clone.1)
}

%fused_computation.590 (param_0.1847: f16[4096,384]) -> f16[4,4,1024,32] {
  %param_0.1847 = f16[4096,384]{1,0} parameter(0)
  %bitcast.1124 = f16[4,1024,384]{2,1,0} bitcast(f16[4096,384]{1,0} %param_0.1847), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %bitcast.1123 = f16[4,1024,128,3]{3,2,1,0} bitcast(f16[4,1024,384]{2,1,0} %bitcast.1124), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %copy.177 = f16[4,1024,128,3]{1,2,0,3} copy(f16[4,1024,128,3]{3,2,1,0} %bitcast.1123), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %slice.55 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.177), slice={[0:4], [0:1024], [0:128], [0:1]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/slice[start_indices=(0, 0, 0, 0) limit_indices=(8, 1024, 1024, 1) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %constant_911 = f16[] constant(0.17676)
  %broadcast.1675 = f16[4,1024,128,1]{1,2,0,3} broadcast(f16[] %constant_911), dimensions={}
  %multiply.2046 = f16[4,1024,128,1]{1,2,0,3} multiply(f16[4,1024,128,1]{1,2,0,3} %slice.55, f16[4,1024,128,1]{1,2,0,3} %broadcast.1675), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %bitcast.900 = f16[4,1024,4,32]{1,3,2,0} bitcast(f16[4,1024,128,1]{1,2,0,3} %multiply.2046), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %copy.145 = f16[4,1024,4,32]{3,1,2,0} copy(f16[4,1024,4,32]{1,3,2,0} %bitcast.900), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  ROOT %transpose.77 = f16[4,4,1024,32]{3,2,1,0} transpose(f16[4,1024,4,32]{3,1,2,0} %copy.145), dimensions={0,2,1,3}
}

%fused_computation.593 (param_0.1205: f32[384]) -> f16[4096,384] {
  %param_0.1205 = f32[384]{0} parameter(0)
  %convert.607 = f16[384]{0} convert(f32[384]{0} %param_0.1205), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.219 = f16[4,1024,384]{2,1,0} broadcast(f16[384]{0} %convert.607), dimensions={2}
  ROOT %bitcast.904 = f16[4096,384]{1,0} bitcast(f16[4,1024,384]{2,1,0} %broadcast.219)
}

%fused_computation.594 (param_0.1208: f32[1024], param_1.1505: f32[1024], param_2.1851: f32[4,1024], param_3.1489: f32[4,1024], param_4.1027: f16[4,1024,1024], param_5.1019: f32[1024], param_6.864: f16[4096,1024]) -> f16[4,1024,1024] {
  %param_6.864 = f16[4096,1024]{1,0} parameter(6)
  %bitcast.1114 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_6.864), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_5.1019 = f32[1024]{0} parameter(5)
  %convert.730 = f16[1024]{0} convert(f32[1024]{0} %param_5.1019), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1949 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.730), dimensions={2}
  %add.1575 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.1114, f16[4,1024,1024]{2,1,0} %broadcast.1949), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_4.1027 = f16[4,1024,1024]{2,1,0} parameter(4)
  %add.1574 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1575, f16[4,1024,1024]{2,1,0} %param_4.1027), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=310}
  %convert.609 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1574), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_3.1489 = f32[4,1024]{1,0} parameter(3)
  %constant_912 = f32[] constant(0.0009765625)
  %broadcast.1676 = f32[4,1024]{1,0} broadcast(f32[] %constant_912), dimensions={}
  %multiply.2051 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_3.1489, f32[4,1024]{1,0} %broadcast.1676), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.223 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.2051), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.81 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.609, f32[4,1024,1024]{2,1,0} %broadcast.223), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.1851 = f32[4,1024]{1,0} parameter(2)
  %multiply.2050 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_2.1851, f32[4,1024]{1,0} %broadcast.1676), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.2049 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %multiply.2051, f32[4,1024]{1,0} %multiply.2051), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.80 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %multiply.2050, f32[4,1024]{1,0} %multiply.2049), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_914 = f32[] constant(0)
  %broadcast.1678 = f32[4,1024]{1,0} broadcast(f32[] %constant_914), dimensions={}
  %maximum.31 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %subtract.80, f32[4,1024]{1,0} %broadcast.1678), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_913 = f32[] constant(1e-12)
  %broadcast.1677 = f32[4,1024]{1,0} broadcast(f32[] %constant_913), dimensions={}
  %add.1413 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.31, f32[4,1024]{1,0} %broadcast.1677), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.6 = f32[4,1024]{1,0} rsqrt(f32[4,1024]{1,0} %add.1413), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %broadcast.222 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %rsqrt.6), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_1.1505 = f32[1024]{0} parameter(1)
  %broadcast.221 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_1.1505), dimensions={2}
  %multiply.2048 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.222, f32[4,1024,1024]{2,1,0} %broadcast.221), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2047 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.81, f32[4,1024,1024]{2,1,0} %multiply.2048), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.1208 = f32[1024]{0} parameter(0)
  %broadcast.220 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_0.1208), dimensions={2}
  %add.1412 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.2047, f32[4,1024,1024]{2,1,0} %broadcast.220), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  ROOT %convert.608 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.1412), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
}

%region_19.1064 (Arg_0.1065: f32[], Arg_1.1066: f32[]) -> f32[] {
  %Arg_0.1065 = f32[] parameter(0)
  %Arg_1.1066 = f32[] parameter(1)
  ROOT %add.1067 = f32[] add(f32[] %Arg_0.1065, f32[] %Arg_1.1066), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%region_18.1052 (Arg_0.1053: f32[], Arg_1.1054: f32[]) -> f32[] {
  %Arg_0.1053 = f32[] parameter(0)
  %Arg_1.1054 = f32[] parameter(1)
  ROOT %add.1055 = f32[] add(f32[] %Arg_0.1053, f32[] %Arg_1.1054), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%fused_computation.595 (param_0.1839: f16[4,1024,1024], param_1.2275: f32[1024], param_2.2033: f16[4096,1024]) -> (f32[4,1024], f32[4,1024]) {
  %param_2.2033 = f16[4096,1024]{1,0} parameter(2)
  %bitcast.1116 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_2.2033), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_1.2275 = f32[1024]{0} parameter(1)
  %convert.732 = f16[1024]{0} convert(f32[1024]{0} %param_1.2275), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1951 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.732), dimensions={2}
  %add.1579 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.1116, f16[4,1024,1024]{2,1,0} %broadcast.1951), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_0.1839 = f16[4,1024,1024]{2,1,0} parameter(0)
  %add.1578 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1579, f16[4,1024,1024]{2,1,0} %param_0.1839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=310}
  %convert.610 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1578), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.2052 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.610, f32[4,1024,1024]{2,1,0} %convert.610), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %constant_916 = f32[] constant(0)
  %reduce.317 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.2052, f32[] %constant_916), dimensions={2}, to_apply=%region_19.1064, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %reduce.318.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.610, f32[] %constant_916), dimensions={2}, to_apply=%region_18.1052, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  ROOT %tuple.303 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) tuple(f32[4,1024]{1,0} %reduce.317, f32[4,1024]{1,0} %reduce.318.clone.1)
}

%fused_computation.598 (param_0.1661: f16[4096,512]) -> f16[4096,512] {
  %param_0.1661 = f16[4096,512]{1,0} parameter(0)
  %bitcast.907 = f16[4,1024,512]{2,1,0} bitcast(f16[4096,512]{1,0} %param_0.1661), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %constant_919 = f32[] constant(-4)
  %broadcast.1681 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_919), dimensions={}
  %constant_917 = f16[] constant(0.70703)
  %broadcast.1679 = f16[4,1024,512]{2,1,0} broadcast(f16[] %constant_917), dimensions={}
  %multiply.2068 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %bitcast.907, f16[4,1024,512]{2,1,0} %broadcast.1679), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.614 = f32[4,1024,512]{2,1,0} convert(f16[4,1024,512]{2,1,0} %multiply.2068), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_918 = f32[] constant(4)
  %broadcast.1680 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_918), dimensions={}
  %clamp.23 = f32[4,1024,512]{2,1,0} clamp(f32[4,1024,512]{2,1,0} %broadcast.1681, f32[4,1024,512]{2,1,0} %convert.614, f32[4,1024,512]{2,1,0} %broadcast.1680), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2067 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.23, f32[4,1024,512]{2,1,0} %clamp.23), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_934 = f32[] constant(0)
  %broadcast.1682 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_934), dimensions={}
  %multiply.2066 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %multiply.2067, f32[4,1024,512]{2,1,0} %broadcast.1682), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_920 = f32[] constant(-2.72614237e-10)
  %broadcast.1683 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_920), dimensions={}
  %add.1428 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2066, f32[4,1024,512]{2,1,0} %broadcast.1683), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2065 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1428, f32[4,1024,512]{2,1,0} %multiply.2067), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_921 = f32[] constant(2.77068146e-08)
  %broadcast.1684 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_921), dimensions={}
  %add.1427 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2065, f32[4,1024,512]{2,1,0} %broadcast.1684), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2064 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1427, f32[4,1024,512]{2,1,0} %multiply.2067), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_922 = f32[] constant(-2.10102394e-06)
  %broadcast.1685 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_922), dimensions={}
  %add.1426 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2064, f32[4,1024,512]{2,1,0} %broadcast.1685), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2063 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1426, f32[4,1024,512]{2,1,0} %multiply.2067), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_923 = f32[] constant(-5.69250624e-05)
  %broadcast.1686 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_923), dimensions={}
  %add.1425 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2063, f32[4,1024,512]{2,1,0} %broadcast.1686), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2062 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1425, f32[4,1024,512]{2,1,0} %multiply.2067), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_924 = f32[] constant(-0.000734990637)
  %broadcast.1687 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_924), dimensions={}
  %add.1424 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2062, f32[4,1024,512]{2,1,0} %broadcast.1687), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2061 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1424, f32[4,1024,512]{2,1,0} %multiply.2067), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_925 = f32[] constant(-0.0029546)
  %broadcast.1688 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_925), dimensions={}
  %add.1423 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2061, f32[4,1024,512]{2,1,0} %broadcast.1688), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2060 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1423, f32[4,1024,512]{2,1,0} %multiply.2067), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_926 = f32[] constant(-0.0160960332)
  %broadcast.1689 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_926), dimensions={}
  %add.1422 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2060, f32[4,1024,512]{2,1,0} %broadcast.1689), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2059 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.23, f32[4,1024,512]{2,1,0} %add.1422), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_927 = f32[] constant(-1.45660715e-05)
  %broadcast.1690 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_927), dimensions={}
  %add.1421 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2066, f32[4,1024,512]{2,1,0} %broadcast.1690), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2058 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1421, f32[4,1024,512]{2,1,0} %multiply.2067), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_928 = f32[] constant(-0.000213374049)
  %broadcast.1691 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_928), dimensions={}
  %add.1420 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2058, f32[4,1024,512]{2,1,0} %broadcast.1691), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2057 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1420, f32[4,1024,512]{2,1,0} %multiply.2067), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_929 = f32[] constant(-0.00168282702)
  %broadcast.1692 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_929), dimensions={}
  %add.1419 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2057, f32[4,1024,512]{2,1,0} %broadcast.1692), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2056 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1419, f32[4,1024,512]{2,1,0} %multiply.2067), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_930 = f32[] constant(-0.00737332925)
  %broadcast.1693 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_930), dimensions={}
  %add.1418 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2056, f32[4,1024,512]{2,1,0} %broadcast.1693), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2055 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1418, f32[4,1024,512]{2,1,0} %multiply.2067), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_931 = f32[] constant(-0.0142647391)
  %broadcast.1694 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_931), dimensions={}
  %add.1417 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2055, f32[4,1024,512]{2,1,0} %broadcast.1694), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.573 = f32[4,1024,512]{2,1,0} divide(f32[4,1024,512]{2,1,0} %multiply.2059, f32[4,1024,512]{2,1,0} %add.1417), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.613 = f16[4,1024,512]{2,1,0} convert(f32[4,1024,512]{2,1,0} %divide.573), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_932 = f16[] constant(1)
  %broadcast.1695 = f16[4,1024,512]{2,1,0} broadcast(f16[] %constant_932), dimensions={}
  %add.1416 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %convert.613, f16[4,1024,512]{2,1,0} %broadcast.1695), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2054 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %bitcast.907, f16[4,1024,512]{2,1,0} %add.1416), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_933 = f16[] constant(0.5)
  %broadcast.1696 = f16[4,1024,512]{2,1,0} broadcast(f16[] %constant_933), dimensions={}
  %multiply.2053 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.2054, f16[4,1024,512]{2,1,0} %broadcast.1696), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  ROOT %bitcast.906 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %multiply.2053)
}

%fused_computation.599 (param_0.1220: f32[512]) -> f16[4096,512] {
  %param_0.1220 = f32[512]{0} parameter(0)
  %convert.615 = f16[512]{0} convert(f32[512]{0} %param_0.1220), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.225 = f16[4,1024,512]{2,1,0} broadcast(f16[512]{0} %convert.615), dimensions={2}
  ROOT %bitcast.908 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %broadcast.225)
}

%fused_computation.600 (param_0.1223: f32[1024], param_1.1520: f32[1024], param_2.1856: f32[4,1024], param_3.1494: f32[4,1024], param_4.1025: f16[4,1024,1024], param_5.1016: f32[1024], param_6.860: f16[4096,1024]) -> f16[4,1024,1024] {
  %param_6.860 = f16[4096,1024]{1,0} parameter(6)
  %bitcast.1108 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_6.860), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_5.1016 = f32[1024]{0} parameter(5)
  %convert.724 = f16[1024]{0} convert(f32[1024]{0} %param_5.1016), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1943 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.724), dimensions={2}
  %add.1563 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.1108, f16[4,1024,1024]{2,1,0} %broadcast.1943), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_4.1025 = f16[4,1024,1024]{2,1,0} parameter(4)
  %add.1562 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1563, f16[4,1024,1024]{2,1,0} %param_4.1025), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=233}
  %convert.617 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1562), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_3.1494 = f32[4,1024]{1,0} parameter(3)
  %constant_935 = f32[] constant(0.0009765625)
  %broadcast.1697 = f32[4,1024]{1,0} broadcast(f32[] %constant_935), dimensions={}
  %multiply.2073 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_3.1494, f32[4,1024]{1,0} %broadcast.1697), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.229 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.2073), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.85 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.617, f32[4,1024,1024]{2,1,0} %broadcast.229), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.1856 = f32[4,1024]{1,0} parameter(2)
  %multiply.2072 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_2.1856, f32[4,1024]{1,0} %broadcast.1697), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.2071 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %multiply.2073, f32[4,1024]{1,0} %multiply.2073), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.84 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %multiply.2072, f32[4,1024]{1,0} %multiply.2071), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_937 = f32[] constant(0)
  %broadcast.1699 = f32[4,1024]{1,0} broadcast(f32[] %constant_937), dimensions={}
  %maximum.32 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %subtract.84, f32[4,1024]{1,0} %broadcast.1699), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_936 = f32[] constant(1e-12)
  %broadcast.1698 = f32[4,1024]{1,0} broadcast(f32[] %constant_936), dimensions={}
  %add.1430 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.32, f32[4,1024]{1,0} %broadcast.1698), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.7 = f32[4,1024]{1,0} rsqrt(f32[4,1024]{1,0} %add.1430), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %broadcast.228 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %rsqrt.7), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_1.1520 = f32[1024]{0} parameter(1)
  %broadcast.227 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_1.1520), dimensions={2}
  %multiply.2070 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.228, f32[4,1024,1024]{2,1,0} %broadcast.227), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2069 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.85, f32[4,1024,1024]{2,1,0} %multiply.2070), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.1223 = f32[1024]{0} parameter(0)
  %broadcast.226 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_0.1223), dimensions={2}
  %add.1429 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.2069, f32[4,1024,1024]{2,1,0} %broadcast.226), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  ROOT %convert.616 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.1429), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
}

%region_17.972 (Arg_0.973: f32[], Arg_1.974: f32[]) -> f32[] {
  %Arg_0.973 = f32[] parameter(0)
  %Arg_1.974 = f32[] parameter(1)
  ROOT %add.975 = f32[] add(f32[] %Arg_0.973, f32[] %Arg_1.974), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%region_16.960 (Arg_0.961: f32[], Arg_1.962: f32[]) -> f32[] {
  %Arg_0.961 = f32[] parameter(0)
  %Arg_1.962 = f32[] parameter(1)
  ROOT %add.963 = f32[] add(f32[] %Arg_0.961, f32[] %Arg_1.962), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%fused_computation.601 (param_0.1832: f16[4,1024,1024], param_1.2266: f32[1024], param_2.2022: f16[4096,1024]) -> (f32[4,1024], f32[4,1024]) {
  %param_2.2022 = f16[4096,1024]{1,0} parameter(2)
  %bitcast.1110 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_2.2022), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_1.2266 = f32[1024]{0} parameter(1)
  %convert.726 = f16[1024]{0} convert(f32[1024]{0} %param_1.2266), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1945 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.726), dimensions={2}
  %add.1567 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.1110, f16[4,1024,1024]{2,1,0} %broadcast.1945), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_0.1832 = f16[4,1024,1024]{2,1,0} parameter(0)
  %add.1566 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1567, f16[4,1024,1024]{2,1,0} %param_0.1832), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=233}
  %convert.618 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1566), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.2074 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.618, f32[4,1024,1024]{2,1,0} %convert.618), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %constant_939 = f32[] constant(0)
  %reduce.319 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.2074, f32[] %constant_939), dimensions={2}, to_apply=%region_17.972, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %reduce.320.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.618, f32[] %constant_939), dimensions={2}, to_apply=%region_16.960, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  ROOT %tuple.304 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) tuple(f32[4,1024]{1,0} %reduce.319, f32[4,1024]{1,0} %reduce.320.clone.1)
}

%fused_computation.604 (param_0.1232: f16[4,4,32,1024]) -> f16[4096,128] {
  %param_0.1232 = f16[4,4,32,1024]{3,2,1,0} parameter(0)
  %transpose.78 = f16[4,1024,4,32]{1,3,2,0} transpose(f16[4,4,32,1024]{3,2,1,0} %param_0.1232), dimensions={0,3,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %copy.147 = f16[4,1024,4,32]{3,2,1,0} copy(f16[4,1024,4,32]{1,3,2,0} %transpose.78), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  ROOT %bitcast.910 = f16[4096,128]{1,0} bitcast(f16[4,1024,4,32]{3,2,1,0} %copy.147)
}

%fused_computation.605 (param_0.1825: f32[4,4,1024], param_1.2257: f16[4,4,1024,1024], param_2.2011: f16[4,4,1024]) -> f16[4,4,1024,1024] {
  %param_1.2257 = f16[4,4,1024,1024]{3,2,1,0} parameter(1)
  %param_2.2011 = f16[4,4,1024]{2,1,0} parameter(2)
  %broadcast.1939 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_2.2011), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.137 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_1.2257, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1939), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.49 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.137), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %param_0.1825 = f32[4,4,1024]{2,1,0} parameter(0)
  %convert.621 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %param_0.1825), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.231 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %convert.621), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  ROOT %divide.574 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %exponential.49, f16[4,4,1024,1024]{3,2,1,0} %broadcast.231), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%region_15.935 (Arg_0.936: f32[], Arg_1.937: f32[]) -> f32[] {
  %Arg_0.936 = f32[] parameter(0)
  %Arg_1.937 = f32[] parameter(1)
  ROOT %add.938 = f32[] add(f32[] %Arg_0.936, f32[] %Arg_1.937), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.606 (param_0.1823: f16[4,4,1024,1024], param_1.2254: f16[4,4,1024]) -> f32[4,4,1024] {
  %param_0.1823 = f16[4,4,1024,1024]{3,2,1,0} parameter(0)
  %param_1.2254 = f16[4,4,1024]{2,1,0} parameter(1)
  %broadcast.1937 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_1.2254), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.135 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_0.1823, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1937), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.47 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.135), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.622 = f32[4,4,1024,1024]{3,2,1,0} convert(f16[4,4,1024,1024]{3,2,1,0} %exponential.47), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %constant_940 = f32[] constant(0)
  ROOT %reduce.321 = f32[4,4,1024]{2,1,0} reduce(f32[4,4,1024,1024]{3,2,1,0} %convert.622, f32[] %constant_940), dimensions={3}, to_apply=%region_15.935, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.608 (param_0.1820: f16[4096,384]) -> (f16[4,4,32,1024], f16[4,4,32,1024]) {
  %param_0.1820 = f16[4096,384]{1,0} parameter(0)
  %bitcast.1104 = f16[4,1024,384]{2,1,0} bitcast(f16[4096,384]{1,0} %param_0.1820), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %bitcast.1103 = f16[4,1024,128,3]{3,2,1,0} bitcast(f16[4,1024,384]{2,1,0} %bitcast.1104), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %copy.173 = f16[4,1024,128,3]{1,2,0,3} copy(f16[4,1024,128,3]{3,2,1,0} %bitcast.1103), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %slice.57 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.173), slice={[0:4], [0:1024], [0:128], [2:3]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/slice[start_indices=(0, 0, 0, 2) limit_indices=(8, 1024, 1024, 3) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.79 = f16[1,4,128,1024]{3,2,1,0} transpose(f16[4,1024,128,1]{1,2,0,3} %slice.57), dimensions={3,0,2,1}
  %bitcast.912 = f16[4,4,32,1024]{3,2,1,0} bitcast(f16[1,4,128,1024]{3,2,1,0} %transpose.79)
  %slice.59.clone.1 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.173), slice={[0:4], [0:1024], [0:128], [1:2]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/slice[start_indices=(0, 0, 0, 1) limit_indices=(8, 1024, 1024, 2) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.80 = f16[1,4,128,1024]{3,2,1,0} transpose(f16[4,1024,128,1]{1,2,0,3} %slice.59.clone.1), dimensions={3,0,2,1}
  %bitcast.915.clone.1 = f16[4,4,32,1024]{3,2,1,0} bitcast(f16[1,4,128,1024]{3,2,1,0} %transpose.80)
  ROOT %tuple.306 = (f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) tuple(f16[4,4,32,1024]{3,2,1,0} %bitcast.912, f16[4,4,32,1024]{3,2,1,0} %bitcast.915.clone.1)
}

%fused_computation.609 (param_0.1816: f16[4096,384]) -> f16[4,4,1024,32] {
  %param_0.1816 = f16[4096,384]{1,0} parameter(0)
  %bitcast.1100 = f16[4,1024,384]{2,1,0} bitcast(f16[4096,384]{1,0} %param_0.1816), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %bitcast.1099 = f16[4,1024,128,3]{3,2,1,0} bitcast(f16[4,1024,384]{2,1,0} %bitcast.1100), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %copy.171 = f16[4,1024,128,3]{1,2,0,3} copy(f16[4,1024,128,3]{3,2,1,0} %bitcast.1099), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %slice.58 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.171), slice={[0:4], [0:1024], [0:128], [0:1]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/slice[start_indices=(0, 0, 0, 0) limit_indices=(8, 1024, 1024, 1) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %constant_941 = f16[] constant(0.17676)
  %broadcast.1700 = f16[4,1024,128,1]{1,2,0,3} broadcast(f16[] %constant_941), dimensions={}
  %multiply.2075 = f16[4,1024,128,1]{1,2,0,3} multiply(f16[4,1024,128,1]{1,2,0,3} %slice.58, f16[4,1024,128,1]{1,2,0,3} %broadcast.1700), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %bitcast.914 = f16[4,1024,4,32]{1,3,2,0} bitcast(f16[4,1024,128,1]{1,2,0,3} %multiply.2075), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %copy.148 = f16[4,1024,4,32]{3,1,2,0} copy(f16[4,1024,4,32]{1,3,2,0} %bitcast.914), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  ROOT %transpose.81 = f16[4,4,1024,32]{3,2,1,0} transpose(f16[4,1024,4,32]{3,1,2,0} %copy.148), dimensions={0,2,1,3}
}

%fused_computation.612 (param_0.1252: f32[384]) -> f16[4096,384] {
  %param_0.1252 = f32[384]{0} parameter(0)
  %convert.623 = f16[384]{0} convert(f32[384]{0} %param_0.1252), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.233 = f16[4,1024,384]{2,1,0} broadcast(f16[384]{0} %convert.623), dimensions={2}
  ROOT %bitcast.918 = f16[4096,384]{1,0} bitcast(f16[4,1024,384]{2,1,0} %broadcast.233)
}

%fused_computation.613 (param_0.1255: f32[1024], param_1.1541: f32[1024], param_2.1858: f32[4,1024], param_3.1496: f32[4,1024], param_4.1023: f16[4,1024,1024], param_5.1013: f32[1024], param_6.856: f16[4096,1024]) -> f16[4,1024,1024] {
  %param_6.856 = f16[4096,1024]{1,0} parameter(6)
  %bitcast.1090 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_6.856), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_5.1013 = f32[1024]{0} parameter(5)
  %convert.718 = f16[1024]{0} convert(f32[1024]{0} %param_5.1013), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1933 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.718), dimensions={2}
  %add.1550 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.1090, f16[4,1024,1024]{2,1,0} %broadcast.1933), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_4.1023 = f16[4,1024,1024]{2,1,0} parameter(4)
  %add.1549 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1550, f16[4,1024,1024]{2,1,0} %param_4.1023), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=310}
  %convert.625 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1549), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_3.1496 = f32[4,1024]{1,0} parameter(3)
  %constant_942 = f32[] constant(0.0009765625)
  %broadcast.1701 = f32[4,1024]{1,0} broadcast(f32[] %constant_942), dimensions={}
  %multiply.2080 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_3.1496, f32[4,1024]{1,0} %broadcast.1701), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.237 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.2080), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.88 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.625, f32[4,1024,1024]{2,1,0} %broadcast.237), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.1858 = f32[4,1024]{1,0} parameter(2)
  %multiply.2079 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_2.1858, f32[4,1024]{1,0} %broadcast.1701), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.2078 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %multiply.2080, f32[4,1024]{1,0} %multiply.2080), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.87 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %multiply.2079, f32[4,1024]{1,0} %multiply.2078), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_944 = f32[] constant(0)
  %broadcast.1703 = f32[4,1024]{1,0} broadcast(f32[] %constant_944), dimensions={}
  %maximum.33 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %subtract.87, f32[4,1024]{1,0} %broadcast.1703), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_943 = f32[] constant(1e-12)
  %broadcast.1702 = f32[4,1024]{1,0} broadcast(f32[] %constant_943), dimensions={}
  %add.1434 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.33, f32[4,1024]{1,0} %broadcast.1702), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.8 = f32[4,1024]{1,0} rsqrt(f32[4,1024]{1,0} %add.1434), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %broadcast.236 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %rsqrt.8), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_1.1541 = f32[1024]{0} parameter(1)
  %broadcast.235 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_1.1541), dimensions={2}
  %multiply.2077 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.236, f32[4,1024,1024]{2,1,0} %broadcast.235), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2076 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.88, f32[4,1024,1024]{2,1,0} %multiply.2077), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.1255 = f32[1024]{0} parameter(0)
  %broadcast.234 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_0.1255), dimensions={2}
  %add.1433 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.2076, f32[4,1024,1024]{2,1,0} %broadcast.234), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  ROOT %convert.624 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.1433), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
}

%region_13.872 (Arg_0.873: f32[], Arg_1.874: f32[]) -> f32[] {
  %Arg_0.873 = f32[] parameter(0)
  %Arg_1.874 = f32[] parameter(1)
  ROOT %add.875 = f32[] add(f32[] %Arg_0.873, f32[] %Arg_1.874), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%region_12.860 (Arg_0.861: f32[], Arg_1.862: f32[]) -> f32[] {
  %Arg_0.861 = f32[] parameter(0)
  %Arg_1.862 = f32[] parameter(1)
  ROOT %add.863 = f32[] add(f32[] %Arg_0.861, f32[] %Arg_1.862), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%fused_computation.614 (param_0.1808: f16[4,1024,1024], param_1.2251: f32[1024], param_2.2009: f16[4096,1024]) -> (f32[4,1024], f32[4,1024]) {
  %param_2.2009 = f16[4096,1024]{1,0} parameter(2)
  %bitcast.1092 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_2.2009), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_1.2251 = f32[1024]{0} parameter(1)
  %convert.720 = f16[1024]{0} convert(f32[1024]{0} %param_1.2251), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1935 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.720), dimensions={2}
  %add.1555 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.1092, f16[4,1024,1024]{2,1,0} %broadcast.1935), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_0.1808 = f16[4,1024,1024]{2,1,0} parameter(0)
  %add.1554 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1555, f16[4,1024,1024]{2,1,0} %param_0.1808), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=310}
  %convert.626 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1554), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.2081 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.626, f32[4,1024,1024]{2,1,0} %convert.626), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %constant_946 = f32[] constant(0)
  %reduce.322 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.2081, f32[] %constant_946), dimensions={2}, to_apply=%region_13.872, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %reduce.323.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.626, f32[] %constant_946), dimensions={2}, to_apply=%region_12.860, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  ROOT %tuple.307 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) tuple(f32[4,1024]{1,0} %reduce.322, f32[4,1024]{1,0} %reduce.323.clone.1)
}

%fused_computation.617 (param_0.1668: f16[4096,512]) -> f16[4096,512] {
  %param_0.1668 = f16[4096,512]{1,0} parameter(0)
  %bitcast.921 = f16[4,1024,512]{2,1,0} bitcast(f16[4096,512]{1,0} %param_0.1668), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %constant_949 = f32[] constant(-4)
  %broadcast.1706 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_949), dimensions={}
  %constant_947 = f16[] constant(0.70703)
  %broadcast.1704 = f16[4,1024,512]{2,1,0} broadcast(f16[] %constant_947), dimensions={}
  %multiply.2097 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %bitcast.921, f16[4,1024,512]{2,1,0} %broadcast.1704), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.630 = f32[4,1024,512]{2,1,0} convert(f16[4,1024,512]{2,1,0} %multiply.2097), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_948 = f32[] constant(4)
  %broadcast.1705 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_948), dimensions={}
  %clamp.24 = f32[4,1024,512]{2,1,0} clamp(f32[4,1024,512]{2,1,0} %broadcast.1706, f32[4,1024,512]{2,1,0} %convert.630, f32[4,1024,512]{2,1,0} %broadcast.1705), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2096 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.24, f32[4,1024,512]{2,1,0} %clamp.24), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_964 = f32[] constant(0)
  %broadcast.1707 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_964), dimensions={}
  %multiply.2095 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %multiply.2096, f32[4,1024,512]{2,1,0} %broadcast.1707), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_950 = f32[] constant(-2.72614237e-10)
  %broadcast.1708 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_950), dimensions={}
  %add.1450 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2095, f32[4,1024,512]{2,1,0} %broadcast.1708), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2094 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1450, f32[4,1024,512]{2,1,0} %multiply.2096), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_951 = f32[] constant(2.77068146e-08)
  %broadcast.1709 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_951), dimensions={}
  %add.1449 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2094, f32[4,1024,512]{2,1,0} %broadcast.1709), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2093 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1449, f32[4,1024,512]{2,1,0} %multiply.2096), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_952 = f32[] constant(-2.10102394e-06)
  %broadcast.1710 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_952), dimensions={}
  %add.1448 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2093, f32[4,1024,512]{2,1,0} %broadcast.1710), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2092 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1448, f32[4,1024,512]{2,1,0} %multiply.2096), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_953 = f32[] constant(-5.69250624e-05)
  %broadcast.1711 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_953), dimensions={}
  %add.1447 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2092, f32[4,1024,512]{2,1,0} %broadcast.1711), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2091 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1447, f32[4,1024,512]{2,1,0} %multiply.2096), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_954 = f32[] constant(-0.000734990637)
  %broadcast.1712 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_954), dimensions={}
  %add.1446 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2091, f32[4,1024,512]{2,1,0} %broadcast.1712), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2090 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1446, f32[4,1024,512]{2,1,0} %multiply.2096), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_955 = f32[] constant(-0.0029546)
  %broadcast.1713 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_955), dimensions={}
  %add.1445 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2090, f32[4,1024,512]{2,1,0} %broadcast.1713), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2089 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1445, f32[4,1024,512]{2,1,0} %multiply.2096), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_956 = f32[] constant(-0.0160960332)
  %broadcast.1714 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_956), dimensions={}
  %add.1444 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2089, f32[4,1024,512]{2,1,0} %broadcast.1714), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2088 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.24, f32[4,1024,512]{2,1,0} %add.1444), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_957 = f32[] constant(-1.45660715e-05)
  %broadcast.1715 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_957), dimensions={}
  %add.1443 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2095, f32[4,1024,512]{2,1,0} %broadcast.1715), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2087 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1443, f32[4,1024,512]{2,1,0} %multiply.2096), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_958 = f32[] constant(-0.000213374049)
  %broadcast.1716 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_958), dimensions={}
  %add.1442 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2087, f32[4,1024,512]{2,1,0} %broadcast.1716), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2086 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1442, f32[4,1024,512]{2,1,0} %multiply.2096), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_959 = f32[] constant(-0.00168282702)
  %broadcast.1717 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_959), dimensions={}
  %add.1441 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2086, f32[4,1024,512]{2,1,0} %broadcast.1717), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2085 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1441, f32[4,1024,512]{2,1,0} %multiply.2096), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_960 = f32[] constant(-0.00737332925)
  %broadcast.1718 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_960), dimensions={}
  %add.1440 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2085, f32[4,1024,512]{2,1,0} %broadcast.1718), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2084 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1440, f32[4,1024,512]{2,1,0} %multiply.2096), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_961 = f32[] constant(-0.0142647391)
  %broadcast.1719 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_961), dimensions={}
  %add.1438 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2084, f32[4,1024,512]{2,1,0} %broadcast.1719), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.575 = f32[4,1024,512]{2,1,0} divide(f32[4,1024,512]{2,1,0} %multiply.2088, f32[4,1024,512]{2,1,0} %add.1438), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.629 = f16[4,1024,512]{2,1,0} convert(f32[4,1024,512]{2,1,0} %divide.575), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_962 = f16[] constant(1)
  %broadcast.1720 = f16[4,1024,512]{2,1,0} broadcast(f16[] %constant_962), dimensions={}
  %add.1437 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %convert.629, f16[4,1024,512]{2,1,0} %broadcast.1720), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2083 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %bitcast.921, f16[4,1024,512]{2,1,0} %add.1437), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_963 = f16[] constant(0.5)
  %broadcast.1721 = f16[4,1024,512]{2,1,0} broadcast(f16[] %constant_963), dimensions={}
  %multiply.2082 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.2083, f16[4,1024,512]{2,1,0} %broadcast.1721), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  ROOT %bitcast.920 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %multiply.2082)
}

%fused_computation.618 (param_0.1267: f32[512]) -> f16[4096,512] {
  %param_0.1267 = f32[512]{0} parameter(0)
  %convert.631 = f16[512]{0} convert(f32[512]{0} %param_0.1267), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.239 = f16[4,1024,512]{2,1,0} broadcast(f16[512]{0} %convert.631), dimensions={2}
  ROOT %bitcast.922 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %broadcast.239)
}

%fused_computation.619 (param_0.1270: f32[1024], param_1.1556: f32[1024], param_2.1863: f32[4,1024], param_3.1501: f32[4,1024], param_4.1021: f16[4,1024,1024], param_5.1010: f32[1024], param_6.852: f16[4096,1024]) -> f16[4,1024,1024] {
  %param_6.852 = f16[4096,1024]{1,0} parameter(6)
  %bitcast.1084 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_6.852), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_5.1010 = f32[1024]{0} parameter(5)
  %convert.712 = f16[1024]{0} convert(f32[1024]{0} %param_5.1010), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1927 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.712), dimensions={2}
  %add.1537 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.1084, f16[4,1024,1024]{2,1,0} %broadcast.1927), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_4.1021 = f16[4,1024,1024]{2,1,0} parameter(4)
  %add.1536 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1537, f16[4,1024,1024]{2,1,0} %param_4.1021), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=233}
  %convert.633 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1536), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_3.1501 = f32[4,1024]{1,0} parameter(3)
  %constant_965 = f32[] constant(0.0009765625)
  %broadcast.1722 = f32[4,1024]{1,0} broadcast(f32[] %constant_965), dimensions={}
  %multiply.2102 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_3.1501, f32[4,1024]{1,0} %broadcast.1722), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.243 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.2102), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.91 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.633, f32[4,1024,1024]{2,1,0} %broadcast.243), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.1863 = f32[4,1024]{1,0} parameter(2)
  %multiply.2101 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_2.1863, f32[4,1024]{1,0} %broadcast.1722), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.2100 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %multiply.2102, f32[4,1024]{1,0} %multiply.2102), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.89 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %multiply.2101, f32[4,1024]{1,0} %multiply.2100), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_967 = f32[] constant(0)
  %broadcast.1724 = f32[4,1024]{1,0} broadcast(f32[] %constant_967), dimensions={}
  %maximum.34 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %subtract.89, f32[4,1024]{1,0} %broadcast.1724), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_966 = f32[] constant(1e-12)
  %broadcast.1723 = f32[4,1024]{1,0} broadcast(f32[] %constant_966), dimensions={}
  %add.1453 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.34, f32[4,1024]{1,0} %broadcast.1723), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.9 = f32[4,1024]{1,0} rsqrt(f32[4,1024]{1,0} %add.1453), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %broadcast.242 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %rsqrt.9), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_1.1556 = f32[1024]{0} parameter(1)
  %broadcast.241 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_1.1556), dimensions={2}
  %multiply.2099 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.242, f32[4,1024,1024]{2,1,0} %broadcast.241), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2098 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.91, f32[4,1024,1024]{2,1,0} %multiply.2099), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.1270 = f32[1024]{0} parameter(0)
  %broadcast.240 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_0.1270), dimensions={2}
  %add.1452 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.2098, f32[4,1024,1024]{2,1,0} %broadcast.240), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  ROOT %convert.632 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.1452), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
}

%region_11.780 (Arg_0.781: f32[], Arg_1.782: f32[]) -> f32[] {
  %Arg_0.781 = f32[] parameter(0)
  %Arg_1.782 = f32[] parameter(1)
  ROOT %add.783 = f32[] add(f32[] %Arg_0.781, f32[] %Arg_1.782), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%region_10.768 (Arg_0.769: f32[], Arg_1.770: f32[]) -> f32[] {
  %Arg_0.769 = f32[] parameter(0)
  %Arg_1.770 = f32[] parameter(1)
  ROOT %add.771 = f32[] add(f32[] %Arg_0.769, f32[] %Arg_1.770), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%fused_computation.620 (param_0.1801: f16[4,1024,1024], param_1.2242: f32[1024], param_2.1998: f16[4096,1024]) -> (f32[4,1024], f32[4,1024]) {
  %param_2.1998 = f16[4096,1024]{1,0} parameter(2)
  %bitcast.1086 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_2.1998), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_1.2242 = f32[1024]{0} parameter(1)
  %convert.714 = f16[1024]{0} convert(f32[1024]{0} %param_1.2242), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1929 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.714), dimensions={2}
  %add.1542 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.1086, f16[4,1024,1024]{2,1,0} %broadcast.1929), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_0.1801 = f16[4,1024,1024]{2,1,0} parameter(0)
  %add.1541 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1542, f16[4,1024,1024]{2,1,0} %param_0.1801), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=233}
  %convert.634 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1541), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.2103 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.634, f32[4,1024,1024]{2,1,0} %convert.634), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %constant_969 = f32[] constant(0)
  %reduce.324 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.2103, f32[] %constant_969), dimensions={2}, to_apply=%region_11.780, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %reduce.325.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.634, f32[] %constant_969), dimensions={2}, to_apply=%region_10.768, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  ROOT %tuple.308 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) tuple(f32[4,1024]{1,0} %reduce.324, f32[4,1024]{1,0} %reduce.325.clone.1)
}

%fused_computation.623 (param_0.1279: f16[4,4,32,1024]) -> f16[4096,128] {
  %param_0.1279 = f16[4,4,32,1024]{3,2,1,0} parameter(0)
  %transpose.82 = f16[4,1024,4,32]{1,3,2,0} transpose(f16[4,4,32,1024]{3,2,1,0} %param_0.1279), dimensions={0,3,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %copy.150 = f16[4,1024,4,32]{3,2,1,0} copy(f16[4,1024,4,32]{1,3,2,0} %transpose.82), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  ROOT %bitcast.924 = f16[4096,128]{1,0} bitcast(f16[4,1024,4,32]{3,2,1,0} %copy.150)
}

%fused_computation.624 (param_0.1794: f32[4,4,1024], param_1.2233: f16[4,4,1024,1024], param_2.1987: f16[4,4,1024]) -> f16[4,4,1024,1024] {
  %param_1.2233 = f16[4,4,1024,1024]{3,2,1,0} parameter(1)
  %param_2.1987 = f16[4,4,1024]{2,1,0} parameter(2)
  %broadcast.1923 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_2.1987), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.133 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_1.2233, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1923), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.45 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.133), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %param_0.1794 = f32[4,4,1024]{2,1,0} parameter(0)
  %convert.637 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %param_0.1794), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.245 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %convert.637), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  ROOT %divide.576 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %exponential.45, f16[4,4,1024,1024]{3,2,1,0} %broadcast.245), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%region_9.743 (Arg_0.744: f32[], Arg_1.745: f32[]) -> f32[] {
  %Arg_0.744 = f32[] parameter(0)
  %Arg_1.745 = f32[] parameter(1)
  ROOT %add.746 = f32[] add(f32[] %Arg_0.744, f32[] %Arg_1.745), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.625 (param_0.1792: f16[4,4,1024,1024], param_1.2230: f16[4,4,1024]) -> f32[4,4,1024] {
  %param_0.1792 = f16[4,4,1024,1024]{3,2,1,0} parameter(0)
  %param_1.2230 = f16[4,4,1024]{2,1,0} parameter(1)
  %broadcast.1921 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_1.2230), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.131 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_0.1792, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1921), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.43 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.131), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.638 = f32[4,4,1024,1024]{3,2,1,0} convert(f16[4,4,1024,1024]{3,2,1,0} %exponential.43), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %constant_970 = f32[] constant(0)
  ROOT %reduce.326 = f32[4,4,1024]{2,1,0} reduce(f32[4,4,1024,1024]{3,2,1,0} %convert.638, f32[] %constant_970), dimensions={3}, to_apply=%region_9.743, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.627 (param_0.1789: f16[4096,384]) -> (f16[4,4,32,1024], f16[4,4,32,1024]) {
  %param_0.1789 = f16[4096,384]{1,0} parameter(0)
  %bitcast.1080 = f16[4,1024,384]{2,1,0} bitcast(f16[4096,384]{1,0} %param_0.1789), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %bitcast.1079 = f16[4,1024,128,3]{3,2,1,0} bitcast(f16[4,1024,384]{2,1,0} %bitcast.1080), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %copy.167 = f16[4,1024,128,3]{1,2,0,3} copy(f16[4,1024,128,3]{3,2,1,0} %bitcast.1079), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %slice.60 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.167), slice={[0:4], [0:1024], [0:128], [2:3]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/slice[start_indices=(0, 0, 0, 2) limit_indices=(8, 1024, 1024, 3) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.83 = f16[1,4,128,1024]{3,2,1,0} transpose(f16[4,1024,128,1]{1,2,0,3} %slice.60), dimensions={3,0,2,1}
  %bitcast.926 = f16[4,4,32,1024]{3,2,1,0} bitcast(f16[1,4,128,1024]{3,2,1,0} %transpose.83)
  %slice.62.clone.1 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.167), slice={[0:4], [0:1024], [0:128], [1:2]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/slice[start_indices=(0, 0, 0, 1) limit_indices=(8, 1024, 1024, 2) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.84 = f16[1,4,128,1024]{3,2,1,0} transpose(f16[4,1024,128,1]{1,2,0,3} %slice.62.clone.1), dimensions={3,0,2,1}
  %bitcast.929.clone.1 = f16[4,4,32,1024]{3,2,1,0} bitcast(f16[1,4,128,1024]{3,2,1,0} %transpose.84)
  ROOT %tuple.310 = (f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) tuple(f16[4,4,32,1024]{3,2,1,0} %bitcast.926, f16[4,4,32,1024]{3,2,1,0} %bitcast.929.clone.1)
}

%fused_computation.628 (param_0.1785: f16[4096,384]) -> f16[4,4,1024,32] {
  %param_0.1785 = f16[4096,384]{1,0} parameter(0)
  %bitcast.1076 = f16[4,1024,384]{2,1,0} bitcast(f16[4096,384]{1,0} %param_0.1785), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %bitcast.1075 = f16[4,1024,128,3]{3,2,1,0} bitcast(f16[4,1024,384]{2,1,0} %bitcast.1076), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %copy.165 = f16[4,1024,128,3]{1,2,0,3} copy(f16[4,1024,128,3]{3,2,1,0} %bitcast.1075), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %slice.61 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.165), slice={[0:4], [0:1024], [0:128], [0:1]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/slice[start_indices=(0, 0, 0, 0) limit_indices=(8, 1024, 1024, 1) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %constant_971 = f16[] constant(0.17676)
  %broadcast.1725 = f16[4,1024,128,1]{1,2,0,3} broadcast(f16[] %constant_971), dimensions={}
  %multiply.2104 = f16[4,1024,128,1]{1,2,0,3} multiply(f16[4,1024,128,1]{1,2,0,3} %slice.61, f16[4,1024,128,1]{1,2,0,3} %broadcast.1725), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %bitcast.928 = f16[4,1024,4,32]{1,3,2,0} bitcast(f16[4,1024,128,1]{1,2,0,3} %multiply.2104), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %copy.151 = f16[4,1024,4,32]{3,1,2,0} copy(f16[4,1024,4,32]{1,3,2,0} %bitcast.928), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  ROOT %transpose.85 = f16[4,4,1024,32]{3,2,1,0} transpose(f16[4,1024,4,32]{3,1,2,0} %copy.151), dimensions={0,2,1,3}
}

%fused_computation.631 (param_0.1299: f32[384]) -> f16[4096,384] {
  %param_0.1299 = f32[384]{0} parameter(0)
  %convert.639 = f16[384]{0} convert(f32[384]{0} %param_0.1299), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.247 = f16[4,1024,384]{2,1,0} broadcast(f16[384]{0} %convert.639), dimensions={2}
  ROOT %bitcast.932 = f16[4096,384]{1,0} bitcast(f16[4,1024,384]{2,1,0} %broadcast.247)
}

%fused_computation.632 (param_0.1302: f32[1024], param_1.1577: f32[1024], param_2.1865: f32[4,1024], param_3.1503: f32[4,1024], param_4.1019: f16[4,1024,1024], param_5.1007: f32[1024], param_6.848: f16[4096,1024]) -> f16[4,1024,1024] {
  %param_6.848 = f16[4096,1024]{1,0} parameter(6)
  %bitcast.1066 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_6.848), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_5.1007 = f32[1024]{0} parameter(5)
  %convert.706 = f16[1024]{0} convert(f32[1024]{0} %param_5.1007), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1917 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.706), dimensions={2}
  %add.1525 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.1066, f16[4,1024,1024]{2,1,0} %broadcast.1917), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_4.1019 = f16[4,1024,1024]{2,1,0} parameter(4)
  %add.1524 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1525, f16[4,1024,1024]{2,1,0} %param_4.1019), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=310}
  %convert.641 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1524), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_3.1503 = f32[4,1024]{1,0} parameter(3)
  %constant_972 = f32[] constant(0.0009765625)
  %broadcast.1726 = f32[4,1024]{1,0} broadcast(f32[] %constant_972), dimensions={}
  %multiply.2109 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_3.1503, f32[4,1024]{1,0} %broadcast.1726), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.251 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.2109), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.96 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.641, f32[4,1024,1024]{2,1,0} %broadcast.251), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.1865 = f32[4,1024]{1,0} parameter(2)
  %multiply.2108 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_2.1865, f32[4,1024]{1,0} %broadcast.1726), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.2107 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %multiply.2109, f32[4,1024]{1,0} %multiply.2109), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.93 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %multiply.2108, f32[4,1024]{1,0} %multiply.2107), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_974 = f32[] constant(0)
  %broadcast.1728 = f32[4,1024]{1,0} broadcast(f32[] %constant_974), dimensions={}
  %maximum.35 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %subtract.93, f32[4,1024]{1,0} %broadcast.1728), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_973 = f32[] constant(1e-12)
  %broadcast.1727 = f32[4,1024]{1,0} broadcast(f32[] %constant_973), dimensions={}
  %add.1457 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.35, f32[4,1024]{1,0} %broadcast.1727), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.10 = f32[4,1024]{1,0} rsqrt(f32[4,1024]{1,0} %add.1457), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %broadcast.250 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %rsqrt.10), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_1.1577 = f32[1024]{0} parameter(1)
  %broadcast.249 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_1.1577), dimensions={2}
  %multiply.2106 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.250, f32[4,1024,1024]{2,1,0} %broadcast.249), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2105 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.96, f32[4,1024,1024]{2,1,0} %multiply.2106), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.1302 = f32[1024]{0} parameter(0)
  %broadcast.248 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_0.1302), dimensions={2}
  %add.1456 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.2105, f32[4,1024,1024]{2,1,0} %broadcast.248), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  ROOT %convert.640 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.1456), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
}

%region_7.680 (Arg_0.681: f32[], Arg_1.682: f32[]) -> f32[] {
  %Arg_0.681 = f32[] parameter(0)
  %Arg_1.682 = f32[] parameter(1)
  ROOT %add.683 = f32[] add(f32[] %Arg_0.681, f32[] %Arg_1.682), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%region_6.668 (Arg_0.669: f32[], Arg_1.670: f32[]) -> f32[] {
  %Arg_0.669 = f32[] parameter(0)
  %Arg_1.670 = f32[] parameter(1)
  ROOT %add.671 = f32[] add(f32[] %Arg_0.669, f32[] %Arg_1.670), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%fused_computation.633 (param_0.1777: f16[4,1024,1024], param_1.2227: f32[1024], param_2.1985: f16[4096,1024]) -> (f32[4,1024], f32[4,1024]) {
  %param_2.1985 = f16[4096,1024]{1,0} parameter(2)
  %bitcast.1068 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_2.1985), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_1.2227 = f32[1024]{0} parameter(1)
  %convert.708 = f16[1024]{0} convert(f32[1024]{0} %param_1.2227), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1919 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.708), dimensions={2}
  %add.1529 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.1068, f16[4,1024,1024]{2,1,0} %broadcast.1919), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_0.1777 = f16[4,1024,1024]{2,1,0} parameter(0)
  %add.1528 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1529, f16[4,1024,1024]{2,1,0} %param_0.1777), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=310}
  %convert.642 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1528), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.2110 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.642, f32[4,1024,1024]{2,1,0} %convert.642), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %constant_976 = f32[] constant(0)
  %reduce.327 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.2110, f32[] %constant_976), dimensions={2}, to_apply=%region_7.680, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %reduce.328.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.642, f32[] %constant_976), dimensions={2}, to_apply=%region_6.668, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  ROOT %tuple.311 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) tuple(f32[4,1024]{1,0} %reduce.327, f32[4,1024]{1,0} %reduce.328.clone.1)
}

%fused_computation.636 (param_0.1675: f16[4096,512]) -> f16[4096,512] {
  %param_0.1675 = f16[4096,512]{1,0} parameter(0)
  %bitcast.935 = f16[4,1024,512]{2,1,0} bitcast(f16[4096,512]{1,0} %param_0.1675), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %constant_979 = f32[] constant(-4)
  %broadcast.1731 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_979), dimensions={}
  %constant_977 = f16[] constant(0.70703)
  %broadcast.1729 = f16[4,1024,512]{2,1,0} broadcast(f16[] %constant_977), dimensions={}
  %multiply.2126 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %bitcast.935, f16[4,1024,512]{2,1,0} %broadcast.1729), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.646 = f32[4,1024,512]{2,1,0} convert(f16[4,1024,512]{2,1,0} %multiply.2126), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_978 = f32[] constant(4)
  %broadcast.1730 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_978), dimensions={}
  %clamp.25 = f32[4,1024,512]{2,1,0} clamp(f32[4,1024,512]{2,1,0} %broadcast.1731, f32[4,1024,512]{2,1,0} %convert.646, f32[4,1024,512]{2,1,0} %broadcast.1730), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2125 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.25, f32[4,1024,512]{2,1,0} %clamp.25), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_994 = f32[] constant(0)
  %broadcast.1732 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_994), dimensions={}
  %multiply.2124 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %multiply.2125, f32[4,1024,512]{2,1,0} %broadcast.1732), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_980 = f32[] constant(-2.72614237e-10)
  %broadcast.1733 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_980), dimensions={}
  %add.1472 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2124, f32[4,1024,512]{2,1,0} %broadcast.1733), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2123 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1472, f32[4,1024,512]{2,1,0} %multiply.2125), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_981 = f32[] constant(2.77068146e-08)
  %broadcast.1734 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_981), dimensions={}
  %add.1471 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2123, f32[4,1024,512]{2,1,0} %broadcast.1734), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2122 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1471, f32[4,1024,512]{2,1,0} %multiply.2125), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_982 = f32[] constant(-2.10102394e-06)
  %broadcast.1735 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_982), dimensions={}
  %add.1470 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2122, f32[4,1024,512]{2,1,0} %broadcast.1735), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2121 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1470, f32[4,1024,512]{2,1,0} %multiply.2125), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_983 = f32[] constant(-5.69250624e-05)
  %broadcast.1736 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_983), dimensions={}
  %add.1469 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2121, f32[4,1024,512]{2,1,0} %broadcast.1736), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2120 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1469, f32[4,1024,512]{2,1,0} %multiply.2125), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_984 = f32[] constant(-0.000734990637)
  %broadcast.1737 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_984), dimensions={}
  %add.1468 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2120, f32[4,1024,512]{2,1,0} %broadcast.1737), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2119 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1468, f32[4,1024,512]{2,1,0} %multiply.2125), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_985 = f32[] constant(-0.0029546)
  %broadcast.1738 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_985), dimensions={}
  %add.1467 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2119, f32[4,1024,512]{2,1,0} %broadcast.1738), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2118 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1467, f32[4,1024,512]{2,1,0} %multiply.2125), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_986 = f32[] constant(-0.0160960332)
  %broadcast.1739 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_986), dimensions={}
  %add.1466 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2118, f32[4,1024,512]{2,1,0} %broadcast.1739), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2117 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.25, f32[4,1024,512]{2,1,0} %add.1466), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_987 = f32[] constant(-1.45660715e-05)
  %broadcast.1740 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_987), dimensions={}
  %add.1465 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2124, f32[4,1024,512]{2,1,0} %broadcast.1740), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2116 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1465, f32[4,1024,512]{2,1,0} %multiply.2125), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_988 = f32[] constant(-0.000213374049)
  %broadcast.1741 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_988), dimensions={}
  %add.1464 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2116, f32[4,1024,512]{2,1,0} %broadcast.1741), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2115 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1464, f32[4,1024,512]{2,1,0} %multiply.2125), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_989 = f32[] constant(-0.00168282702)
  %broadcast.1742 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_989), dimensions={}
  %add.1463 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2115, f32[4,1024,512]{2,1,0} %broadcast.1742), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2114 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1463, f32[4,1024,512]{2,1,0} %multiply.2125), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_990 = f32[] constant(-0.00737332925)
  %broadcast.1743 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_990), dimensions={}
  %add.1462 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2114, f32[4,1024,512]{2,1,0} %broadcast.1743), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2113 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.1462, f32[4,1024,512]{2,1,0} %multiply.2125), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_991 = f32[] constant(-0.0142647391)
  %broadcast.1744 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant_991), dimensions={}
  %add.1461 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.2113, f32[4,1024,512]{2,1,0} %broadcast.1744), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.577 = f32[4,1024,512]{2,1,0} divide(f32[4,1024,512]{2,1,0} %multiply.2117, f32[4,1024,512]{2,1,0} %add.1461), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.645 = f16[4,1024,512]{2,1,0} convert(f32[4,1024,512]{2,1,0} %divide.577), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_992 = f16[] constant(1)
  %broadcast.1745 = f16[4,1024,512]{2,1,0} broadcast(f16[] %constant_992), dimensions={}
  %add.1460 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %convert.645, f16[4,1024,512]{2,1,0} %broadcast.1745), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.2112 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %bitcast.935, f16[4,1024,512]{2,1,0} %add.1460), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant_993 = f16[] constant(0.5)
  %broadcast.1746 = f16[4,1024,512]{2,1,0} broadcast(f16[] %constant_993), dimensions={}
  %multiply.2111 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.2112, f16[4,1024,512]{2,1,0} %broadcast.1746), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  ROOT %bitcast.934 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %multiply.2111)
}

%fused_computation.637 (param_0.1314: f32[512]) -> f16[4096,512] {
  %param_0.1314 = f32[512]{0} parameter(0)
  %convert.647 = f16[512]{0} convert(f32[512]{0} %param_0.1314), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.253 = f16[4,1024,512]{2,1,0} broadcast(f16[512]{0} %convert.647), dimensions={2}
  ROOT %bitcast.936 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %broadcast.253)
}

%fused_computation.638 (param_0.1317: f32[1024], param_1.1592: f32[1024], param_2.1870: f32[4,1024], param_3.1508: f32[4,1024], param_4.1017: f16[4,1024,1024], param_5.1004: f32[1024], param_6.844: f16[4096,1024]) -> f16[4,1024,1024] {
  %param_6.844 = f16[4096,1024]{1,0} parameter(6)
  %bitcast.1060 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_6.844), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_5.1004 = f32[1024]{0} parameter(5)
  %convert.700 = f16[1024]{0} convert(f32[1024]{0} %param_5.1004), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1911 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.700), dimensions={2}
  %add.1512 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.1060, f16[4,1024,1024]{2,1,0} %broadcast.1911), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_4.1017 = f16[4,1024,1024]{2,1,0} parameter(4)
  %add.1511 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1512, f16[4,1024,1024]{2,1,0} %param_4.1017), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=233}
  %convert.649 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1511), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_3.1508 = f32[4,1024]{1,0} parameter(3)
  %constant_995 = f32[] constant(0.0009765625)
  %broadcast.1747 = f32[4,1024]{1,0} broadcast(f32[] %constant_995), dimensions={}
  %multiply.2131 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_3.1508, f32[4,1024]{1,0} %broadcast.1747), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.257 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.2131), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.98 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.649, f32[4,1024,1024]{2,1,0} %broadcast.257), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.1870 = f32[4,1024]{1,0} parameter(2)
  %multiply.2130 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_2.1870, f32[4,1024]{1,0} %broadcast.1747), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.2129 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %multiply.2131, f32[4,1024]{1,0} %multiply.2131), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.97 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %multiply.2130, f32[4,1024]{1,0} %multiply.2129), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_997 = f32[] constant(0)
  %broadcast.1749 = f32[4,1024]{1,0} broadcast(f32[] %constant_997), dimensions={}
  %maximum.36 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %subtract.97, f32[4,1024]{1,0} %broadcast.1749), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_996 = f32[] constant(1e-12)
  %broadcast.1748 = f32[4,1024]{1,0} broadcast(f32[] %constant_996), dimensions={}
  %add.1474 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.36, f32[4,1024]{1,0} %broadcast.1748), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.11 = f32[4,1024]{1,0} rsqrt(f32[4,1024]{1,0} %add.1474), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %broadcast.256 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %rsqrt.11), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_1.1592 = f32[1024]{0} parameter(1)
  %broadcast.255 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_1.1592), dimensions={2}
  %multiply.2128 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.256, f32[4,1024,1024]{2,1,0} %broadcast.255), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2127 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.98, f32[4,1024,1024]{2,1,0} %multiply.2128), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.1317 = f32[1024]{0} parameter(0)
  %broadcast.254 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_0.1317), dimensions={2}
  %add.1473 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.2127, f32[4,1024,1024]{2,1,0} %broadcast.254), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  ROOT %convert.648 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.1473), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
}

%region_5.588 (Arg_0.589: f32[], Arg_1.590: f32[]) -> f32[] {
  %Arg_0.589 = f32[] parameter(0)
  %Arg_1.590 = f32[] parameter(1)
  ROOT %add.591 = f32[] add(f32[] %Arg_0.589, f32[] %Arg_1.590), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%region_4.576 (Arg_0.577: f32[], Arg_1.578: f32[]) -> f32[] {
  %Arg_0.577 = f32[] parameter(0)
  %Arg_1.578 = f32[] parameter(1)
  ROOT %add.579 = f32[] add(f32[] %Arg_0.577, f32[] %Arg_1.578), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%fused_computation.639 (param_0.1770: f16[4,1024,1024], param_1.2218: f32[1024], param_2.1974: f16[4096,1024]) -> (f32[4,1024], f32[4,1024]) {
  %param_2.1974 = f16[4096,1024]{1,0} parameter(2)
  %bitcast.1062 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %param_2.1974), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param_1.2218 = f32[1024]{0} parameter(1)
  %convert.702 = f16[1024]{0} convert(f32[1024]{0} %param_1.2218), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1913 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.702), dimensions={2}
  %add.1517 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %bitcast.1062, f16[4,1024,1024]{2,1,0} %broadcast.1913), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param_0.1770 = f16[4,1024,1024]{2,1,0} parameter(0)
  %add.1516 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.1517, f16[4,1024,1024]{2,1,0} %param_0.1770), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=233}
  %convert.650 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.1516), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.2132 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.650, f32[4,1024,1024]{2,1,0} %convert.650), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %constant_999 = f32[] constant(0)
  %reduce.329 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.2132, f32[] %constant_999), dimensions={2}, to_apply=%region_5.588, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %reduce.330.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.650, f32[] %constant_999), dimensions={2}, to_apply=%region_4.576, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  ROOT %tuple.312 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) tuple(f32[4,1024]{1,0} %reduce.329, f32[4,1024]{1,0} %reduce.330.clone.1)
}

%fused_computation.642 (param_0.1326: f16[4,4,32,1024]) -> f16[4096,128] {
  %param_0.1326 = f16[4,4,32,1024]{3,2,1,0} parameter(0)
  %transpose.86 = f16[4,1024,4,32]{1,3,2,0} transpose(f16[4,4,32,1024]{3,2,1,0} %param_0.1326), dimensions={0,3,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %copy.153 = f16[4,1024,4,32]{3,2,1,0} copy(f16[4,1024,4,32]{1,3,2,0} %transpose.86), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  ROOT %bitcast.938 = f16[4096,128]{1,0} bitcast(f16[4,1024,4,32]{3,2,1,0} %copy.153)
}

%fused_computation.643 (param_0.1763: f32[4,4,1024], param_1.2209: f16[4,4,1024,1024], param_2.1963: f16[4,4,1024]) -> f16[4,4,1024,1024] {
  %param_1.2209 = f16[4,4,1024,1024]{3,2,1,0} parameter(1)
  %param_2.1963 = f16[4,4,1024]{2,1,0} parameter(2)
  %broadcast.1907 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_2.1963), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.129 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_1.2209, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1907), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.41 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.129), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %param_0.1763 = f32[4,4,1024]{2,1,0} parameter(0)
  %convert.653 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %param_0.1763), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.259 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %convert.653), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  ROOT %divide.578 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %exponential.41, f16[4,4,1024,1024]{3,2,1,0} %broadcast.259), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%region_3.551 (Arg_0.552: f32[], Arg_1.553: f32[]) -> f32[] {
  %Arg_0.552 = f32[] parameter(0)
  %Arg_1.553 = f32[] parameter(1)
  ROOT %add.554 = f32[] add(f32[] %Arg_0.552, f32[] %Arg_1.553), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.644 (param_0.1761: f16[4,4,1024,1024], param_1.2206: f16[4,4,1024]) -> f32[4,4,1024] {
  %param_0.1761 = f16[4,4,1024,1024]{3,2,1,0} parameter(0)
  %param_1.2206 = f16[4,4,1024]{2,1,0} parameter(1)
  %broadcast.1905 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_1.2206), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.127 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_0.1761, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1905), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.39 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.127), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.654 = f32[4,4,1024,1024]{3,2,1,0} convert(f16[4,4,1024,1024]{3,2,1,0} %exponential.39), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %constant_1000 = f32[] constant(0)
  ROOT %reduce.331 = f32[4,4,1024]{2,1,0} reduce(f32[4,4,1024,1024]{3,2,1,0} %convert.654, f32[] %constant_1000), dimensions={3}, to_apply=%region_3.551, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.647 (param_0.1746: f16[4096,384]) -> (f16[4,4,32,1024], f16[4,4,32,1024]) {
  %param_0.1746 = f16[4096,384]{1,0} parameter(0)
  %bitcast.1056 = f16[4,1024,384]{2,1,0} bitcast(f16[4096,384]{1,0} %param_0.1746), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %bitcast.1055 = f16[4,1024,128,3]{3,2,1,0} bitcast(f16[4,1024,384]{2,1,0} %bitcast.1056), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %copy.161 = f16[4,1024,128,3]{1,2,0,3} copy(f16[4,1024,128,3]{3,2,1,0} %bitcast.1055), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %slice.63 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.161), slice={[0:4], [0:1024], [0:128], [2:3]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/slice[start_indices=(0, 0, 0, 2) limit_indices=(8, 1024, 1024, 3) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.87 = f16[1,4,128,1024]{3,2,1,0} transpose(f16[4,1024,128,1]{1,2,0,3} %slice.63), dimensions={3,0,2,1}
  %bitcast.940 = f16[4,4,32,1024]{3,2,1,0} bitcast(f16[1,4,128,1024]{3,2,1,0} %transpose.87)
  %slice.65.clone.1 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.161), slice={[0:4], [0:1024], [0:128], [1:2]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/slice[start_indices=(0, 0, 0, 1) limit_indices=(8, 1024, 1024, 2) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.88 = f16[1,4,128,1024]{3,2,1,0} transpose(f16[4,1024,128,1]{1,2,0,3} %slice.65.clone.1), dimensions={3,0,2,1}
  %bitcast.943.clone.1 = f16[4,4,32,1024]{3,2,1,0} bitcast(f16[1,4,128,1024]{3,2,1,0} %transpose.88)
  ROOT %tuple.314 = (f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) tuple(f16[4,4,32,1024]{3,2,1,0} %bitcast.940, f16[4,4,32,1024]{3,2,1,0} %bitcast.943.clone.1)
}

%fused_computation.648 (param_0.1742: f16[4096,384]) -> f16[4,4,1024,32] {
  %param_0.1742 = f16[4096,384]{1,0} parameter(0)
  %bitcast.1052 = f16[4,1024,384]{2,1,0} bitcast(f16[4096,384]{1,0} %param_0.1742), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %bitcast.1051 = f16[4,1024,128,3]{3,2,1,0} bitcast(f16[4,1024,384]{2,1,0} %bitcast.1052), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %copy.159 = f16[4,1024,128,3]{1,2,0,3} copy(f16[4,1024,128,3]{3,2,1,0} %bitcast.1051), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %slice.64 = f16[4,1024,128,1]{1,2,0,3} slice(f16[4,1024,128,3]{1,2,0,3} %copy.159), slice={[0:4], [0:1024], [0:128], [0:1]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/slice[start_indices=(0, 0, 0, 0) limit_indices=(8, 1024, 1024, 1) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %constant_1004 = f16[] constant(0.17676)
  %broadcast.1751 = f16[4,1024,128,1]{1,2,0,3} broadcast(f16[] %constant_1004), dimensions={}
  %multiply.2133 = f16[4,1024,128,1]{1,2,0,3} multiply(f16[4,1024,128,1]{1,2,0,3} %slice.64, f16[4,1024,128,1]{1,2,0,3} %broadcast.1751), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %bitcast.942 = f16[4,1024,4,32]{1,3,2,0} bitcast(f16[4,1024,128,1]{1,2,0,3} %multiply.2133), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %copy.154 = f16[4,1024,4,32]{3,1,2,0} copy(f16[4,1024,4,32]{1,3,2,0} %bitcast.942), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  ROOT %transpose.89 = f16[4,4,1024,32]{3,2,1,0} transpose(f16[4,1024,4,32]{3,1,2,0} %copy.154), dimensions={0,2,1,3}
}

%fused_computation.651 (param_0.1349: f32[384]) -> f16[4096,384] {
  %param_0.1349 = f32[384]{0} parameter(0)
  %convert.655 = f16[384]{0} convert(f32[384]{0} %param_0.1349), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.264 = f16[4,1024,384]{2,1,0} broadcast(f16[384]{0} %convert.655), dimensions={2}
  ROOT %bitcast.946 = f16[4096,384]{1,0} bitcast(f16[4,1024,384]{2,1,0} %broadcast.264)
}

%fused_computation.652 (param_0.1352: f32[1024], param_1.2168: f32[1024], param_2.1920: f32[4,1024], param_3.1546: f16[4096,1024], param_4.1012: f32[4,1024]) -> f16[4,1024,1024] {
  %param_3.1546 = f16[4096,1024]{1,0} parameter(3)
  %convert.694 = f32[4096,1024]{1,0} convert(f16[4096,1024]{1,0} %param_3.1546), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %bitcast.1026 = f32[4,1024,1024]{2,1,0} bitcast(f32[4096,1024]{1,0} %convert.694), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.1920 = f32[4,1024]{1,0} parameter(2)
  %constant_1073 = f32[] constant(0.0009765625)
  %broadcast.1811 = f32[4,1024]{1,0} broadcast(f32[] %constant_1073), dimensions={}
  %multiply.2170 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_2.1920, f32[4,1024]{1,0} %broadcast.1811), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.1810 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.2170), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.103 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %bitcast.1026, f32[4,1024,1024]{2,1,0} %broadcast.1810), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %param_4.1012 = f32[4,1024]{1,0} parameter(4)
  %multiply.2202 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %param_4.1012, f32[4,1024]{1,0} %broadcast.1811), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.2200 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %multiply.2170, f32[4,1024]{1,0} %multiply.2170), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.118 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %multiply.2202, f32[4,1024]{1,0} %multiply.2200), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_1102 = f32[] constant(0)
  %broadcast.1838 = f32[4,1024]{1,0} broadcast(f32[] %constant_1102), dimensions={}
  %maximum.68 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %subtract.118, f32[4,1024]{1,0} %broadcast.1838), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_1101 = f32[] constant(1e-12)
  %broadcast.1837 = f32[4,1024]{1,0} broadcast(f32[] %constant_1101), dimensions={}
  %add.1494 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.68, f32[4,1024]{1,0} %broadcast.1837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1036 = f32[4,1024,1]{1,0,2} bitcast(f32[4,1024]{1,0} %add.1494), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.64 = f32[4,1024,1]{1,0,2} rsqrt(f32[4,1024,1]{1,0,2} %bitcast.1036), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %bitcast.1035 = f32[4,1024]{1,0} bitcast(f32[4,1024,1]{1,0,2} %rsqrt.64), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.1836 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %bitcast.1035), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param_1.2168 = f32[1024]{0} parameter(1)
  %broadcast.1752 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_1.2168), dimensions={2}
  %multiply.2154 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.1836, f32[4,1024,1024]{2,1,0} %broadcast.1752), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.2134 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.103, f32[4,1024,1024]{2,1,0} %multiply.2154), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.1352 = f32[1024]{0} parameter(0)
  %broadcast.265 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param_0.1352), dimensions={2}
  %add.1477 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.2134, f32[4,1024,1024]{2,1,0} %broadcast.265), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  ROOT %convert.656 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.1477), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
}

%fused_computation.659 (param_0.1698: s32[4,1024], param_1.2124: s32[16], param_2.1886: u32[]) -> f16[4096,128] {
  %param_0.1698 = s32[4,1024]{1,0} parameter(0)
  %broadcast.269 = s32[4,1024,128]{2,1,0} broadcast(s32[4,1024]{1,0} %param_0.1698), dimensions={0,1}
  %iota.1 = s32[4,1024,128]{2,1,0} iota(), iota_dimension=2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/position_embeddings.embed_call_one_hot/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %param_1.2124 = s32[16]{0} parameter(1)
  %param_2.1886 = u32[] parameter(2)
  %dynamic-slice.74 = s32[1]{0} dynamic-slice(s32[16]{0} %param_1.2124, u32[] %param_2.1886), dynamic_slice_sizes={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %bitcast.1008 = s32[] bitcast(s32[1]{0} %dynamic-slice.74), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %constant_9 = s32[] constant(128), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/position_embeddings.embed_call_one_hot/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %multiply.2138 = s32[] multiply(s32[] %bitcast.1008, s32[] %constant_9), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/position_embeddings.embed_call_one_hot/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %broadcast.268 = s32[4,1024,128]{2,1,0} broadcast(s32[] %multiply.2138), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/position_embeddings.embed_call_one_hot/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %add.1479 = s32[4,1024,128]{2,1,0} add(s32[4,1024,128]{2,1,0} %iota.1, s32[4,1024,128]{2,1,0} %broadcast.268), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/position_embeddings.embed_call_one_hot/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %compare.76 = pred[4,1024,128]{2,1,0} compare(s32[4,1024,128]{2,1,0} %broadcast.269, s32[4,1024,128]{2,1,0} %add.1479), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/position_embeddings.embed_call_one_hot/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %convert.658 = f16[4,1024,128]{2,1,0} convert(pred[4,1024,128]{2,1,0} %compare.76), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/position_embeddings.embed_call_one_hot/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  ROOT %bitcast.950 = f16[4096,128]{1,0} bitcast(f16[4,1024,128]{2,1,0} %convert.658)
}

%fused_computation.660 (param_0.1701: s32[4,1024], param_1.2129: s32[16], param_2.1894: u32[]) -> f16[4096,6400] {
  %param_0.1701 = s32[4,1024]{1,0} parameter(0)
  %broadcast.270 = s32[4,1024,6400]{2,1,0} broadcast(s32[4,1024]{1,0} %param_0.1701), dimensions={0,1}
  %iota.6 = s32[4,1024,6400]{2,1,0} iota(), iota_dimension=2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %param_1.2129 = s32[16]{0} parameter(1)
  %param_2.1894 = u32[] parameter(2)
  %dynamic-slice.78 = s32[1]{0} dynamic-slice(s32[16]{0} %param_1.2129, u32[] %param_2.1894), dynamic_slice_sizes={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %bitcast.1012 = s32[] bitcast(s32[1]{0} %dynamic-slice.78), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %constant_1040 = s32[] constant(6400), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %multiply.2160 = s32[] multiply(s32[] %bitcast.1012, s32[] %constant_1040), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %broadcast.1775 = s32[4,1024,6400]{2,1,0} broadcast(s32[] %multiply.2160), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %add.1484 = s32[4,1024,6400]{2,1,0} add(s32[4,1024,6400]{2,1,0} %iota.6, s32[4,1024,6400]{2,1,0} %broadcast.1775), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %compare.77 = pred[4,1024,6400]{2,1,0} compare(s32[4,1024,6400]{2,1,0} %broadcast.270, s32[4,1024,6400]{2,1,0} %add.1484), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/word_embeddings.embed_call_one_hot/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %convert.659 = f16[4,1024,6400]{2,1,0} convert(pred[4,1024,6400]{2,1,0} %compare.77), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/word_embeddings.embed_call_one_hot/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  ROOT %bitcast.951 = f16[4096,6400]{1,0} bitcast(f16[4,1024,6400]{2,1,0} %convert.659)
}

%region_41.1720 (Arg_0.1721: f16[], Arg_1.1722: f16[]) -> f16[] {
  %Arg_0.1721 = f16[] parameter(0)
  %Arg_1.1722 = f16[] parameter(1)
  ROOT %add.1723 = f16[] add(f16[] %Arg_0.1721, f16[] %Arg_1.1722), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(2,)]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
}

%fused_computation.661 (param_0.1705: f32[], param_1.2136: s32[4,1024], param_2.1905: s32[16], param_3.1534: u32[]) -> f16[4,1024] {
  %param_1.2136 = s32[4,1024]{1,0} parameter(1)
  %broadcast.1791 = s32[4,1024,6400]{2,1,0} broadcast(s32[4,1024]{1,0} %param_1.2136), dimensions={0,1}
  %iota.8 = s32[4,1024,6400]{2,1,0} iota(), iota_dimension=2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %param_2.1905 = s32[16]{0} parameter(2)
  %param_3.1534 = u32[] parameter(3)
  %dynamic-slice.80 = s32[1]{0} dynamic-slice(s32[16]{0} %param_2.1905, u32[] %param_3.1534), dynamic_slice_sizes={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %bitcast.1014 = s32[] bitcast(s32[1]{0} %dynamic-slice.80), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %constant_1053 = s32[] constant(6400), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %multiply.2164 = s32[] multiply(s32[] %bitcast.1014, s32[] %constant_1053), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %broadcast.1790 = s32[4,1024,6400]{2,1,0} broadcast(s32[] %multiply.2164), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %add.1486 = s32[4,1024,6400]{2,1,0} add(s32[4,1024,6400]{2,1,0} %iota.8, s32[4,1024,6400]{2,1,0} %broadcast.1790), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %compare.87 = pred[4,1024,6400]{2,1,0} compare(s32[4,1024,6400]{2,1,0} %broadcast.1791, s32[4,1024,6400]{2,1,0} %add.1486), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %constant_1052 = s32[] constant(0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %broadcast.1789 = s32[4,1024]{1,0} broadcast(s32[] %constant_1052), dimensions={}
  %compare.86 = pred[4,1024]{1,0} compare(s32[4,1024]{1,0} %param_1.2136, s32[4,1024]{1,0} %broadcast.1789), direction=GT, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/gt" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=86}
  %constant_1049 = f32[] constant(1)
  %broadcast.1788 = f32[4,1024]{1,0} broadcast(f32[] %constant_1049), dimensions={}
  %constant_1048 = f32[] constant(0)
  %broadcast.1787 = f32[4,1024]{1,0} broadcast(f32[] %constant_1048), dimensions={}
  %select.79 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.86, f32[4,1024]{1,0} %broadcast.1788, f32[4,1024]{1,0} %broadcast.1787), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/select_n" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=86}
  %param_0.1705 = f32[] parameter(0)
  %divide.635 = f32[] divide(f32[] %constant_1049, f32[] %param_0.1705), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=90}
  %broadcast.1786 = f32[4,1024]{1,0} broadcast(f32[] %divide.635), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/broadcast_in_dim[shape=(8, 1024) broadcast_dimensions=()]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=90}
  %multiply.2163 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %select.79, f32[4,1024]{1,0} %broadcast.1786), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=90}
  %negate.81 = f32[4,1024]{1,0} negate(f32[4,1024]{1,0} %multiply.2163), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/neg" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %broadcast.1785 = f32[4,1024,6400]{2,1,0} broadcast(f32[4,1024]{1,0} %negate.81), dimensions={0,1}
  %broadcast.1784 = f32[4,1024,6400]{2,1,0} broadcast(f32[] %constant_1048), dimensions={}
  %select.78 = f32[4,1024,6400]{2,1,0} select(pred[4,1024,6400]{2,1,0} %compare.87, f32[4,1024,6400]{2,1,0} %broadcast.1785, f32[4,1024,6400]{2,1,0} %broadcast.1784), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %convert.682 = f16[4,1024,6400]{2,1,0} convert(f32[4,1024,6400]{2,1,0} %select.78), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %negate.78 = f16[4,1024,6400]{2,1,0} negate(f16[4,1024,6400]{2,1,0} %convert.682), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/neg" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %constant_808 = f16[] constant(0)
  ROOT %reduce.333 = f16[4,1024]{1,0} reduce(f16[4,1024,6400]{2,1,0} %negate.78, f16[] %constant_808), dimensions={2}, to_apply=%region_41.1720, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(2,)]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
}

%fused_computation.663 (param_0.1691: s32[4,1024]) -> f32[] {
  %param_0.1691 = s32[4,1024]{1,0} parameter(0)
  %constant_1023 = s32[] constant(0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %broadcast.1765 = s32[4,1024]{1,0} broadcast(s32[] %constant_1023), dimensions={}
  %compare.81 = pred[4,1024]{1,0} compare(s32[4,1024]{1,0} %param_0.1691, s32[4,1024]{1,0} %broadcast.1765), direction=GT, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/gt" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=86}
  %constant_1022 = f32[] constant(1)
  %broadcast.1764 = f32[4,1024]{1,0} broadcast(f32[] %constant_1022), dimensions={}
  %constant_1013 = f32[] constant(0)
  %broadcast.1763 = f32[4,1024]{1,0} broadcast(f32[] %constant_1013), dimensions={}
  %select.73 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.81, f32[4,1024]{1,0} %broadcast.1764, f32[4,1024]{1,0} %broadcast.1763), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/select_n" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=86}
  %bitcast.952 = f32[4096]{0} bitcast(f32[4,1024]{1,0} %select.73)
  ROOT %reduce.334 = f32[] reduce(f32[4096]{0} %bitcast.952, f32[] %constant_1013), dimensions={0}, to_apply=%region_40.1707, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(0, 1)]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=90}
}

%region_55.2064 (Arg_0.2065: f16[], Arg_1.2066: f16[]) -> f16[] {
  %Arg_0.2065 = f16[] parameter(0)
  %Arg_1.2066 = f16[] parameter(1)
  ROOT %add.2067 = f16[] add(f16[] %Arg_0.2065, f16[] %Arg_1.2066), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%fused_computation.667 (param_0.1387: f16[4,1024,1024]) -> f16[1024] {
  %param_0.1387 = f16[4,1024,1024]{2,1,0} parameter(0)
  %bitcast.954 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %param_0.1387)
  %constant_24 = f16[] constant(0)
  ROOT %reduce.335 = f16[1024]{0} reduce(f16[4096,1024]{1,0} %bitcast.954, f16[] %constant_24), dimensions={0}, to_apply=%region_55.2064, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%region_56.2083 (Arg_0.2084: f16[], Arg_1.2085: f16[]) -> f16[] {
  %Arg_0.2084 = f16[] parameter(0)
  %Arg_1.2085 = f16[] parameter(1)
  ROOT %add.2086 = f16[] add(f16[] %Arg_0.2084, f16[] %Arg_1.2085), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%fused_computation.668 (param_0.1392: f16[4,1024,512]) -> f16[512] {
  %param_0.1392 = f16[4,1024,512]{2,1,0} parameter(0)
  %bitcast.955 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %param_0.1392)
  %constant_55 = f16[] constant(0)
  ROOT %reduce.336 = f16[512]{0} reduce(f16[4096,512]{1,0} %bitcast.955, f16[] %constant_55), dimensions={0}, to_apply=%region_56.2083, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%region_64.2200 (Arg_0.2201: f16[], Arg_1.2202: f16[]) -> f16[] {
  %Arg_0.2201 = f16[] parameter(0)
  %Arg_1.2202 = f16[] parameter(1)
  ROOT %add.2203 = f16[] add(f16[] %Arg_0.2201, f16[] %Arg_1.2202), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%fused_computation.669 (param_0.1395: f16[4,1024,128,3]) -> f16[384] {
  %param_0.1395 = f16[4,1024,128,3]{3,2,1,0} parameter(0)
  %bitcast.956 = f16[4096,384]{1,0} bitcast(f16[4,1024,128,3]{3,2,1,0} %param_0.1395)
  %constant_75 = f16[] constant(0)
  ROOT %reduce.337 = f16[384]{0} reduce(f16[4096,384]{1,0} %bitcast.956, f16[] %constant_75), dimensions={0}, to_apply=%region_64.2200, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%region_61.2144 (Arg_0.2145: f16[], Arg_1.2146: f16[]) -> f16[] {
  %Arg_0.2145 = f16[] parameter(0)
  %Arg_1.2146 = f16[] parameter(1)
  ROOT %add.2147 = f16[] add(f16[] %Arg_0.2145, f16[] %Arg_1.2146), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%fused_computation.670 (param_0.1398: f16[4,1024,1024]) -> f16[1024] {
  %param_0.1398 = f16[4,1024,1024]{2,1,0} parameter(0)
  %bitcast.957 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %param_0.1398)
  %constant_90 = f16[] constant(0)
  ROOT %reduce.338 = f16[1024]{0} reduce(f16[4096,1024]{1,0} %bitcast.957, f16[] %constant_90), dimensions={0}, to_apply=%region_61.2144, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%region_75.2523 (Arg_0.2524: f16[], Arg_1.2525: f16[]) -> f16[] {
  %Arg_0.2524 = f16[] parameter(0)
  %Arg_1.2525 = f16[] parameter(1)
  ROOT %add.2526 = f16[] add(f16[] %Arg_0.2524, f16[] %Arg_1.2525), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%fused_computation.671 (param_0.1403: f16[4,1024,1024]) -> f16[1024] {
  %param_0.1403 = f16[4,1024,1024]{2,1,0} parameter(0)
  %bitcast.958 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %param_0.1403)
  %constant_121 = f16[] constant(0)
  ROOT %reduce.339 = f16[1024]{0} reduce(f16[4096,1024]{1,0} %bitcast.958, f16[] %constant_121), dimensions={0}, to_apply=%region_75.2523, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%region_76.2542 (Arg_0.2543: f16[], Arg_1.2544: f16[]) -> f16[] {
  %Arg_0.2543 = f16[] parameter(0)
  %Arg_1.2544 = f16[] parameter(1)
  ROOT %add.2545 = f16[] add(f16[] %Arg_0.2543, f16[] %Arg_1.2544), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%fused_computation.672 (param_0.1408: f16[4,1024,512]) -> f16[512] {
  %param_0.1408 = f16[4,1024,512]{2,1,0} parameter(0)
  %bitcast.959 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %param_0.1408)
  %constant_150 = f16[] constant(0)
  ROOT %reduce.340 = f16[512]{0} reduce(f16[4096,512]{1,0} %bitcast.959, f16[] %constant_150), dimensions={0}, to_apply=%region_76.2542, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%region_84.2659 (Arg_0.2660: f16[], Arg_1.2661: f16[]) -> f16[] {
  %Arg_0.2660 = f16[] parameter(0)
  %Arg_1.2661 = f16[] parameter(1)
  ROOT %add.2662 = f16[] add(f16[] %Arg_0.2660, f16[] %Arg_1.2661), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%fused_computation.673 (param_0.1411: f16[4,1024,128,3]) -> f16[384] {
  %param_0.1411 = f16[4,1024,128,3]{3,2,1,0} parameter(0)
  %bitcast.960 = f16[4096,384]{1,0} bitcast(f16[4,1024,128,3]{3,2,1,0} %param_0.1411)
  %constant_164 = f16[] constant(0)
  ROOT %reduce.341 = f16[384]{0} reduce(f16[4096,384]{1,0} %bitcast.960, f16[] %constant_164), dimensions={0}, to_apply=%region_84.2659, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%region_81.2603 (Arg_0.2604: f16[], Arg_1.2605: f16[]) -> f16[] {
  %Arg_0.2604 = f16[] parameter(0)
  %Arg_1.2605 = f16[] parameter(1)
  ROOT %add.2606 = f16[] add(f16[] %Arg_0.2604, f16[] %Arg_1.2605), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%fused_computation.674 (param_0.1414: f16[4,1024,1024]) -> f16[1024] {
  %param_0.1414 = f16[4,1024,1024]{2,1,0} parameter(0)
  %bitcast.961 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %param_0.1414)
  %constant_180 = f16[] constant(0)
  ROOT %reduce.342 = f16[1024]{0} reduce(f16[4096,1024]{1,0} %bitcast.961, f16[] %constant_180), dimensions={0}, to_apply=%region_81.2603, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%region_95.2982 (Arg_0.2983: f16[], Arg_1.2984: f16[]) -> f16[] {
  %Arg_0.2983 = f16[] parameter(0)
  %Arg_1.2984 = f16[] parameter(1)
  ROOT %add.2985 = f16[] add(f16[] %Arg_0.2983, f16[] %Arg_1.2984), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%fused_computation.675 (param_0.1419: f16[4,1024,1024]) -> f16[1024] {
  %param_0.1419 = f16[4,1024,1024]{2,1,0} parameter(0)
  %bitcast.962 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %param_0.1419)
  %constant_208 = f16[] constant(0)
  ROOT %reduce.343 = f16[1024]{0} reduce(f16[4096,1024]{1,0} %bitcast.962, f16[] %constant_208), dimensions={0}, to_apply=%region_95.2982, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%region_96.3001 (Arg_0.3002: f16[], Arg_1.3003: f16[]) -> f16[] {
  %Arg_0.3002 = f16[] parameter(0)
  %Arg_1.3003 = f16[] parameter(1)
  ROOT %add.3004 = f16[] add(f16[] %Arg_0.3002, f16[] %Arg_1.3003), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%fused_computation.676 (param_0.1424: f16[4,1024,512]) -> f16[512] {
  %param_0.1424 = f16[4,1024,512]{2,1,0} parameter(0)
  %bitcast.963 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %param_0.1424)
  %constant_239 = f16[] constant(0)
  ROOT %reduce.344 = f16[512]{0} reduce(f16[4096,512]{1,0} %bitcast.963, f16[] %constant_239), dimensions={0}, to_apply=%region_96.3001, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%region_104.3118 (Arg_0.3119: f16[], Arg_1.3120: f16[]) -> f16[] {
  %Arg_0.3119 = f16[] parameter(0)
  %Arg_1.3120 = f16[] parameter(1)
  ROOT %add.3121 = f16[] add(f16[] %Arg_0.3119, f16[] %Arg_1.3120), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%fused_computation.677 (param_0.1427: f16[4,1024,128,3]) -> f16[384] {
  %param_0.1427 = f16[4,1024,128,3]{3,2,1,0} parameter(0)
  %bitcast.964 = f16[4096,384]{1,0} bitcast(f16[4,1024,128,3]{3,2,1,0} %param_0.1427)
  %constant_255 = f16[] constant(0)
  ROOT %reduce.345 = f16[384]{0} reduce(f16[4096,384]{1,0} %bitcast.964, f16[] %constant_255), dimensions={0}, to_apply=%region_104.3118, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%region_101.3062 (Arg_0.3063: f16[], Arg_1.3064: f16[]) -> f16[] {
  %Arg_0.3063 = f16[] parameter(0)
  %Arg_1.3064 = f16[] parameter(1)
  ROOT %add.3065 = f16[] add(f16[] %Arg_0.3063, f16[] %Arg_1.3064), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%fused_computation.678 (param_0.1430: f16[4,1024,1024]) -> f16[1024] {
  %param_0.1430 = f16[4,1024,1024]{2,1,0} parameter(0)
  %bitcast.965 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %param_0.1430)
  %constant_271 = f16[] constant(0)
  ROOT %reduce.346 = f16[1024]{0} reduce(f16[4096,1024]{1,0} %bitcast.965, f16[] %constant_271), dimensions={0}, to_apply=%region_101.3062, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%region_115.3441 (Arg_0.3442: f16[], Arg_1.3443: f16[]) -> f16[] {
  %Arg_0.3442 = f16[] parameter(0)
  %Arg_1.3443 = f16[] parameter(1)
  ROOT %add.3444 = f16[] add(f16[] %Arg_0.3442, f16[] %Arg_1.3443), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%fused_computation.679 (param_0.1435: f16[4,1024,1024]) -> f16[1024] {
  %param_0.1435 = f16[4,1024,1024]{2,1,0} parameter(0)
  %bitcast.966 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %param_0.1435)
  %constant_303 = f16[] constant(0)
  ROOT %reduce.347 = f16[1024]{0} reduce(f16[4096,1024]{1,0} %bitcast.966, f16[] %constant_303), dimensions={0}, to_apply=%region_115.3441, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%region_116.3460 (Arg_0.3461: f16[], Arg_1.3462: f16[]) -> f16[] {
  %Arg_0.3461 = f16[] parameter(0)
  %Arg_1.3462 = f16[] parameter(1)
  ROOT %add.3463 = f16[] add(f16[] %Arg_0.3461, f16[] %Arg_1.3462), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%fused_computation.680 (param_0.1440: f16[4,1024,512]) -> f16[512] {
  %param_0.1440 = f16[4,1024,512]{2,1,0} parameter(0)
  %bitcast.967 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %param_0.1440)
  %constant_335 = f16[] constant(0)
  ROOT %reduce.348 = f16[512]{0} reduce(f16[4096,512]{1,0} %bitcast.967, f16[] %constant_335), dimensions={0}, to_apply=%region_116.3460, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%region_124.3577 (Arg_0.3578: f16[], Arg_1.3579: f16[]) -> f16[] {
  %Arg_0.3578 = f16[] parameter(0)
  %Arg_1.3579 = f16[] parameter(1)
  ROOT %add.3580 = f16[] add(f16[] %Arg_0.3578, f16[] %Arg_1.3579), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%fused_computation.681 (param_0.1443: f16[4,1024,128,3]) -> f16[384] {
  %param_0.1443 = f16[4,1024,128,3]{3,2,1,0} parameter(0)
  %bitcast.968 = f16[4096,384]{1,0} bitcast(f16[4,1024,128,3]{3,2,1,0} %param_0.1443)
  %constant_351 = f16[] constant(0)
  ROOT %reduce.349 = f16[384]{0} reduce(f16[4096,384]{1,0} %bitcast.968, f16[] %constant_351), dimensions={0}, to_apply=%region_124.3577, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%region_121.3521 (Arg_0.3522: f16[], Arg_1.3523: f16[]) -> f16[] {
  %Arg_0.3522 = f16[] parameter(0)
  %Arg_1.3523 = f16[] parameter(1)
  ROOT %add.3524 = f16[] add(f16[] %Arg_0.3522, f16[] %Arg_1.3523), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%fused_computation.682 (param_0.1446: f16[4,1024,1024]) -> f16[1024] {
  %param_0.1446 = f16[4,1024,1024]{2,1,0} parameter(0)
  %bitcast.969 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %param_0.1446)
  %constant_365 = f16[] constant(0)
  ROOT %reduce.350 = f16[1024]{0} reduce(f16[4096,1024]{1,0} %bitcast.969, f16[] %constant_365), dimensions={0}, to_apply=%region_121.3521, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%region_135.3900 (Arg_0.3901: f16[], Arg_1.3902: f16[]) -> f16[] {
  %Arg_0.3901 = f16[] parameter(0)
  %Arg_1.3902 = f16[] parameter(1)
  ROOT %add.3903 = f16[] add(f16[] %Arg_0.3901, f16[] %Arg_1.3902), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%fused_computation.683 (param_0.1451: f16[4,1024,1024]) -> f16[1024] {
  %param_0.1451 = f16[4,1024,1024]{2,1,0} parameter(0)
  %bitcast.970 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %param_0.1451)
  %constant_393 = f16[] constant(0)
  ROOT %reduce.351 = f16[1024]{0} reduce(f16[4096,1024]{1,0} %bitcast.970, f16[] %constant_393), dimensions={0}, to_apply=%region_135.3900, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%region_136.3919 (Arg_0.3920: f16[], Arg_1.3921: f16[]) -> f16[] {
  %Arg_0.3920 = f16[] parameter(0)
  %Arg_1.3921 = f16[] parameter(1)
  ROOT %add.3922 = f16[] add(f16[] %Arg_0.3920, f16[] %Arg_1.3921), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%fused_computation.684 (param_0.1456: f16[4,1024,512]) -> f16[512] {
  %param_0.1456 = f16[4,1024,512]{2,1,0} parameter(0)
  %bitcast.971 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %param_0.1456)
  %constant_421 = f16[] constant(0)
  ROOT %reduce.352 = f16[512]{0} reduce(f16[4096,512]{1,0} %bitcast.971, f16[] %constant_421), dimensions={0}, to_apply=%region_136.3919, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%region_144.4036 (Arg_0.4037: f16[], Arg_1.4038: f16[]) -> f16[] {
  %Arg_0.4037 = f16[] parameter(0)
  %Arg_1.4038 = f16[] parameter(1)
  ROOT %add.4039 = f16[] add(f16[] %Arg_0.4037, f16[] %Arg_1.4038), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%fused_computation.685 (param_0.1459: f16[4,1024,128,3]) -> f16[384] {
  %param_0.1459 = f16[4,1024,128,3]{3,2,1,0} parameter(0)
  %bitcast.972 = f16[4096,384]{1,0} bitcast(f16[4,1024,128,3]{3,2,1,0} %param_0.1459)
  %constant_435 = f16[] constant(0)
  ROOT %reduce.353 = f16[384]{0} reduce(f16[4096,384]{1,0} %bitcast.972, f16[] %constant_435), dimensions={0}, to_apply=%region_144.4036, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%region_141.3980 (Arg_0.3981: f16[], Arg_1.3982: f16[]) -> f16[] {
  %Arg_0.3981 = f16[] parameter(0)
  %Arg_1.3982 = f16[] parameter(1)
  ROOT %add.3983 = f16[] add(f16[] %Arg_0.3981, f16[] %Arg_1.3982), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%fused_computation.686 (param_0.1462: f16[4,1024,1024]) -> f16[1024] {
  %param_0.1462 = f16[4,1024,1024]{2,1,0} parameter(0)
  %bitcast.973 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %param_0.1462)
  %constant_449 = f16[] constant(0)
  ROOT %reduce.354 = f16[1024]{0} reduce(f16[4096,1024]{1,0} %bitcast.973, f16[] %constant_449), dimensions={0}, to_apply=%region_141.3980, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%region_155.4359 (Arg_0.4360: f16[], Arg_1.4361: f16[]) -> f16[] {
  %Arg_0.4360 = f16[] parameter(0)
  %Arg_1.4361 = f16[] parameter(1)
  ROOT %add.4362 = f16[] add(f16[] %Arg_0.4360, f16[] %Arg_1.4361), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%fused_computation.687 (param_0.1467: f16[4,1024,1024]) -> f16[1024] {
  %param_0.1467 = f16[4,1024,1024]{2,1,0} parameter(0)
  %bitcast.974 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %param_0.1467)
  %constant_477 = f16[] constant(0)
  ROOT %reduce.355 = f16[1024]{0} reduce(f16[4096,1024]{1,0} %bitcast.974, f16[] %constant_477), dimensions={0}, to_apply=%region_155.4359, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%region_156.4378 (Arg_0.4379: f16[], Arg_1.4380: f16[]) -> f16[] {
  %Arg_0.4379 = f16[] parameter(0)
  %Arg_1.4380 = f16[] parameter(1)
  ROOT %add.4381 = f16[] add(f16[] %Arg_0.4379, f16[] %Arg_1.4380), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%fused_computation.688 (param_0.1472: f16[4,1024,512]) -> f16[512] {
  %param_0.1472 = f16[4,1024,512]{2,1,0} parameter(0)
  %bitcast.975 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %param_0.1472)
  %constant_505 = f16[] constant(0)
  ROOT %reduce.356 = f16[512]{0} reduce(f16[4096,512]{1,0} %bitcast.975, f16[] %constant_505), dimensions={0}, to_apply=%region_156.4378, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%region_164.4495 (Arg_0.4496: f16[], Arg_1.4497: f16[]) -> f16[] {
  %Arg_0.4496 = f16[] parameter(0)
  %Arg_1.4497 = f16[] parameter(1)
  ROOT %add.4498 = f16[] add(f16[] %Arg_0.4496, f16[] %Arg_1.4497), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%fused_computation.689 (param_0.1475: f16[4,1024,128,3]) -> f16[384] {
  %param_0.1475 = f16[4,1024,128,3]{3,2,1,0} parameter(0)
  %bitcast.976 = f16[4096,384]{1,0} bitcast(f16[4,1024,128,3]{3,2,1,0} %param_0.1475)
  %constant_519 = f16[] constant(0)
  ROOT %reduce.357 = f16[384]{0} reduce(f16[4096,384]{1,0} %bitcast.976, f16[] %constant_519), dimensions={0}, to_apply=%region_164.4495, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%region_161.4439 (Arg_0.4440: f16[], Arg_1.4441: f16[]) -> f16[] {
  %Arg_0.4440 = f16[] parameter(0)
  %Arg_1.4441 = f16[] parameter(1)
  ROOT %add.4442 = f16[] add(f16[] %Arg_0.4440, f16[] %Arg_1.4441), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%fused_computation.690 (param_0.1478: f16[4,1024,1024]) -> f16[1024] {
  %param_0.1478 = f16[4,1024,1024]{2,1,0} parameter(0)
  %bitcast.977 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %param_0.1478)
  %constant_533 = f16[] constant(0)
  ROOT %reduce.358 = f16[1024]{0} reduce(f16[4096,1024]{1,0} %bitcast.977, f16[] %constant_533), dimensions={0}, to_apply=%region_161.4439, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%fused_computation.691 (param_0.2326: f32[4,4,1024], param_1.2817: f16[4,4,1024,1024], param_2.2623: f16[4,4,1024]) -> f16[4,4,1024,1024] {
  %param_1.2817 = f16[4,4,1024,1024]{3,2,1,0} parameter(1)
  %param_2.2623 = f16[4,4,1024]{2,1,0} parameter(2)
  %broadcast.3237 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_2.2623), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.452 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_1.2817, f16[4,4,1024,1024]{3,2,1,0} %broadcast.3237), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.109 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.452), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %param_0.2326 = f32[4,4,1024]{2,1,0} parameter(0)
  %convert.663 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %param_0.2326), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.1380 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %convert.663), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  ROOT %divide.588 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %exponential.109, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1380), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.693 (param_0.2252: f32[4,4,1024], param_1.2726: f16[4,4,1024,1024], param_2.2520: f16[4,4,1024]) -> f16[4,4,1024,1024] {
  %param_1.2726 = f16[4,4,1024,1024]{3,2,1,0} parameter(1)
  %param_2.2520 = f16[4,4,1024]{2,1,0} parameter(2)
  %broadcast.2991 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_2.2520), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.398 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_1.2726, f16[4,4,1024,1024]{3,2,1,0} %broadcast.2991), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.101 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.398), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %param_0.2252 = f32[4,4,1024]{2,1,0} parameter(0)
  %convert.666 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %param_0.2252), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.1424 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %convert.666), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  ROOT %divide.597 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %exponential.101, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1424), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.695 (param_0.2178: f32[4,4,1024], param_1.2635: f16[4,4,1024,1024], param_2.2417: f16[4,4,1024]) -> f16[4,4,1024,1024] {
  %param_1.2635 = f16[4,4,1024,1024]{3,2,1,0} parameter(1)
  %param_2.2417 = f16[4,4,1024]{2,1,0} parameter(2)
  %broadcast.2745 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_2.2417), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.344 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_1.2635, f16[4,4,1024,1024]{3,2,1,0} %broadcast.2745), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.93 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.344), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %param_0.2178 = f32[4,4,1024]{2,1,0} parameter(0)
  %convert.669 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %param_0.2178), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.1468 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %convert.669), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  ROOT %divide.606 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %exponential.93, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1468), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.697 (param_0.2104: f32[4,4,1024], param_1.2544: f16[4,4,1024,1024], param_2.2314: f16[4,4,1024]) -> f16[4,4,1024,1024] {
  %param_1.2544 = f16[4,4,1024,1024]{3,2,1,0} parameter(1)
  %param_2.2314 = f16[4,4,1024]{2,1,0} parameter(2)
  %broadcast.2499 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_2.2314), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.288 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_1.2544, f16[4,4,1024,1024]{3,2,1,0} %broadcast.2499), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.85 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.288), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %param_0.2104 = f32[4,4,1024]{2,1,0} parameter(0)
  %convert.672 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %param_0.2104), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.1512 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %convert.672), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  ROOT %divide.615 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %exponential.85, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1512), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.699 (param_0.2030: f32[4,4,1024], param_1.2453: f16[4,4,1024,1024], param_2.2211: f16[4,4,1024]) -> f16[4,4,1024,1024] {
  %param_1.2453 = f16[4,4,1024,1024]{3,2,1,0} parameter(1)
  %param_2.2211 = f16[4,4,1024]{2,1,0} parameter(2)
  %broadcast.2253 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_2.2211), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.226 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_1.2453, f16[4,4,1024,1024]{3,2,1,0} %broadcast.2253), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.77 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.226), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %param_0.2030 = f32[4,4,1024]{2,1,0} parameter(0)
  %convert.675 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %param_0.2030), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.1556 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %convert.675), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  ROOT %divide.624 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %exponential.77, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1556), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.701 (param_0.1956: f32[4,4,1024], param_1.2362: f16[4,4,1024,1024], param_2.2108: f16[4,4,1024]) -> f16[4,4,1024,1024] {
  %param_1.2362 = f16[4,4,1024,1024]{3,2,1,0} parameter(1)
  %param_2.2108 = f16[4,4,1024]{2,1,0} parameter(2)
  %broadcast.2007 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %param_2.2108), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.162 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %param_1.2362, f16[4,4,1024,1024]{3,2,1,0} %broadcast.2007), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.69 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.162), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %param_0.1956 = f32[4,4,1024]{2,1,0} parameter(0)
  %convert.678 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %param_0.1956), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.1600 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %convert.678), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  ROOT %divide.633 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %exponential.69, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1600), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%region_43.1737 (Arg_0.1738: f16[], Arg_1.1739: f16[]) -> f16[] {
  %Arg_0.1738 = f16[] parameter(0)
  %Arg_1.1739 = f16[] parameter(1)
  ROOT %add.1740 = f16[] add(f16[] %Arg_0.1738, f16[] %Arg_1.1739), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/reduce_sum[axes=(0, 1)]" source_file="/code/alpa/alpa/model/gpt_model.py" source_line=75}
}

%fused_computation.703 (param_0.1631: f16[4,1024,6400]) -> f16[6400] {
  %param_0.1631 = f16[4,1024,6400]{2,1,0} parameter(0)
  %bitcast.1002 = f16[4096,6400]{1,0} bitcast(f16[4,1024,6400]{2,1,0} %param_0.1631)
  %constant_806 = f16[] constant(0)
  ROOT %reduce.359 = f16[6400]{0} reduce(f16[4096,6400]{1,0} %bitcast.1002, f16[] %constant_806), dimensions={0}, to_apply=%region_43.1737, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/reduce_sum[axes=(0, 1)]" source_file="/code/alpa/alpa/model/gpt_model.py" source_line=75}
}

%fused_computation.704 (param_0.1637: f16[4096,6400]) -> f16[4,1024] {
  %param_0.1637 = f16[4096,6400]{1,0} parameter(0)
  %bitcast.1004 = f16[4,1024,6400]{2,1,0} bitcast(f16[4096,6400]{1,0} %param_0.1637), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/add" source_file="/code/alpa/alpa/model/gpt_model.py" source_line=75}
  %constant_809 = f16[] constant(-inf)
  ROOT %reduce.360 = f16[4,1024]{1,0} reduce(f16[4,1024,6400]{2,1,0} %bitcast.1004, f16[] %constant_809), dimensions={2}, to_apply=%region_38.1688, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/reduce_max[axes=(2,)]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
}

%region_0.466 (Arg_0.467: f32[], Arg_1.468: f32[]) -> f32[] {
  %Arg_0.467 = f32[] parameter(0)
  %Arg_1.468 = f32[] parameter(1)
  ROOT %add.469 = f32[] add(f32[] %Arg_0.467, f32[] %Arg_1.468), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_1.474 (Arg_0.475: f32[], Arg_1.476: f32[]) -> f32[] {
  %Arg_0.475 = f32[] parameter(0)
  %Arg_1.476 = f32[] parameter(1)
  ROOT %add.477 = f32[] add(f32[] %Arg_0.475, f32[] %Arg_1.476), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%fused_computation.705 (param_0.2445: f16[4096,1024]) -> (f32[4,1024], f32[4,1024]) {
  %param_0.2445 = f16[4096,1024]{1,0} parameter(0)
  %convert.686 = f32[4096,1024]{1,0} convert(f16[4096,1024]{1,0} %param_0.2445), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %bitcast.1018 = f32[4,1024,1024]{2,1,0} bitcast(f32[4096,1024]{1,0} %convert.686), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %constant_1008_clone_1 = f32[] constant(0)
  %reduce.361 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %bitcast.1018, f32[] %constant_1008_clone_1), dimensions={2}, to_apply=%region_0.466, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.2137.clone.1 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %bitcast.1018, f32[4,1024,1024]{2,1,0} %bitcast.1018), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.332.clone.1 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.2137.clone.1, f32[] %constant_1008_clone_1), dimensions={2}, to_apply=%region_1.474, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  ROOT %tuple.315 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) tuple(f32[4,1024]{1,0} %reduce.361, f32[4,1024]{1,0} %reduce.332.clone.1)
}

%region_32.1499 (Arg_0.1500: f16[], Arg_1.1501: f16[]) -> f16[] {
  %Arg_0.1500 = f16[] parameter(0)
  %Arg_1.1501 = f16[] parameter(1)
  ROOT %maximum.1502 = f16[] maximum(f16[] %Arg_0.1500, f16[] %Arg_1.1501), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.712 (param_0.2439: f16[4,4,1024,1024], param_1.3156: s32[4,1024]) -> (f16[4,4,1024], f16[4,4,1024,1024]) {
  %param_0.2439 = f16[4,4,1024,1024]{3,2,1,0} parameter(0)
  %param_1.3156 = s32[4,1024]{1,0} parameter(1)
  %constant_1162_clone_1 = s32[] constant(0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %broadcast.1903.clone.1 = s32[4,1024]{1,0} broadcast(s32[] %constant_1162_clone_1), dimensions={}
  %compare.103.clone.1 = pred[4,1024]{1,0} compare(s32[4,1024]{1,0} %param_1.3156, s32[4,1024]{1,0} %broadcast.1903.clone.1), direction=GT, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %constant_1161_clone_1 = f16[] constant(0)
  %broadcast.1902.clone.1 = f16[4,1024]{1,0} broadcast(f16[] %constant_1161_clone_1), dimensions={}
  %constant_1160_clone_1 = f16[] constant(-inf)
  %broadcast.1901.clone.1 = f16[4,1024]{1,0} broadcast(f16[] %constant_1160_clone_1), dimensions={}
  %select.95.clone.1 = f16[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.103.clone.1, f16[4,1024]{1,0} %broadcast.1902.clone.1, f16[4,1024]{1,0} %broadcast.1901.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %broadcast.1900.clone.1 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,1024]{1,0} %select.95.clone.1), dimensions={0,3}
  %add.1504.clone.1 = f16[4,4,1024,1024]{3,2,1,0} add(f16[4,4,1024,1024]{3,2,1,0} %param_0.2439, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1900.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=100}
  %reduce.362 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %add.1504.clone.1, f16[] %constant_1160_clone_1), dimensions={3}, to_apply=%region_32.1499, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  ROOT %tuple.293 = (f16[4,4,1024]{2,1,0}, f16[4,4,1024,1024]{3,2,1,0}) tuple(f16[4,4,1024]{2,1,0} %reduce.362, f16[4,4,1024,1024]{3,2,1,0} %add.1504.clone.1)
}

%region_26.1307 (Arg_0.1308: f16[], Arg_1.1309: f16[]) -> f16[] {
  %Arg_0.1308 = f16[] parameter(0)
  %Arg_1.1309 = f16[] parameter(1)
  ROOT %maximum.1310 = f16[] maximum(f16[] %Arg_0.1308, f16[] %Arg_1.1309), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.713 (param_0.2440: f16[4,4,1024,1024], param_1.3157: s32[4,1024]) -> (f16[4,4,1024], f16[4,4,1024,1024]) {
  %param_0.2440 = f16[4,4,1024,1024]{3,2,1,0} parameter(0)
  %param_1.3157 = s32[4,1024]{1,0} parameter(1)
  %constant_1155_clone_1 = s32[] constant(0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %broadcast.1895.clone.1 = s32[4,1024]{1,0} broadcast(s32[] %constant_1155_clone_1), dimensions={}
  %compare.101.clone.1 = pred[4,1024]{1,0} compare(s32[4,1024]{1,0} %param_1.3157, s32[4,1024]{1,0} %broadcast.1895.clone.1), direction=GT, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %constant_1154_clone_1 = f16[] constant(0)
  %broadcast.1894.clone.1 = f16[4,1024]{1,0} broadcast(f16[] %constant_1154_clone_1), dimensions={}
  %constant_1153_clone_1 = f16[] constant(-inf)
  %broadcast.1893.clone.1 = f16[4,1024]{1,0} broadcast(f16[] %constant_1153_clone_1), dimensions={}
  %select.93.clone.1 = f16[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.101.clone.1, f16[4,1024]{1,0} %broadcast.1894.clone.1, f16[4,1024]{1,0} %broadcast.1893.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %broadcast.1892.clone.1 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,1024]{1,0} %select.93.clone.1), dimensions={0,3}
  %add.1503.clone.1 = f16[4,4,1024,1024]{3,2,1,0} add(f16[4,4,1024,1024]{3,2,1,0} %param_0.2440, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1892.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=100}
  %reduce.363 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %add.1503.clone.1, f16[] %constant_1153_clone_1), dimensions={3}, to_apply=%region_26.1307, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  ROOT %tuple.297 = (f16[4,4,1024]{2,1,0}, f16[4,4,1024,1024]{3,2,1,0}) tuple(f16[4,4,1024]{2,1,0} %reduce.363, f16[4,4,1024,1024]{3,2,1,0} %add.1503.clone.1)
}

%region_20.1115 (Arg_0.1116: f16[], Arg_1.1117: f16[]) -> f16[] {
  %Arg_0.1116 = f16[] parameter(0)
  %Arg_1.1117 = f16[] parameter(1)
  ROOT %maximum.1118 = f16[] maximum(f16[] %Arg_0.1116, f16[] %Arg_1.1117), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.714 (param_0.2441: f16[4,4,1024,1024], param_1.3158: s32[4,1024]) -> (f16[4,4,1024], f16[4,4,1024,1024]) {
  %param_0.2441 = f16[4,4,1024,1024]{3,2,1,0} parameter(0)
  %param_1.3158 = s32[4,1024]{1,0} parameter(1)
  %constant_1148_clone_1 = s32[] constant(0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %broadcast.1887.clone.1 = s32[4,1024]{1,0} broadcast(s32[] %constant_1148_clone_1), dimensions={}
  %compare.99.clone.1 = pred[4,1024]{1,0} compare(s32[4,1024]{1,0} %param_1.3158, s32[4,1024]{1,0} %broadcast.1887.clone.1), direction=GT, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %constant_1147_clone_1 = f16[] constant(0)
  %broadcast.1886.clone.1 = f16[4,1024]{1,0} broadcast(f16[] %constant_1147_clone_1), dimensions={}
  %constant_1146_clone_1 = f16[] constant(-inf)
  %broadcast.1885.clone.1 = f16[4,1024]{1,0} broadcast(f16[] %constant_1146_clone_1), dimensions={}
  %select.91.clone.1 = f16[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.99.clone.1, f16[4,1024]{1,0} %broadcast.1886.clone.1, f16[4,1024]{1,0} %broadcast.1885.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %broadcast.1884.clone.1 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,1024]{1,0} %select.91.clone.1), dimensions={0,3}
  %add.1502.clone.1 = f16[4,4,1024,1024]{3,2,1,0} add(f16[4,4,1024,1024]{3,2,1,0} %param_0.2441, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1884.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=100}
  %reduce.364 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %add.1502.clone.1, f16[] %constant_1146_clone_1), dimensions={3}, to_apply=%region_20.1115, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  ROOT %tuple.301 = (f16[4,4,1024]{2,1,0}, f16[4,4,1024,1024]{3,2,1,0}) tuple(f16[4,4,1024]{2,1,0} %reduce.364, f16[4,4,1024,1024]{3,2,1,0} %add.1502.clone.1)
}

%region_14.923 (Arg_0.924: f16[], Arg_1.925: f16[]) -> f16[] {
  %Arg_0.924 = f16[] parameter(0)
  %Arg_1.925 = f16[] parameter(1)
  ROOT %maximum.926 = f16[] maximum(f16[] %Arg_0.924, f16[] %Arg_1.925), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.715 (param_0.2442: f16[4,4,1024,1024], param_1.3159: s32[4,1024]) -> (f16[4,4,1024], f16[4,4,1024,1024]) {
  %param_0.2442 = f16[4,4,1024,1024]{3,2,1,0} parameter(0)
  %param_1.3159 = s32[4,1024]{1,0} parameter(1)
  %constant_1141_clone_1 = s32[] constant(0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %broadcast.1879.clone.1 = s32[4,1024]{1,0} broadcast(s32[] %constant_1141_clone_1), dimensions={}
  %compare.97.clone.1 = pred[4,1024]{1,0} compare(s32[4,1024]{1,0} %param_1.3159, s32[4,1024]{1,0} %broadcast.1879.clone.1), direction=GT, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %constant_1140_clone_1 = f16[] constant(0)
  %broadcast.1878.clone.1 = f16[4,1024]{1,0} broadcast(f16[] %constant_1140_clone_1), dimensions={}
  %constant_1139_clone_1 = f16[] constant(-inf)
  %broadcast.1877.clone.1 = f16[4,1024]{1,0} broadcast(f16[] %constant_1139_clone_1), dimensions={}
  %select.89.clone.1 = f16[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.97.clone.1, f16[4,1024]{1,0} %broadcast.1878.clone.1, f16[4,1024]{1,0} %broadcast.1877.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %broadcast.1876.clone.1 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,1024]{1,0} %select.89.clone.1), dimensions={0,3}
  %add.1501.clone.1 = f16[4,4,1024,1024]{3,2,1,0} add(f16[4,4,1024,1024]{3,2,1,0} %param_0.2442, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1876.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=100}
  %reduce.365 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %add.1501.clone.1, f16[] %constant_1139_clone_1), dimensions={3}, to_apply=%region_14.923, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  ROOT %tuple.305 = (f16[4,4,1024]{2,1,0}, f16[4,4,1024,1024]{3,2,1,0}) tuple(f16[4,4,1024]{2,1,0} %reduce.365, f16[4,4,1024,1024]{3,2,1,0} %add.1501.clone.1)
}

%region_8.731 (Arg_0.732: f16[], Arg_1.733: f16[]) -> f16[] {
  %Arg_0.732 = f16[] parameter(0)
  %Arg_1.733 = f16[] parameter(1)
  ROOT %maximum.734 = f16[] maximum(f16[] %Arg_0.732, f16[] %Arg_1.733), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.716 (param_0.2443: f16[4,4,1024,1024], param_1.3160: s32[4,1024]) -> (f16[4,4,1024], f16[4,4,1024,1024]) {
  %param_0.2443 = f16[4,4,1024,1024]{3,2,1,0} parameter(0)
  %param_1.3160 = s32[4,1024]{1,0} parameter(1)
  %constant_1134_clone_1 = s32[] constant(0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %broadcast.1871.clone.1 = s32[4,1024]{1,0} broadcast(s32[] %constant_1134_clone_1), dimensions={}
  %compare.95.clone.1 = pred[4,1024]{1,0} compare(s32[4,1024]{1,0} %param_1.3160, s32[4,1024]{1,0} %broadcast.1871.clone.1), direction=GT, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %constant_1133_clone_1 = f16[] constant(0)
  %broadcast.1870.clone.1 = f16[4,1024]{1,0} broadcast(f16[] %constant_1133_clone_1), dimensions={}
  %constant_1132_clone_1 = f16[] constant(-inf)
  %broadcast.1869.clone.1 = f16[4,1024]{1,0} broadcast(f16[] %constant_1132_clone_1), dimensions={}
  %select.87.clone.1 = f16[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.95.clone.1, f16[4,1024]{1,0} %broadcast.1870.clone.1, f16[4,1024]{1,0} %broadcast.1869.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %broadcast.1868.clone.1 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,1024]{1,0} %select.87.clone.1), dimensions={0,3}
  %add.1500.clone.1 = f16[4,4,1024,1024]{3,2,1,0} add(f16[4,4,1024,1024]{3,2,1,0} %param_0.2443, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1868.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=100}
  %reduce.366 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %add.1500.clone.1, f16[] %constant_1132_clone_1), dimensions={3}, to_apply=%region_8.731, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  ROOT %tuple.309 = (f16[4,4,1024]{2,1,0}, f16[4,4,1024,1024]{3,2,1,0}) tuple(f16[4,4,1024]{2,1,0} %reduce.366, f16[4,4,1024,1024]{3,2,1,0} %add.1500.clone.1)
}

%region_2.539 (Arg_0.540: f16[], Arg_1.541: f16[]) -> f16[] {
  %Arg_0.540 = f16[] parameter(0)
  %Arg_1.541 = f16[] parameter(1)
  ROOT %maximum.542 = f16[] maximum(f16[] %Arg_0.540, f16[] %Arg_1.541), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%fused_computation.717 (param_0.2444: f16[4,4,1024,1024], param_1.3161: s32[4,1024]) -> (f16[4,4,1024], f16[4,4,1024,1024]) {
  %param_0.2444 = f16[4,4,1024,1024]{3,2,1,0} parameter(0)
  %param_1.3161 = s32[4,1024]{1,0} parameter(1)
  %constant_1127_clone_1 = s32[] constant(0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %broadcast.1863.clone.1 = s32[4,1024]{1,0} broadcast(s32[] %constant_1127_clone_1), dimensions={}
  %compare.93.clone.1 = pred[4,1024]{1,0} compare(s32[4,1024]{1,0} %param_1.3161, s32[4,1024]{1,0} %broadcast.1863.clone.1), direction=GT, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %constant_1126_clone_1 = f16[] constant(0)
  %broadcast.1862.clone.1 = f16[4,1024]{1,0} broadcast(f16[] %constant_1126_clone_1), dimensions={}
  %constant_1125_clone_1 = f16[] constant(-inf)
  %broadcast.1861.clone.1 = f16[4,1024]{1,0} broadcast(f16[] %constant_1125_clone_1), dimensions={}
  %select.85.clone.1 = f16[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.93.clone.1, f16[4,1024]{1,0} %broadcast.1862.clone.1, f16[4,1024]{1,0} %broadcast.1861.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %broadcast.1860.clone.1 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,1024]{1,0} %select.85.clone.1), dimensions={0,3}
  %add.1499.clone.1 = f16[4,4,1024,1024]{3,2,1,0} add(f16[4,4,1024,1024]{3,2,1,0} %param_0.2444, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1860.clone.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=100}
  %reduce.367 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %add.1499.clone.1, f16[] %constant_1125_clone_1), dimensions={3}, to_apply=%region_2.539, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  ROOT %tuple.313 = (f16[4,4,1024]{2,1,0}, f16[4,4,1024,1024]{3,2,1,0}) tuple(f16[4,4,1024]{2,1,0} %reduce.367, f16[4,4,1024,1024]{3,2,1,0} %add.1499.clone.1)
}

ENTRY %main.5888_spmd (param.2: s32[], param.3: f32[6400], param.10: f32[1024], param.9: f32[1024], param.8: f32[128,1024], param.6: f32[6400,1024], param.17: f32[1024], param.16: f32[1024], param.15: f32[1024], param.14: f32[128,1024], param.12: f32[384], param.11: f32[1024,384], param.19: f32[512], param.18: f32[1024,512], param.23: f32[1024], param.22: f32[1024], param.21: f32[1024], param.20: f32[512,1024], param.29: f32[1024], param.28: f32[1024], param.27: f32[1024], param.26: f32[128,1024], param.25: f32[384], param.24: f32[1024,384], param.31: f32[512], param.30: f32[1024,512], param.35: f32[1024], param.34: f32[1024], param.33: f32[1024], param.32: f32[512,1024], param.41: f32[1024], param.40: f32[1024], param.39: f32[1024], param.38: f32[128,1024], param.37: f32[384], param.36: f32[1024,384], param.43: f32[512], param.42: f32[1024,512], param.47: f32[1024], param.46: f32[1024], param.45: f32[1024], param.44: f32[512,1024], param.53: f32[1024], param.52: f32[1024], param.51: f32[1024], param.50: f32[128,1024], param.49: f32[384], param.48: f32[1024,384], param.55: f32[512], param.54: f32[1024,512], param.59: f32[1024], param.58: f32[1024], param.57: f32[1024], param.56: f32[512,1024], param.65: f32[1024], param.64: f32[1024], param.63: f32[1024], param.62: f32[128,1024], param.61: f32[384], param.60: f32[1024,384], param.67: f32[512], param.66: f32[1024,512], param.71: f32[1024], param.70: f32[1024], param.69: f32[1024], param.68: f32[512,1024], param.77: f32[1024], param.76: f32[1024], param.75: f32[1024], param.74: f32[128,1024], param.73: f32[384], param.72: f32[1024,384], param.79: f32[512], param.78: f32[1024,512], param.83: f32[1024], param.82: f32[1024], param.81: f32[1024], param.80: f32[512,1024], param.85: s32[], param.84: f32[6400], param.87: f32[1024], param.89: f32[1024], param.91: f32[128,1024], param.93: f32[6400,1024], param.95: f32[1024], param.97: f32[1024], param.99: f32[1024], param.101: f32[128,1024], param.103: f32[384], param.105: f32[1024,384], param.107: f32[512], param.109: f32[1024,512], param.111: f32[1024], param.113: f32[1024], param.115: f32[1024], param.117: f32[512,1024], param.119: f32[1024], param.121: f32[1024], param.123: f32[1024], param.125: f32[128,1024], param.127: f32[384], param.129: f32[1024,384], param.131: f32[512], param.133: f32[1024,512], param.135: f32[1024], param.137: f32[1024], param.139: f32[1024], param.141: f32[512,1024], param.143: f32[1024], param.145: f32[1024], param.147: f32[1024], param.149: f32[128,1024], param.151: f32[384], param.153: f32[1024,384], param.155: f32[512], param.157: f32[1024,512], param.159: f32[1024], param.161: f32[1024], param.163: f32[1024], param.165: f32[512,1024], param.167: f32[1024], param.169: f32[1024], param.171: f32[1024], param.173: f32[128,1024], param.175: f32[384], param.177: f32[1024,384], param.179: f32[512], param.181: f32[1024,512], param.183: f32[1024], param.185: f32[1024], param.187: f32[1024], param.189: f32[512,1024], param.191: f32[1024], param.193: f32[1024], param.195: f32[1024], param.197: f32[128,1024], param.199: f32[384], param.201: f32[1024,384], param.203: f32[512], param.205: f32[1024,512], param.207: f32[1024], param.209: f32[1024], param.211: f32[1024], param.213: f32[512,1024], param.215: f32[1024], param.217: f32[1024], param.219: f32[1024], param.221: f32[128,1024], param.223: f32[384], param.225: f32[1024,384], param.227: f32[512], param.229: f32[1024,512], param.231: f32[1024], param.233: f32[1024], param.235: f32[1024], param.237: f32[512,1024], param.86: f32[6400], param.88: f32[1024], param.90: f32[1024], param.92: f32[128,1024], param.94: f32[6400,1024], param.96: f32[1024], param.98: f32[1024], param.100: f32[1024], param.102: f32[128,1024], param.104: f32[384], param.106: f32[1024,384], param.108: f32[512], param.110: f32[1024,512], param.112: f32[1024], param.114: f32[1024], param.116: f32[1024], param.118: f32[512,1024], param.120: f32[1024], param.122: f32[1024], param.124: f32[1024], param.126: f32[128,1024], param.128: f32[384], param.130: f32[1024,384], param.132: f32[512], param.134: f32[1024,512], param.136: f32[1024], param.138: f32[1024], param.140: f32[1024], param.142: f32[512,1024], param.144: f32[1024], param.146: f32[1024], param.148: f32[1024], param.150: f32[128,1024], param.152: f32[384], param.154: f32[1024,384], param.156: f32[512], param.158: f32[1024,512], param.160: f32[1024], param.162: f32[1024], param.164: f32[1024], param.166: f32[512,1024], param.168: f32[1024], param.170: f32[1024], param.172: f32[1024], param.174: f32[128,1024], param.176: f32[384], param.178: f32[1024,384], param.180: f32[512], param.182: f32[1024,512], param.184: f32[1024], param.186: f32[1024], param.188: f32[1024], param.190: f32[512,1024], param.192: f32[1024], param.194: f32[1024], param.196: f32[1024], param.198: f32[128,1024], param.200: f32[384], param.202: f32[1024,384], param.204: f32[512], param.206: f32[1024,512], param.208: f32[1024], param.210: f32[1024], param.212: f32[1024], param.214: f32[512,1024], param.216: f32[1024], param.218: f32[1024], param.220: f32[1024], param.222: f32[128,1024], param.224: f32[384], param.226: f32[1024,384], param.228: f32[512], param.230: f32[1024,512], param.232: f32[1024], param.234: f32[1024], param.236: f32[1024], param.238: f32[512,1024], param.13: s32[4,1024], param.5: s32[4,1024], param.4: s32[4,1024], param.7: s32[4,1024], param: s32[4,1024], param.1: u32[1]) -> (s32[], f32[6400], f32[1024], f32[1024], f32[128,1024], /*index=5*/f32[6400,1024], f32[1024], f32[1024], f32[1024], f32[128,1024], /*index=10*/f32[384], f32[1024,384], f32[512], f32[1024,512], f32[1024], /*index=15*/f32[1024], f32[1024], f32[512,1024], f32[1024], f32[1024], /*index=20*/f32[1024], f32[128,1024], f32[384], f32[1024,384], f32[512], /*index=25*/f32[1024,512], f32[1024], f32[1024], f32[1024], f32[512,1024], /*index=30*/f32[1024], f32[1024], f32[1024], f32[128,1024], f32[384], /*index=35*/f32[1024,384], f32[512], f32[1024,512], f32[1024], f32[1024], /*index=40*/f32[1024], f32[512,1024], f32[1024], f32[1024], f32[1024], /*index=45*/f32[128,1024], f32[384], f32[1024,384], f32[512], f32[1024,512], /*index=50*/f32[1024], f32[1024], f32[1024], f32[512,1024], f32[1024], /*index=55*/f32[1024], f32[1024], f32[128,1024], f32[384], f32[1024,384], /*index=60*/f32[512], f32[1024,512], f32[1024], f32[1024], f32[1024], /*index=65*/f32[512,1024], f32[1024], f32[1024], f32[1024], f32[128,1024], /*index=70*/f32[384], f32[1024,384], f32[512], f32[1024,512], f32[1024], /*index=75*/f32[1024], f32[1024], f32[512,1024], s32[], f32[6400], /*index=80*/f32[1024], f32[1024], f32[128,1024], f32[6400,1024], f32[1024], /*index=85*/f32[1024], f32[1024], f32[128,1024], f32[384], f32[1024,384], /*index=90*/f32[512], f32[1024,512], f32[1024], f32[1024], f32[1024], /*index=95*/f32[512,1024], f32[1024], f32[1024], f32[1024], f32[128,1024], /*index=100*/f32[384], f32[1024,384], f32[512], f32[1024,512], f32[1024], /*index=105*/f32[1024], f32[1024], f32[512,1024], f32[1024], f32[1024], /*index=110*/f32[1024], f32[128,1024], f32[384], f32[1024,384], f32[512], /*index=115*/f32[1024,512], f32[1024], f32[1024], f32[1024], f32[512,1024], /*index=120*/f32[1024], f32[1024], f32[1024], f32[128,1024], f32[384], /*index=125*/f32[1024,384], f32[512], f32[1024,512], f32[1024], f32[1024], /*index=130*/f32[1024], f32[512,1024], f32[1024], f32[1024], f32[1024], /*index=135*/f32[128,1024], f32[384], f32[1024,384], f32[512], f32[1024,512], /*index=140*/f32[1024], f32[1024], f32[1024], f32[512,1024], f32[1024], /*index=145*/f32[1024], f32[1024], f32[128,1024], f32[384], f32[1024,384], /*index=150*/f32[512], f32[1024,512], f32[1024], f32[1024], f32[1024], /*index=155*/f32[512,1024], f32[6400], f32[1024], f32[1024], f32[128,1024], /*index=160*/f32[6400,1024], f32[1024], f32[1024], f32[1024], f32[128,1024], /*index=165*/f32[384], f32[1024,384], f32[512], f32[1024,512], f32[1024], /*index=170*/f32[1024], f32[1024], f32[512,1024], f32[1024], f32[1024], /*index=175*/f32[1024], f32[128,1024], f32[384], f32[1024,384], f32[512], /*index=180*/f32[1024,512], f32[1024], f32[1024], f32[1024], f32[512,1024], /*index=185*/f32[1024], f32[1024], f32[1024], f32[128,1024], f32[384], /*index=190*/f32[1024,384], f32[512], f32[1024,512], f32[1024], f32[1024], /*index=195*/f32[1024], f32[512,1024], f32[1024], f32[1024], f32[1024], /*index=200*/f32[128,1024], f32[384], f32[1024,384], f32[512], f32[1024,512], /*index=205*/f32[1024], f32[1024], f32[1024], f32[512,1024], f32[1024], /*index=210*/f32[1024], f32[1024], f32[128,1024], f32[384], f32[1024,384], /*index=215*/f32[512], f32[1024,512], f32[1024], f32[1024], f32[1024], /*index=220*/f32[512,1024], f32[1024], f32[1024], f32[1024], f32[128,1024], /*index=225*/f32[384], f32[1024,384], f32[512], f32[1024,512], f32[1024], /*index=230*/f32[1024], f32[1024], f32[512,1024]) {
  %param = s32[4,1024]{1,0} parameter(237), sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}
  %param.1 = u32[1]{0} parameter(238), sharding={devices=[2,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}
  %param.2 = s32[] parameter(0), sharding={replicated}
  %constant_49 = s32[] constant(1)
  %add.92 = s32[] add(s32[] %param.2, s32[] %constant_49), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/code/alpa/alpa/model/model_util.py" source_line=301}
  %param.3 = f32[6400]{0} parameter(1), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.85 = s32[] parameter(78), sharding={replicated}
  %fusion.528 = (f32[], f32[], s32[]) fusion(s32[] %param.85), kind=kLoop, calls=%fused_computation.528, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sub" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=112}
  %get-tuple-element.760 = f32[] get-tuple-element((f32[], f32[], s32[]) %fusion.528), index=0
  %get-tuple-element.761 = f32[] get-tuple-element((f32[], f32[], s32[]) %fusion.528), index=1
  %param.86 = f32[6400]{0} parameter(156), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.4 = s32[4,1024]{1,0} parameter(235), sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}
  %fusion.663 = f32[] fusion(s32[4,1024]{1,0} %param.4), kind=kInput, calls=%fused_computation.663, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(0, 1)]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=90}
  %all-reduce = f32[] all-reduce(f32[] %fusion.663), channel_id=1, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_40.1707, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(0, 1)]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=90}
  %constant_52 = s32[16]{0} constant({...}), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %partition-id = u32[] partition-id()
  %fusion.661 = f16[4,1024]{1,0} fusion(f32[] %all-reduce, s32[4,1024]{1,0} %param.4, s32[16]{0} %constant_52, u32[] %partition-id), kind=kInput, calls=%fused_computation.661, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(2,)]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %bitcast.1687 = f16[4096]{0} bitcast(f16[4,1024]{1,0} %fusion.661)
  %param.5 = s32[4,1024]{1,0} parameter(234), sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}
  %fusion.660 = f16[4096,6400]{1,0} fusion(s32[4,1024]{1,0} %param.5, s32[16]{0} %constant_52, u32[] %partition-id), kind=kLoop, calls=%fused_computation.660
  %param.6 = f32[6400,1024]{1,0} parameter(5), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.44 = f16[6400,1024]{1,0} convert(f32[6400,1024]{1,0} %param.6), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/word_embeddings.embed_call_one_hot/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=237}
  %param.7 = s32[4,1024]{1,0} parameter(236), sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}
  %fusion.659 = f16[4096,128]{1,0} fusion(s32[4,1024]{1,0} %param.7, s32[16]{0} %constant_52, u32[] %partition-id), kind=kLoop, calls=%fused_computation.659
  %param.8 = f32[128,1024]{1,0} parameter(4), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.46 = f16[128,1024]{1,0} convert(f32[128,1024]{1,0} %param.8), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/position_embeddings.embed_call_one_hot/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=237}
  %cublas-gemm.3 = f16[4096,1024]{1,0} custom-call(f16[4096,128]{1,0} %fusion.659, f16[128,1024]{1,0} %convert.46), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/position_embeddings.embed_call_one_hot/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=237}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %cublas-gemm.5 = f16[4096,1024]{1,0} custom-call(f16[4096,6400]{1,0} %fusion.660, f16[6400,1024]{1,0} %convert.44, f16[4096,1024]{1,0} %cublas-gemm.3), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/word_embeddings.embed_call_one_hot/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=237}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %bitcast.1688 = f16[4194304]{0} bitcast(f16[4096,1024]{1,0} %cublas-gemm.5)
  %concatenate = f16[4198400]{0} concatenate(f16[4096]{0} %bitcast.1687, f16[4194304]{0} %bitcast.1688), dimensions={0}
  %all-reduce.123 = f16[4198400]{0} all-reduce(f16[4198400]{0} %concatenate), channel_id=2, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%region_41.1720
  %slice.78 = f16[4096]{0} slice(f16[4198400]{0} %all-reduce.123), slice={[0:4096]}
  %bitcast.1689 = f16[4,1024]{1,0} bitcast(f16[4096]{0} %slice.78), sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(2,)]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %param.83 = f32[1024]{0} parameter(74), sharding={replicated}
  %param.82 = f32[1024]{0} parameter(75), sharding={replicated}
  %param.77 = f32[1024]{0} parameter(66), sharding={replicated}
  %param.76 = f32[1024]{0} parameter(67), sharding={replicated}
  %param.71 = f32[1024]{0} parameter(62), sharding={replicated}
  %param.70 = f32[1024]{0} parameter(63), sharding={replicated}
  %param.65 = f32[1024]{0} parameter(54), sharding={replicated}
  %param.64 = f32[1024]{0} parameter(55), sharding={replicated}
  %param.59 = f32[1024]{0} parameter(50), sharding={replicated}
  %param.58 = f32[1024]{0} parameter(51), sharding={replicated}
  %param.53 = f32[1024]{0} parameter(42), sharding={replicated}
  %param.52 = f32[1024]{0} parameter(43), sharding={replicated}
  %param.47 = f32[1024]{0} parameter(38), sharding={replicated}
  %param.46 = f32[1024]{0} parameter(39), sharding={replicated}
  %param.41 = f32[1024]{0} parameter(30), sharding={replicated}
  %param.40 = f32[1024]{0} parameter(31), sharding={replicated}
  %param.35 = f32[1024]{0} parameter(26), sharding={replicated}
  %param.34 = f32[1024]{0} parameter(27), sharding={replicated}
  %param.29 = f32[1024]{0} parameter(18), sharding={replicated}
  %param.28 = f32[1024]{0} parameter(19), sharding={replicated}
  %param.23 = f32[1024]{0} parameter(14), sharding={replicated}
  %param.22 = f32[1024]{0} parameter(15), sharding={replicated}
  %param.17 = f32[1024]{0} parameter(6), sharding={replicated}
  %param.16 = f32[1024]{0} parameter(7), sharding={replicated}
  %param.10 = f32[1024]{0} parameter(2), sharding={replicated}
  %param.9 = f32[1024]{0} parameter(3), sharding={replicated}
  %slice.79 = f16[4194304]{0} slice(f16[4198400]{0} %all-reduce.123), slice={[4096:4198400]}
  %bitcast.1690 = f16[4096,1024]{1,0} bitcast(f16[4194304]{0} %slice.79), sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/word_embeddings.embed_call_one_hot/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=237}
  %fusion.705 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) fusion(f16[4096,1024]{1,0} %bitcast.1690), kind=kInput, calls=%fused_computation.705, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.814 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.705), index=0
  %get-tuple-element.815 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.705), index=1
  %fusion.652 = f16[4,1024,1024]{2,1,0} fusion(f32[1024]{0} %param.10, f32[1024]{0} %param.9, f32[4,1024]{1,0} %get-tuple-element.814, f16[4096,1024]{1,0} %bitcast.1690, f32[4,1024]{1,0} %get-tuple-element.815), kind=kLoop, calls=%fused_computation.652, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %param.15 = f32[1024]{0} parameter(8), sharding={replicated}
  %bitcast.58 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %fusion.652)
  %param.11 = f32[1024,384]{1,0} parameter(11), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.49 = f16[1024,384]{1,0} convert(f32[1024,384]{1,0} %param.11), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %param.12 = f32[384]{0} parameter(10), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %fusion.651 = f16[4096,384]{1,0} fusion(f32[384]{0} %param.12), kind=kLoop, calls=%fused_computation.651
  %cublas-gemm.9 = f16[4096,384]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.58, f16[1024,384]{1,0} %convert.49, f16[4096,384]{1,0} %fusion.651), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.647 = (f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) fusion(f16[4096,384]{1,0} %cublas-gemm.9), kind=kLoop, calls=%fused_computation.647
  %get-tuple-element.813 = f16[4,4,32,1024]{3,2,1,0} get-tuple-element((f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) %fusion.647), index=1
  %fusion.648 = f16[4,4,1024,32]{3,2,1,0} fusion(f16[4096,384]{1,0} %cublas-gemm.9), kind=kLoop, calls=%fused_computation.648
  %get-tuple-element.812 = f16[4,4,32,1024]{3,2,1,0} get-tuple-element((f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) %fusion.647), index=0
  %cublas-batch-gemm.1 = f16[4,4,1024,1024]{3,2,1,0} custom-call(f16[4,4,1024,32]{3,2,1,0} %fusion.648, f16[4,4,32,1024]{3,2,1,0} %get-tuple-element.812), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/dot_general[dimension_numbers=(((3,), (3,)), ((0, 2), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %param.13 = s32[4,1024]{1,0} parameter(233), sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}
  %fusion.717 = (f16[4,4,1024]{2,1,0}, f16[4,4,1024,1024]{3,2,1,0}) fusion(f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.1, s32[4,1024]{1,0} %param.13), kind=kInput, calls=%fused_computation.717, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %get-tuple-element.811 = f16[4,4,1024,1024]{3,2,1,0} get-tuple-element((f16[4,4,1024]{2,1,0}, f16[4,4,1024,1024]{3,2,1,0}) %fusion.717), index=1
  %get-tuple-element.810 = f16[4,4,1024]{2,1,0} get-tuple-element((f16[4,4,1024]{2,1,0}, f16[4,4,1024,1024]{3,2,1,0}) %fusion.717), index=0
  %fusion.644 = f32[4,4,1024]{2,1,0} fusion(f16[4,4,1024,1024]{3,2,1,0} %get-tuple-element.811, f16[4,4,1024]{2,1,0} %get-tuple-element.810), kind=kInput, calls=%fused_computation.644, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %fusion.643 = f16[4,4,1024,1024]{3,2,1,0} fusion(f32[4,4,1024]{2,1,0} %fusion.644, f16[4,4,1024,1024]{3,2,1,0} %get-tuple-element.811, f16[4,4,1024]{2,1,0} %get-tuple-element.810), kind=kLoop, calls=%fused_computation.643, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %cublas-batch-gemm.3 = f16[4,4,32,1024]{3,2,1,0} custom-call(f16[4,4,32,1024]{3,2,1,0} %get-tuple-element.813, f16[4,4,1024,1024]{3,2,1,0} %fusion.643), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/dot_general[dimension_numbers=(((1,), (3,)), ((0, 2), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"3\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.642 = f16[4096,128]{1,0} fusion(f16[4,4,32,1024]{3,2,1,0} %cublas-batch-gemm.3), kind=kLoop, calls=%fused_computation.642
  %param.14 = f32[128,1024]{1,0} parameter(9), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.53 = f16[128,1024]{1,0} convert(f32[128,1024]{1,0} %param.14), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %cublas-gemm.11 = f16[4096,1024]{1,0} custom-call(f16[4096,128]{1,0} %fusion.642, f16[128,1024]{1,0} %convert.53), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.4 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.11), channel_id=5, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.2, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.639 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) fusion(f16[4,1024,1024]{2,1,0} %fusion.652, f32[1024]{0} %param.15, f16[4096,1024]{1,0} %all-reduce.4), kind=kInput, calls=%fused_computation.639, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %get-tuple-element.808 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.639), index=0
  %get-tuple-element.809 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.639), index=1
  %fusion.638 = f16[4,1024,1024]{2,1,0} fusion(f32[1024]{0} %param.17, f32[1024]{0} %param.16, f32[4,1024]{1,0} %get-tuple-element.808, f32[4,1024]{1,0} %get-tuple-element.809, f16[4,1024,1024]{2,1,0} %fusion.652, /*index=5*/f32[1024]{0} %param.15, f16[4096,1024]{1,0} %all-reduce.4), kind=kLoop, calls=%fused_computation.638, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %param.21 = f32[1024]{0} parameter(16), sharding={replicated}
  %bitcast.70 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %fusion.638)
  %param.18 = f32[1024,512]{1,0} parameter(13), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.57 = f16[1024,512]{1,0} convert(f32[1024,512]{1,0} %param.18), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %param.19 = f32[512]{0} parameter(12), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %fusion.637 = f16[4096,512]{1,0} fusion(f32[512]{0} %param.19), kind=kLoop, calls=%fused_computation.637
  %cublas-gemm.15 = f16[4096,512]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.70, f16[1024,512]{1,0} %convert.57, f16[4096,512]{1,0} %fusion.637), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.636 = f16[4096,512]{1,0} fusion(f16[4096,512]{1,0} %cublas-gemm.15), kind=kLoop, calls=%fused_computation.636
  %param.20 = f32[512,1024]{1,0} parameter(17), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.61 = f16[512,1024]{1,0} convert(f32[512,1024]{1,0} %param.20), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %cublas-gemm.17 = f16[4096,1024]{1,0} custom-call(f16[4096,512]{1,0} %fusion.636, f16[512,1024]{1,0} %convert.61), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.5 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.17), channel_id=6, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.3, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.633 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) fusion(f16[4,1024,1024]{2,1,0} %fusion.638, f32[1024]{0} %param.21, f16[4096,1024]{1,0} %all-reduce.5), kind=kInput, calls=%fused_computation.633, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %get-tuple-element.806 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.633), index=0
  %get-tuple-element.807 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.633), index=1
  %fusion.632 = f16[4,1024,1024]{2,1,0} fusion(f32[1024]{0} %param.23, f32[1024]{0} %param.22, f32[4,1024]{1,0} %get-tuple-element.806, f32[4,1024]{1,0} %get-tuple-element.807, f16[4,1024,1024]{2,1,0} %fusion.638, /*index=5*/f32[1024]{0} %param.21, f16[4096,1024]{1,0} %all-reduce.5), kind=kLoop, calls=%fused_computation.632, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %param.27 = f32[1024]{0} parameter(20), sharding={replicated}
  %bitcast.74 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %fusion.632)
  %param.24 = f32[1024,384]{1,0} parameter(23), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.65 = f16[1024,384]{1,0} convert(f32[1024,384]{1,0} %param.24), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %param.25 = f32[384]{0} parameter(22), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %fusion.631 = f16[4096,384]{1,0} fusion(f32[384]{0} %param.25), kind=kLoop, calls=%fused_computation.631
  %cublas-gemm.21 = f16[4096,384]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.74, f16[1024,384]{1,0} %convert.65, f16[4096,384]{1,0} %fusion.631), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.627 = (f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) fusion(f16[4096,384]{1,0} %cublas-gemm.21), kind=kLoop, calls=%fused_computation.627
  %get-tuple-element.805 = f16[4,4,32,1024]{3,2,1,0} get-tuple-element((f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) %fusion.627), index=1
  %fusion.628 = f16[4,4,1024,32]{3,2,1,0} fusion(f16[4096,384]{1,0} %cublas-gemm.21), kind=kLoop, calls=%fused_computation.628
  %get-tuple-element.804 = f16[4,4,32,1024]{3,2,1,0} get-tuple-element((f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) %fusion.627), index=0
  %cublas-batch-gemm.5 = f16[4,4,1024,1024]{3,2,1,0} custom-call(f16[4,4,1024,32]{3,2,1,0} %fusion.628, f16[4,4,32,1024]{3,2,1,0} %get-tuple-element.804), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/dot_general[dimension_numbers=(((3,), (3,)), ((0, 2), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.716 = (f16[4,4,1024]{2,1,0}, f16[4,4,1024,1024]{3,2,1,0}) fusion(f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.5, s32[4,1024]{1,0} %param.13), kind=kInput, calls=%fused_computation.716, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %get-tuple-element.803 = f16[4,4,1024,1024]{3,2,1,0} get-tuple-element((f16[4,4,1024]{2,1,0}, f16[4,4,1024,1024]{3,2,1,0}) %fusion.716), index=1
  %get-tuple-element.802 = f16[4,4,1024]{2,1,0} get-tuple-element((f16[4,4,1024]{2,1,0}, f16[4,4,1024,1024]{3,2,1,0}) %fusion.716), index=0
  %fusion.625 = f32[4,4,1024]{2,1,0} fusion(f16[4,4,1024,1024]{3,2,1,0} %get-tuple-element.803, f16[4,4,1024]{2,1,0} %get-tuple-element.802), kind=kInput, calls=%fused_computation.625, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %fusion.624 = f16[4,4,1024,1024]{3,2,1,0} fusion(f32[4,4,1024]{2,1,0} %fusion.625, f16[4,4,1024,1024]{3,2,1,0} %get-tuple-element.803, f16[4,4,1024]{2,1,0} %get-tuple-element.802), kind=kLoop, calls=%fused_computation.624, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %cublas-batch-gemm.7 = f16[4,4,32,1024]{3,2,1,0} custom-call(f16[4,4,32,1024]{3,2,1,0} %get-tuple-element.805, f16[4,4,1024,1024]{3,2,1,0} %fusion.624), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/dot_general[dimension_numbers=(((1,), (3,)), ((0, 2), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"3\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.623 = f16[4096,128]{1,0} fusion(f16[4,4,32,1024]{3,2,1,0} %cublas-batch-gemm.7), kind=kLoop, calls=%fused_computation.623
  %param.26 = f32[128,1024]{1,0} parameter(21), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.69 = f16[128,1024]{1,0} convert(f32[128,1024]{1,0} %param.26), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %cublas-gemm.23 = f16[4096,1024]{1,0} custom-call(f16[4096,128]{1,0} %fusion.623, f16[128,1024]{1,0} %convert.69), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.6 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.23), channel_id=7, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.4, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.620 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) fusion(f16[4,1024,1024]{2,1,0} %fusion.632, f32[1024]{0} %param.27, f16[4096,1024]{1,0} %all-reduce.6), kind=kInput, calls=%fused_computation.620, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %get-tuple-element.800 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.620), index=0
  %get-tuple-element.801 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.620), index=1
  %fusion.619 = f16[4,1024,1024]{2,1,0} fusion(f32[1024]{0} %param.29, f32[1024]{0} %param.28, f32[4,1024]{1,0} %get-tuple-element.800, f32[4,1024]{1,0} %get-tuple-element.801, f16[4,1024,1024]{2,1,0} %fusion.632, /*index=5*/f32[1024]{0} %param.27, f16[4096,1024]{1,0} %all-reduce.6), kind=kLoop, calls=%fused_computation.619, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %param.33 = f32[1024]{0} parameter(28), sharding={replicated}
  %bitcast.86 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %fusion.619)
  %param.30 = f32[1024,512]{1,0} parameter(25), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.73 = f16[1024,512]{1,0} convert(f32[1024,512]{1,0} %param.30), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %param.31 = f32[512]{0} parameter(24), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %fusion.618 = f16[4096,512]{1,0} fusion(f32[512]{0} %param.31), kind=kLoop, calls=%fused_computation.618
  %cublas-gemm.27 = f16[4096,512]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.86, f16[1024,512]{1,0} %convert.73, f16[4096,512]{1,0} %fusion.618), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.617 = f16[4096,512]{1,0} fusion(f16[4096,512]{1,0} %cublas-gemm.27), kind=kLoop, calls=%fused_computation.617
  %param.32 = f32[512,1024]{1,0} parameter(29), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.77 = f16[512,1024]{1,0} convert(f32[512,1024]{1,0} %param.32), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %cublas-gemm.29 = f16[4096,1024]{1,0} custom-call(f16[4096,512]{1,0} %fusion.617, f16[512,1024]{1,0} %convert.77), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.7 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.29), channel_id=8, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.5, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.614 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) fusion(f16[4,1024,1024]{2,1,0} %fusion.619, f32[1024]{0} %param.33, f16[4096,1024]{1,0} %all-reduce.7), kind=kInput, calls=%fused_computation.614, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %get-tuple-element.798 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.614), index=0
  %get-tuple-element.799 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.614), index=1
  %fusion.613 = f16[4,1024,1024]{2,1,0} fusion(f32[1024]{0} %param.35, f32[1024]{0} %param.34, f32[4,1024]{1,0} %get-tuple-element.798, f32[4,1024]{1,0} %get-tuple-element.799, f16[4,1024,1024]{2,1,0} %fusion.619, /*index=5*/f32[1024]{0} %param.33, f16[4096,1024]{1,0} %all-reduce.7), kind=kLoop, calls=%fused_computation.613, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %param.39 = f32[1024]{0} parameter(32), sharding={replicated}
  %bitcast.90 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %fusion.613)
  %param.36 = f32[1024,384]{1,0} parameter(35), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.81 = f16[1024,384]{1,0} convert(f32[1024,384]{1,0} %param.36), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %param.37 = f32[384]{0} parameter(34), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %fusion.612 = f16[4096,384]{1,0} fusion(f32[384]{0} %param.37), kind=kLoop, calls=%fused_computation.612
  %cublas-gemm.33 = f16[4096,384]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.90, f16[1024,384]{1,0} %convert.81, f16[4096,384]{1,0} %fusion.612), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.608 = (f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) fusion(f16[4096,384]{1,0} %cublas-gemm.33), kind=kLoop, calls=%fused_computation.608
  %get-tuple-element.797 = f16[4,4,32,1024]{3,2,1,0} get-tuple-element((f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) %fusion.608), index=1
  %fusion.609 = f16[4,4,1024,32]{3,2,1,0} fusion(f16[4096,384]{1,0} %cublas-gemm.33), kind=kLoop, calls=%fused_computation.609
  %get-tuple-element.796 = f16[4,4,32,1024]{3,2,1,0} get-tuple-element((f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) %fusion.608), index=0
  %cublas-batch-gemm.9 = f16[4,4,1024,1024]{3,2,1,0} custom-call(f16[4,4,1024,32]{3,2,1,0} %fusion.609, f16[4,4,32,1024]{3,2,1,0} %get-tuple-element.796), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/dot_general[dimension_numbers=(((3,), (3,)), ((0, 2), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.715 = (f16[4,4,1024]{2,1,0}, f16[4,4,1024,1024]{3,2,1,0}) fusion(f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.9, s32[4,1024]{1,0} %param.13), kind=kInput, calls=%fused_computation.715, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %get-tuple-element.795 = f16[4,4,1024,1024]{3,2,1,0} get-tuple-element((f16[4,4,1024]{2,1,0}, f16[4,4,1024,1024]{3,2,1,0}) %fusion.715), index=1
  %get-tuple-element.794 = f16[4,4,1024]{2,1,0} get-tuple-element((f16[4,4,1024]{2,1,0}, f16[4,4,1024,1024]{3,2,1,0}) %fusion.715), index=0
  %fusion.606 = f32[4,4,1024]{2,1,0} fusion(f16[4,4,1024,1024]{3,2,1,0} %get-tuple-element.795, f16[4,4,1024]{2,1,0} %get-tuple-element.794), kind=kInput, calls=%fused_computation.606, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %fusion.605 = f16[4,4,1024,1024]{3,2,1,0} fusion(f32[4,4,1024]{2,1,0} %fusion.606, f16[4,4,1024,1024]{3,2,1,0} %get-tuple-element.795, f16[4,4,1024]{2,1,0} %get-tuple-element.794), kind=kLoop, calls=%fused_computation.605, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %cublas-batch-gemm.11 = f16[4,4,32,1024]{3,2,1,0} custom-call(f16[4,4,32,1024]{3,2,1,0} %get-tuple-element.797, f16[4,4,1024,1024]{3,2,1,0} %fusion.605), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/dot_general[dimension_numbers=(((1,), (3,)), ((0, 2), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"3\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.604 = f16[4096,128]{1,0} fusion(f16[4,4,32,1024]{3,2,1,0} %cublas-batch-gemm.11), kind=kLoop, calls=%fused_computation.604
  %param.38 = f32[128,1024]{1,0} parameter(33), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.85 = f16[128,1024]{1,0} convert(f32[128,1024]{1,0} %param.38), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %cublas-gemm.35 = f16[4096,1024]{1,0} custom-call(f16[4096,128]{1,0} %fusion.604, f16[128,1024]{1,0} %convert.85), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.8 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.35), channel_id=9, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.6, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.601 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) fusion(f16[4,1024,1024]{2,1,0} %fusion.613, f32[1024]{0} %param.39, f16[4096,1024]{1,0} %all-reduce.8), kind=kInput, calls=%fused_computation.601, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %get-tuple-element.792 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.601), index=0
  %get-tuple-element.793 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.601), index=1
  %fusion.600 = f16[4,1024,1024]{2,1,0} fusion(f32[1024]{0} %param.41, f32[1024]{0} %param.40, f32[4,1024]{1,0} %get-tuple-element.792, f32[4,1024]{1,0} %get-tuple-element.793, f16[4,1024,1024]{2,1,0} %fusion.613, /*index=5*/f32[1024]{0} %param.39, f16[4096,1024]{1,0} %all-reduce.8), kind=kLoop, calls=%fused_computation.600, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %param.45 = f32[1024]{0} parameter(40), sharding={replicated}
  %bitcast.102 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %fusion.600)
  %param.42 = f32[1024,512]{1,0} parameter(37), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.89 = f16[1024,512]{1,0} convert(f32[1024,512]{1,0} %param.42), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %param.43 = f32[512]{0} parameter(36), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %fusion.599 = f16[4096,512]{1,0} fusion(f32[512]{0} %param.43), kind=kLoop, calls=%fused_computation.599
  %cublas-gemm.39 = f16[4096,512]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.102, f16[1024,512]{1,0} %convert.89, f16[4096,512]{1,0} %fusion.599), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.598 = f16[4096,512]{1,0} fusion(f16[4096,512]{1,0} %cublas-gemm.39), kind=kLoop, calls=%fused_computation.598
  %param.44 = f32[512,1024]{1,0} parameter(41), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.93 = f16[512,1024]{1,0} convert(f32[512,1024]{1,0} %param.44), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %cublas-gemm.41 = f16[4096,1024]{1,0} custom-call(f16[4096,512]{1,0} %fusion.598, f16[512,1024]{1,0} %convert.93), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.9 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.41), channel_id=10, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.7, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.595 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) fusion(f16[4,1024,1024]{2,1,0} %fusion.600, f32[1024]{0} %param.45, f16[4096,1024]{1,0} %all-reduce.9), kind=kInput, calls=%fused_computation.595, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %get-tuple-element.790 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.595), index=0
  %get-tuple-element.791 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.595), index=1
  %fusion.594 = f16[4,1024,1024]{2,1,0} fusion(f32[1024]{0} %param.47, f32[1024]{0} %param.46, f32[4,1024]{1,0} %get-tuple-element.790, f32[4,1024]{1,0} %get-tuple-element.791, f16[4,1024,1024]{2,1,0} %fusion.600, /*index=5*/f32[1024]{0} %param.45, f16[4096,1024]{1,0} %all-reduce.9), kind=kLoop, calls=%fused_computation.594, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %param.51 = f32[1024]{0} parameter(44), sharding={replicated}
  %bitcast.106 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %fusion.594)
  %param.48 = f32[1024,384]{1,0} parameter(47), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.97 = f16[1024,384]{1,0} convert(f32[1024,384]{1,0} %param.48), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %param.49 = f32[384]{0} parameter(46), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %fusion.593 = f16[4096,384]{1,0} fusion(f32[384]{0} %param.49), kind=kLoop, calls=%fused_computation.593
  %cublas-gemm.45 = f16[4096,384]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.106, f16[1024,384]{1,0} %convert.97, f16[4096,384]{1,0} %fusion.593), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.589 = (f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) fusion(f16[4096,384]{1,0} %cublas-gemm.45), kind=kLoop, calls=%fused_computation.589
  %get-tuple-element.789 = f16[4,4,32,1024]{3,2,1,0} get-tuple-element((f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) %fusion.589), index=1
  %fusion.590 = f16[4,4,1024,32]{3,2,1,0} fusion(f16[4096,384]{1,0} %cublas-gemm.45), kind=kLoop, calls=%fused_computation.590
  %get-tuple-element.788 = f16[4,4,32,1024]{3,2,1,0} get-tuple-element((f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) %fusion.589), index=0
  %cublas-batch-gemm.13 = f16[4,4,1024,1024]{3,2,1,0} custom-call(f16[4,4,1024,32]{3,2,1,0} %fusion.590, f16[4,4,32,1024]{3,2,1,0} %get-tuple-element.788), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/dot_general[dimension_numbers=(((3,), (3,)), ((0, 2), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.714 = (f16[4,4,1024]{2,1,0}, f16[4,4,1024,1024]{3,2,1,0}) fusion(f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.13, s32[4,1024]{1,0} %param.13), kind=kInput, calls=%fused_computation.714, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %get-tuple-element.787 = f16[4,4,1024,1024]{3,2,1,0} get-tuple-element((f16[4,4,1024]{2,1,0}, f16[4,4,1024,1024]{3,2,1,0}) %fusion.714), index=1
  %get-tuple-element.786 = f16[4,4,1024]{2,1,0} get-tuple-element((f16[4,4,1024]{2,1,0}, f16[4,4,1024,1024]{3,2,1,0}) %fusion.714), index=0
  %fusion.587 = f32[4,4,1024]{2,1,0} fusion(f16[4,4,1024,1024]{3,2,1,0} %get-tuple-element.787, f16[4,4,1024]{2,1,0} %get-tuple-element.786), kind=kInput, calls=%fused_computation.587, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %fusion.586 = f16[4,4,1024,1024]{3,2,1,0} fusion(f32[4,4,1024]{2,1,0} %fusion.587, f16[4,4,1024,1024]{3,2,1,0} %get-tuple-element.787, f16[4,4,1024]{2,1,0} %get-tuple-element.786), kind=kLoop, calls=%fused_computation.586, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %cublas-batch-gemm.15 = f16[4,4,32,1024]{3,2,1,0} custom-call(f16[4,4,32,1024]{3,2,1,0} %get-tuple-element.789, f16[4,4,1024,1024]{3,2,1,0} %fusion.586), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/dot_general[dimension_numbers=(((1,), (3,)), ((0, 2), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"3\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.585 = f16[4096,128]{1,0} fusion(f16[4,4,32,1024]{3,2,1,0} %cublas-batch-gemm.15), kind=kLoop, calls=%fused_computation.585
  %param.50 = f32[128,1024]{1,0} parameter(45), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.101 = f16[128,1024]{1,0} convert(f32[128,1024]{1,0} %param.50), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %cublas-gemm.47 = f16[4096,1024]{1,0} custom-call(f16[4096,128]{1,0} %fusion.585, f16[128,1024]{1,0} %convert.101), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.10 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.47), channel_id=11, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.8, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.582 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) fusion(f16[4,1024,1024]{2,1,0} %fusion.594, f32[1024]{0} %param.51, f16[4096,1024]{1,0} %all-reduce.10), kind=kInput, calls=%fused_computation.582, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %get-tuple-element.784 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.582), index=0
  %get-tuple-element.785 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.582), index=1
  %fusion.581 = f16[4,1024,1024]{2,1,0} fusion(f32[1024]{0} %param.53, f32[1024]{0} %param.52, f32[4,1024]{1,0} %get-tuple-element.784, f32[4,1024]{1,0} %get-tuple-element.785, f16[4,1024,1024]{2,1,0} %fusion.594, /*index=5*/f32[1024]{0} %param.51, f16[4096,1024]{1,0} %all-reduce.10), kind=kLoop, calls=%fused_computation.581, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %param.57 = f32[1024]{0} parameter(52), sharding={replicated}
  %bitcast.118 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %fusion.581)
  %param.54 = f32[1024,512]{1,0} parameter(49), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.105 = f16[1024,512]{1,0} convert(f32[1024,512]{1,0} %param.54), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %param.55 = f32[512]{0} parameter(48), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %fusion.580 = f16[4096,512]{1,0} fusion(f32[512]{0} %param.55), kind=kLoop, calls=%fused_computation.580
  %cublas-gemm.51 = f16[4096,512]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.118, f16[1024,512]{1,0} %convert.105, f16[4096,512]{1,0} %fusion.580), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.579 = f16[4096,512]{1,0} fusion(f16[4096,512]{1,0} %cublas-gemm.51), kind=kLoop, calls=%fused_computation.579
  %param.56 = f32[512,1024]{1,0} parameter(53), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.109 = f16[512,1024]{1,0} convert(f32[512,1024]{1,0} %param.56), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %cublas-gemm.53 = f16[4096,1024]{1,0} custom-call(f16[4096,512]{1,0} %fusion.579, f16[512,1024]{1,0} %convert.109), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.11 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.53), channel_id=12, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.9, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.576 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) fusion(f16[4,1024,1024]{2,1,0} %fusion.581, f32[1024]{0} %param.57, f16[4096,1024]{1,0} %all-reduce.11), kind=kInput, calls=%fused_computation.576, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %get-tuple-element.782 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.576), index=0
  %get-tuple-element.783 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.576), index=1
  %fusion.575 = f16[4,1024,1024]{2,1,0} fusion(f32[1024]{0} %param.59, f32[1024]{0} %param.58, f32[4,1024]{1,0} %get-tuple-element.782, f32[4,1024]{1,0} %get-tuple-element.783, f16[4,1024,1024]{2,1,0} %fusion.581, /*index=5*/f32[1024]{0} %param.57, f16[4096,1024]{1,0} %all-reduce.11), kind=kLoop, calls=%fused_computation.575, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %param.63 = f32[1024]{0} parameter(56), sharding={replicated}
  %bitcast.122 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %fusion.575)
  %param.60 = f32[1024,384]{1,0} parameter(59), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.113 = f16[1024,384]{1,0} convert(f32[1024,384]{1,0} %param.60), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %param.61 = f32[384]{0} parameter(58), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %fusion.574 = f16[4096,384]{1,0} fusion(f32[384]{0} %param.61), kind=kLoop, calls=%fused_computation.574
  %cublas-gemm.57 = f16[4096,384]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.122, f16[1024,384]{1,0} %convert.113, f16[4096,384]{1,0} %fusion.574), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.570 = (f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) fusion(f16[4096,384]{1,0} %cublas-gemm.57), kind=kLoop, calls=%fused_computation.570
  %get-tuple-element.781 = f16[4,4,32,1024]{3,2,1,0} get-tuple-element((f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) %fusion.570), index=1
  %fusion.571 = f16[4,4,1024,32]{3,2,1,0} fusion(f16[4096,384]{1,0} %cublas-gemm.57), kind=kLoop, calls=%fused_computation.571
  %get-tuple-element.780 = f16[4,4,32,1024]{3,2,1,0} get-tuple-element((f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) %fusion.570), index=0
  %cublas-batch-gemm.17 = f16[4,4,1024,1024]{3,2,1,0} custom-call(f16[4,4,1024,32]{3,2,1,0} %fusion.571, f16[4,4,32,1024]{3,2,1,0} %get-tuple-element.780), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/dot_general[dimension_numbers=(((3,), (3,)), ((0, 2), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.713 = (f16[4,4,1024]{2,1,0}, f16[4,4,1024,1024]{3,2,1,0}) fusion(f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.17, s32[4,1024]{1,0} %param.13), kind=kInput, calls=%fused_computation.713, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %get-tuple-element.779 = f16[4,4,1024,1024]{3,2,1,0} get-tuple-element((f16[4,4,1024]{2,1,0}, f16[4,4,1024,1024]{3,2,1,0}) %fusion.713), index=1
  %get-tuple-element.778 = f16[4,4,1024]{2,1,0} get-tuple-element((f16[4,4,1024]{2,1,0}, f16[4,4,1024,1024]{3,2,1,0}) %fusion.713), index=0
  %fusion.568 = f32[4,4,1024]{2,1,0} fusion(f16[4,4,1024,1024]{3,2,1,0} %get-tuple-element.779, f16[4,4,1024]{2,1,0} %get-tuple-element.778), kind=kInput, calls=%fused_computation.568, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %fusion.567 = f16[4,4,1024,1024]{3,2,1,0} fusion(f32[4,4,1024]{2,1,0} %fusion.568, f16[4,4,1024,1024]{3,2,1,0} %get-tuple-element.779, f16[4,4,1024]{2,1,0} %get-tuple-element.778), kind=kLoop, calls=%fused_computation.567, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %cublas-batch-gemm.19 = f16[4,4,32,1024]{3,2,1,0} custom-call(f16[4,4,32,1024]{3,2,1,0} %get-tuple-element.781, f16[4,4,1024,1024]{3,2,1,0} %fusion.567), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/dot_general[dimension_numbers=(((1,), (3,)), ((0, 2), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"3\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.566 = f16[4096,128]{1,0} fusion(f16[4,4,32,1024]{3,2,1,0} %cublas-batch-gemm.19), kind=kLoop, calls=%fused_computation.566
  %param.62 = f32[128,1024]{1,0} parameter(57), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.117 = f16[128,1024]{1,0} convert(f32[128,1024]{1,0} %param.62), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %cublas-gemm.59 = f16[4096,1024]{1,0} custom-call(f16[4096,128]{1,0} %fusion.566, f16[128,1024]{1,0} %convert.117), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.12 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.59), channel_id=13, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.10, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.563 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) fusion(f16[4,1024,1024]{2,1,0} %fusion.575, f32[1024]{0} %param.63, f16[4096,1024]{1,0} %all-reduce.12), kind=kInput, calls=%fused_computation.563, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %get-tuple-element.776 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.563), index=0
  %get-tuple-element.777 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.563), index=1
  %fusion.562 = f16[4,1024,1024]{2,1,0} fusion(f32[1024]{0} %param.65, f32[1024]{0} %param.64, f32[4,1024]{1,0} %get-tuple-element.776, f32[4,1024]{1,0} %get-tuple-element.777, f16[4,1024,1024]{2,1,0} %fusion.575, /*index=5*/f32[1024]{0} %param.63, f16[4096,1024]{1,0} %all-reduce.12), kind=kLoop, calls=%fused_computation.562, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %param.69 = f32[1024]{0} parameter(64), sharding={replicated}
  %bitcast.134 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %fusion.562)
  %param.66 = f32[1024,512]{1,0} parameter(61), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.121 = f16[1024,512]{1,0} convert(f32[1024,512]{1,0} %param.66), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %param.67 = f32[512]{0} parameter(60), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %fusion.561 = f16[4096,512]{1,0} fusion(f32[512]{0} %param.67), kind=kLoop, calls=%fused_computation.561
  %cublas-gemm.63 = f16[4096,512]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.134, f16[1024,512]{1,0} %convert.121, f16[4096,512]{1,0} %fusion.561), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.560 = f16[4096,512]{1,0} fusion(f16[4096,512]{1,0} %cublas-gemm.63), kind=kLoop, calls=%fused_computation.560
  %param.68 = f32[512,1024]{1,0} parameter(65), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.125 = f16[512,1024]{1,0} convert(f32[512,1024]{1,0} %param.68), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %cublas-gemm.65 = f16[4096,1024]{1,0} custom-call(f16[4096,512]{1,0} %fusion.560, f16[512,1024]{1,0} %convert.125), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.13 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.65), channel_id=14, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.11, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.557 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) fusion(f16[4,1024,1024]{2,1,0} %fusion.562, f32[1024]{0} %param.69, f16[4096,1024]{1,0} %all-reduce.13), kind=kInput, calls=%fused_computation.557, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %get-tuple-element.774 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.557), index=0
  %get-tuple-element.775 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.557), index=1
  %fusion.556 = f16[4,1024,1024]{2,1,0} fusion(f32[1024]{0} %param.71, f32[1024]{0} %param.70, f32[4,1024]{1,0} %get-tuple-element.774, f32[4,1024]{1,0} %get-tuple-element.775, f16[4,1024,1024]{2,1,0} %fusion.562, /*index=5*/f32[1024]{0} %param.69, f16[4096,1024]{1,0} %all-reduce.13), kind=kLoop, calls=%fused_computation.556, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %param.75 = f32[1024]{0} parameter(68), sharding={replicated}
  %bitcast.138 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %fusion.556)
  %param.72 = f32[1024,384]{1,0} parameter(71), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.129 = f16[1024,384]{1,0} convert(f32[1024,384]{1,0} %param.72), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %param.73 = f32[384]{0} parameter(70), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %fusion.555 = f16[4096,384]{1,0} fusion(f32[384]{0} %param.73), kind=kLoop, calls=%fused_computation.555
  %cublas-gemm.69 = f16[4096,384]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.138, f16[1024,384]{1,0} %convert.129, f16[4096,384]{1,0} %fusion.555), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.551 = (f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) fusion(f16[4096,384]{1,0} %cublas-gemm.69), kind=kLoop, calls=%fused_computation.551
  %get-tuple-element.773 = f16[4,4,32,1024]{3,2,1,0} get-tuple-element((f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) %fusion.551), index=1
  %fusion.552 = f16[4,4,1024,32]{3,2,1,0} fusion(f16[4096,384]{1,0} %cublas-gemm.69), kind=kLoop, calls=%fused_computation.552
  %get-tuple-element.772 = f16[4,4,32,1024]{3,2,1,0} get-tuple-element((f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) %fusion.551), index=0
  %cublas-batch-gemm.21 = f16[4,4,1024,1024]{3,2,1,0} custom-call(f16[4,4,1024,32]{3,2,1,0} %fusion.552, f16[4,4,32,1024]{3,2,1,0} %get-tuple-element.772), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/dot_general[dimension_numbers=(((3,), (3,)), ((0, 2), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.712 = (f16[4,4,1024]{2,1,0}, f16[4,4,1024,1024]{3,2,1,0}) fusion(f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.21, s32[4,1024]{1,0} %param.13), kind=kInput, calls=%fused_computation.712, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %get-tuple-element.771 = f16[4,4,1024,1024]{3,2,1,0} get-tuple-element((f16[4,4,1024]{2,1,0}, f16[4,4,1024,1024]{3,2,1,0}) %fusion.712), index=1
  %get-tuple-element.770 = f16[4,4,1024]{2,1,0} get-tuple-element((f16[4,4,1024]{2,1,0}, f16[4,4,1024,1024]{3,2,1,0}) %fusion.712), index=0
  %fusion.549 = f32[4,4,1024]{2,1,0} fusion(f16[4,4,1024,1024]{3,2,1,0} %get-tuple-element.771, f16[4,4,1024]{2,1,0} %get-tuple-element.770), kind=kInput, calls=%fused_computation.549, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %fusion.548 = f16[4,4,1024,1024]{3,2,1,0} fusion(f32[4,4,1024]{2,1,0} %fusion.549, f16[4,4,1024,1024]{3,2,1,0} %get-tuple-element.771, f16[4,4,1024]{2,1,0} %get-tuple-element.770), kind=kLoop, calls=%fused_computation.548, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %cublas-batch-gemm.23 = f16[4,4,32,1024]{3,2,1,0} custom-call(f16[4,4,32,1024]{3,2,1,0} %get-tuple-element.773, f16[4,4,1024,1024]{3,2,1,0} %fusion.548), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/dot_general[dimension_numbers=(((1,), (3,)), ((0, 2), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"3\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.547 = f16[4096,128]{1,0} fusion(f16[4,4,32,1024]{3,2,1,0} %cublas-batch-gemm.23), kind=kLoop, calls=%fused_computation.547
  %param.74 = f32[128,1024]{1,0} parameter(69), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.133 = f16[128,1024]{1,0} convert(f32[128,1024]{1,0} %param.74), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %cublas-gemm.71 = f16[4096,1024]{1,0} custom-call(f16[4096,128]{1,0} %fusion.547, f16[128,1024]{1,0} %convert.133), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.14 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.71), channel_id=15, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.12, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.544 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) fusion(f16[4,1024,1024]{2,1,0} %fusion.556, f32[1024]{0} %param.75, f16[4096,1024]{1,0} %all-reduce.14), kind=kInput, calls=%fused_computation.544, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %get-tuple-element.768 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.544), index=0
  %get-tuple-element.769 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.544), index=1
  %fusion.543 = f16[4,1024,1024]{2,1,0} fusion(f32[1024]{0} %param.77, f32[1024]{0} %param.76, f32[4,1024]{1,0} %get-tuple-element.768, f32[4,1024]{1,0} %get-tuple-element.769, f16[4,1024,1024]{2,1,0} %fusion.556, /*index=5*/f32[1024]{0} %param.75, f16[4096,1024]{1,0} %all-reduce.14), kind=kLoop, calls=%fused_computation.543, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %param.81 = f32[1024]{0} parameter(76), sharding={replicated}
  %bitcast.150 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %fusion.543)
  %param.78 = f32[1024,512]{1,0} parameter(73), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.137 = f16[1024,512]{1,0} convert(f32[1024,512]{1,0} %param.78), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %param.79 = f32[512]{0} parameter(72), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %fusion.542 = f16[4096,512]{1,0} fusion(f32[512]{0} %param.79), kind=kLoop, calls=%fused_computation.542
  %cublas-gemm.75 = f16[4096,512]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.150, f16[1024,512]{1,0} %convert.137, f16[4096,512]{1,0} %fusion.542), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.541 = f16[4096,512]{1,0} fusion(f16[4096,512]{1,0} %cublas-gemm.75), kind=kLoop, calls=%fused_computation.541
  %param.80 = f32[512,1024]{1,0} parameter(77), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.141 = f16[512,1024]{1,0} convert(f32[512,1024]{1,0} %param.80), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %cublas-gemm.77 = f16[4096,1024]{1,0} custom-call(f16[4096,512]{1,0} %fusion.541, f16[512,1024]{1,0} %convert.141), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.15 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.77), channel_id=16, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.13, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.538 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) fusion(f16[4,1024,1024]{2,1,0} %fusion.543, f32[1024]{0} %param.81, f16[4096,1024]{1,0} %all-reduce.15), kind=kInput, calls=%fused_computation.538, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %get-tuple-element.766 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.538), index=0
  %get-tuple-element.767 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.538), index=1
  %fusion.537 = f16[4096,1024]{1,0} fusion(f32[1024]{0} %param.83, f32[1024]{0} %param.82, f32[4,1024]{1,0} %get-tuple-element.766, f32[4,1024]{1,0} %get-tuple-element.767, f16[4,1024,1024]{2,1,0} %fusion.543, /*index=5*/f32[1024]{0} %param.81, f16[4096,1024]{1,0} %all-reduce.15), kind=kLoop, calls=%fused_computation.537
  %fusion.536 = f16[4096,6400]{1,0} fusion(f32[6400]{0} %param.3), kind=kLoop, calls=%fused_computation.536
  %cublas-gemm.81 = f16[4096,6400]{1,0} custom-call(f16[4096,1024]{1,0} %fusion.537, f16[6400,1024]{1,0} %convert.44, f16[4096,6400]{1,0} %fusion.536), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/gpt_model.py" source_line=70}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.704 = f16[4,1024]{1,0} fusion(f16[4096,6400]{1,0} %cublas-gemm.81), kind=kInput, calls=%fused_computation.704, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/reduce_max[axes=(2,)]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %all-reduce.16 = f16[4,1024]{1,0} all-reduce(f16[4,1024]{1,0} %fusion.704), channel_id=17, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%region_38.1688, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/reduce_max[axes=(2,)]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %fusion.534 = f32[4,1024]{1,0} fusion(f16[4,1024]{1,0} %all-reduce.16, f16[4096,6400]{1,0} %cublas-gemm.81), kind=kInput, calls=%fused_computation.534, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(2,)]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %all-reduce.17 = f32[4,1024]{1,0} all-reduce(f32[4,1024]{1,0} %fusion.534), channel_id=18, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%region_39.1700, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(2,)]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %fusion.533 = f16[4,1024,6400]{2,1,0} fusion(f16[4,1024]{1,0} %bitcast.1689, f32[4,1024]{1,0} %all-reduce.17, f32[] %all-reduce, s32[4,1024]{1,0} %param.4, s32[16]{0} %constant_52, /*index=5*/u32[] %partition-id, f16[4,1024]{1,0} %all-reduce.16, f16[4096,6400]{1,0} %cublas-gemm.81), kind=kLoop, calls=%fused_computation.533, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add_any" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %fusion.703 = f16[6400]{0} fusion(f16[4,1024,6400]{2,1,0} %fusion.533), kind=kInput, calls=%fused_computation.703, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/reduce_sum[axes=(0, 1)]" source_file="/code/alpa/alpa/model/gpt_model.py" source_line=75}
  %constant_862 = f16[] constant(5.6562)
  %constant_59 = s32[] constant(0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %constant_179 = f16[] constant(-inf)
  %broadcast.580 = f16[8,1,1,1024]{3,0,2,1} broadcast(f16[] %constant_179), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=184}
  %constant_89 = f16[] constant(0)
  %broadcast.581 = f16[8,1,1,1024]{3,0,2,1} broadcast(f16[] %constant_89), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=183}
  %constant_863 = f32[] constant(1024)
  %constant_69 = f32[] constant(0)
  %constant_124 = f32[] constant(1e-12)
  %constant_864 = f16[] constant(1.4141)
  %constant_337 = f16[] constant(1)
  %constant_865 = f16[] constant(2)
  %broadcast.421 = f32[8,1024]{1,0} broadcast(f32[] %constant_69), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_68 = f32[] constant(1)
  %broadcast.419 = f32[8,1024]{1,0} broadcast(f32[] %constant_68), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/broadcast_in_dim[shape=(8, 1024) broadcast_dimensions=()]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_866 = f32[] constant(2)
  %broadcast.582 = f32[8,1024]{1,0} broadcast(f32[] %constant_866), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %bitcast.1 = f16[4096,6400]{1,0} bitcast(f16[4,1024,6400]{2,1,0} %fusion.533)
  %cublas-gemm.83 = f16[4096,1024]{1,0} custom-call(f16[4096,6400]{1,0} %bitcast.1, f16[6400,1024]{1,0} %convert.44), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/gpt_model.py" source_line=70}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.19 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.83), channel_id=20, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.14, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/gpt_model.py" source_line=70}
  %bitcast.157 = f16[4,1024,1024]{2,1,0} bitcast(f16[4096,1024]{1,0} %all-reduce.19), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/gpt_model.py" source_line=70}
  %tuple = (f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) tuple(f16[] %constant_862, s32[] %constant_59, f16[8,1,1,1024]{3,0,2,1} %broadcast.580, f16[8,1,1,1024]{3,0,2,1} %broadcast.581, f32[] %constant_863, /*index=5*/f32[] %constant_69, f32[] %constant_863, f32[] %constant_124, f16[] %constant_864, f16[] %constant_337, /*index=10*/f16[] %constant_865, f32[] %constant_863, f32[] %constant_69, f32[] %constant_863, f32[] %constant_124, /*index=15*/f32[8,1024]{1,0} %broadcast.421, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.582, f32[8,1024]{1,0} %broadcast.421, /*index=20*/f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.582, f32[1024]{0} %param.77, f32[1024]{0} %param.76, /*index=25*/f32[1024]{0} %param.75, f32[128,1024]{1,0} %param.74, f32[384]{0} %param.73, f32[1024,384]{1,0} %param.72, f32[512]{0} %param.79, /*index=30*/f32[1024,512]{1,0} %param.78, f32[1024]{0} %param.82, f32[1024]{0} %param.81, f32[512,1024]{1,0} %param.80, f16[4,1024,1024]{2,1,0} %fusion.556, /*index=35*/s32[4,1024]{1,0} %param.13, f16[4,1024,1024]{2,1,0} %bitcast.157), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %bitcast.1847 = (f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) bitcast((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %tuple)
  %get-tuple-element.25 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=31, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.28 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=14, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.15 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=23, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.14 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=24, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element = f16[4,1024,1024]{2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=34, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.9 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=25, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %bitcast.158 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %get-tuple-element)
  %get-tuple-element.1 = f32[1024,384]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=28, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.150 = f16[1024,384]{1,0} convert(f32[1024,384]{1,0} %get-tuple-element.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %get-tuple-element.2 = f32[384]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=27, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.526 = f16[4096,384]{1,0} fusion(f32[384]{0} %get-tuple-element.2), kind=kLoop, calls=%fused_computation.526
  %cublas-gemm.87 = f16[4096,384]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.158, f16[1024,384]{1,0} %convert.150, f16[4096,384]{1,0} %fusion.526), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.524 = (f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) fusion(f16[4096,384]{1,0} %cublas-gemm.87), kind=kLoop, calls=%fused_computation.524
  %get-tuple-element.566 = f16[4,4,32,1024]{3,2,1,0} get-tuple-element((f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) %fusion.524), index=0
  %get-tuple-element.3 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=0, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.485 = (f16[4,4,1024,32]{3,2,1,0}, f16[4,4,1024,32]{3,2,1,0}) fusion(f16[4096,384]{1,0} %cublas-gemm.87, f16[] %get-tuple-element.3), kind=kLoop, calls=%fused_computation.485
  %get-tuple-element.565 = f16[4,4,1024,32]{3,2,1,0} get-tuple-element((f16[4,4,1024,32]{3,2,1,0}, f16[4,4,1024,32]{3,2,1,0}) %fusion.485), index=1
  %get-tuple-element.567 = f16[4,4,32,1024]{3,2,1,0} get-tuple-element((f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) %fusion.524), index=1
  %get-tuple-element.7 = f16[8,1,1,1024]{3,0,2,1} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %constant_65 = u32[16]{0} constant({...}), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/gt" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=86}
  %fusion.522 = s32[] fusion(u32[16]{0} %constant_65, u32[] %partition-id), kind=kLoop, calls=%fused_computation.522, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/gt" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=86}
  %get-tuple-element.6 = f16[8,1,1,1024]{3,0,2,1} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=3, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.4 = s32[4,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=35, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.5 = s32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=1, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.521 = f16[4,4,1024,1024]{3,2,1,0} fusion(f16[8,1,1,1024]{3,0,2,1} %get-tuple-element.7, s32[] %fusion.522, f16[8,1,1,1024]{3,0,2,1} %get-tuple-element.6, s32[4,1024]{1,0} %get-tuple-element.4, s32[] %get-tuple-element.5), kind=kLoop, calls=%fused_computation.521
  %cublas-batch-gemm.27 = f16[4,4,1024,1024]{3,2,1,0} custom-call(f16[4,4,1024,32]{3,2,1,0} %get-tuple-element.565, f16[4,4,32,1024]{3,2,1,0} %get-tuple-element.567, f16[4,4,1024,1024]{3,2,1,0} %fusion.521), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/dot_general[dimension_numbers=(((3,), (3,)), ((0, 2), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %reduce.43 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.27, f16[] %constant_179), dimensions={3}, to_apply=%region_45.1817, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %fusion.519 = f32[4,4,1024]{2,1,0} fusion(f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.27, f16[4,4,1024]{2,1,0} %reduce.43), kind=kInput, calls=%fused_computation.519, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %fusion.701 = f16[4,4,1024,1024]{3,2,1,0} fusion(f32[4,4,1024]{2,1,0} %fusion.519, f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.27, f16[4,4,1024]{2,1,0} %reduce.43), kind=kLoop, calls=%fused_computation.701, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %cublas-batch-gemm.29 = f16[4,4,32,1024]{3,2,1,0} custom-call(f16[4,4,32,1024]{3,2,1,0} %get-tuple-element.566, f16[4,4,1024,1024]{3,2,1,0} %fusion.701), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/dot_general[dimension_numbers=(((1,), (3,)), ((0, 2), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"3\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.518 = f16[4096,128]{1,0} fusion(f16[4,4,32,1024]{3,2,1,0} %cublas-batch-gemm.29), kind=kLoop, calls=%fused_computation.518
  %get-tuple-element.8 = f32[128,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=26, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.154 = f16[128,1024]{1,0} convert(f32[128,1024]{1,0} %get-tuple-element.8), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %cublas-gemm.89 = f16[4096,1024]{1,0} custom-call(f16[4096,128]{1,0} %fusion.518, f16[128,1024]{1,0} %convert.154), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.20 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.89), channel_id=21, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.15, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.516 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) fusion(f16[4,1024,1024]{2,1,0} %get-tuple-element, f32[1024]{0} %get-tuple-element.9, f16[4096,1024]{1,0} %all-reduce.20), kind=kInput, calls=%fused_computation.516, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.561 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.516), index=0
  %get-tuple-element.10 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=4, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.563 = f16[4,1024,1024]{2,1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.516), index=2
  %get-tuple-element.13 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=7, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.562 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.516), index=1
  %get-tuple-element.12 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=6, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.11 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=5, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.510 = f16[4,1024,1024]{2,1,0} fusion(f32[1024]{0} %get-tuple-element.15, f32[1024]{0} %get-tuple-element.14, f32[4,1024]{1,0} %get-tuple-element.561, f32[] %get-tuple-element.10, f16[4,1024,1024]{2,1,0} %get-tuple-element.563, /*index=5*/f32[] %get-tuple-element.13, f32[4,1024]{1,0} %get-tuple-element.562, f32[] %get-tuple-element.12, f32[] %get-tuple-element.11), kind=kLoop, calls=%fused_computation.510, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %get-tuple-element.22 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=32, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.20 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=10, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %bitcast.175 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %fusion.510)
  %get-tuple-element.16 = f32[1024,512]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=30, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.158 = f16[1024,512]{1,0} convert(f32[1024,512]{1,0} %get-tuple-element.16), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %get-tuple-element.17 = f32[512]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=29, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.509 = f16[4096,512]{1,0} fusion(f32[512]{0} %get-tuple-element.17), kind=kLoop, calls=%fused_computation.509
  %cublas-gemm.93 = f16[4096,512]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.175, f16[1024,512]{1,0} %convert.158, f16[4096,512]{1,0} %fusion.509), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %get-tuple-element.19 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=9, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.18 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=8, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.507 = f16[4096,512]{1,0} fusion(f16[] %get-tuple-element.20, f16[4096,512]{1,0} %cublas-gemm.93, f16[] %get-tuple-element.19, f16[] %get-tuple-element.18), kind=kLoop, calls=%fused_computation.507
  %get-tuple-element.21 = f32[512,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=33, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.162 = f16[512,1024]{1,0} convert(f32[512,1024]{1,0} %get-tuple-element.21), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %cublas-gemm.95 = f16[4096,1024]{1,0} custom-call(f16[4096,512]{1,0} %fusion.507, f16[512,1024]{1,0} %convert.162), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.21 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.95), channel_id=22, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.16, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.505 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) fusion(f16[4,1024,1024]{2,1,0} %fusion.510, f32[1024]{0} %get-tuple-element.22, f16[4096,1024]{1,0} %all-reduce.21), kind=kInput, calls=%fused_computation.505, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.558 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.505), index=0
  %get-tuple-element.23 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=11, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.559 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.505), index=1
  %get-tuple-element.27 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=13, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.26 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=12, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.24 = f16[4,1024,1024]{2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=36, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.560 = f16[4,1024,1024]{2,1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.505), index=2
  %fusion.497 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) fusion(f32[1024]{0} %get-tuple-element.25, f32[] %get-tuple-element.28, f32[4,1024]{1,0} %get-tuple-element.558, f32[] %get-tuple-element.23, f32[4,1024]{1,0} %get-tuple-element.559, /*index=5*/f32[] %get-tuple-element.27, f32[] %get-tuple-element.26, f16[4,1024,1024]{2,1,0} %get-tuple-element.24, f16[4,1024,1024]{2,1,0} %get-tuple-element.560), kind=kInput, calls=%fused_computation.497, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %get-tuple-element.556 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.497), index=0
  %get-tuple-element.32 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=21, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.31 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=22, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.30 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=19, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.29 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=20, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.557 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.497), index=1
  %fusion.496 = f16[4,1024,1024]{2,1,0} fusion(f32[4,1024]{1,0} %get-tuple-element.556, f32[8,1024]{1,0} %get-tuple-element.32, s32[] %fusion.522, f32[8,1024]{1,0} %get-tuple-element.31, f32[8,1024]{1,0} %get-tuple-element.30, /*index=5*/f32[8,1024]{1,0} %get-tuple-element.29, f32[4,1024]{1,0} %get-tuple-element.557, f16[4,1024,1024]{2,1,0} %get-tuple-element.560, f32[] %get-tuple-element.27, f32[4,1024]{1,0} %get-tuple-element.558, /*index=10*/f32[] %get-tuple-element.23, f32[] %get-tuple-element.26, f32[4,1024]{1,0} %get-tuple-element.559, f32[] %get-tuple-element.28, f32[1024]{0} %get-tuple-element.25, /*index=15*/f16[4,1024,1024]{2,1,0} %get-tuple-element.24), kind=kLoop, calls=%fused_computation.496, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %fusion.667 = f16[1024]{0} fusion(f16[4,1024,1024]{2,1,0} %fusion.496), kind=kInput, calls=%fused_computation.667, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %bitcast.183 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %fusion.496)
  %cublas-gemm.97 = f16[4096,512]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.183, f16[512,1024]{1,0} %convert.162), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.495 = f16[4,1024,512]{2,1,0} fusion(f16[4096,512]{1,0} %cublas-gemm.97, f16[] %get-tuple-element.20, f16[] %get-tuple-element.18, f16[4096,512]{1,0} %cublas-gemm.93, f16[] %get-tuple-element.19), kind=kLoop, calls=%fused_computation.495, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %bitcast.185 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %fusion.495)
  %cublas-gemm.99 = f16[4096,1024]{1,0} custom-call(f16[4096,512]{1,0} %bitcast.185, f16[1024,512]{1,0} %convert.158), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.22 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.99), channel_id=23, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.17, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.490 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) fusion(f32[1024]{0} %get-tuple-element.14, f32[] %get-tuple-element.13, f32[4,1024]{1,0} %get-tuple-element.561, f32[] %get-tuple-element.10, f32[4,1024]{1,0} %get-tuple-element.562, /*index=5*/f32[] %get-tuple-element.12, f32[] %get-tuple-element.11, f16[4,1024,1024]{2,1,0} %fusion.496, f16[4096,1024]{1,0} %all-reduce.22, f16[4,1024,1024]{2,1,0} %get-tuple-element.563), kind=kInput, calls=%fused_computation.490, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %get-tuple-element.552 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.490), index=0
  %get-tuple-element.36 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=17, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.35 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=18, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.34 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=15, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.33 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1847), index=16, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.553 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.490), index=1
  %fusion.489 = f16[4,1024,1024]{2,1,0} fusion(f32[4,1024]{1,0} %get-tuple-element.552, f32[8,1024]{1,0} %get-tuple-element.36, s32[] %fusion.522, f32[8,1024]{1,0} %get-tuple-element.35, f32[8,1024]{1,0} %get-tuple-element.34, /*index=5*/f32[8,1024]{1,0} %get-tuple-element.33, f32[4,1024]{1,0} %get-tuple-element.553, f16[4,1024,1024]{2,1,0} %get-tuple-element.563, f32[] %get-tuple-element.12, f32[4,1024]{1,0} %get-tuple-element.561, /*index=10*/f32[] %get-tuple-element.10, f32[] %get-tuple-element.11, f32[4,1024]{1,0} %get-tuple-element.562, f32[] %get-tuple-element.13, f32[1024]{0} %get-tuple-element.14, /*index=15*/f16[4,1024,1024]{2,1,0} %fusion.496, f16[4096,1024]{1,0} %all-reduce.22), kind=kLoop, calls=%fused_computation.489, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %fusion.670 = f16[1024]{0} fusion(f16[4,1024,1024]{2,1,0} %fusion.489), kind=kInput, calls=%fused_computation.670, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %bitcast.189 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %fusion.489)
  %cublas-gemm.101 = f16[4096,128]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.189, f16[128,1024]{1,0} %convert.154), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.484 = f16[4,4,32,1024]{3,2,1,0} fusion(f16[4096,128]{1,0} %cublas-gemm.101), kind=kLoop, calls=%fused_computation.484, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/transpose[permutation=(0, 2, 3, 1)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %cublas-batch-gemm.37 = f16[4,4,32,1024]{3,2,1,0} custom-call(f16[4,4,32,1024]{3,2,1,0} %fusion.484, f16[4,4,1024,1024]{3,2,1,0} %fusion.701), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/dot_general[dimension_numbers=(((3,), (2,)), ((0, 1), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.488 = f16[4,4,1024,32]{3,2,1,0} fusion(f16[4096,128]{1,0} %cublas-gemm.101), kind=kLoop, calls=%fused_computation.488
  %cublas-batch-gemm.31 = f16[4,4,1024,1024]{3,2,1,0} custom-call(f16[4,4,1024,32]{3,2,1,0} %fusion.488, f16[4,4,32,1024]{3,2,1,0} %get-tuple-element.566), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/dot_general[dimension_numbers=(((2,), (3,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.487 = f16[4,4,1024]{2,1,0} fusion(f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.31, f32[4,4,1024]{2,1,0} %fusion.519, f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.27, f16[4,4,1024]{2,1,0} %reduce.43), kind=kInput, calls=%fused_computation.487, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %fusion.486 = f16[4,4,1024,1024]{3,2,1,0} fusion(f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.31, f16[4,4,1024]{2,1,0} %fusion.487, f32[4,4,1024]{2,1,0} %fusion.519, f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.27, f16[4,4,1024]{2,1,0} %reduce.43), kind=kLoop, calls=%fused_computation.486, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %get-tuple-element.564 = f16[4,4,1024,32]{3,2,1,0} get-tuple-element((f16[4,4,1024,32]{3,2,1,0}, f16[4,4,1024,32]{3,2,1,0}) %fusion.485), index=0
  %cublas-batch-gemm.35 = f16[4,4,1024,32]{3,2,1,0} custom-call(f16[4,4,1024,1024]{3,2,1,0} %fusion.486, f16[4,4,1024,32]{3,2,1,0} %get-tuple-element.564), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/dot_general[dimension_numbers=(((3,), (1,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %cublas-batch-gemm.33 = f16[4,4,1024,32]{3,2,1,0} custom-call(f16[4,4,1024,1024]{3,2,1,0} %fusion.486, f16[4,4,1024,32]{3,2,1,0} %get-tuple-element.565), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/dot_general[dimension_numbers=(((2,), (1,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"2\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.483 = f16[4,1024,128,3]{3,2,1,0} fusion(f16[4,4,32,1024]{3,2,1,0} %cublas-batch-gemm.37, f16[4,4,1024,32]{3,2,1,0} %cublas-batch-gemm.35, f16[] %get-tuple-element.3, f16[4,4,1024,32]{3,2,1,0} %cublas-batch-gemm.33), kind=kLoop, calls=%fused_computation.483, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %bitcast.202 = f16[4096,384]{1,0} bitcast(f16[4,1024,128,3]{3,2,1,0} %fusion.483)
  %cublas-gemm.103 = f16[4096,1024]{1,0} custom-call(f16[4096,384]{1,0} %bitcast.202, f16[1024,384]{1,0} %convert.150), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.23 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.103), channel_id=24, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.18, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.482 = f16[4,1024,1024]{2,1,0} fusion(f16[4,1024,1024]{2,1,0} %fusion.489, f16[4096,1024]{1,0} %all-reduce.23), kind=kLoop, calls=%fused_computation.482, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %tuple.1 = (f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) tuple(f16[] %constant_862, s32[] %constant_59, f16[8,1,1,1024]{3,0,2,1} %broadcast.580, f16[8,1,1,1024]{3,0,2,1} %broadcast.581, f32[] %constant_863, /*index=5*/f32[] %constant_69, f32[] %constant_863, f32[] %constant_124, f16[] %constant_864, f16[] %constant_337, /*index=10*/f16[] %constant_865, f32[] %constant_863, f32[] %constant_69, f32[] %constant_863, f32[] %constant_124, /*index=15*/f32[8,1024]{1,0} %broadcast.421, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.582, f32[8,1024]{1,0} %broadcast.421, /*index=20*/f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.582, f32[1024]{0} %param.65, f32[1024]{0} %param.64, /*index=25*/f32[1024]{0} %param.63, f32[128,1024]{1,0} %param.62, f32[384]{0} %param.61, f32[1024,384]{1,0} %param.60, f32[512]{0} %param.67, /*index=30*/f32[1024,512]{1,0} %param.66, f32[1024]{0} %param.70, f32[1024]{0} %param.69, f32[512,1024]{1,0} %param.68, f16[4,1024,1024]{2,1,0} %fusion.575, /*index=35*/s32[4,1024]{1,0} %param.13, f16[4,1024,1024]{2,1,0} %fusion.482), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %bitcast.1848 = (f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) bitcast((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %tuple.1)
  %get-tuple-element.62 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=31, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.65 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=14, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.52 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=23, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.51 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=24, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.37 = f16[4,1024,1024]{2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=34, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.46 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=25, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %bitcast.204 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %get-tuple-element.37)
  %get-tuple-element.38 = f32[1024,384]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=28, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.171 = f16[1024,384]{1,0} convert(f32[1024,384]{1,0} %get-tuple-element.38), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %get-tuple-element.39 = f32[384]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=27, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.481 = f16[4096,384]{1,0} fusion(f32[384]{0} %get-tuple-element.39), kind=kLoop, calls=%fused_computation.481
  %cublas-gemm.107 = f16[4096,384]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.204, f16[1024,384]{1,0} %convert.171, f16[4096,384]{1,0} %fusion.481), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.479 = (f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) fusion(f16[4096,384]{1,0} %cublas-gemm.107), kind=kLoop, calls=%fused_computation.479
  %get-tuple-element.548 = f16[4,4,32,1024]{3,2,1,0} get-tuple-element((f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) %fusion.479), index=0
  %get-tuple-element.40 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=0, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.441 = (f16[4,4,1024,32]{3,2,1,0}, f16[4,4,1024,32]{3,2,1,0}) fusion(f16[4096,384]{1,0} %cublas-gemm.107, f16[] %get-tuple-element.40), kind=kLoop, calls=%fused_computation.441
  %get-tuple-element.547 = f16[4,4,1024,32]{3,2,1,0} get-tuple-element((f16[4,4,1024,32]{3,2,1,0}, f16[4,4,1024,32]{3,2,1,0}) %fusion.441), index=1
  %get-tuple-element.549 = f16[4,4,32,1024]{3,2,1,0} get-tuple-element((f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) %fusion.479), index=1
  %get-tuple-element.44 = f16[8,1,1,1024]{3,0,2,1} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.43 = f16[8,1,1,1024]{3,0,2,1} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=3, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.41 = s32[4,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=35, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.42 = s32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=1, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.477 = f16[4,4,1024,1024]{3,2,1,0} fusion(f16[8,1,1,1024]{3,0,2,1} %get-tuple-element.44, s32[] %fusion.522, f16[8,1,1,1024]{3,0,2,1} %get-tuple-element.43, s32[4,1024]{1,0} %get-tuple-element.41, s32[] %get-tuple-element.42), kind=kLoop, calls=%fused_computation.477
  %cublas-batch-gemm.41 = f16[4,4,1024,1024]{3,2,1,0} custom-call(f16[4,4,1024,32]{3,2,1,0} %get-tuple-element.547, f16[4,4,32,1024]{3,2,1,0} %get-tuple-element.549, f16[4,4,1024,1024]{3,2,1,0} %fusion.477), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/dot_general[dimension_numbers=(((3,), (3,)), ((0, 2), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %reduce.54 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.41, f16[] %constant_179), dimensions={3}, to_apply=%region_65.2276, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %fusion.475 = f32[4,4,1024]{2,1,0} fusion(f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.41, f16[4,4,1024]{2,1,0} %reduce.54), kind=kInput, calls=%fused_computation.475, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %fusion.699 = f16[4,4,1024,1024]{3,2,1,0} fusion(f32[4,4,1024]{2,1,0} %fusion.475, f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.41, f16[4,4,1024]{2,1,0} %reduce.54), kind=kLoop, calls=%fused_computation.699, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %cublas-batch-gemm.43 = f16[4,4,32,1024]{3,2,1,0} custom-call(f16[4,4,32,1024]{3,2,1,0} %get-tuple-element.548, f16[4,4,1024,1024]{3,2,1,0} %fusion.699), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/dot_general[dimension_numbers=(((1,), (3,)), ((0, 2), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"3\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.474 = f16[4096,128]{1,0} fusion(f16[4,4,32,1024]{3,2,1,0} %cublas-batch-gemm.43), kind=kLoop, calls=%fused_computation.474
  %get-tuple-element.45 = f32[128,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=26, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.175 = f16[128,1024]{1,0} convert(f32[128,1024]{1,0} %get-tuple-element.45), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %cublas-gemm.109 = f16[4096,1024]{1,0} custom-call(f16[4096,128]{1,0} %fusion.474, f16[128,1024]{1,0} %convert.175), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.24 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.109), channel_id=25, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.19, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.472 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) fusion(f16[4,1024,1024]{2,1,0} %get-tuple-element.37, f32[1024]{0} %get-tuple-element.46, f16[4096,1024]{1,0} %all-reduce.24), kind=kInput, calls=%fused_computation.472, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.543 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.472), index=0
  %get-tuple-element.47 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=4, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.545 = f16[4,1024,1024]{2,1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.472), index=2
  %get-tuple-element.50 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=7, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.544 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.472), index=1
  %get-tuple-element.49 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=6, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.48 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=5, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.466 = f16[4,1024,1024]{2,1,0} fusion(f32[1024]{0} %get-tuple-element.52, f32[1024]{0} %get-tuple-element.51, f32[4,1024]{1,0} %get-tuple-element.543, f32[] %get-tuple-element.47, f16[4,1024,1024]{2,1,0} %get-tuple-element.545, /*index=5*/f32[] %get-tuple-element.50, f32[4,1024]{1,0} %get-tuple-element.544, f32[] %get-tuple-element.49, f32[] %get-tuple-element.48), kind=kLoop, calls=%fused_computation.466, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %get-tuple-element.59 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=32, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.57 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=10, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %bitcast.220 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %fusion.466)
  %get-tuple-element.53 = f32[1024,512]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=30, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.179 = f16[1024,512]{1,0} convert(f32[1024,512]{1,0} %get-tuple-element.53), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %get-tuple-element.54 = f32[512]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=29, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.465 = f16[4096,512]{1,0} fusion(f32[512]{0} %get-tuple-element.54), kind=kLoop, calls=%fused_computation.465
  %cublas-gemm.113 = f16[4096,512]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.220, f16[1024,512]{1,0} %convert.179, f16[4096,512]{1,0} %fusion.465), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %get-tuple-element.56 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=9, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.55 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=8, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.463 = f16[4096,512]{1,0} fusion(f16[] %get-tuple-element.57, f16[4096,512]{1,0} %cublas-gemm.113, f16[] %get-tuple-element.56, f16[] %get-tuple-element.55), kind=kLoop, calls=%fused_computation.463
  %get-tuple-element.58 = f32[512,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=33, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.183 = f16[512,1024]{1,0} convert(f32[512,1024]{1,0} %get-tuple-element.58), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %cublas-gemm.115 = f16[4096,1024]{1,0} custom-call(f16[4096,512]{1,0} %fusion.463, f16[512,1024]{1,0} %convert.183), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.25 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.115), channel_id=26, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.20, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.461 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) fusion(f16[4,1024,1024]{2,1,0} %fusion.466, f32[1024]{0} %get-tuple-element.59, f16[4096,1024]{1,0} %all-reduce.25), kind=kInput, calls=%fused_computation.461, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.540 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.461), index=0
  %get-tuple-element.60 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=11, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.541 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.461), index=1
  %get-tuple-element.64 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=13, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.63 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=12, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.61 = f16[4,1024,1024]{2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=36, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.542 = f16[4,1024,1024]{2,1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.461), index=2
  %fusion.453 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) fusion(f32[1024]{0} %get-tuple-element.62, f32[] %get-tuple-element.65, f32[4,1024]{1,0} %get-tuple-element.540, f32[] %get-tuple-element.60, f32[4,1024]{1,0} %get-tuple-element.541, /*index=5*/f32[] %get-tuple-element.64, f32[] %get-tuple-element.63, f16[4,1024,1024]{2,1,0} %get-tuple-element.61, f16[4,1024,1024]{2,1,0} %get-tuple-element.542), kind=kInput, calls=%fused_computation.453, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %get-tuple-element.538 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.453), index=0
  %get-tuple-element.69 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=21, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.68 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=22, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.67 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=19, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.66 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=20, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.539 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.453), index=1
  %fusion.452 = f16[4,1024,1024]{2,1,0} fusion(f32[4,1024]{1,0} %get-tuple-element.538, f32[8,1024]{1,0} %get-tuple-element.69, s32[] %fusion.522, f32[8,1024]{1,0} %get-tuple-element.68, f32[8,1024]{1,0} %get-tuple-element.67, /*index=5*/f32[8,1024]{1,0} %get-tuple-element.66, f32[4,1024]{1,0} %get-tuple-element.539, f16[4,1024,1024]{2,1,0} %get-tuple-element.542, f32[] %get-tuple-element.64, f32[4,1024]{1,0} %get-tuple-element.540, /*index=10*/f32[] %get-tuple-element.60, f32[] %get-tuple-element.63, f32[4,1024]{1,0} %get-tuple-element.541, f32[] %get-tuple-element.65, f32[1024]{0} %get-tuple-element.62, /*index=15*/f16[4,1024,1024]{2,1,0} %get-tuple-element.61), kind=kLoop, calls=%fused_computation.452, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %fusion.671 = f16[1024]{0} fusion(f16[4,1024,1024]{2,1,0} %fusion.452), kind=kInput, calls=%fused_computation.671, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %bitcast.228 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %fusion.452)
  %cublas-gemm.117 = f16[4096,512]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.228, f16[512,1024]{1,0} %convert.183), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.451 = f16[4,1024,512]{2,1,0} fusion(f16[4096,512]{1,0} %cublas-gemm.117, f16[] %get-tuple-element.57, f16[] %get-tuple-element.55, f16[4096,512]{1,0} %cublas-gemm.113, f16[] %get-tuple-element.56), kind=kLoop, calls=%fused_computation.451, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %bitcast.230 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %fusion.451)
  %cublas-gemm.119 = f16[4096,1024]{1,0} custom-call(f16[4096,512]{1,0} %bitcast.230, f16[1024,512]{1,0} %convert.179), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.26 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.119), channel_id=27, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.21, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.446 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) fusion(f32[1024]{0} %get-tuple-element.51, f32[] %get-tuple-element.50, f32[4,1024]{1,0} %get-tuple-element.543, f32[] %get-tuple-element.47, f32[4,1024]{1,0} %get-tuple-element.544, /*index=5*/f32[] %get-tuple-element.49, f32[] %get-tuple-element.48, f16[4,1024,1024]{2,1,0} %fusion.452, f16[4096,1024]{1,0} %all-reduce.26, f16[4,1024,1024]{2,1,0} %get-tuple-element.545), kind=kInput, calls=%fused_computation.446, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %get-tuple-element.534 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.446), index=0
  %get-tuple-element.73 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=17, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.72 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=18, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.71 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=15, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.70 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1848), index=16, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.535 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.446), index=1
  %fusion.445 = f16[4,1024,1024]{2,1,0} fusion(f32[4,1024]{1,0} %get-tuple-element.534, f32[8,1024]{1,0} %get-tuple-element.73, s32[] %fusion.522, f32[8,1024]{1,0} %get-tuple-element.72, f32[8,1024]{1,0} %get-tuple-element.71, /*index=5*/f32[8,1024]{1,0} %get-tuple-element.70, f32[4,1024]{1,0} %get-tuple-element.535, f16[4,1024,1024]{2,1,0} %get-tuple-element.545, f32[] %get-tuple-element.49, f32[4,1024]{1,0} %get-tuple-element.543, /*index=10*/f32[] %get-tuple-element.47, f32[] %get-tuple-element.48, f32[4,1024]{1,0} %get-tuple-element.544, f32[] %get-tuple-element.50, f32[1024]{0} %get-tuple-element.51, /*index=15*/f16[4,1024,1024]{2,1,0} %fusion.452, f16[4096,1024]{1,0} %all-reduce.26), kind=kLoop, calls=%fused_computation.445, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %bitcast.234 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %fusion.445)
  %cublas-gemm.121 = f16[4096,128]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.234, f16[128,1024]{1,0} %convert.175), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.440 = f16[4,4,32,1024]{3,2,1,0} fusion(f16[4096,128]{1,0} %cublas-gemm.121), kind=kLoop, calls=%fused_computation.440, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/transpose[permutation=(0, 2, 3, 1)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %cublas-batch-gemm.51 = f16[4,4,32,1024]{3,2,1,0} custom-call(f16[4,4,32,1024]{3,2,1,0} %fusion.440, f16[4,4,1024,1024]{3,2,1,0} %fusion.699), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/dot_general[dimension_numbers=(((3,), (2,)), ((0, 1), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.444 = f16[4,4,1024,32]{3,2,1,0} fusion(f16[4096,128]{1,0} %cublas-gemm.121), kind=kLoop, calls=%fused_computation.444
  %cublas-batch-gemm.45 = f16[4,4,1024,1024]{3,2,1,0} custom-call(f16[4,4,1024,32]{3,2,1,0} %fusion.444, f16[4,4,32,1024]{3,2,1,0} %get-tuple-element.548), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/dot_general[dimension_numbers=(((2,), (3,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.443 = f16[4,4,1024]{2,1,0} fusion(f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.45, f32[4,4,1024]{2,1,0} %fusion.475, f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.41, f16[4,4,1024]{2,1,0} %reduce.54), kind=kInput, calls=%fused_computation.443, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %fusion.442 = f16[4,4,1024,1024]{3,2,1,0} fusion(f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.45, f16[4,4,1024]{2,1,0} %fusion.443, f32[4,4,1024]{2,1,0} %fusion.475, f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.41, f16[4,4,1024]{2,1,0} %reduce.54), kind=kLoop, calls=%fused_computation.442, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %get-tuple-element.546 = f16[4,4,1024,32]{3,2,1,0} get-tuple-element((f16[4,4,1024,32]{3,2,1,0}, f16[4,4,1024,32]{3,2,1,0}) %fusion.441), index=0
  %cublas-batch-gemm.49 = f16[4,4,1024,32]{3,2,1,0} custom-call(f16[4,4,1024,1024]{3,2,1,0} %fusion.442, f16[4,4,1024,32]{3,2,1,0} %get-tuple-element.546), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/dot_general[dimension_numbers=(((3,), (1,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %cublas-batch-gemm.47 = f16[4,4,1024,32]{3,2,1,0} custom-call(f16[4,4,1024,1024]{3,2,1,0} %fusion.442, f16[4,4,1024,32]{3,2,1,0} %get-tuple-element.547), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/dot_general[dimension_numbers=(((2,), (1,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"2\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.439 = f16[4,1024,128,3]{3,2,1,0} fusion(f16[4,4,32,1024]{3,2,1,0} %cublas-batch-gemm.51, f16[4,4,1024,32]{3,2,1,0} %cublas-batch-gemm.49, f16[] %get-tuple-element.40, f16[4,4,1024,32]{3,2,1,0} %cublas-batch-gemm.47), kind=kLoop, calls=%fused_computation.439, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %bitcast.247 = f16[4096,384]{1,0} bitcast(f16[4,1024,128,3]{3,2,1,0} %fusion.439)
  %cublas-gemm.123 = f16[4096,1024]{1,0} custom-call(f16[4096,384]{1,0} %bitcast.247, f16[1024,384]{1,0} %convert.171), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.27 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.123), channel_id=28, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.22, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.438 = f16[4,1024,1024]{2,1,0} fusion(f16[4,1024,1024]{2,1,0} %fusion.445, f16[4096,1024]{1,0} %all-reduce.27), kind=kLoop, calls=%fused_computation.438, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %tuple.2 = (f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) tuple(f16[] %constant_862, s32[] %constant_59, f16[8,1,1,1024]{3,0,2,1} %broadcast.580, f16[8,1,1,1024]{3,0,2,1} %broadcast.581, f32[] %constant_863, /*index=5*/f32[] %constant_69, f32[] %constant_863, f32[] %constant_124, f16[] %constant_864, f16[] %constant_337, /*index=10*/f16[] %constant_865, f32[] %constant_863, f32[] %constant_69, f32[] %constant_863, f32[] %constant_124, /*index=15*/f32[8,1024]{1,0} %broadcast.421, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.582, f32[8,1024]{1,0} %broadcast.421, /*index=20*/f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.582, f32[1024]{0} %param.53, f32[1024]{0} %param.52, /*index=25*/f32[1024]{0} %param.51, f32[128,1024]{1,0} %param.50, f32[384]{0} %param.49, f32[1024,384]{1,0} %param.48, f32[512]{0} %param.55, /*index=30*/f32[1024,512]{1,0} %param.54, f32[1024]{0} %param.58, f32[1024]{0} %param.57, f32[512,1024]{1,0} %param.56, f16[4,1024,1024]{2,1,0} %fusion.594, /*index=35*/s32[4,1024]{1,0} %param.13, f16[4,1024,1024]{2,1,0} %fusion.438), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %bitcast.1849 = (f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) bitcast((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %tuple.2)
  %get-tuple-element.88 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=24, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.87 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=7, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.74 = f16[4,1024,1024]{2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=34, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.83 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=25, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %bitcast.249 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %get-tuple-element.74)
  %get-tuple-element.75 = f32[1024,384]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=28, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.192 = f16[1024,384]{1,0} convert(f32[1024,384]{1,0} %get-tuple-element.75), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %get-tuple-element.76 = f32[384]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=27, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.437 = f16[4096,384]{1,0} fusion(f32[384]{0} %get-tuple-element.76), kind=kLoop, calls=%fused_computation.437
  %cublas-gemm.127 = f16[4096,384]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.249, f16[1024,384]{1,0} %convert.192, f16[4096,384]{1,0} %fusion.437), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.435 = (f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) fusion(f16[4096,384]{1,0} %cublas-gemm.127), kind=kLoop, calls=%fused_computation.435
  %get-tuple-element.530 = f16[4,4,32,1024]{3,2,1,0} get-tuple-element((f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) %fusion.435), index=0
  %get-tuple-element.77 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=0, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.397 = (f16[4,4,1024,32]{3,2,1,0}, f16[4,4,1024,32]{3,2,1,0}) fusion(f16[4096,384]{1,0} %cublas-gemm.127, f16[] %get-tuple-element.77), kind=kLoop, calls=%fused_computation.397
  %get-tuple-element.529 = f16[4,4,1024,32]{3,2,1,0} get-tuple-element((f16[4,4,1024,32]{3,2,1,0}, f16[4,4,1024,32]{3,2,1,0}) %fusion.397), index=1
  %get-tuple-element.531 = f16[4,4,32,1024]{3,2,1,0} get-tuple-element((f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) %fusion.435), index=1
  %get-tuple-element.81 = f16[8,1,1,1024]{3,0,2,1} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.80 = f16[8,1,1,1024]{3,0,2,1} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=3, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.78 = s32[4,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=35, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.79 = s32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=1, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.433 = f16[4,4,1024,1024]{3,2,1,0} fusion(f16[8,1,1,1024]{3,0,2,1} %get-tuple-element.81, s32[] %fusion.522, f16[8,1,1,1024]{3,0,2,1} %get-tuple-element.80, s32[4,1024]{1,0} %get-tuple-element.78, s32[] %get-tuple-element.79), kind=kLoop, calls=%fused_computation.433
  %cublas-batch-gemm.55 = f16[4,4,1024,1024]{3,2,1,0} custom-call(f16[4,4,1024,32]{3,2,1,0} %get-tuple-element.529, f16[4,4,32,1024]{3,2,1,0} %get-tuple-element.531, f16[4,4,1024,1024]{3,2,1,0} %fusion.433), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/dot_general[dimension_numbers=(((3,), (3,)), ((0, 2), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %reduce.65 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.55, f16[] %constant_179), dimensions={3}, to_apply=%region_85.2735, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %fusion.431 = f32[4,4,1024]{2,1,0} fusion(f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.55, f16[4,4,1024]{2,1,0} %reduce.65), kind=kInput, calls=%fused_computation.431, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %fusion.697 = f16[4,4,1024,1024]{3,2,1,0} fusion(f32[4,4,1024]{2,1,0} %fusion.431, f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.55, f16[4,4,1024]{2,1,0} %reduce.65), kind=kLoop, calls=%fused_computation.697, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %cublas-batch-gemm.57 = f16[4,4,32,1024]{3,2,1,0} custom-call(f16[4,4,32,1024]{3,2,1,0} %get-tuple-element.530, f16[4,4,1024,1024]{3,2,1,0} %fusion.697), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/dot_general[dimension_numbers=(((1,), (3,)), ((0, 2), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"3\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.430 = f16[4096,128]{1,0} fusion(f16[4,4,32,1024]{3,2,1,0} %cublas-batch-gemm.57), kind=kLoop, calls=%fused_computation.430
  %get-tuple-element.82 = f32[128,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=26, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.196 = f16[128,1024]{1,0} convert(f32[128,1024]{1,0} %get-tuple-element.82), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %cublas-gemm.129 = f16[4096,1024]{1,0} custom-call(f16[4096,128]{1,0} %fusion.430, f16[128,1024]{1,0} %convert.196), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.28 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.129), channel_id=29, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.23, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.428 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) fusion(f16[4,1024,1024]{2,1,0} %get-tuple-element.74, f32[1024]{0} %get-tuple-element.83, f16[4096,1024]{1,0} %all-reduce.28), kind=kInput, calls=%fused_computation.428, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.525 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.428), index=0
  %get-tuple-element.84 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=4, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.526 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.428), index=1
  %get-tuple-element.86 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=6, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.85 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=5, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.99 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=31, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.102 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=14, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.89 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=23, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.527 = f16[4,1024,1024]{2,1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.428), index=2
  %fusion.422 = f16[4,1024,1024]{2,1,0} fusion(f32[1024]{0} %get-tuple-element.89, f32[1024]{0} %get-tuple-element.88, f32[4,1024]{1,0} %get-tuple-element.525, f32[] %get-tuple-element.84, f16[4,1024,1024]{2,1,0} %get-tuple-element.527, /*index=5*/f32[] %get-tuple-element.87, f32[4,1024]{1,0} %get-tuple-element.526, f32[] %get-tuple-element.86, f32[] %get-tuple-element.85), kind=kLoop, calls=%fused_computation.422, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %get-tuple-element.96 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=32, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.94 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=10, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %bitcast.265 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %fusion.422)
  %get-tuple-element.90 = f32[1024,512]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=30, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.200 = f16[1024,512]{1,0} convert(f32[1024,512]{1,0} %get-tuple-element.90), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %get-tuple-element.91 = f32[512]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=29, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.421 = f16[4096,512]{1,0} fusion(f32[512]{0} %get-tuple-element.91), kind=kLoop, calls=%fused_computation.421
  %cublas-gemm.133 = f16[4096,512]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.265, f16[1024,512]{1,0} %convert.200, f16[4096,512]{1,0} %fusion.421), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %get-tuple-element.93 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=9, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.92 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=8, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.419 = f16[4096,512]{1,0} fusion(f16[] %get-tuple-element.94, f16[4096,512]{1,0} %cublas-gemm.133, f16[] %get-tuple-element.93, f16[] %get-tuple-element.92), kind=kLoop, calls=%fused_computation.419
  %get-tuple-element.95 = f32[512,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=33, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.204 = f16[512,1024]{1,0} convert(f32[512,1024]{1,0} %get-tuple-element.95), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %cublas-gemm.135 = f16[4096,1024]{1,0} custom-call(f16[4096,512]{1,0} %fusion.419, f16[512,1024]{1,0} %convert.204), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.29 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.135), channel_id=30, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.24, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.417 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) fusion(f16[4,1024,1024]{2,1,0} %fusion.422, f32[1024]{0} %get-tuple-element.96, f16[4096,1024]{1,0} %all-reduce.29), kind=kInput, calls=%fused_computation.417, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.522 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.417), index=0
  %get-tuple-element.97 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=11, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.523 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.417), index=1
  %get-tuple-element.101 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=13, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.100 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=12, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.98 = f16[4,1024,1024]{2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=36, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.524 = f16[4,1024,1024]{2,1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.417), index=2
  %fusion.409 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) fusion(f32[1024]{0} %get-tuple-element.99, f32[] %get-tuple-element.102, f32[4,1024]{1,0} %get-tuple-element.522, f32[] %get-tuple-element.97, f32[4,1024]{1,0} %get-tuple-element.523, /*index=5*/f32[] %get-tuple-element.101, f32[] %get-tuple-element.100, f16[4,1024,1024]{2,1,0} %get-tuple-element.98, f16[4,1024,1024]{2,1,0} %get-tuple-element.524), kind=kInput, calls=%fused_computation.409, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %get-tuple-element.520 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.409), index=0
  %get-tuple-element.106 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=21, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.105 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=22, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.104 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=19, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.103 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=20, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.521 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.409), index=1
  %fusion.408 = f16[4,1024,1024]{2,1,0} fusion(f32[4,1024]{1,0} %get-tuple-element.520, f32[8,1024]{1,0} %get-tuple-element.106, s32[] %fusion.522, f32[8,1024]{1,0} %get-tuple-element.105, f32[8,1024]{1,0} %get-tuple-element.104, /*index=5*/f32[8,1024]{1,0} %get-tuple-element.103, f32[4,1024]{1,0} %get-tuple-element.521, f16[4,1024,1024]{2,1,0} %get-tuple-element.524, f32[] %get-tuple-element.101, f32[4,1024]{1,0} %get-tuple-element.522, /*index=10*/f32[] %get-tuple-element.97, f32[] %get-tuple-element.100, f32[4,1024]{1,0} %get-tuple-element.523, f32[] %get-tuple-element.102, f32[1024]{0} %get-tuple-element.99, /*index=15*/f16[4,1024,1024]{2,1,0} %get-tuple-element.98), kind=kLoop, calls=%fused_computation.408, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %bitcast.273 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %fusion.408)
  %cublas-gemm.137 = f16[4096,512]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.273, f16[512,1024]{1,0} %convert.204), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.407 = f16[4,1024,512]{2,1,0} fusion(f16[4096,512]{1,0} %cublas-gemm.137, f16[] %get-tuple-element.94, f16[] %get-tuple-element.92, f16[4096,512]{1,0} %cublas-gemm.133, f16[] %get-tuple-element.93), kind=kLoop, calls=%fused_computation.407, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %bitcast.275 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %fusion.407)
  %cublas-gemm.139 = f16[4096,1024]{1,0} custom-call(f16[4096,512]{1,0} %bitcast.275, f16[1024,512]{1,0} %convert.200), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.30 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.139), channel_id=31, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.25, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.402 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) fusion(f32[1024]{0} %get-tuple-element.88, f32[] %get-tuple-element.87, f32[4,1024]{1,0} %get-tuple-element.525, f32[] %get-tuple-element.84, f32[4,1024]{1,0} %get-tuple-element.526, /*index=5*/f32[] %get-tuple-element.86, f32[] %get-tuple-element.85, f16[4,1024,1024]{2,1,0} %fusion.408, f16[4096,1024]{1,0} %all-reduce.30, f16[4,1024,1024]{2,1,0} %get-tuple-element.527), kind=kInput, calls=%fused_computation.402, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %get-tuple-element.516 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.402), index=0
  %get-tuple-element.110 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=17, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.109 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=18, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.108 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=15, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.107 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1849), index=16, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.517 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.402), index=1
  %fusion.401 = f16[4,1024,1024]{2,1,0} fusion(f32[4,1024]{1,0} %get-tuple-element.516, f32[8,1024]{1,0} %get-tuple-element.110, s32[] %fusion.522, f32[8,1024]{1,0} %get-tuple-element.109, f32[8,1024]{1,0} %get-tuple-element.108, /*index=5*/f32[8,1024]{1,0} %get-tuple-element.107, f32[4,1024]{1,0} %get-tuple-element.517, f16[4,1024,1024]{2,1,0} %get-tuple-element.527, f32[] %get-tuple-element.86, f32[4,1024]{1,0} %get-tuple-element.525, /*index=10*/f32[] %get-tuple-element.84, f32[] %get-tuple-element.85, f32[4,1024]{1,0} %get-tuple-element.526, f32[] %get-tuple-element.87, f32[1024]{0} %get-tuple-element.88, /*index=15*/f16[4,1024,1024]{2,1,0} %fusion.408, f16[4096,1024]{1,0} %all-reduce.30), kind=kLoop, calls=%fused_computation.401, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %bitcast.279 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %fusion.401)
  %cublas-gemm.141 = f16[4096,128]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.279, f16[128,1024]{1,0} %convert.196), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.396 = f16[4,4,32,1024]{3,2,1,0} fusion(f16[4096,128]{1,0} %cublas-gemm.141), kind=kLoop, calls=%fused_computation.396, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/transpose[permutation=(0, 2, 3, 1)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %cublas-batch-gemm.65 = f16[4,4,32,1024]{3,2,1,0} custom-call(f16[4,4,32,1024]{3,2,1,0} %fusion.396, f16[4,4,1024,1024]{3,2,1,0} %fusion.697), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/dot_general[dimension_numbers=(((3,), (2,)), ((0, 1), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.400 = f16[4,4,1024,32]{3,2,1,0} fusion(f16[4096,128]{1,0} %cublas-gemm.141), kind=kLoop, calls=%fused_computation.400
  %cublas-batch-gemm.59 = f16[4,4,1024,1024]{3,2,1,0} custom-call(f16[4,4,1024,32]{3,2,1,0} %fusion.400, f16[4,4,32,1024]{3,2,1,0} %get-tuple-element.530), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/dot_general[dimension_numbers=(((2,), (3,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.399 = f16[4,4,1024]{2,1,0} fusion(f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.59, f32[4,4,1024]{2,1,0} %fusion.431, f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.55, f16[4,4,1024]{2,1,0} %reduce.65), kind=kInput, calls=%fused_computation.399, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %fusion.398 = f16[4,4,1024,1024]{3,2,1,0} fusion(f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.59, f16[4,4,1024]{2,1,0} %fusion.399, f32[4,4,1024]{2,1,0} %fusion.431, f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.55, f16[4,4,1024]{2,1,0} %reduce.65), kind=kLoop, calls=%fused_computation.398, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %get-tuple-element.528 = f16[4,4,1024,32]{3,2,1,0} get-tuple-element((f16[4,4,1024,32]{3,2,1,0}, f16[4,4,1024,32]{3,2,1,0}) %fusion.397), index=0
  %cublas-batch-gemm.63 = f16[4,4,1024,32]{3,2,1,0} custom-call(f16[4,4,1024,1024]{3,2,1,0} %fusion.398, f16[4,4,1024,32]{3,2,1,0} %get-tuple-element.528), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/dot_general[dimension_numbers=(((3,), (1,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %cublas-batch-gemm.61 = f16[4,4,1024,32]{3,2,1,0} custom-call(f16[4,4,1024,1024]{3,2,1,0} %fusion.398, f16[4,4,1024,32]{3,2,1,0} %get-tuple-element.529), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/dot_general[dimension_numbers=(((2,), (1,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"2\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.395 = f16[4,1024,128,3]{3,2,1,0} fusion(f16[4,4,32,1024]{3,2,1,0} %cublas-batch-gemm.65, f16[4,4,1024,32]{3,2,1,0} %cublas-batch-gemm.63, f16[] %get-tuple-element.77, f16[4,4,1024,32]{3,2,1,0} %cublas-batch-gemm.61), kind=kLoop, calls=%fused_computation.395, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %bitcast.292 = f16[4096,384]{1,0} bitcast(f16[4,1024,128,3]{3,2,1,0} %fusion.395)
  %cublas-gemm.143 = f16[4096,1024]{1,0} custom-call(f16[4096,384]{1,0} %bitcast.292, f16[1024,384]{1,0} %convert.192), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.31 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.143), channel_id=32, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.26, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.394 = f16[4,1024,1024]{2,1,0} fusion(f16[4,1024,1024]{2,1,0} %fusion.401, f16[4096,1024]{1,0} %all-reduce.31), kind=kLoop, calls=%fused_computation.394, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %tuple.3 = (f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) tuple(f16[] %constant_862, s32[] %constant_59, f16[8,1,1,1024]{3,0,2,1} %broadcast.580, f16[8,1,1,1024]{3,0,2,1} %broadcast.581, f32[] %constant_863, /*index=5*/f32[] %constant_69, f32[] %constant_863, f32[] %constant_124, f16[] %constant_864, f16[] %constant_337, /*index=10*/f16[] %constant_865, f32[] %constant_863, f32[] %constant_69, f32[] %constant_863, f32[] %constant_124, /*index=15*/f32[8,1024]{1,0} %broadcast.421, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.582, f32[8,1024]{1,0} %broadcast.421, /*index=20*/f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.582, f32[1024]{0} %param.41, f32[1024]{0} %param.40, /*index=25*/f32[1024]{0} %param.39, f32[128,1024]{1,0} %param.38, f32[384]{0} %param.37, f32[1024,384]{1,0} %param.36, f32[512]{0} %param.43, /*index=30*/f32[1024,512]{1,0} %param.42, f32[1024]{0} %param.46, f32[1024]{0} %param.45, f32[512,1024]{1,0} %param.44, f16[4,1024,1024]{2,1,0} %fusion.613, /*index=35*/s32[4,1024]{1,0} %param.13, f16[4,1024,1024]{2,1,0} %fusion.394), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %bitcast.1850 = (f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) bitcast((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %tuple.3)
  %get-tuple-element.125 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=24, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.124 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=7, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.111 = f16[4,1024,1024]{2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=34, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.120 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=25, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %bitcast.294 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %get-tuple-element.111)
  %get-tuple-element.112 = f32[1024,384]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=28, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.213 = f16[1024,384]{1,0} convert(f32[1024,384]{1,0} %get-tuple-element.112), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %get-tuple-element.113 = f32[384]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=27, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.393 = f16[4096,384]{1,0} fusion(f32[384]{0} %get-tuple-element.113), kind=kLoop, calls=%fused_computation.393
  %cublas-gemm.147 = f16[4096,384]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.294, f16[1024,384]{1,0} %convert.213, f16[4096,384]{1,0} %fusion.393), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.391 = (f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) fusion(f16[4096,384]{1,0} %cublas-gemm.147), kind=kLoop, calls=%fused_computation.391
  %get-tuple-element.512 = f16[4,4,32,1024]{3,2,1,0} get-tuple-element((f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) %fusion.391), index=0
  %get-tuple-element.114 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=0, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.353 = (f16[4,4,1024,32]{3,2,1,0}, f16[4,4,1024,32]{3,2,1,0}) fusion(f16[4096,384]{1,0} %cublas-gemm.147, f16[] %get-tuple-element.114), kind=kLoop, calls=%fused_computation.353
  %get-tuple-element.511 = f16[4,4,1024,32]{3,2,1,0} get-tuple-element((f16[4,4,1024,32]{3,2,1,0}, f16[4,4,1024,32]{3,2,1,0}) %fusion.353), index=1
  %get-tuple-element.513 = f16[4,4,32,1024]{3,2,1,0} get-tuple-element((f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) %fusion.391), index=1
  %get-tuple-element.118 = f16[8,1,1,1024]{3,0,2,1} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.117 = f16[8,1,1,1024]{3,0,2,1} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=3, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.115 = s32[4,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=35, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.116 = s32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=1, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.389 = f16[4,4,1024,1024]{3,2,1,0} fusion(f16[8,1,1,1024]{3,0,2,1} %get-tuple-element.118, s32[] %fusion.522, f16[8,1,1,1024]{3,0,2,1} %get-tuple-element.117, s32[4,1024]{1,0} %get-tuple-element.115, s32[] %get-tuple-element.116), kind=kLoop, calls=%fused_computation.389
  %cublas-batch-gemm.69 = f16[4,4,1024,1024]{3,2,1,0} custom-call(f16[4,4,1024,32]{3,2,1,0} %get-tuple-element.511, f16[4,4,32,1024]{3,2,1,0} %get-tuple-element.513, f16[4,4,1024,1024]{3,2,1,0} %fusion.389), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/dot_general[dimension_numbers=(((3,), (3,)), ((0, 2), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %reduce.76 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.69, f16[] %constant_179), dimensions={3}, to_apply=%region_105.3194, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %fusion.387 = f32[4,4,1024]{2,1,0} fusion(f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.69, f16[4,4,1024]{2,1,0} %reduce.76), kind=kInput, calls=%fused_computation.387, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %fusion.695 = f16[4,4,1024,1024]{3,2,1,0} fusion(f32[4,4,1024]{2,1,0} %fusion.387, f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.69, f16[4,4,1024]{2,1,0} %reduce.76), kind=kLoop, calls=%fused_computation.695, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %cublas-batch-gemm.71 = f16[4,4,32,1024]{3,2,1,0} custom-call(f16[4,4,32,1024]{3,2,1,0} %get-tuple-element.512, f16[4,4,1024,1024]{3,2,1,0} %fusion.695), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/dot_general[dimension_numbers=(((1,), (3,)), ((0, 2), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"3\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.386 = f16[4096,128]{1,0} fusion(f16[4,4,32,1024]{3,2,1,0} %cublas-batch-gemm.71), kind=kLoop, calls=%fused_computation.386
  %get-tuple-element.119 = f32[128,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=26, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.217 = f16[128,1024]{1,0} convert(f32[128,1024]{1,0} %get-tuple-element.119), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %cublas-gemm.149 = f16[4096,1024]{1,0} custom-call(f16[4096,128]{1,0} %fusion.386, f16[128,1024]{1,0} %convert.217), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.32 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.149), channel_id=33, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.27, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.384 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) fusion(f16[4,1024,1024]{2,1,0} %get-tuple-element.111, f32[1024]{0} %get-tuple-element.120, f16[4096,1024]{1,0} %all-reduce.32), kind=kInput, calls=%fused_computation.384, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.507 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.384), index=0
  %get-tuple-element.121 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=4, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.508 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.384), index=1
  %get-tuple-element.123 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=6, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.122 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=5, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.136 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=31, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.139 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=14, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.126 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=23, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.509 = f16[4,1024,1024]{2,1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.384), index=2
  %fusion.378 = f16[4,1024,1024]{2,1,0} fusion(f32[1024]{0} %get-tuple-element.126, f32[1024]{0} %get-tuple-element.125, f32[4,1024]{1,0} %get-tuple-element.507, f32[] %get-tuple-element.121, f16[4,1024,1024]{2,1,0} %get-tuple-element.509, /*index=5*/f32[] %get-tuple-element.124, f32[4,1024]{1,0} %get-tuple-element.508, f32[] %get-tuple-element.123, f32[] %get-tuple-element.122), kind=kLoop, calls=%fused_computation.378, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %get-tuple-element.133 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=32, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.131 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=10, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %bitcast.310 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %fusion.378)
  %get-tuple-element.127 = f32[1024,512]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=30, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.221 = f16[1024,512]{1,0} convert(f32[1024,512]{1,0} %get-tuple-element.127), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %get-tuple-element.128 = f32[512]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=29, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.377 = f16[4096,512]{1,0} fusion(f32[512]{0} %get-tuple-element.128), kind=kLoop, calls=%fused_computation.377
  %cublas-gemm.153 = f16[4096,512]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.310, f16[1024,512]{1,0} %convert.221, f16[4096,512]{1,0} %fusion.377), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %get-tuple-element.130 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=9, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.129 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=8, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.375 = f16[4096,512]{1,0} fusion(f16[] %get-tuple-element.131, f16[4096,512]{1,0} %cublas-gemm.153, f16[] %get-tuple-element.130, f16[] %get-tuple-element.129), kind=kLoop, calls=%fused_computation.375
  %get-tuple-element.132 = f32[512,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=33, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.225 = f16[512,1024]{1,0} convert(f32[512,1024]{1,0} %get-tuple-element.132), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %cublas-gemm.155 = f16[4096,1024]{1,0} custom-call(f16[4096,512]{1,0} %fusion.375, f16[512,1024]{1,0} %convert.225), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.33 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.155), channel_id=34, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.28, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.373 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) fusion(f16[4,1024,1024]{2,1,0} %fusion.378, f32[1024]{0} %get-tuple-element.133, f16[4096,1024]{1,0} %all-reduce.33), kind=kInput, calls=%fused_computation.373, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.504 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.373), index=0
  %get-tuple-element.134 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=11, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.505 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.373), index=1
  %get-tuple-element.138 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=13, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.137 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=12, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.135 = f16[4,1024,1024]{2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=36, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.506 = f16[4,1024,1024]{2,1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.373), index=2
  %fusion.365 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) fusion(f32[1024]{0} %get-tuple-element.136, f32[] %get-tuple-element.139, f32[4,1024]{1,0} %get-tuple-element.504, f32[] %get-tuple-element.134, f32[4,1024]{1,0} %get-tuple-element.505, /*index=5*/f32[] %get-tuple-element.138, f32[] %get-tuple-element.137, f16[4,1024,1024]{2,1,0} %get-tuple-element.135, f16[4,1024,1024]{2,1,0} %get-tuple-element.506), kind=kInput, calls=%fused_computation.365, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %get-tuple-element.502 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.365), index=0
  %get-tuple-element.143 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=21, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.142 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=22, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.141 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=19, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.140 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=20, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.503 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.365), index=1
  %fusion.364 = f16[4,1024,1024]{2,1,0} fusion(f32[4,1024]{1,0} %get-tuple-element.502, f32[8,1024]{1,0} %get-tuple-element.143, s32[] %fusion.522, f32[8,1024]{1,0} %get-tuple-element.142, f32[8,1024]{1,0} %get-tuple-element.141, /*index=5*/f32[8,1024]{1,0} %get-tuple-element.140, f32[4,1024]{1,0} %get-tuple-element.503, f16[4,1024,1024]{2,1,0} %get-tuple-element.506, f32[] %get-tuple-element.138, f32[4,1024]{1,0} %get-tuple-element.504, /*index=10*/f32[] %get-tuple-element.134, f32[] %get-tuple-element.137, f32[4,1024]{1,0} %get-tuple-element.505, f32[] %get-tuple-element.139, f32[1024]{0} %get-tuple-element.136, /*index=15*/f16[4,1024,1024]{2,1,0} %get-tuple-element.135), kind=kLoop, calls=%fused_computation.364, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %bitcast.318 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %fusion.364)
  %cublas-gemm.157 = f16[4096,512]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.318, f16[512,1024]{1,0} %convert.225), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.363 = f16[4,1024,512]{2,1,0} fusion(f16[4096,512]{1,0} %cublas-gemm.157, f16[] %get-tuple-element.131, f16[] %get-tuple-element.129, f16[4096,512]{1,0} %cublas-gemm.153, f16[] %get-tuple-element.130), kind=kLoop, calls=%fused_computation.363, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %bitcast.320 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %fusion.363)
  %cublas-gemm.159 = f16[4096,1024]{1,0} custom-call(f16[4096,512]{1,0} %bitcast.320, f16[1024,512]{1,0} %convert.221), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.34 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.159), channel_id=35, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.29, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.358 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) fusion(f32[1024]{0} %get-tuple-element.125, f32[] %get-tuple-element.124, f32[4,1024]{1,0} %get-tuple-element.507, f32[] %get-tuple-element.121, f32[4,1024]{1,0} %get-tuple-element.508, /*index=5*/f32[] %get-tuple-element.123, f32[] %get-tuple-element.122, f16[4,1024,1024]{2,1,0} %fusion.364, f16[4096,1024]{1,0} %all-reduce.34, f16[4,1024,1024]{2,1,0} %get-tuple-element.509), kind=kInput, calls=%fused_computation.358, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %get-tuple-element.498 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.358), index=0
  %get-tuple-element.147 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=17, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.146 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=18, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.145 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=15, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.144 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1850), index=16, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.499 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.358), index=1
  %fusion.357 = f16[4,1024,1024]{2,1,0} fusion(f32[4,1024]{1,0} %get-tuple-element.498, f32[8,1024]{1,0} %get-tuple-element.147, s32[] %fusion.522, f32[8,1024]{1,0} %get-tuple-element.146, f32[8,1024]{1,0} %get-tuple-element.145, /*index=5*/f32[8,1024]{1,0} %get-tuple-element.144, f32[4,1024]{1,0} %get-tuple-element.499, f16[4,1024,1024]{2,1,0} %get-tuple-element.509, f32[] %get-tuple-element.123, f32[4,1024]{1,0} %get-tuple-element.507, /*index=10*/f32[] %get-tuple-element.121, f32[] %get-tuple-element.122, f32[4,1024]{1,0} %get-tuple-element.508, f32[] %get-tuple-element.124, f32[1024]{0} %get-tuple-element.125, /*index=15*/f16[4,1024,1024]{2,1,0} %fusion.364, f16[4096,1024]{1,0} %all-reduce.34), kind=kLoop, calls=%fused_computation.357, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %bitcast.324 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %fusion.357)
  %cublas-gemm.161 = f16[4096,128]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.324, f16[128,1024]{1,0} %convert.217), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.352 = f16[4,4,32,1024]{3,2,1,0} fusion(f16[4096,128]{1,0} %cublas-gemm.161), kind=kLoop, calls=%fused_computation.352, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/transpose[permutation=(0, 2, 3, 1)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %cublas-batch-gemm.79 = f16[4,4,32,1024]{3,2,1,0} custom-call(f16[4,4,32,1024]{3,2,1,0} %fusion.352, f16[4,4,1024,1024]{3,2,1,0} %fusion.695), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/dot_general[dimension_numbers=(((3,), (2,)), ((0, 1), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.356 = f16[4,4,1024,32]{3,2,1,0} fusion(f16[4096,128]{1,0} %cublas-gemm.161), kind=kLoop, calls=%fused_computation.356
  %cublas-batch-gemm.73 = f16[4,4,1024,1024]{3,2,1,0} custom-call(f16[4,4,1024,32]{3,2,1,0} %fusion.356, f16[4,4,32,1024]{3,2,1,0} %get-tuple-element.512), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/dot_general[dimension_numbers=(((2,), (3,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.355 = f16[4,4,1024]{2,1,0} fusion(f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.73, f32[4,4,1024]{2,1,0} %fusion.387, f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.69, f16[4,4,1024]{2,1,0} %reduce.76), kind=kInput, calls=%fused_computation.355, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %fusion.354 = f16[4,4,1024,1024]{3,2,1,0} fusion(f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.73, f16[4,4,1024]{2,1,0} %fusion.355, f32[4,4,1024]{2,1,0} %fusion.387, f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.69, f16[4,4,1024]{2,1,0} %reduce.76), kind=kLoop, calls=%fused_computation.354, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %get-tuple-element.510 = f16[4,4,1024,32]{3,2,1,0} get-tuple-element((f16[4,4,1024,32]{3,2,1,0}, f16[4,4,1024,32]{3,2,1,0}) %fusion.353), index=0
  %cublas-batch-gemm.77 = f16[4,4,1024,32]{3,2,1,0} custom-call(f16[4,4,1024,1024]{3,2,1,0} %fusion.354, f16[4,4,1024,32]{3,2,1,0} %get-tuple-element.510), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/dot_general[dimension_numbers=(((3,), (1,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %cublas-batch-gemm.75 = f16[4,4,1024,32]{3,2,1,0} custom-call(f16[4,4,1024,1024]{3,2,1,0} %fusion.354, f16[4,4,1024,32]{3,2,1,0} %get-tuple-element.511), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/dot_general[dimension_numbers=(((2,), (1,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"2\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.351 = f16[4,1024,128,3]{3,2,1,0} fusion(f16[4,4,32,1024]{3,2,1,0} %cublas-batch-gemm.79, f16[4,4,1024,32]{3,2,1,0} %cublas-batch-gemm.77, f16[] %get-tuple-element.114, f16[4,4,1024,32]{3,2,1,0} %cublas-batch-gemm.75), kind=kLoop, calls=%fused_computation.351, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %bitcast.337 = f16[4096,384]{1,0} bitcast(f16[4,1024,128,3]{3,2,1,0} %fusion.351)
  %cublas-gemm.163 = f16[4096,1024]{1,0} custom-call(f16[4096,384]{1,0} %bitcast.337, f16[1024,384]{1,0} %convert.213), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.35 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.163), channel_id=36, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.30, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.350 = f16[4,1024,1024]{2,1,0} fusion(f16[4,1024,1024]{2,1,0} %fusion.357, f16[4096,1024]{1,0} %all-reduce.35), kind=kLoop, calls=%fused_computation.350, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %tuple.4 = (f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) tuple(f16[] %constant_862, s32[] %constant_59, f16[8,1,1,1024]{3,0,2,1} %broadcast.580, f16[8,1,1,1024]{3,0,2,1} %broadcast.581, f32[] %constant_863, /*index=5*/f32[] %constant_69, f32[] %constant_863, f32[] %constant_124, f16[] %constant_864, f16[] %constant_337, /*index=10*/f16[] %constant_865, f32[] %constant_863, f32[] %constant_69, f32[] %constant_863, f32[] %constant_124, /*index=15*/f32[8,1024]{1,0} %broadcast.421, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.582, f32[8,1024]{1,0} %broadcast.421, /*index=20*/f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.582, f32[1024]{0} %param.29, f32[1024]{0} %param.28, /*index=25*/f32[1024]{0} %param.27, f32[128,1024]{1,0} %param.26, f32[384]{0} %param.25, f32[1024,384]{1,0} %param.24, f32[512]{0} %param.31, /*index=30*/f32[1024,512]{1,0} %param.30, f32[1024]{0} %param.34, f32[1024]{0} %param.33, f32[512,1024]{1,0} %param.32, f16[4,1024,1024]{2,1,0} %fusion.632, /*index=35*/s32[4,1024]{1,0} %param.13, f16[4,1024,1024]{2,1,0} %fusion.350), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %bitcast.1851 = (f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) bitcast((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %tuple.4)
  %get-tuple-element.162 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=24, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.161 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=7, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.148 = f16[4,1024,1024]{2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=34, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.157 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=25, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %bitcast.339 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %get-tuple-element.148)
  %get-tuple-element.149 = f32[1024,384]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=28, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.234 = f16[1024,384]{1,0} convert(f32[1024,384]{1,0} %get-tuple-element.149), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %get-tuple-element.150 = f32[384]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=27, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.349 = f16[4096,384]{1,0} fusion(f32[384]{0} %get-tuple-element.150), kind=kLoop, calls=%fused_computation.349
  %cublas-gemm.167 = f16[4096,384]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.339, f16[1024,384]{1,0} %convert.234, f16[4096,384]{1,0} %fusion.349), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.347 = (f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) fusion(f16[4096,384]{1,0} %cublas-gemm.167), kind=kLoop, calls=%fused_computation.347
  %get-tuple-element.494 = f16[4,4,32,1024]{3,2,1,0} get-tuple-element((f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) %fusion.347), index=0
  %get-tuple-element.151 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=0, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.309 = (f16[4,4,1024,32]{3,2,1,0}, f16[4,4,1024,32]{3,2,1,0}) fusion(f16[4096,384]{1,0} %cublas-gemm.167, f16[] %get-tuple-element.151), kind=kLoop, calls=%fused_computation.309
  %get-tuple-element.493 = f16[4,4,1024,32]{3,2,1,0} get-tuple-element((f16[4,4,1024,32]{3,2,1,0}, f16[4,4,1024,32]{3,2,1,0}) %fusion.309), index=1
  %get-tuple-element.495 = f16[4,4,32,1024]{3,2,1,0} get-tuple-element((f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) %fusion.347), index=1
  %get-tuple-element.155 = f16[8,1,1,1024]{3,0,2,1} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.154 = f16[8,1,1,1024]{3,0,2,1} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=3, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.152 = s32[4,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=35, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.153 = s32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=1, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.345 = f16[4,4,1024,1024]{3,2,1,0} fusion(f16[8,1,1,1024]{3,0,2,1} %get-tuple-element.155, s32[] %fusion.522, f16[8,1,1,1024]{3,0,2,1} %get-tuple-element.154, s32[4,1024]{1,0} %get-tuple-element.152, s32[] %get-tuple-element.153), kind=kLoop, calls=%fused_computation.345
  %cublas-batch-gemm.83 = f16[4,4,1024,1024]{3,2,1,0} custom-call(f16[4,4,1024,32]{3,2,1,0} %get-tuple-element.493, f16[4,4,32,1024]{3,2,1,0} %get-tuple-element.495, f16[4,4,1024,1024]{3,2,1,0} %fusion.345), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/dot_general[dimension_numbers=(((3,), (3,)), ((0, 2), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %reduce.87 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.83, f16[] %constant_179), dimensions={3}, to_apply=%region_125.3653, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %fusion.343 = f32[4,4,1024]{2,1,0} fusion(f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.83, f16[4,4,1024]{2,1,0} %reduce.87), kind=kInput, calls=%fused_computation.343, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %fusion.693 = f16[4,4,1024,1024]{3,2,1,0} fusion(f32[4,4,1024]{2,1,0} %fusion.343, f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.83, f16[4,4,1024]{2,1,0} %reduce.87), kind=kLoop, calls=%fused_computation.693, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %cublas-batch-gemm.85 = f16[4,4,32,1024]{3,2,1,0} custom-call(f16[4,4,32,1024]{3,2,1,0} %get-tuple-element.494, f16[4,4,1024,1024]{3,2,1,0} %fusion.693), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/dot_general[dimension_numbers=(((1,), (3,)), ((0, 2), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"3\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.342 = f16[4096,128]{1,0} fusion(f16[4,4,32,1024]{3,2,1,0} %cublas-batch-gemm.85), kind=kLoop, calls=%fused_computation.342
  %get-tuple-element.156 = f32[128,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=26, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.238 = f16[128,1024]{1,0} convert(f32[128,1024]{1,0} %get-tuple-element.156), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %cublas-gemm.169 = f16[4096,1024]{1,0} custom-call(f16[4096,128]{1,0} %fusion.342, f16[128,1024]{1,0} %convert.238), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.36 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.169), channel_id=37, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.31, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.340 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) fusion(f16[4,1024,1024]{2,1,0} %get-tuple-element.148, f32[1024]{0} %get-tuple-element.157, f16[4096,1024]{1,0} %all-reduce.36), kind=kInput, calls=%fused_computation.340, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.489 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.340), index=0
  %get-tuple-element.158 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=4, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.490 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.340), index=1
  %get-tuple-element.160 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=6, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.159 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=5, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.173 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=31, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.176 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=14, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.163 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=23, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.491 = f16[4,1024,1024]{2,1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.340), index=2
  %fusion.334 = f16[4,1024,1024]{2,1,0} fusion(f32[1024]{0} %get-tuple-element.163, f32[1024]{0} %get-tuple-element.162, f32[4,1024]{1,0} %get-tuple-element.489, f32[] %get-tuple-element.158, f16[4,1024,1024]{2,1,0} %get-tuple-element.491, /*index=5*/f32[] %get-tuple-element.161, f32[4,1024]{1,0} %get-tuple-element.490, f32[] %get-tuple-element.160, f32[] %get-tuple-element.159), kind=kLoop, calls=%fused_computation.334, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %get-tuple-element.170 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=32, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.168 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=10, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %bitcast.355 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %fusion.334)
  %get-tuple-element.164 = f32[1024,512]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=30, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.242 = f16[1024,512]{1,0} convert(f32[1024,512]{1,0} %get-tuple-element.164), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %get-tuple-element.165 = f32[512]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=29, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.333 = f16[4096,512]{1,0} fusion(f32[512]{0} %get-tuple-element.165), kind=kLoop, calls=%fused_computation.333
  %cublas-gemm.173 = f16[4096,512]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.355, f16[1024,512]{1,0} %convert.242, f16[4096,512]{1,0} %fusion.333), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %get-tuple-element.167 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=9, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.166 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=8, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.331 = f16[4096,512]{1,0} fusion(f16[] %get-tuple-element.168, f16[4096,512]{1,0} %cublas-gemm.173, f16[] %get-tuple-element.167, f16[] %get-tuple-element.166), kind=kLoop, calls=%fused_computation.331
  %get-tuple-element.169 = f32[512,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=33, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.246 = f16[512,1024]{1,0} convert(f32[512,1024]{1,0} %get-tuple-element.169), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %cublas-gemm.175 = f16[4096,1024]{1,0} custom-call(f16[4096,512]{1,0} %fusion.331, f16[512,1024]{1,0} %convert.246), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.37 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.175), channel_id=38, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.32, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.329 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) fusion(f16[4,1024,1024]{2,1,0} %fusion.334, f32[1024]{0} %get-tuple-element.170, f16[4096,1024]{1,0} %all-reduce.37), kind=kInput, calls=%fused_computation.329, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.486 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.329), index=0
  %get-tuple-element.171 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=11, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.487 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.329), index=1
  %get-tuple-element.175 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=13, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.174 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=12, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.172 = f16[4,1024,1024]{2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=36, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.488 = f16[4,1024,1024]{2,1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.329), index=2
  %fusion.321 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) fusion(f32[1024]{0} %get-tuple-element.173, f32[] %get-tuple-element.176, f32[4,1024]{1,0} %get-tuple-element.486, f32[] %get-tuple-element.171, f32[4,1024]{1,0} %get-tuple-element.487, /*index=5*/f32[] %get-tuple-element.175, f32[] %get-tuple-element.174, f16[4,1024,1024]{2,1,0} %get-tuple-element.172, f16[4,1024,1024]{2,1,0} %get-tuple-element.488), kind=kInput, calls=%fused_computation.321, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %get-tuple-element.484 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.321), index=0
  %get-tuple-element.180 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=21, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.179 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=22, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.178 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=19, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.177 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=20, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.485 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.321), index=1
  %fusion.320 = f16[4,1024,1024]{2,1,0} fusion(f32[4,1024]{1,0} %get-tuple-element.484, f32[8,1024]{1,0} %get-tuple-element.180, s32[] %fusion.522, f32[8,1024]{1,0} %get-tuple-element.179, f32[8,1024]{1,0} %get-tuple-element.178, /*index=5*/f32[8,1024]{1,0} %get-tuple-element.177, f32[4,1024]{1,0} %get-tuple-element.485, f16[4,1024,1024]{2,1,0} %get-tuple-element.488, f32[] %get-tuple-element.175, f32[4,1024]{1,0} %get-tuple-element.486, /*index=10*/f32[] %get-tuple-element.171, f32[] %get-tuple-element.174, f32[4,1024]{1,0} %get-tuple-element.487, f32[] %get-tuple-element.176, f32[1024]{0} %get-tuple-element.173, /*index=15*/f16[4,1024,1024]{2,1,0} %get-tuple-element.172), kind=kLoop, calls=%fused_computation.320, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %bitcast.363 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %fusion.320)
  %cublas-gemm.177 = f16[4096,512]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.363, f16[512,1024]{1,0} %convert.246), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.319 = f16[4,1024,512]{2,1,0} fusion(f16[4096,512]{1,0} %cublas-gemm.177, f16[] %get-tuple-element.168, f16[] %get-tuple-element.166, f16[4096,512]{1,0} %cublas-gemm.173, f16[] %get-tuple-element.167), kind=kLoop, calls=%fused_computation.319, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %bitcast.365 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %fusion.319)
  %cublas-gemm.179 = f16[4096,1024]{1,0} custom-call(f16[4096,512]{1,0} %bitcast.365, f16[1024,512]{1,0} %convert.242), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.38 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.179), channel_id=39, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.33, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.314 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) fusion(f32[1024]{0} %get-tuple-element.162, f32[] %get-tuple-element.161, f32[4,1024]{1,0} %get-tuple-element.489, f32[] %get-tuple-element.158, f32[4,1024]{1,0} %get-tuple-element.490, /*index=5*/f32[] %get-tuple-element.160, f32[] %get-tuple-element.159, f16[4,1024,1024]{2,1,0} %fusion.320, f16[4096,1024]{1,0} %all-reduce.38, f16[4,1024,1024]{2,1,0} %get-tuple-element.491), kind=kInput, calls=%fused_computation.314, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %get-tuple-element.480 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.314), index=0
  %get-tuple-element.184 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=17, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.183 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=18, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.182 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=15, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.181 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1851), index=16, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.481 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.314), index=1
  %fusion.313 = f16[4,1024,1024]{2,1,0} fusion(f32[4,1024]{1,0} %get-tuple-element.480, f32[8,1024]{1,0} %get-tuple-element.184, s32[] %fusion.522, f32[8,1024]{1,0} %get-tuple-element.183, f32[8,1024]{1,0} %get-tuple-element.182, /*index=5*/f32[8,1024]{1,0} %get-tuple-element.181, f32[4,1024]{1,0} %get-tuple-element.481, f16[4,1024,1024]{2,1,0} %get-tuple-element.491, f32[] %get-tuple-element.160, f32[4,1024]{1,0} %get-tuple-element.489, /*index=10*/f32[] %get-tuple-element.158, f32[] %get-tuple-element.159, f32[4,1024]{1,0} %get-tuple-element.490, f32[] %get-tuple-element.161, f32[1024]{0} %get-tuple-element.162, /*index=15*/f16[4,1024,1024]{2,1,0} %fusion.320, f16[4096,1024]{1,0} %all-reduce.38), kind=kLoop, calls=%fused_computation.313, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %bitcast.369 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %fusion.313)
  %cublas-gemm.181 = f16[4096,128]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.369, f16[128,1024]{1,0} %convert.238), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.308 = f16[4,4,32,1024]{3,2,1,0} fusion(f16[4096,128]{1,0} %cublas-gemm.181), kind=kLoop, calls=%fused_computation.308, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/transpose[permutation=(0, 2, 3, 1)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %cublas-batch-gemm.93 = f16[4,4,32,1024]{3,2,1,0} custom-call(f16[4,4,32,1024]{3,2,1,0} %fusion.308, f16[4,4,1024,1024]{3,2,1,0} %fusion.693), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/dot_general[dimension_numbers=(((3,), (2,)), ((0, 1), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.312 = f16[4,4,1024,32]{3,2,1,0} fusion(f16[4096,128]{1,0} %cublas-gemm.181), kind=kLoop, calls=%fused_computation.312
  %cublas-batch-gemm.87 = f16[4,4,1024,1024]{3,2,1,0} custom-call(f16[4,4,1024,32]{3,2,1,0} %fusion.312, f16[4,4,32,1024]{3,2,1,0} %get-tuple-element.494), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/dot_general[dimension_numbers=(((2,), (3,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.311 = f16[4,4,1024]{2,1,0} fusion(f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.87, f32[4,4,1024]{2,1,0} %fusion.343, f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.83, f16[4,4,1024]{2,1,0} %reduce.87), kind=kInput, calls=%fused_computation.311, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %fusion.310 = f16[4,4,1024,1024]{3,2,1,0} fusion(f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.87, f16[4,4,1024]{2,1,0} %fusion.311, f32[4,4,1024]{2,1,0} %fusion.343, f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.83, f16[4,4,1024]{2,1,0} %reduce.87), kind=kLoop, calls=%fused_computation.310, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %get-tuple-element.492 = f16[4,4,1024,32]{3,2,1,0} get-tuple-element((f16[4,4,1024,32]{3,2,1,0}, f16[4,4,1024,32]{3,2,1,0}) %fusion.309), index=0
  %cublas-batch-gemm.91 = f16[4,4,1024,32]{3,2,1,0} custom-call(f16[4,4,1024,1024]{3,2,1,0} %fusion.310, f16[4,4,1024,32]{3,2,1,0} %get-tuple-element.492), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/dot_general[dimension_numbers=(((3,), (1,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %cublas-batch-gemm.89 = f16[4,4,1024,32]{3,2,1,0} custom-call(f16[4,4,1024,1024]{3,2,1,0} %fusion.310, f16[4,4,1024,32]{3,2,1,0} %get-tuple-element.493), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/dot_general[dimension_numbers=(((2,), (1,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"2\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.307 = f16[4,1024,128,3]{3,2,1,0} fusion(f16[4,4,32,1024]{3,2,1,0} %cublas-batch-gemm.93, f16[4,4,1024,32]{3,2,1,0} %cublas-batch-gemm.91, f16[] %get-tuple-element.151, f16[4,4,1024,32]{3,2,1,0} %cublas-batch-gemm.89), kind=kLoop, calls=%fused_computation.307, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %bitcast.382 = f16[4096,384]{1,0} bitcast(f16[4,1024,128,3]{3,2,1,0} %fusion.307)
  %cublas-gemm.183 = f16[4096,1024]{1,0} custom-call(f16[4096,384]{1,0} %bitcast.382, f16[1024,384]{1,0} %convert.234), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.39 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.183), channel_id=40, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.34, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.306 = f16[4,1024,1024]{2,1,0} fusion(f16[4,1024,1024]{2,1,0} %fusion.313, f16[4096,1024]{1,0} %all-reduce.39), kind=kLoop, calls=%fused_computation.306, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %tuple.5 = (f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) tuple(f16[] %constant_862, s32[] %constant_59, f16[8,1,1,1024]{3,0,2,1} %broadcast.580, f16[8,1,1,1024]{3,0,2,1} %broadcast.581, f32[] %constant_863, /*index=5*/f32[] %constant_69, f32[] %constant_863, f32[] %constant_124, f16[] %constant_864, f16[] %constant_337, /*index=10*/f16[] %constant_865, f32[] %constant_863, f32[] %constant_69, f32[] %constant_863, f32[] %constant_124, /*index=15*/f32[8,1024]{1,0} %broadcast.421, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.582, f32[8,1024]{1,0} %broadcast.421, /*index=20*/f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.582, f32[1024]{0} %param.17, f32[1024]{0} %param.16, /*index=25*/f32[1024]{0} %param.15, f32[128,1024]{1,0} %param.14, f32[384]{0} %param.12, f32[1024,384]{1,0} %param.11, f32[512]{0} %param.19, /*index=30*/f32[1024,512]{1,0} %param.18, f32[1024]{0} %param.22, f32[1024]{0} %param.21, f32[512,1024]{1,0} %param.20, f16[4,1024,1024]{2,1,0} %fusion.652, /*index=35*/s32[4,1024]{1,0} %param.13, f16[4,1024,1024]{2,1,0} %fusion.306), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %bitcast.1852 = (f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) bitcast((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %tuple.5)
  %get-tuple-element.199 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=24, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.198 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=7, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.185 = f16[4,1024,1024]{2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=34, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.194 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=25, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %bitcast.384 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %get-tuple-element.185)
  %get-tuple-element.186 = f32[1024,384]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=28, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.255 = f16[1024,384]{1,0} convert(f32[1024,384]{1,0} %get-tuple-element.186), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %get-tuple-element.187 = f32[384]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=27, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.305 = f16[4096,384]{1,0} fusion(f32[384]{0} %get-tuple-element.187), kind=kLoop, calls=%fused_computation.305
  %cublas-gemm.187 = f16[4096,384]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.384, f16[1024,384]{1,0} %convert.255, f16[4096,384]{1,0} %fusion.305), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.303 = (f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) fusion(f16[4096,384]{1,0} %cublas-gemm.187), kind=kLoop, calls=%fused_computation.303
  %get-tuple-element.476 = f16[4,4,32,1024]{3,2,1,0} get-tuple-element((f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) %fusion.303), index=0
  %get-tuple-element.188 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=0, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.265 = (f16[4,4,1024,32]{3,2,1,0}, f16[4,4,1024,32]{3,2,1,0}) fusion(f16[4096,384]{1,0} %cublas-gemm.187, f16[] %get-tuple-element.188), kind=kLoop, calls=%fused_computation.265
  %get-tuple-element.475 = f16[4,4,1024,32]{3,2,1,0} get-tuple-element((f16[4,4,1024,32]{3,2,1,0}, f16[4,4,1024,32]{3,2,1,0}) %fusion.265), index=1
  %get-tuple-element.477 = f16[4,4,32,1024]{3,2,1,0} get-tuple-element((f16[4,4,32,1024]{3,2,1,0}, f16[4,4,32,1024]{3,2,1,0}) %fusion.303), index=1
  %get-tuple-element.192 = f16[8,1,1,1024]{3,0,2,1} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.191 = f16[8,1,1,1024]{3,0,2,1} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=3, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.189 = s32[4,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=35, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.190 = s32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=1, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.301 = f16[4,4,1024,1024]{3,2,1,0} fusion(f16[8,1,1,1024]{3,0,2,1} %get-tuple-element.192, s32[] %fusion.522, f16[8,1,1,1024]{3,0,2,1} %get-tuple-element.191, s32[4,1024]{1,0} %get-tuple-element.189, s32[] %get-tuple-element.190), kind=kLoop, calls=%fused_computation.301
  %cublas-batch-gemm.97 = f16[4,4,1024,1024]{3,2,1,0} custom-call(f16[4,4,1024,32]{3,2,1,0} %get-tuple-element.475, f16[4,4,32,1024]{3,2,1,0} %get-tuple-element.477, f16[4,4,1024,1024]{3,2,1,0} %fusion.301), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/dot_general[dimension_numbers=(((3,), (3,)), ((0, 2), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %reduce.98 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.97, f16[] %constant_179), dimensions={3}, to_apply=%region_145.4112, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %fusion.299 = f32[4,4,1024]{2,1,0} fusion(f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.97, f16[4,4,1024]{2,1,0} %reduce.98), kind=kInput, calls=%fused_computation.299, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %fusion.691 = f16[4,4,1024,1024]{3,2,1,0} fusion(f32[4,4,1024]{2,1,0} %fusion.299, f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.97, f16[4,4,1024]{2,1,0} %reduce.98), kind=kLoop, calls=%fused_computation.691, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %cublas-batch-gemm.99 = f16[4,4,32,1024]{3,2,1,0} custom-call(f16[4,4,32,1024]{3,2,1,0} %get-tuple-element.476, f16[4,4,1024,1024]{3,2,1,0} %fusion.691), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/dot_general[dimension_numbers=(((1,), (3,)), ((0, 2), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"3\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.298 = f16[4096,128]{1,0} fusion(f16[4,4,32,1024]{3,2,1,0} %cublas-batch-gemm.99), kind=kLoop, calls=%fused_computation.298
  %get-tuple-element.193 = f32[128,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=26, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.259 = f16[128,1024]{1,0} convert(f32[128,1024]{1,0} %get-tuple-element.193), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %cublas-gemm.189 = f16[4096,1024]{1,0} custom-call(f16[4096,128]{1,0} %fusion.298, f16[128,1024]{1,0} %convert.259), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.40 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.189), channel_id=41, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.35, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.296 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) fusion(f16[4,1024,1024]{2,1,0} %get-tuple-element.185, f32[1024]{0} %get-tuple-element.194, f16[4096,1024]{1,0} %all-reduce.40), kind=kInput, calls=%fused_computation.296, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.471 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.296), index=0
  %get-tuple-element.195 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=4, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.472 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.296), index=1
  %get-tuple-element.197 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=6, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.196 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=5, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.210 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=31, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.213 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=14, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.200 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=23, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.473 = f16[4,1024,1024]{2,1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.296), index=2
  %fusion.290 = f16[4,1024,1024]{2,1,0} fusion(f32[1024]{0} %get-tuple-element.200, f32[1024]{0} %get-tuple-element.199, f32[4,1024]{1,0} %get-tuple-element.471, f32[] %get-tuple-element.195, f16[4,1024,1024]{2,1,0} %get-tuple-element.473, /*index=5*/f32[] %get-tuple-element.198, f32[4,1024]{1,0} %get-tuple-element.472, f32[] %get-tuple-element.197, f32[] %get-tuple-element.196), kind=kLoop, calls=%fused_computation.290, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %get-tuple-element.207 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=32, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.205 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=10, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %bitcast.400 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %fusion.290)
  %get-tuple-element.201 = f32[1024,512]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=30, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.263 = f16[1024,512]{1,0} convert(f32[1024,512]{1,0} %get-tuple-element.201), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %get-tuple-element.202 = f32[512]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=29, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.289 = f16[4096,512]{1,0} fusion(f32[512]{0} %get-tuple-element.202), kind=kLoop, calls=%fused_computation.289
  %cublas-gemm.193 = f16[4096,512]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.400, f16[1024,512]{1,0} %convert.263, f16[4096,512]{1,0} %fusion.289), custom_call_target="__cublas$gemm", output_to_operand_aliasing={{}: (2, {})}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":1,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %get-tuple-element.204 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=9, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.203 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=8, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.287 = f16[4096,512]{1,0} fusion(f16[] %get-tuple-element.205, f16[4096,512]{1,0} %cublas-gemm.193, f16[] %get-tuple-element.204, f16[] %get-tuple-element.203), kind=kLoop, calls=%fused_computation.287
  %get-tuple-element.206 = f32[512,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=33, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.267 = f16[512,1024]{1,0} convert(f32[512,1024]{1,0} %get-tuple-element.206), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %cublas-gemm.195 = f16[4096,1024]{1,0} custom-call(f16[4096,512]{1,0} %fusion.287, f16[512,1024]{1,0} %convert.267), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.41 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.195), channel_id=42, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.36, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.285 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) fusion(f16[4,1024,1024]{2,1,0} %fusion.290, f32[1024]{0} %get-tuple-element.207, f16[4096,1024]{1,0} %all-reduce.41), kind=kInput, calls=%fused_computation.285, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.468 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.285), index=0
  %get-tuple-element.208 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=11, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.469 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.285), index=1
  %get-tuple-element.212 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=13, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.211 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=12, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.209 = f16[4,1024,1024]{2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=36, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.470 = f16[4,1024,1024]{2,1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %fusion.285), index=2
  %fusion.277 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) fusion(f32[1024]{0} %get-tuple-element.210, f32[] %get-tuple-element.213, f32[4,1024]{1,0} %get-tuple-element.468, f32[] %get-tuple-element.208, f32[4,1024]{1,0} %get-tuple-element.469, /*index=5*/f32[] %get-tuple-element.212, f32[] %get-tuple-element.211, f16[4,1024,1024]{2,1,0} %get-tuple-element.209, f16[4,1024,1024]{2,1,0} %get-tuple-element.470), kind=kInput, calls=%fused_computation.277, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %get-tuple-element.466 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.277), index=0
  %get-tuple-element.217 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=21, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.216 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=22, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.215 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=19, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.214 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=20, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.467 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.277), index=1
  %fusion.276 = f16[4,1024,1024]{2,1,0} fusion(f32[4,1024]{1,0} %get-tuple-element.466, f32[8,1024]{1,0} %get-tuple-element.217, s32[] %fusion.522, f32[8,1024]{1,0} %get-tuple-element.216, f32[8,1024]{1,0} %get-tuple-element.215, /*index=5*/f32[8,1024]{1,0} %get-tuple-element.214, f32[4,1024]{1,0} %get-tuple-element.467, f16[4,1024,1024]{2,1,0} %get-tuple-element.470, f32[] %get-tuple-element.212, f32[4,1024]{1,0} %get-tuple-element.468, /*index=10*/f32[] %get-tuple-element.208, f32[] %get-tuple-element.211, f32[4,1024]{1,0} %get-tuple-element.469, f32[] %get-tuple-element.213, f32[1024]{0} %get-tuple-element.210, /*index=15*/f16[4,1024,1024]{2,1,0} %get-tuple-element.209), kind=kLoop, calls=%fused_computation.276, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %bitcast.408 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %fusion.276)
  %cublas-gemm.197 = f16[4096,512]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.408, f16[512,1024]{1,0} %convert.267), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.275 = f16[4,1024,512]{2,1,0} fusion(f16[4096,512]{1,0} %cublas-gemm.197, f16[] %get-tuple-element.205, f16[] %get-tuple-element.203, f16[4096,512]{1,0} %cublas-gemm.193, f16[] %get-tuple-element.204), kind=kLoop, calls=%fused_computation.275, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %bitcast.410 = f16[4096,512]{1,0} bitcast(f16[4,1024,512]{2,1,0} %fusion.275)
  %cublas-gemm.199 = f16[4096,1024]{1,0} custom-call(f16[4096,512]{1,0} %bitcast.410, f16[1024,512]{1,0} %convert.263), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.42 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.199), channel_id=43, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.37, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.270 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) fusion(f32[1024]{0} %get-tuple-element.199, f32[] %get-tuple-element.198, f32[4,1024]{1,0} %get-tuple-element.471, f32[] %get-tuple-element.195, f32[4,1024]{1,0} %get-tuple-element.472, /*index=5*/f32[] %get-tuple-element.197, f32[] %get-tuple-element.196, f16[4,1024,1024]{2,1,0} %fusion.276, f16[4096,1024]{1,0} %all-reduce.42, f16[4,1024,1024]{2,1,0} %get-tuple-element.473), kind=kInput, calls=%fused_computation.270, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %get-tuple-element.462 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.270), index=0
  %get-tuple-element.221 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=17, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.220 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=18, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.219 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=15, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.218 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,0,2,1}, f16[8,1,1,1024]{3,0,2,1}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1852), index=16, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.463 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.270), index=1
  %fusion.269 = f16[4,1024,1024]{2,1,0} fusion(f32[4,1024]{1,0} %get-tuple-element.462, f32[8,1024]{1,0} %get-tuple-element.221, s32[] %fusion.522, f32[8,1024]{1,0} %get-tuple-element.220, f32[8,1024]{1,0} %get-tuple-element.219, /*index=5*/f32[8,1024]{1,0} %get-tuple-element.218, f32[4,1024]{1,0} %get-tuple-element.463, f16[4,1024,1024]{2,1,0} %get-tuple-element.473, f32[] %get-tuple-element.197, f32[4,1024]{1,0} %get-tuple-element.471, /*index=10*/f32[] %get-tuple-element.195, f32[] %get-tuple-element.196, f32[4,1024]{1,0} %get-tuple-element.472, f32[] %get-tuple-element.198, f32[1024]{0} %get-tuple-element.199, /*index=15*/f16[4,1024,1024]{2,1,0} %fusion.276, f16[4096,1024]{1,0} %all-reduce.42), kind=kLoop, calls=%fused_computation.269, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %fusion.690 = f16[1024]{0} fusion(f16[4,1024,1024]{2,1,0} %fusion.269), kind=kInput, calls=%fused_computation.690, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %constant_113 = s32[16]{0} constant({...}), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/position_embeddings.embed_call_one_hot/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %iota.11 = s32[1,1,1024]{2,1,0} iota(), iota_dimension=2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/position_embeddings.embed_call_one_hot/remat(core_fn)/iota[dtype=int32 shape=(1, 1, 1024) dimension=2]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %bitcast.414 = f16[4096,1024]{1,0} bitcast(f16[4,1024,1024]{2,1,0} %fusion.269)
  %cublas-gemm.201 = f16[4096,128]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.414, f16[128,1024]{1,0} %convert.259), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.264 = f16[4,4,32,1024]{3,2,1,0} fusion(f16[4096,128]{1,0} %cublas-gemm.201), kind=kLoop, calls=%fused_computation.264, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/transpose[permutation=(0, 2, 3, 1)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %cublas-batch-gemm.107 = f16[4,4,32,1024]{3,2,1,0} custom-call(f16[4,4,32,1024]{3,2,1,0} %fusion.264, f16[4,4,1024,1024]{3,2,1,0} %fusion.691), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/dot_general[dimension_numbers=(((3,), (2,)), ((0, 1), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.268 = f16[4,4,1024,32]{3,2,1,0} fusion(f16[4096,128]{1,0} %cublas-gemm.201), kind=kLoop, calls=%fused_computation.268
  %cublas-batch-gemm.101 = f16[4,4,1024,1024]{3,2,1,0} custom-call(f16[4,4,1024,32]{3,2,1,0} %fusion.268, f16[4,4,32,1024]{3,2,1,0} %get-tuple-element.476), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/dot_general[dimension_numbers=(((2,), (3,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.267 = f16[4,4,1024]{2,1,0} fusion(f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.101, f32[4,4,1024]{2,1,0} %fusion.299, f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.97, f16[4,4,1024]{2,1,0} %reduce.98), kind=kInput, calls=%fused_computation.267, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %fusion.266 = f16[4,4,1024,1024]{3,2,1,0} fusion(f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.101, f16[4,4,1024]{2,1,0} %fusion.267, f32[4,4,1024]{2,1,0} %fusion.299, f16[4,4,1024,1024]{3,2,1,0} %cublas-batch-gemm.97, f16[4,4,1024]{2,1,0} %reduce.98), kind=kLoop, calls=%fused_computation.266, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %get-tuple-element.474 = f16[4,4,1024,32]{3,2,1,0} get-tuple-element((f16[4,4,1024,32]{3,2,1,0}, f16[4,4,1024,32]{3,2,1,0}) %fusion.265), index=0
  %cublas-batch-gemm.105 = f16[4,4,1024,32]{3,2,1,0} custom-call(f16[4,4,1024,1024]{3,2,1,0} %fusion.266, f16[4,4,1024,32]{3,2,1,0} %get-tuple-element.474), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/dot_general[dimension_numbers=(((3,), (1,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"3\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %cublas-batch-gemm.103 = f16[4,4,1024,32]{3,2,1,0} custom-call(f16[4,4,1024,1024]{3,2,1,0} %fusion.266, f16[4,4,1024,32]{3,2,1,0} %get-tuple-element.475), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/dot_general[dimension_numbers=(((2,), (1,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"2\"],\"rhs_contracting_dimensions\":[\"2\"],\"lhs_batch_dimensions\":[\"0\",\"1\"],\"rhs_batch_dimensions\":[\"0\",\"1\"]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %fusion.263 = f16[4,1024,128,3]{3,2,1,0} fusion(f16[4,4,32,1024]{3,2,1,0} %cublas-batch-gemm.107, f16[4,4,1024,32]{3,2,1,0} %cublas-batch-gemm.105, f16[] %get-tuple-element.188, f16[4,4,1024,32]{3,2,1,0} %cublas-batch-gemm.103), kind=kLoop, calls=%fused_computation.263, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %bitcast.427 = f16[4096,384]{1,0} bitcast(f16[4,1024,128,3]{3,2,1,0} %fusion.263)
  %cublas-gemm.203 = f16[4096,1024]{1,0} custom-call(f16[4096,384]{1,0} %bitcast.427, f16[1024,384]{1,0} %convert.255), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %all-reduce.43 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %cublas-gemm.203), channel_id=44, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.38, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %fusion.252 = (f32[4,1024]{1,0}, f32[4,1024]{1,0}) fusion(f32[1024]{0} %param.9, f16[4,1024,1024]{2,1,0} %fusion.269, f16[4096,1024]{1,0} %all-reduce.43, f32[4,1024]{1,0} %get-tuple-element.814, f16[4096,1024]{1,0} %bitcast.1690, /*index=5*/f32[4,1024]{1,0} %get-tuple-element.815), kind=kInput, calls=%fused_computation.252, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %get-tuple-element.457 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.252), index=1
  %get-tuple-element.456 = f32[4,1024]{1,0} get-tuple-element((f32[4,1024]{1,0}, f32[4,1024]{1,0}) %fusion.252), index=0
  %fusion.249 = f16[4,1024,1024]{2,1,0} fusion(f32[4,1024]{1,0} %get-tuple-element.457, f32[4,1024]{1,0} %get-tuple-element.814, f32[4,1024]{1,0} %get-tuple-element.456, f16[4096,1024]{1,0} %bitcast.1690, f32[4,1024]{1,0} %get-tuple-element.815, /*index=5*/f32[1024]{0} %param.9, f16[4,1024,1024]{2,1,0} %fusion.269, f16[4096,1024]{1,0} %all-reduce.43), kind=kLoop, calls=%fused_computation.249, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %tuple.6 = (s32[1,1,1024]{2,1,0}, s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) tuple(s32[1,1,1024]{2,1,0} %iota.11, s32[4,1024]{1,0} %param.7, f16[4,1024,1024]{2,1,0} %fusion.249), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/position_embeddings.embed_call_one_hot/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %bitcast.1853 = (s32[1,1,1024]{2,1,0}, s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) bitcast((s32[1,1,1024]{2,1,0}, s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %tuple.6)
  %get-tuple-element.223 = s32[1,1,1024]{2,1,0} get-tuple-element((s32[1,1,1024]{2,1,0}, s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1853), index=0, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/position_embeddings.embed_call_one_hot/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.222 = s32[4,1024]{1,0} get-tuple-element((s32[1,1,1024]{2,1,0}, s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1853), index=1, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/position_embeddings.embed_call_one_hot/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.248 = f16[4096,128]{1,0} fusion(s32[16]{0} %constant_113, u32[] %partition-id, s32[1,1,1024]{2,1,0} %get-tuple-element.223, s32[4,1024]{1,0} %get-tuple-element.222), kind=kLoop, calls=%fused_computation.248
  %get-tuple-element.224 = f16[4,1024,1024]{2,1,0} get-tuple-element((s32[1,1,1024]{2,1,0}, s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1853), index=2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/position_embeddings.embed_call_one_hot/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %bitcast.435 = f16[1024,4096]{0,1} bitcast(f16[4,1024,1024]{2,1,0} %get-tuple-element.224)
  %cublas-gemm.205 = f16[128,1024]{1,0} custom-call(f16[4096,128]{1,0} %fusion.248, f16[1024,4096]{0,1} %bitcast.435), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/position_embeddings.embed_call_one_hot/remat(core_fn)/transpose[permutation=(1, 0)]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=237}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %bitcast.1696 = f16[131072]{0} bitcast(f16[128,1024]{1,0} %cublas-gemm.205)
  %bitcast.494 = f16[1024,4096]{0,1} bitcast(f16[4,1024,1024]{2,1,0} %fusion.489)
  %cublas-gemm.251 = f16[128,1024]{1,0} custom-call(f16[4096,128]{1,0} %fusion.518, f16[1024,4096]{0,1} %bitcast.494), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %bitcast.1697 = f16[131072]{0} bitcast(f16[128,1024]{1,0} %cublas-gemm.251)
  %bitcast.484 = f16[1024,4096]{0,1} bitcast(f16[4,1024,1024]{2,1,0} %fusion.445)
  %cublas-gemm.243 = f16[128,1024]{1,0} custom-call(f16[4096,128]{1,0} %fusion.474, f16[1024,4096]{0,1} %bitcast.484), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %bitcast.1698 = f16[131072]{0} bitcast(f16[128,1024]{1,0} %cublas-gemm.243)
  %bitcast.474 = f16[1024,4096]{0,1} bitcast(f16[4,1024,1024]{2,1,0} %fusion.401)
  %cublas-gemm.235 = f16[128,1024]{1,0} custom-call(f16[4096,128]{1,0} %fusion.430, f16[1024,4096]{0,1} %bitcast.474), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %bitcast.1699 = f16[131072]{0} bitcast(f16[128,1024]{1,0} %cublas-gemm.235)
  %bitcast.464 = f16[1024,4096]{0,1} bitcast(f16[4,1024,1024]{2,1,0} %fusion.357)
  %cublas-gemm.227 = f16[128,1024]{1,0} custom-call(f16[4096,128]{1,0} %fusion.386, f16[1024,4096]{0,1} %bitcast.464), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %bitcast.1700 = f16[131072]{0} bitcast(f16[128,1024]{1,0} %cublas-gemm.227)
  %bitcast.454 = f16[1024,4096]{0,1} bitcast(f16[4,1024,1024]{2,1,0} %fusion.313)
  %cublas-gemm.219 = f16[128,1024]{1,0} custom-call(f16[4096,128]{1,0} %fusion.342, f16[1024,4096]{0,1} %bitcast.454), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %bitcast.1701 = f16[131072]{0} bitcast(f16[128,1024]{1,0} %cublas-gemm.219)
  %bitcast.444 = f16[1024,4096]{0,1} bitcast(f16[4,1024,1024]{2,1,0} %fusion.269)
  %cublas-gemm.211 = f16[128,1024]{1,0} custom-call(f16[4096,128]{1,0} %fusion.298, f16[1024,4096]{0,1} %bitcast.444), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %bitcast.1702 = f16[131072]{0} bitcast(f16[128,1024]{1,0} %cublas-gemm.211)
  %constant_60 = s32[16]{0} constant({...}), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %iota.12 = s32[1,1,51200]{2,1,0} iota(), iota_dimension=2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/word_embeddings.embed_call_one_hot/remat(core_fn)/iota[dtype=int32 shape=(1, 1, 51200) dimension=2]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %tuple.7 = (s32[1,1,51200]{2,1,0}, s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) tuple(s32[1,1,51200]{2,1,0} %iota.12, s32[4,1024]{1,0} %param.5, f16[4,1024,1024]{2,1,0} %fusion.249), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/word_embeddings.embed_call_one_hot/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %bitcast.1854 = (s32[1,1,51200]{2,1,0}, s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) bitcast((s32[1,1,51200]{2,1,0}, s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %tuple.7)
  %get-tuple-element.226 = s32[1,1,51200]{2,1,0} get-tuple-element((s32[1,1,51200]{2,1,0}, s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1854), index=0, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/word_embeddings.embed_call_one_hot/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.225 = s32[4,1024]{1,0} get-tuple-element((s32[1,1,51200]{2,1,0}, s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1854), index=1, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/word_embeddings.embed_call_one_hot/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %fusion.244 = f16[4096,6400]{1,0} fusion(s32[16]{0} %constant_60, u32[] %partition-id, s32[1,1,51200]{2,1,0} %get-tuple-element.226, s32[4,1024]{1,0} %get-tuple-element.225), kind=kLoop, calls=%fused_computation.244
  %get-tuple-element.227 = f16[4,1024,1024]{2,1,0} get-tuple-element((s32[1,1,51200]{2,1,0}, s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %bitcast.1854), index=2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/word_embeddings.embed_call_one_hot/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %bitcast.442 = f16[1024,4096]{0,1} bitcast(f16[4,1024,1024]{2,1,0} %get-tuple-element.227)
  %cublas-gemm.209 = f16[6400,1024]{1,0} custom-call(f16[4096,6400]{1,0} %fusion.244, f16[1024,4096]{0,1} %bitcast.442), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/word_embeddings.embed_call_one_hot/remat(core_fn)/transpose[permutation=(1, 0)]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=237}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %bitcast.1703 = f16[6553600]{0} bitcast(f16[6400,1024]{1,0} %cublas-gemm.209)
  %bitcast.437 = f16[6400,4096]{0,1} bitcast(f16[4,1024,6400]{2,1,0} %fusion.533)
  %cublas-gemm.207 = f16[6400,1024]{1,0} custom-call(f16[6400,4096]{0,1} %bitcast.437, f16[4096,1024]{1,0} %fusion.537), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/dot_general[dimension_numbers=(((0, 1), (0, 1)), ((), ())) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/gpt_model.py" source_line=70}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %bitcast.1704 = f16[6553600]{0} bitcast(f16[6400,1024]{1,0} %cublas-gemm.207)
  %fusion.689 = f16[384]{0} fusion(f16[4,1024,128,3]{3,2,1,0} %fusion.263), kind=kInput, calls=%fused_computation.689, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %fusion.669 = f16[384]{0} fusion(f16[4,1024,128,3]{3,2,1,0} %fusion.483), kind=kInput, calls=%fused_computation.669, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %fusion.673 = f16[384]{0} fusion(f16[4,1024,128,3]{3,2,1,0} %fusion.439), kind=kInput, calls=%fused_computation.673, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %fusion.677 = f16[384]{0} fusion(f16[4,1024,128,3]{3,2,1,0} %fusion.395), kind=kInput, calls=%fused_computation.677, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %fusion.681 = f16[384]{0} fusion(f16[4,1024,128,3]{3,2,1,0} %fusion.351), kind=kInput, calls=%fused_computation.681, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %fusion.685 = f16[384]{0} fusion(f16[4,1024,128,3]{3,2,1,0} %fusion.307), kind=kInput, calls=%fused_computation.685, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %bitcast.448 = f16[384,4096]{0,1} bitcast(f16[4,1024,128,3]{3,2,1,0} %fusion.263)
  %cublas-gemm.213 = f16[1024,384]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.384, f16[384,4096]{0,1} %bitcast.448), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %bitcast.1711 = f16[393216]{0} bitcast(f16[1024,384]{1,0} %cublas-gemm.213)
  %bitcast.498 = f16[384,4096]{0,1} bitcast(f16[4,1024,128,3]{3,2,1,0} %fusion.483)
  %cublas-gemm.253 = f16[1024,384]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.158, f16[384,4096]{0,1} %bitcast.498), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %bitcast.1712 = f16[393216]{0} bitcast(f16[1024,384]{1,0} %cublas-gemm.253)
  %bitcast.488 = f16[384,4096]{0,1} bitcast(f16[4,1024,128,3]{3,2,1,0} %fusion.439)
  %cublas-gemm.245 = f16[1024,384]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.204, f16[384,4096]{0,1} %bitcast.488), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %bitcast.1713 = f16[393216]{0} bitcast(f16[1024,384]{1,0} %cublas-gemm.245)
  %bitcast.478 = f16[384,4096]{0,1} bitcast(f16[4,1024,128,3]{3,2,1,0} %fusion.395)
  %cublas-gemm.237 = f16[1024,384]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.249, f16[384,4096]{0,1} %bitcast.478), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %bitcast.1714 = f16[393216]{0} bitcast(f16[1024,384]{1,0} %cublas-gemm.237)
  %bitcast.468 = f16[384,4096]{0,1} bitcast(f16[4,1024,128,3]{3,2,1,0} %fusion.351)
  %cublas-gemm.229 = f16[1024,384]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.294, f16[384,4096]{0,1} %bitcast.468), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %bitcast.1715 = f16[393216]{0} bitcast(f16[1024,384]{1,0} %cublas-gemm.229)
  %bitcast.458 = f16[384,4096]{0,1} bitcast(f16[4,1024,128,3]{3,2,1,0} %fusion.307)
  %cublas-gemm.221 = f16[1024,384]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.339, f16[384,4096]{0,1} %bitcast.458), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %bitcast.1716 = f16[393216]{0} bitcast(f16[1024,384]{1,0} %cublas-gemm.221)
  %fusion.688 = f16[512]{0} fusion(f16[4,1024,512]{2,1,0} %fusion.275), kind=kInput, calls=%fused_computation.688, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %fusion.668 = f16[512]{0} fusion(f16[4,1024,512]{2,1,0} %fusion.495), kind=kInput, calls=%fused_computation.668, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %fusion.672 = f16[512]{0} fusion(f16[4,1024,512]{2,1,0} %fusion.451), kind=kInput, calls=%fused_computation.672, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %fusion.676 = f16[512]{0} fusion(f16[4,1024,512]{2,1,0} %fusion.407), kind=kInput, calls=%fused_computation.676, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %fusion.680 = f16[512]{0} fusion(f16[4,1024,512]{2,1,0} %fusion.363), kind=kInput, calls=%fused_computation.680, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %fusion.684 = f16[512]{0} fusion(f16[4,1024,512]{2,1,0} %fusion.319), kind=kInput, calls=%fused_computation.684, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %bitcast.450 = f16[512,4096]{0,1} bitcast(f16[4,1024,512]{2,1,0} %fusion.275)
  %cublas-gemm.215 = f16[1024,512]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.400, f16[512,4096]{0,1} %bitcast.450), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %bitcast.1723 = f16[524288]{0} bitcast(f16[1024,512]{1,0} %cublas-gemm.215)
  %bitcast.500 = f16[512,4096]{0,1} bitcast(f16[4,1024,512]{2,1,0} %fusion.495)
  %cublas-gemm.255 = f16[1024,512]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.175, f16[512,4096]{0,1} %bitcast.500), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %bitcast.1724 = f16[524288]{0} bitcast(f16[1024,512]{1,0} %cublas-gemm.255)
  %bitcast.490 = f16[512,4096]{0,1} bitcast(f16[4,1024,512]{2,1,0} %fusion.451)
  %cublas-gemm.247 = f16[1024,512]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.220, f16[512,4096]{0,1} %bitcast.490), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %bitcast.1725 = f16[524288]{0} bitcast(f16[1024,512]{1,0} %cublas-gemm.247)
  %bitcast.480 = f16[512,4096]{0,1} bitcast(f16[4,1024,512]{2,1,0} %fusion.407)
  %cublas-gemm.239 = f16[1024,512]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.265, f16[512,4096]{0,1} %bitcast.480), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %bitcast.1726 = f16[524288]{0} bitcast(f16[1024,512]{1,0} %cublas-gemm.239)
  %bitcast.470 = f16[512,4096]{0,1} bitcast(f16[4,1024,512]{2,1,0} %fusion.363)
  %cublas-gemm.231 = f16[1024,512]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.310, f16[512,4096]{0,1} %bitcast.470), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %bitcast.1727 = f16[524288]{0} bitcast(f16[1024,512]{1,0} %cublas-gemm.231)
  %bitcast.460 = f16[512,4096]{0,1} bitcast(f16[4,1024,512]{2,1,0} %fusion.319)
  %cublas-gemm.223 = f16[1024,512]{1,0} custom-call(f16[4096,1024]{1,0} %bitcast.355, f16[512,4096]{0,1} %bitcast.460), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %bitcast.1728 = f16[524288]{0} bitcast(f16[1024,512]{1,0} %cublas-gemm.223)
  %fusion.683 = f16[1024]{0} fusion(f16[4,1024,1024]{2,1,0} %fusion.320), kind=kInput, calls=%fused_computation.683, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %fusion.686 = f16[1024]{0} fusion(f16[4,1024,1024]{2,1,0} %fusion.313), kind=kInput, calls=%fused_computation.686, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %fusion.687 = f16[1024]{0} fusion(f16[4,1024,1024]{2,1,0} %fusion.276), kind=kInput, calls=%fused_computation.687, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %bitcast.452 = f16[1024,4096]{0,1} bitcast(f16[4,1024,1024]{2,1,0} %fusion.276)
  %cublas-gemm.217 = f16[512,1024]{1,0} custom-call(f16[4096,512]{1,0} %fusion.287, f16[1024,4096]{0,1} %bitcast.452), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %bitcast.1732 = f16[524288]{0} bitcast(f16[512,1024]{1,0} %cublas-gemm.217)
  %bitcast.502 = f16[1024,4096]{0,1} bitcast(f16[4,1024,1024]{2,1,0} %fusion.496)
  %cublas-gemm.257 = f16[512,1024]{1,0} custom-call(f16[4096,512]{1,0} %fusion.507, f16[1024,4096]{0,1} %bitcast.502), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %bitcast.1733 = f16[524288]{0} bitcast(f16[512,1024]{1,0} %cublas-gemm.257)
  %bitcast.492 = f16[1024,4096]{0,1} bitcast(f16[4,1024,1024]{2,1,0} %fusion.452)
  %cublas-gemm.249 = f16[512,1024]{1,0} custom-call(f16[4096,512]{1,0} %fusion.463, f16[1024,4096]{0,1} %bitcast.492), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %bitcast.1734 = f16[524288]{0} bitcast(f16[512,1024]{1,0} %cublas-gemm.249)
  %bitcast.482 = f16[1024,4096]{0,1} bitcast(f16[4,1024,1024]{2,1,0} %fusion.408)
  %cublas-gemm.241 = f16[512,1024]{1,0} custom-call(f16[4096,512]{1,0} %fusion.419, f16[1024,4096]{0,1} %bitcast.482), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %bitcast.1735 = f16[524288]{0} bitcast(f16[512,1024]{1,0} %cublas-gemm.241)
  %bitcast.472 = f16[1024,4096]{0,1} bitcast(f16[4,1024,1024]{2,1,0} %fusion.364)
  %cublas-gemm.233 = f16[512,1024]{1,0} custom-call(f16[4096,512]{1,0} %fusion.375, f16[1024,4096]{0,1} %bitcast.472), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %bitcast.1736 = f16[524288]{0} bitcast(f16[512,1024]{1,0} %cublas-gemm.233)
  %bitcast.462 = f16[1024,4096]{0,1} bitcast(f16[4,1024,1024]{2,1,0} %fusion.320)
  %cublas-gemm.225 = f16[512,1024]{1,0} custom-call(f16[4096,512]{1,0} %fusion.331, f16[1024,4096]{0,1} %bitcast.462), custom_call_target="__cublas$gemm", metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"precision_config\":{\"operand_precision\":[\"DEFAULT\",\"DEFAULT\"]},\"epilogue\":\"DEFAULT\"}"
  %bitcast.1737 = f16[524288]{0} bitcast(f16[512,1024]{1,0} %cublas-gemm.225)
  %fusion.678 = f16[1024]{0} fusion(f16[4,1024,1024]{2,1,0} %fusion.401), kind=kInput, calls=%fused_computation.678, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %fusion.679 = f16[1024]{0} fusion(f16[4,1024,1024]{2,1,0} %fusion.364), kind=kInput, calls=%fused_computation.679, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %fusion.682 = f16[1024]{0} fusion(f16[4,1024,1024]{2,1,0} %fusion.357), kind=kInput, calls=%fused_computation.682, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %fusion.674 = f16[1024]{0} fusion(f16[4,1024,1024]{2,1,0} %fusion.445), kind=kInput, calls=%fused_computation.674, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %fusion.675 = f16[1024]{0} fusion(f16[4,1024,1024]{2,1,0} %fusion.408), kind=kInput, calls=%fused_computation.675, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %concatenate.1 = f16[22699520]{0} concatenate(f16[6400]{0} %fusion.703, f16[1024]{0} %fusion.667, f16[1024]{0} %fusion.670, f16[1024]{0} %fusion.671, f16[1024]{0} %fusion.690, /*index=5*/f16[131072]{0} %bitcast.1696, f16[131072]{0} %bitcast.1697, f16[131072]{0} %bitcast.1698, f16[131072]{0} %bitcast.1699, f16[131072]{0} %bitcast.1700, /*index=10*/f16[131072]{0} %bitcast.1701, f16[131072]{0} %bitcast.1702, f16[6553600]{0} %bitcast.1703, f16[6553600]{0} %bitcast.1704, f16[384]{0} %fusion.689, /*index=15*/f16[384]{0} %fusion.669, f16[384]{0} %fusion.673, f16[384]{0} %fusion.677, f16[384]{0} %fusion.681, f16[384]{0} %fusion.685, /*index=20*/f16[393216]{0} %bitcast.1711, f16[393216]{0} %bitcast.1712, f16[393216]{0} %bitcast.1713, f16[393216]{0} %bitcast.1714, f16[393216]{0} %bitcast.1715, /*index=25*/f16[393216]{0} %bitcast.1716, f16[512]{0} %fusion.688, f16[512]{0} %fusion.668, f16[512]{0} %fusion.672, f16[512]{0} %fusion.676, /*index=30*/f16[512]{0} %fusion.680, f16[512]{0} %fusion.684, f16[524288]{0} %bitcast.1723, f16[524288]{0} %bitcast.1724, f16[524288]{0} %bitcast.1725, /*index=35*/f16[524288]{0} %bitcast.1726, f16[524288]{0} %bitcast.1727, f16[524288]{0} %bitcast.1728, f16[1024]{0} %fusion.683, f16[1024]{0} %fusion.686, /*index=40*/f16[1024]{0} %fusion.687, f16[524288]{0} %bitcast.1732, f16[524288]{0} %bitcast.1733, f16[524288]{0} %bitcast.1734, f16[524288]{0} %bitcast.1735, /*index=45*/f16[524288]{0} %bitcast.1736, f16[524288]{0} %bitcast.1737, f16[1024]{0} %fusion.678, f16[1024]{0} %fusion.679, f16[1024]{0} %fusion.682, /*index=50*/f16[1024]{0} %fusion.674, f16[1024]{0} %fusion.675), dimensions={0}
  %all-reduce.124 = f16[22699520]{0} all-reduce(f16[22699520]{0} %concatenate.1), channel_id=19, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_43.1737
  %slice.80 = f16[6400]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[0:6400]}, sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/reduce_sum[axes=(0, 1)]" source_file="/code/alpa/alpa/model/gpt_model.py" source_line=75}
  %param.84 = f32[6400]{0} parameter(79), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %fusion.527 = (f32[6400]{0}, f32[6400]{0}, f32[6400]{0}) fusion(f32[6400]{0} %param.3, f32[] %get-tuple-element.760, f32[] %get-tuple-element.761, f32[6400]{0} %param.86, f16[6400]{0} %slice.80, /*index=5*/f32[6400]{0} %param.84), kind=kLoop, calls=%fused_computation.527, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %get-tuple-element.897 = f32[6400]{0} get-tuple-element((f32[6400]{0}, f32[6400]{0}, f32[6400]{0}) %fusion.527), index=0
  %param.88 = f32[1024]{0} parameter(157), sharding={replicated}
  %fusion.261 = (f32[1024]{0}, f32[1024]{0}) fusion(f16[4,1024,1024]{2,1,0} %fusion.269, f16[4096,1024]{1,0} %all-reduce.43, f32[4,1024]{1,0} %get-tuple-element.814, f16[4096,1024]{1,0} %bitcast.1690, f32[4,1024]{1,0} %get-tuple-element.815), kind=kInput, calls=%fused_computation.261, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %get-tuple-element.458 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}) %fusion.261), index=0
  %fusion.9 = (f32[1024]{0}, f32[1024]{0}) fusion(f32[] %get-tuple-element.28, f32[4,1024]{1,0} %get-tuple-element.558, f32[] %get-tuple-element.23, f32[4,1024]{1,0} %get-tuple-element.559, f32[] %get-tuple-element.27, /*index=5*/f32[] %get-tuple-element.26, f16[4,1024,1024]{2,1,0} %get-tuple-element.24, f16[4,1024,1024]{2,1,0} %get-tuple-element.560), kind=kInput, calls=%fused_computation.9, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %get-tuple-element.554 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}) %fusion.9), index=0
  %get-tuple-element.555 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}) %fusion.9), index=1
  %fusion.35 = (f32[1024]{0}, f32[1024]{0}) fusion(f32[] %get-tuple-element.13, f32[4,1024]{1,0} %get-tuple-element.561, f32[] %get-tuple-element.10, f32[4,1024]{1,0} %get-tuple-element.562, f32[] %get-tuple-element.12, /*index=5*/f32[] %get-tuple-element.11, f16[4,1024,1024]{2,1,0} %fusion.496, f16[4096,1024]{1,0} %all-reduce.22, f16[4,1024,1024]{2,1,0} %get-tuple-element.563), kind=kInput, calls=%fused_computation.35, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %get-tuple-element.550 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}) %fusion.35), index=0
  %get-tuple-element.551 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}) %fusion.35), index=1
  %fusion.209 = (f32[1024]{0}, f32[1024]{0}) fusion(f32[] %get-tuple-element.213, f32[4,1024]{1,0} %get-tuple-element.468, f32[] %get-tuple-element.208, f32[4,1024]{1,0} %get-tuple-element.469, f32[] %get-tuple-element.212, /*index=5*/f32[] %get-tuple-element.211, f16[4,1024,1024]{2,1,0} %get-tuple-element.209, f16[4,1024,1024]{2,1,0} %get-tuple-element.470), kind=kInput, calls=%fused_computation.209, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %get-tuple-element.464 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}) %fusion.209), index=0
  %get-tuple-element.465 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}) %fusion.209), index=1
  %fusion.235 = (f32[1024]{0}, f32[1024]{0}) fusion(f32[] %get-tuple-element.198, f32[4,1024]{1,0} %get-tuple-element.471, f32[] %get-tuple-element.195, f32[4,1024]{1,0} %get-tuple-element.472, f32[] %get-tuple-element.197, /*index=5*/f32[] %get-tuple-element.196, f16[4,1024,1024]{2,1,0} %fusion.276, f16[4096,1024]{1,0} %all-reduce.42, f16[4,1024,1024]{2,1,0} %get-tuple-element.473), kind=kInput, calls=%fused_computation.235, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %get-tuple-element.460 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}) %fusion.235), index=0
  %get-tuple-element.461 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}) %fusion.235), index=1
  %get-tuple-element.459 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}) %fusion.261), index=1
  %fusion.155 = (f32[1024]{0}, f32[1024]{0}) fusion(f32[] %get-tuple-element.124, f32[4,1024]{1,0} %get-tuple-element.507, f32[] %get-tuple-element.121, f32[4,1024]{1,0} %get-tuple-element.508, f32[] %get-tuple-element.123, /*index=5*/f32[] %get-tuple-element.122, f16[4,1024,1024]{2,1,0} %fusion.364, f16[4096,1024]{1,0} %all-reduce.34, f16[4,1024,1024]{2,1,0} %get-tuple-element.509), kind=kInput, calls=%fused_computation.155, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %get-tuple-element.497 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}) %fusion.155), index=1
  %fusion.169 = (f32[1024]{0}, f32[1024]{0}) fusion(f32[] %get-tuple-element.176, f32[4,1024]{1,0} %get-tuple-element.486, f32[] %get-tuple-element.171, f32[4,1024]{1,0} %get-tuple-element.487, f32[] %get-tuple-element.175, /*index=5*/f32[] %get-tuple-element.174, f16[4,1024,1024]{2,1,0} %get-tuple-element.172, f16[4,1024,1024]{2,1,0} %get-tuple-element.488), kind=kInput, calls=%fused_computation.169, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %get-tuple-element.482 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}) %fusion.169), index=0
  %get-tuple-element.483 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}) %fusion.169), index=1
  %fusion.195 = (f32[1024]{0}, f32[1024]{0}) fusion(f32[] %get-tuple-element.161, f32[4,1024]{1,0} %get-tuple-element.489, f32[] %get-tuple-element.158, f32[4,1024]{1,0} %get-tuple-element.490, f32[] %get-tuple-element.160, /*index=5*/f32[] %get-tuple-element.159, f16[4,1024,1024]{2,1,0} %fusion.320, f16[4096,1024]{1,0} %all-reduce.38, f16[4,1024,1024]{2,1,0} %get-tuple-element.491), kind=kInput, calls=%fused_computation.195, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %get-tuple-element.478 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}) %fusion.195), index=0
  %get-tuple-element.479 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}) %fusion.195), index=1
  %fusion.115 = (f32[1024]{0}, f32[1024]{0}) fusion(f32[] %get-tuple-element.87, f32[4,1024]{1,0} %get-tuple-element.525, f32[] %get-tuple-element.84, f32[4,1024]{1,0} %get-tuple-element.526, f32[] %get-tuple-element.86, /*index=5*/f32[] %get-tuple-element.85, f16[4,1024,1024]{2,1,0} %fusion.408, f16[4096,1024]{1,0} %all-reduce.30, f16[4,1024,1024]{2,1,0} %get-tuple-element.527), kind=kInput, calls=%fused_computation.115, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %get-tuple-element.514 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}) %fusion.115), index=0
  %get-tuple-element.515 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}) %fusion.115), index=1
  %fusion.129 = (f32[1024]{0}, f32[1024]{0}) fusion(f32[] %get-tuple-element.139, f32[4,1024]{1,0} %get-tuple-element.504, f32[] %get-tuple-element.134, f32[4,1024]{1,0} %get-tuple-element.505, f32[] %get-tuple-element.138, /*index=5*/f32[] %get-tuple-element.137, f16[4,1024,1024]{2,1,0} %get-tuple-element.135, f16[4,1024,1024]{2,1,0} %get-tuple-element.506), kind=kInput, calls=%fused_computation.129, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %get-tuple-element.500 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}) %fusion.129), index=0
  %get-tuple-element.501 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}) %fusion.129), index=1
  %get-tuple-element.496 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}) %fusion.155), index=0
  %fusion.49 = (f32[1024]{0}, f32[1024]{0}) fusion(f32[] %get-tuple-element.65, f32[4,1024]{1,0} %get-tuple-element.540, f32[] %get-tuple-element.60, f32[4,1024]{1,0} %get-tuple-element.541, f32[] %get-tuple-element.64, /*index=5*/f32[] %get-tuple-element.63, f16[4,1024,1024]{2,1,0} %get-tuple-element.61, f16[4,1024,1024]{2,1,0} %get-tuple-element.542), kind=kInput, calls=%fused_computation.49, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %get-tuple-element.536 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}) %fusion.49), index=0
  %get-tuple-element.537 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}) %fusion.49), index=1
  %fusion.75 = (f32[1024]{0}, f32[1024]{0}) fusion(f32[] %get-tuple-element.50, f32[4,1024]{1,0} %get-tuple-element.543, f32[] %get-tuple-element.47, f32[4,1024]{1,0} %get-tuple-element.544, f32[] %get-tuple-element.49, /*index=5*/f32[] %get-tuple-element.48, f16[4,1024,1024]{2,1,0} %fusion.452, f16[4096,1024]{1,0} %all-reduce.26, f16[4,1024,1024]{2,1,0} %get-tuple-element.545), kind=kInput, calls=%fused_computation.75, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %get-tuple-element.532 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}) %fusion.75), index=0
  %get-tuple-element.533 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}) %fusion.75), index=1
  %fusion.89 = (f32[1024]{0}, f32[1024]{0}) fusion(f32[] %get-tuple-element.102, f32[4,1024]{1,0} %get-tuple-element.522, f32[] %get-tuple-element.97, f32[4,1024]{1,0} %get-tuple-element.523, f32[] %get-tuple-element.101, /*index=5*/f32[] %get-tuple-element.100, f16[4,1024,1024]{2,1,0} %get-tuple-element.98, f16[4,1024,1024]{2,1,0} %get-tuple-element.524), kind=kInput, calls=%fused_computation.89, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %get-tuple-element.518 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}) %fusion.89), index=0
  %get-tuple-element.519 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}) %fusion.89), index=1
  %concatenate.2 = f32[26624]{0} concatenate(f32[1024]{0} %get-tuple-element.458, f32[1024]{0} %get-tuple-element.554, f32[1024]{0} %get-tuple-element.555, f32[1024]{0} %get-tuple-element.550, f32[1024]{0} %get-tuple-element.551, /*index=5*/f32[1024]{0} %get-tuple-element.464, f32[1024]{0} %get-tuple-element.465, f32[1024]{0} %get-tuple-element.460, f32[1024]{0} %get-tuple-element.461, f32[1024]{0} %get-tuple-element.459, /*index=10*/f32[1024]{0} %get-tuple-element.497, f32[1024]{0} %get-tuple-element.482, f32[1024]{0} %get-tuple-element.483, f32[1024]{0} %get-tuple-element.478, f32[1024]{0} %get-tuple-element.479, /*index=15*/f32[1024]{0} %get-tuple-element.514, f32[1024]{0} %get-tuple-element.515, f32[1024]{0} %get-tuple-element.500, f32[1024]{0} %get-tuple-element.501, f32[1024]{0} %get-tuple-element.496, /*index=20*/f32[1024]{0} %get-tuple-element.536, f32[1024]{0} %get-tuple-element.537, f32[1024]{0} %get-tuple-element.532, f32[1024]{0} %get-tuple-element.533, f32[1024]{0} %get-tuple-element.518, /*index=25*/f32[1024]{0} %get-tuple-element.519), dimensions={0}
  %all-reduce.125 = f32[26624]{0} all-reduce(f32[26624]{0} %concatenate.2), channel_id=45, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_165.4507, control-predecessors={%all-reduce.124}
  %slice.132 = f32[1024]{0} slice(f32[26624]{0} %all-reduce.125), slice={[0:1024]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %param.87 = f32[1024]{0} parameter(80), sharding={replicated}
  %param.236 = f32[1024]{0} parameter(231), sharding={replicated}
  %slice.81 = f16[1024]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[6400:7424]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param.235 = f32[1024]{0} parameter(154), sharding={replicated}
  %param.234 = f32[1024]{0} parameter(230), sharding={replicated}
  %slice.133 = f32[1024]{0} slice(f32[26624]{0} %all-reduce.125), slice={[1024:2048]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param.233 = f32[1024]{0} parameter(153), sharding={replicated}
  %param.232 = f32[1024]{0} parameter(229), sharding={replicated}
  %slice.134 = f32[1024]{0} slice(f32[26624]{0} %all-reduce.125), slice={[2048:3072]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %param.231 = f32[1024]{0} parameter(152), sharding={replicated}
  %param.220 = f32[1024]{0} parameter(223), sharding={replicated}
  %slice.82 = f16[1024]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[7424:8448]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param.219 = f32[1024]{0} parameter(146), sharding={replicated}
  %param.218 = f32[1024]{0} parameter(222), sharding={replicated}
  %slice.135 = f32[1024]{0} slice(f32[26624]{0} %all-reduce.125), slice={[3072:4096]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param.217 = f32[1024]{0} parameter(145), sharding={replicated}
  %param.216 = f32[1024]{0} parameter(221), sharding={replicated}
  %slice.136 = f32[1024]{0} slice(f32[26624]{0} %all-reduce.125), slice={[4096:5120]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %param.215 = f32[1024]{0} parameter(144), sharding={replicated}
  %param.212 = f32[1024]{0} parameter(219), sharding={replicated}
  %slice.83 = f16[1024]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[8448:9472]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param.211 = f32[1024]{0} parameter(142), sharding={replicated}
  %fusion.258 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %param.10, f32[] %get-tuple-element.760, f32[] %get-tuple-element.761, f32[1024]{0} %param.88, f32[1024]{0} %slice.132, /*index=5*/f32[1024]{0} %param.87, f32[1024]{0} %param.81, f32[1024]{0} %param.236, f16[1024]{0} %slice.81, f32[1024]{0} %param.235, /*index=10*/f32[1024]{0} %param.82, f32[1024]{0} %param.234, f32[1024]{0} %slice.133, f32[1024]{0} %param.233, f32[1024]{0} %param.83, /*index=15*/f32[1024]{0} %param.232, f32[1024]{0} %slice.134, f32[1024]{0} %param.231, f32[1024]{0} %param.75, f32[1024]{0} %param.220, /*index=20*/f16[1024]{0} %slice.82, f32[1024]{0} %param.219, f32[1024]{0} %param.76, f32[1024]{0} %param.218, f32[1024]{0} %slice.135, /*index=25*/f32[1024]{0} %param.217, f32[1024]{0} %param.77, f32[1024]{0} %param.216, f32[1024]{0} %slice.136, f32[1024]{0} %param.215, /*index=30*/f32[1024]{0} %param.69, f32[1024]{0} %param.212, f16[1024]{0} %slice.83, f32[1024]{0} %param.211), kind=kLoop, calls=%fused_computation.258, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %get-tuple-element.898 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.258), index=0
  %param.114 = f32[1024]{0} parameter(170), sharding={replicated}
  %slice.137 = f32[1024]{0} slice(f32[26624]{0} %all-reduce.125), slice={[5120:6144]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param.113 = f32[1024]{0} parameter(93), sharding={replicated}
  %param.112 = f32[1024]{0} parameter(169), sharding={replicated}
  %slice.138 = f32[1024]{0} slice(f32[26624]{0} %all-reduce.125), slice={[6144:7168]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %param.111 = f32[1024]{0} parameter(92), sharding={replicated}
  %param.100 = f32[1024]{0} parameter(163), sharding={replicated}
  %slice.84 = f16[1024]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[9472:10496]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param.99 = f32[1024]{0} parameter(86), sharding={replicated}
  %param.98 = f32[1024]{0} parameter(162), sharding={replicated}
  %slice.139 = f32[1024]{0} slice(f32[26624]{0} %all-reduce.125), slice={[7168:8192]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param.97 = f32[1024]{0} parameter(85), sharding={replicated}
  %param.96 = f32[1024]{0} parameter(161), sharding={replicated}
  %slice.140 = f32[1024]{0} slice(f32[26624]{0} %all-reduce.125), slice={[8192:9216]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %param.95 = f32[1024]{0} parameter(84), sharding={replicated}
  %param.90 = f32[1024]{0} parameter(158), sharding={replicated}
  %slice.141 = f32[1024]{0} slice(f32[26624]{0} %all-reduce.125), slice={[9216:10240]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param.89 = f32[1024]{0} parameter(81), sharding={replicated}
  %fusion.206 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %param.22, f32[] %get-tuple-element.760, f32[] %get-tuple-element.761, f32[1024]{0} %param.114, f32[1024]{0} %slice.137, /*index=5*/f32[1024]{0} %param.113, f32[1024]{0} %param.23, f32[1024]{0} %param.112, f32[1024]{0} %slice.138, f32[1024]{0} %param.111, /*index=10*/f32[1024]{0} %param.15, f32[1024]{0} %param.100, f16[1024]{0} %slice.84, f32[1024]{0} %param.99, f32[1024]{0} %param.16, /*index=15*/f32[1024]{0} %param.98, f32[1024]{0} %slice.139, f32[1024]{0} %param.97, f32[1024]{0} %param.17, f32[1024]{0} %param.96, /*index=20*/f32[1024]{0} %slice.140, f32[1024]{0} %param.95, f32[1024]{0} %param.9, f32[1024]{0} %param.90, f32[1024]{0} %slice.141, /*index=25*/f32[1024]{0} %param.89), kind=kLoop, calls=%fused_computation.206, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %get-tuple-element.899 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.206), index=15
  %param.92 = f32[128,1024]{1,0} parameter(159), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.85 = f16[131072]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[10496:141568]}
  %bitcast.1748 = f16[128,1024]{1,0} bitcast(f16[131072]{0} %slice.85), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/position_embeddings.embed_call_one_hot/remat(core_fn)/transpose[permutation=(1, 0)]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=237}
  %param.91 = f32[128,1024]{1,0} parameter(82), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.222 = f32[128,1024]{1,0} parameter(224), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.86 = f16[131072]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[141568:272640]}
  %bitcast.1749 = f16[128,1024]{1,0} bitcast(f16[131072]{0} %slice.86), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.221 = f32[128,1024]{1,0} parameter(147), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.198 = f32[128,1024]{1,0} parameter(212), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.87 = f16[131072]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[272640:403712]}
  %bitcast.1750 = f16[128,1024]{1,0} bitcast(f16[131072]{0} %slice.87), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.197 = f32[128,1024]{1,0} parameter(135), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.174 = f32[128,1024]{1,0} parameter(200), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.88 = f16[131072]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[403712:534784]}
  %bitcast.1751 = f16[128,1024]{1,0} bitcast(f16[131072]{0} %slice.88), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.173 = f32[128,1024]{1,0} parameter(123), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.150 = f32[128,1024]{1,0} parameter(188), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.89 = f16[131072]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[534784:665856]}
  %bitcast.1752 = f16[128,1024]{1,0} bitcast(f16[131072]{0} %slice.89), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.149 = f32[128,1024]{1,0} parameter(111), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.126 = f32[128,1024]{1,0} parameter(176), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.90 = f16[131072]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[665856:796928]}
  %bitcast.1753 = f16[128,1024]{1,0} bitcast(f16[131072]{0} %slice.90), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.125 = f32[128,1024]{1,0} parameter(99), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.102 = f32[128,1024]{1,0} parameter(164), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.91 = f16[131072]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[796928:928000]}
  %bitcast.1754 = f16[128,1024]{1,0} bitcast(f16[131072]{0} %slice.91), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.101 = f32[128,1024]{1,0} parameter(87), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %fusion.245 = (f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=5*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=10*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=15*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=20*/f32[128,1024]{1,0}) fusion(f32[128,1024]{1,0} %param.8, f32[] %get-tuple-element.760, f32[] %get-tuple-element.761, f32[128,1024]{1,0} %param.92, f16[128,1024]{1,0} %bitcast.1748, /*index=5*/f32[128,1024]{1,0} %param.91, f32[128,1024]{1,0} %param.74, f32[128,1024]{1,0} %param.222, f16[128,1024]{1,0} %bitcast.1749, f32[128,1024]{1,0} %param.221, /*index=10*/f32[128,1024]{1,0} %param.62, f32[128,1024]{1,0} %param.198, f16[128,1024]{1,0} %bitcast.1750, f32[128,1024]{1,0} %param.197, f32[128,1024]{1,0} %param.50, /*index=15*/f32[128,1024]{1,0} %param.174, f16[128,1024]{1,0} %bitcast.1751, f32[128,1024]{1,0} %param.173, f32[128,1024]{1,0} %param.38, f32[128,1024]{1,0} %param.150, /*index=20*/f16[128,1024]{1,0} %bitcast.1752, f32[128,1024]{1,0} %param.149, f32[128,1024]{1,0} %param.26, f32[128,1024]{1,0} %param.126, f16[128,1024]{1,0} %bitcast.1753, /*index=25*/f32[128,1024]{1,0} %param.125, f32[128,1024]{1,0} %param.14, f32[128,1024]{1,0} %param.102, f16[128,1024]{1,0} %bitcast.1754, f32[128,1024]{1,0} %param.101), kind=kLoop, calls=%fused_computation.245, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %get-tuple-element.900 = f32[128,1024]{1,0} get-tuple-element((f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=5*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=10*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=15*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=20*/f32[128,1024]{1,0}) %fusion.245), index=0
  %param.94 = f32[6400,1024]{1,0} parameter(160), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.92 = f16[6553600]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[928000:7481600]}
  %bitcast.1755 = f16[6400,1024]{1,0} bitcast(f16[6553600]{0} %slice.92), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/word_embeddings.embed_call_one_hot/remat(core_fn)/transpose[permutation=(1, 0)]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=237}
  %slice.93 = f16[6553600]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[7481600:14035200]}
  %bitcast.1756 = f16[6400,1024]{1,0} bitcast(f16[6553600]{0} %slice.93), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/dot_general[dimension_numbers=(((0, 1), (0, 1)), ((), ())) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/gpt_model.py" source_line=70}
  %param.93 = f32[6400,1024]{1,0} parameter(83), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %fusion.240 = (f32[6400,1024]{1,0}, f32[6400,1024]{1,0}, f32[6400,1024]{1,0}) fusion(f32[6400,1024]{1,0} %param.6, f32[] %get-tuple-element.760, f32[] %get-tuple-element.761, f32[6400,1024]{1,0} %param.94, f16[6400,1024]{1,0} %bitcast.1755, /*index=5*/f16[6400,1024]{1,0} %bitcast.1756, f32[6400,1024]{1,0} %param.93), kind=kLoop, calls=%fused_computation.240, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %get-tuple-element.901 = f32[6400,1024]{1,0} get-tuple-element((f32[6400,1024]{1,0}, f32[6400,1024]{1,0}, f32[6400,1024]{1,0}) %fusion.240), index=0
  %get-tuple-element.902 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.206), index=12
  %get-tuple-element.903 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.206), index=9
  %get-tuple-element.904 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.206), index=6
  %get-tuple-element.905 = f32[128,1024]{1,0} get-tuple-element((f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=5*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=10*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=15*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=20*/f32[128,1024]{1,0}) %fusion.245), index=18
  %param.104 = f32[384]{0} parameter(165), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.94 = f16[384]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[14035200:14035584]}, sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param.103 = f32[384]{0} parameter(88), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.224 = f32[384]{0} parameter(225), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.95 = f16[384]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[14035584:14035968]}, sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param.223 = f32[384]{0} parameter(148), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.200 = f32[384]{0} parameter(213), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.96 = f16[384]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[14035968:14036352]}, sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param.199 = f32[384]{0} parameter(136), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.176 = f32[384]{0} parameter(201), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.97 = f16[384]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[14036352:14036736]}, sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param.175 = f32[384]{0} parameter(124), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.152 = f32[384]{0} parameter(189), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.98 = f16[384]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[14036736:14037120]}, sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param.151 = f32[384]{0} parameter(112), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.128 = f32[384]{0} parameter(177), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.99 = f16[384]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[14037120:14037504]}, sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param.127 = f32[384]{0} parameter(100), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %fusion.223 = (f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=5*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=10*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=15*/f32[384]{0}, f32[384]{0}, f32[384]{0}) fusion(f32[384]{0} %param.12, f32[] %get-tuple-element.760, f32[] %get-tuple-element.761, f32[384]{0} %param.104, f16[384]{0} %slice.94, /*index=5*/f32[384]{0} %param.103, f32[384]{0} %param.73, f32[384]{0} %param.224, f16[384]{0} %slice.95, f32[384]{0} %param.223, /*index=10*/f32[384]{0} %param.61, f32[384]{0} %param.200, f16[384]{0} %slice.96, f32[384]{0} %param.199, f32[384]{0} %param.49, /*index=15*/f32[384]{0} %param.176, f16[384]{0} %slice.97, f32[384]{0} %param.175, f32[384]{0} %param.37, f32[384]{0} %param.152, /*index=20*/f16[384]{0} %slice.98, f32[384]{0} %param.151, f32[384]{0} %param.25, f32[384]{0} %param.128, f16[384]{0} %slice.99, /*index=25*/f32[384]{0} %param.127), kind=kLoop, calls=%fused_computation.223, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %get-tuple-element.906 = f32[384]{0} get-tuple-element((f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=5*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=10*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=15*/f32[384]{0}, f32[384]{0}, f32[384]{0}) %fusion.223), index=0
  %param.106 = f32[1024,384]{1,0} parameter(166), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.100 = f16[393216]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[14037504:14430720]}
  %bitcast.1763 = f16[1024,384]{1,0} bitcast(f16[393216]{0} %slice.100), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.105 = f32[1024,384]{1,0} parameter(89), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.226 = f32[1024,384]{1,0} parameter(226), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.101 = f16[393216]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[14430720:14823936]}
  %bitcast.1764 = f16[1024,384]{1,0} bitcast(f16[393216]{0} %slice.101), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.225 = f32[1024,384]{1,0} parameter(149), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.202 = f32[1024,384]{1,0} parameter(214), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.102 = f16[393216]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[14823936:15217152]}
  %bitcast.1765 = f16[1024,384]{1,0} bitcast(f16[393216]{0} %slice.102), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.201 = f32[1024,384]{1,0} parameter(137), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.178 = f32[1024,384]{1,0} parameter(202), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.103 = f16[393216]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[15217152:15610368]}
  %bitcast.1766 = f16[1024,384]{1,0} bitcast(f16[393216]{0} %slice.103), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.177 = f32[1024,384]{1,0} parameter(125), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.154 = f32[1024,384]{1,0} parameter(190), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.104 = f16[393216]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[15610368:16003584]}
  %bitcast.1767 = f16[1024,384]{1,0} bitcast(f16[393216]{0} %slice.104), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.153 = f32[1024,384]{1,0} parameter(113), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.130 = f32[1024,384]{1,0} parameter(178), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.105 = f16[393216]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[16003584:16396800]}
  %bitcast.1768 = f16[1024,384]{1,0} bitcast(f16[393216]{0} %slice.105), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.129 = f32[1024,384]{1,0} parameter(101), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %fusion.220 = (f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=5*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=10*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=15*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}) fusion(f32[1024,384]{1,0} %param.11, f32[] %get-tuple-element.760, f32[] %get-tuple-element.761, f32[1024,384]{1,0} %param.106, f16[1024,384]{1,0} %bitcast.1763, /*index=5*/f32[1024,384]{1,0} %param.105, f32[1024,384]{1,0} %param.72, f32[1024,384]{1,0} %param.226, f16[1024,384]{1,0} %bitcast.1764, f32[1024,384]{1,0} %param.225, /*index=10*/f32[1024,384]{1,0} %param.60, f32[1024,384]{1,0} %param.202, f16[1024,384]{1,0} %bitcast.1765, f32[1024,384]{1,0} %param.201, f32[1024,384]{1,0} %param.48, /*index=15*/f32[1024,384]{1,0} %param.178, f16[1024,384]{1,0} %bitcast.1766, f32[1024,384]{1,0} %param.177, f32[1024,384]{1,0} %param.36, f32[1024,384]{1,0} %param.154, /*index=20*/f16[1024,384]{1,0} %bitcast.1767, f32[1024,384]{1,0} %param.153, f32[1024,384]{1,0} %param.24, f32[1024,384]{1,0} %param.130, f16[1024,384]{1,0} %bitcast.1768, /*index=25*/f32[1024,384]{1,0} %param.129), kind=kLoop, calls=%fused_computation.220, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %get-tuple-element.907 = f32[1024,384]{1,0} get-tuple-element((f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=5*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=10*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=15*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}) %fusion.220), index=0
  %param.108 = f32[512]{0} parameter(167), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.106 = f16[512]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[16396800:16397312]}, sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param.107 = f32[512]{0} parameter(90), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.228 = f32[512]{0} parameter(227), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.107 = f16[512]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[16397312:16397824]}, sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param.227 = f32[512]{0} parameter(150), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.204 = f32[512]{0} parameter(215), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.108 = f16[512]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[16397824:16398336]}, sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param.203 = f32[512]{0} parameter(138), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.180 = f32[512]{0} parameter(203), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.109 = f16[512]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[16398336:16398848]}, sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param.179 = f32[512]{0} parameter(126), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.156 = f32[512]{0} parameter(191), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.110 = f16[512]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[16398848:16399360]}, sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param.155 = f32[512]{0} parameter(114), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.132 = f32[512]{0} parameter(179), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.111 = f16[512]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[16399360:16399872]}, sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param.131 = f32[512]{0} parameter(102), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %fusion.217 = (f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=5*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=10*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=15*/f32[512]{0}, f32[512]{0}, f32[512]{0}) fusion(f32[512]{0} %param.19, f32[] %get-tuple-element.760, f32[] %get-tuple-element.761, f32[512]{0} %param.108, f16[512]{0} %slice.106, /*index=5*/f32[512]{0} %param.107, f32[512]{0} %param.79, f32[512]{0} %param.228, f16[512]{0} %slice.107, f32[512]{0} %param.227, /*index=10*/f32[512]{0} %param.67, f32[512]{0} %param.204, f16[512]{0} %slice.108, f32[512]{0} %param.203, f32[512]{0} %param.55, /*index=15*/f32[512]{0} %param.180, f16[512]{0} %slice.109, f32[512]{0} %param.179, f32[512]{0} %param.43, f32[512]{0} %param.156, /*index=20*/f16[512]{0} %slice.110, f32[512]{0} %param.155, f32[512]{0} %param.31, f32[512]{0} %param.132, f16[512]{0} %slice.111, /*index=25*/f32[512]{0} %param.131), kind=kLoop, calls=%fused_computation.217, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %get-tuple-element.908 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=5*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=10*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=15*/f32[512]{0}, f32[512]{0}, f32[512]{0}) %fusion.217), index=0
  %param.110 = f32[1024,512]{1,0} parameter(168), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.112 = f16[524288]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[16399872:16924160]}
  %bitcast.1775 = f16[1024,512]{1,0} bitcast(f16[524288]{0} %slice.112), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.109 = f32[1024,512]{1,0} parameter(91), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.230 = f32[1024,512]{1,0} parameter(228), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.113 = f16[524288]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[16924160:17448448]}
  %bitcast.1776 = f16[1024,512]{1,0} bitcast(f16[524288]{0} %slice.113), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.229 = f32[1024,512]{1,0} parameter(151), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.206 = f32[1024,512]{1,0} parameter(216), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.114 = f16[524288]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[17448448:17972736]}
  %bitcast.1777 = f16[1024,512]{1,0} bitcast(f16[524288]{0} %slice.114), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.205 = f32[1024,512]{1,0} parameter(139), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.182 = f32[1024,512]{1,0} parameter(204), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.115 = f16[524288]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[17972736:18497024]}
  %bitcast.1778 = f16[1024,512]{1,0} bitcast(f16[524288]{0} %slice.115), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.181 = f32[1024,512]{1,0} parameter(127), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.158 = f32[1024,512]{1,0} parameter(192), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.116 = f16[524288]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[18497024:19021312]}
  %bitcast.1779 = f16[1024,512]{1,0} bitcast(f16[524288]{0} %slice.116), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.157 = f32[1024,512]{1,0} parameter(115), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.134 = f32[1024,512]{1,0} parameter(180), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.117 = f16[524288]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[19021312:19545600]}
  %bitcast.1780 = f16[1024,512]{1,0} bitcast(f16[524288]{0} %slice.117), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.133 = f32[1024,512]{1,0} parameter(103), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %fusion.214 = (f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=5*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=10*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=15*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}) fusion(f32[1024,512]{1,0} %param.18, f32[] %get-tuple-element.760, f32[] %get-tuple-element.761, f32[1024,512]{1,0} %param.110, f16[1024,512]{1,0} %bitcast.1775, /*index=5*/f32[1024,512]{1,0} %param.109, f32[1024,512]{1,0} %param.78, f32[1024,512]{1,0} %param.230, f16[1024,512]{1,0} %bitcast.1776, f32[1024,512]{1,0} %param.229, /*index=10*/f32[1024,512]{1,0} %param.66, f32[1024,512]{1,0} %param.206, f16[1024,512]{1,0} %bitcast.1777, f32[1024,512]{1,0} %param.205, f32[1024,512]{1,0} %param.54, /*index=15*/f32[1024,512]{1,0} %param.182, f16[1024,512]{1,0} %bitcast.1778, f32[1024,512]{1,0} %param.181, f32[1024,512]{1,0} %param.42, f32[1024,512]{1,0} %param.158, /*index=20*/f16[1024,512]{1,0} %bitcast.1779, f32[1024,512]{1,0} %param.157, f32[1024,512]{1,0} %param.30, f32[1024,512]{1,0} %param.134, f16[1024,512]{1,0} %bitcast.1780, /*index=25*/f32[1024,512]{1,0} %param.133), kind=kLoop, calls=%fused_computation.214, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %get-tuple-element.909 = f32[1024,512]{1,0} get-tuple-element((f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=5*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=10*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=15*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}) %fusion.214), index=0
  %get-tuple-element.910 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.206), index=3
  %get-tuple-element.911 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.206), index=0
  %param.144 = f32[1024]{0} parameter(185), sharding={replicated}
  %slice.142 = f32[1024]{0} slice(f32[26624]{0} %all-reduce.125), slice={[10240:11264]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %param.143 = f32[1024]{0} parameter(108), sharding={replicated}
  %param.140 = f32[1024]{0} parameter(183), sharding={replicated}
  %slice.118 = f16[1024]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[19545600:19546624]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param.139 = f32[1024]{0} parameter(106), sharding={replicated}
  %param.138 = f32[1024]{0} parameter(182), sharding={replicated}
  %slice.143 = f32[1024]{0} slice(f32[26624]{0} %all-reduce.125), slice={[11264:12288]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param.137 = f32[1024]{0} parameter(105), sharding={replicated}
  %param.136 = f32[1024]{0} parameter(181), sharding={replicated}
  %slice.144 = f32[1024]{0} slice(f32[26624]{0} %all-reduce.125), slice={[12288:13312]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %param.135 = f32[1024]{0} parameter(104), sharding={replicated}
  %param.124 = f32[1024]{0} parameter(175), sharding={replicated}
  %slice.119 = f16[1024]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[19546624:19547648]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param.123 = f32[1024]{0} parameter(98), sharding={replicated}
  %param.122 = f32[1024]{0} parameter(174), sharding={replicated}
  %slice.145 = f32[1024]{0} slice(f32[26624]{0} %all-reduce.125), slice={[13312:14336]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param.121 = f32[1024]{0} parameter(97), sharding={replicated}
  %param.120 = f32[1024]{0} parameter(173), sharding={replicated}
  %slice.146 = f32[1024]{0} slice(f32[26624]{0} %all-reduce.125), slice={[14336:15360]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %param.119 = f32[1024]{0} parameter(96), sharding={replicated}
  %param.116 = f32[1024]{0} parameter(171), sharding={replicated}
  %slice.120 = f16[1024]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[19547648:19548672]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param.115 = f32[1024]{0} parameter(94), sharding={replicated}
  %fusion.156 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %param.41, f32[] %get-tuple-element.760, f32[] %get-tuple-element.761, f32[1024]{0} %param.144, f32[1024]{0} %slice.142, /*index=5*/f32[1024]{0} %param.143, f32[1024]{0} %param.33, f32[1024]{0} %param.140, f16[1024]{0} %slice.118, f32[1024]{0} %param.139, /*index=10*/f32[1024]{0} %param.34, f32[1024]{0} %param.138, f32[1024]{0} %slice.143, f32[1024]{0} %param.137, f32[1024]{0} %param.35, /*index=15*/f32[1024]{0} %param.136, f32[1024]{0} %slice.144, f32[1024]{0} %param.135, f32[1024]{0} %param.27, f32[1024]{0} %param.124, /*index=20*/f16[1024]{0} %slice.119, f32[1024]{0} %param.123, f32[1024]{0} %param.28, f32[1024]{0} %param.122, f32[1024]{0} %slice.145, /*index=25*/f32[1024]{0} %param.121, f32[1024]{0} %param.29, f32[1024]{0} %param.120, f32[1024]{0} %slice.146, f32[1024]{0} %param.119, /*index=30*/f32[1024]{0} %param.21, f32[1024]{0} %param.116, f16[1024]{0} %slice.120, f32[1024]{0} %param.115), kind=kLoop, calls=%fused_computation.156, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %get-tuple-element.912 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.156), index=21
  %param.118 = f32[512,1024]{1,0} parameter(172), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.121 = f16[524288]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[19548672:20072960]}
  %bitcast.1784 = f16[512,1024]{1,0} bitcast(f16[524288]{0} %slice.121), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.117 = f32[512,1024]{1,0} parameter(95), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.238 = f32[512,1024]{1,0} parameter(232), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.122 = f16[524288]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[20072960:20597248]}
  %bitcast.1785 = f16[512,1024]{1,0} bitcast(f16[524288]{0} %slice.122), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.237 = f32[512,1024]{1,0} parameter(155), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.214 = f32[512,1024]{1,0} parameter(220), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.123 = f16[524288]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[20597248:21121536]}
  %bitcast.1786 = f16[512,1024]{1,0} bitcast(f16[524288]{0} %slice.123), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.213 = f32[512,1024]{1,0} parameter(143), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.190 = f32[512,1024]{1,0} parameter(208), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.124 = f16[524288]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[21121536:21645824]}
  %bitcast.1787 = f16[512,1024]{1,0} bitcast(f16[524288]{0} %slice.124), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.189 = f32[512,1024]{1,0} parameter(131), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.166 = f32[512,1024]{1,0} parameter(196), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.125 = f16[524288]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[21645824:22170112]}
  %bitcast.1788 = f16[512,1024]{1,0} bitcast(f16[524288]{0} %slice.125), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.165 = f32[512,1024]{1,0} parameter(119), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.142 = f32[512,1024]{1,0} parameter(184), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %slice.126 = f16[524288]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[22170112:22694400]}
  %bitcast.1789 = f16[512,1024]{1,0} bitcast(f16[524288]{0} %slice.126), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.141 = f32[512,1024]{1,0} parameter(107), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %fusion.200 = (f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=5*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=10*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=15*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}) fusion(f32[512,1024]{1,0} %param.20, f32[] %get-tuple-element.760, f32[] %get-tuple-element.761, f32[512,1024]{1,0} %param.118, f16[512,1024]{1,0} %bitcast.1784, /*index=5*/f32[512,1024]{1,0} %param.117, f32[512,1024]{1,0} %param.80, f32[512,1024]{1,0} %param.238, f16[512,1024]{1,0} %bitcast.1785, f32[512,1024]{1,0} %param.237, /*index=10*/f32[512,1024]{1,0} %param.68, f32[512,1024]{1,0} %param.214, f16[512,1024]{1,0} %bitcast.1786, f32[512,1024]{1,0} %param.213, f32[512,1024]{1,0} %param.56, /*index=15*/f32[512,1024]{1,0} %param.190, f16[512,1024]{1,0} %bitcast.1787, f32[512,1024]{1,0} %param.189, f32[512,1024]{1,0} %param.44, f32[512,1024]{1,0} %param.166, /*index=20*/f16[512,1024]{1,0} %bitcast.1788, f32[512,1024]{1,0} %param.165, f32[512,1024]{1,0} %param.32, f32[512,1024]{1,0} %param.142, f16[512,1024]{1,0} %bitcast.1789, /*index=25*/f32[512,1024]{1,0} %param.141), kind=kLoop, calls=%fused_computation.200, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %get-tuple-element.913 = f32[512,1024]{1,0} get-tuple-element((f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=5*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=10*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=15*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}) %fusion.200), index=0
  %get-tuple-element.914 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.156), index=18
  %get-tuple-element.915 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.156), index=15
  %get-tuple-element.916 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.156), index=12
  %get-tuple-element.917 = f32[128,1024]{1,0} get-tuple-element((f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=5*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=10*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=15*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=20*/f32[128,1024]{1,0}) %fusion.245), index=15
  %get-tuple-element.918 = f32[384]{0} get-tuple-element((f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=5*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=10*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=15*/f32[384]{0}, f32[384]{0}, f32[384]{0}) %fusion.223), index=15
  %get-tuple-element.919 = f32[1024,384]{1,0} get-tuple-element((f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=5*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=10*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=15*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}) %fusion.220), index=15
  %get-tuple-element.920 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=5*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=10*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=15*/f32[512]{0}, f32[512]{0}, f32[512]{0}) %fusion.217), index=15
  %get-tuple-element.921 = f32[1024,512]{1,0} get-tuple-element((f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=5*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=10*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=15*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}) %fusion.214), index=15
  %get-tuple-element.922 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.156), index=9
  %get-tuple-element.923 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.156), index=6
  %get-tuple-element.924 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.156), index=3
  %get-tuple-element.925 = f32[512,1024]{1,0} get-tuple-element((f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=5*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=10*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=15*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}) %fusion.200), index=15
  %get-tuple-element.926 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.156), index=0
  %param.172 = f32[1024]{0} parameter(199), sharding={replicated}
  %slice.127 = f16[1024]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[22694400:22695424]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param.171 = f32[1024]{0} parameter(122), sharding={replicated}
  %param.170 = f32[1024]{0} parameter(198), sharding={replicated}
  %slice.147 = f32[1024]{0} slice(f32[26624]{0} %all-reduce.125), slice={[15360:16384]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param.169 = f32[1024]{0} parameter(121), sharding={replicated}
  %param.168 = f32[1024]{0} parameter(197), sharding={replicated}
  %slice.148 = f32[1024]{0} slice(f32[26624]{0} %all-reduce.125), slice={[16384:17408]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %param.167 = f32[1024]{0} parameter(120), sharding={replicated}
  %param.164 = f32[1024]{0} parameter(195), sharding={replicated}
  %slice.128 = f16[1024]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[22695424:22696448]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param.163 = f32[1024]{0} parameter(118), sharding={replicated}
  %param.162 = f32[1024]{0} parameter(194), sharding={replicated}
  %slice.149 = f32[1024]{0} slice(f32[26624]{0} %all-reduce.125), slice={[17408:18432]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param.161 = f32[1024]{0} parameter(117), sharding={replicated}
  %param.160 = f32[1024]{0} parameter(193), sharding={replicated}
  %slice.150 = f32[1024]{0} slice(f32[26624]{0} %all-reduce.125), slice={[18432:19456]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %param.159 = f32[1024]{0} parameter(116), sharding={replicated}
  %param.148 = f32[1024]{0} parameter(187), sharding={replicated}
  %slice.129 = f16[1024]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[22696448:22697472]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param.147 = f32[1024]{0} parameter(110), sharding={replicated}
  %param.146 = f32[1024]{0} parameter(186), sharding={replicated}
  %slice.151 = f32[1024]{0} slice(f32[26624]{0} %all-reduce.125), slice={[19456:20480]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param.145 = f32[1024]{0} parameter(109), sharding={replicated}
  %fusion.109 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %param.51, f32[] %get-tuple-element.760, f32[] %get-tuple-element.761, f32[1024]{0} %param.172, f16[1024]{0} %slice.127, /*index=5*/f32[1024]{0} %param.171, f32[1024]{0} %param.52, f32[1024]{0} %param.170, f32[1024]{0} %slice.147, f32[1024]{0} %param.169, /*index=10*/f32[1024]{0} %param.53, f32[1024]{0} %param.168, f32[1024]{0} %slice.148, f32[1024]{0} %param.167, f32[1024]{0} %param.45, /*index=15*/f32[1024]{0} %param.164, f16[1024]{0} %slice.128, f32[1024]{0} %param.163, f32[1024]{0} %param.46, f32[1024]{0} %param.162, /*index=20*/f32[1024]{0} %slice.149, f32[1024]{0} %param.161, f32[1024]{0} %param.47, f32[1024]{0} %param.160, f32[1024]{0} %slice.150, /*index=25*/f32[1024]{0} %param.159, f32[1024]{0} %param.39, f32[1024]{0} %param.148, f16[1024]{0} %slice.129, f32[1024]{0} %param.147, /*index=30*/f32[1024]{0} %param.40, f32[1024]{0} %param.146, f32[1024]{0} %slice.151, f32[1024]{0} %param.145), kind=kLoop, calls=%fused_computation.109, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %get-tuple-element.927 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.109), index=21
  %get-tuple-element.928 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.109), index=18
  %get-tuple-element.929 = f32[128,1024]{1,0} get-tuple-element((f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=5*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=10*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=15*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=20*/f32[128,1024]{1,0}) %fusion.245), index=12
  %get-tuple-element.930 = f32[384]{0} get-tuple-element((f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=5*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=10*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=15*/f32[384]{0}, f32[384]{0}, f32[384]{0}) %fusion.223), index=12
  %get-tuple-element.931 = f32[1024,384]{1,0} get-tuple-element((f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=5*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=10*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=15*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}) %fusion.220), index=12
  %get-tuple-element.932 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=5*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=10*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=15*/f32[512]{0}, f32[512]{0}, f32[512]{0}) %fusion.217), index=12
  %get-tuple-element.933 = f32[1024,512]{1,0} get-tuple-element((f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=5*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=10*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=15*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}) %fusion.214), index=12
  %get-tuple-element.934 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.109), index=15
  %get-tuple-element.935 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.109), index=12
  %get-tuple-element.936 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.109), index=9
  %get-tuple-element.937 = f32[512,1024]{1,0} get-tuple-element((f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=5*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=10*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=15*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}) %fusion.200), index=12
  %get-tuple-element.938 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.109), index=6
  %get-tuple-element.939 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.109), index=3
  %get-tuple-element.940 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.109), index=0
  %get-tuple-element.941 = f32[128,1024]{1,0} get-tuple-element((f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=5*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=10*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=15*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=20*/f32[128,1024]{1,0}) %fusion.245), index=9
  %get-tuple-element.942 = f32[384]{0} get-tuple-element((f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=5*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=10*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=15*/f32[384]{0}, f32[384]{0}, f32[384]{0}) %fusion.223), index=9
  %get-tuple-element.943 = f32[1024,384]{1,0} get-tuple-element((f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=5*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=10*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=15*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}) %fusion.220), index=9
  %get-tuple-element.944 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=5*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=10*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=15*/f32[512]{0}, f32[512]{0}, f32[512]{0}) %fusion.217), index=9
  %get-tuple-element.945 = f32[1024,512]{1,0} get-tuple-element((f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=5*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=10*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=15*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}) %fusion.214), index=9
  %param.210 = f32[1024]{0} parameter(218), sharding={replicated}
  %slice.152 = f32[1024]{0} slice(f32[26624]{0} %all-reduce.125), slice={[20480:21504]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param.209 = f32[1024]{0} parameter(141), sharding={replicated}
  %param.208 = f32[1024]{0} parameter(217), sharding={replicated}
  %slice.153 = f32[1024]{0} slice(f32[26624]{0} %all-reduce.125), slice={[21504:22528]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %param.207 = f32[1024]{0} parameter(140), sharding={replicated}
  %param.196 = f32[1024]{0} parameter(211), sharding={replicated}
  %slice.130 = f16[1024]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[22697472:22698496]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param.195 = f32[1024]{0} parameter(134), sharding={replicated}
  %param.194 = f32[1024]{0} parameter(210), sharding={replicated}
  %slice.154 = f32[1024]{0} slice(f32[26624]{0} %all-reduce.125), slice={[22528:23552]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param.193 = f32[1024]{0} parameter(133), sharding={replicated}
  %param.192 = f32[1024]{0} parameter(209), sharding={replicated}
  %slice.155 = f32[1024]{0} slice(f32[26624]{0} %all-reduce.125), slice={[23552:24576]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %param.191 = f32[1024]{0} parameter(132), sharding={replicated}
  %param.188 = f32[1024]{0} parameter(207), sharding={replicated}
  %slice.131 = f16[1024]{0} slice(f16[22699520]{0} %all-reduce.124), slice={[22698496:22699520]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %param.187 = f32[1024]{0} parameter(130), sharding={replicated}
  %param.186 = f32[1024]{0} parameter(206), sharding={replicated}
  %slice.156 = f32[1024]{0} slice(f32[26624]{0} %all-reduce.125), slice={[24576:25600]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param.185 = f32[1024]{0} parameter(129), sharding={replicated}
  %param.184 = f32[1024]{0} parameter(205), sharding={replicated}
  %slice.157 = f32[1024]{0} slice(f32[26624]{0} %all-reduce.125), slice={[25600:26624]}, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %param.183 = f32[1024]{0} parameter(128), sharding={replicated}
  %fusion.46 = (f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) fusion(f32[1024]{0} %param.70, f32[] %get-tuple-element.760, f32[] %get-tuple-element.761, f32[1024]{0} %param.210, f32[1024]{0} %slice.152, /*index=5*/f32[1024]{0} %param.209, f32[1024]{0} %param.71, f32[1024]{0} %param.208, f32[1024]{0} %slice.153, f32[1024]{0} %param.207, /*index=10*/f32[1024]{0} %param.63, f32[1024]{0} %param.196, f16[1024]{0} %slice.130, f32[1024]{0} %param.195, f32[1024]{0} %param.64, /*index=15*/f32[1024]{0} %param.194, f32[1024]{0} %slice.154, f32[1024]{0} %param.193, f32[1024]{0} %param.65, f32[1024]{0} %param.192, /*index=20*/f32[1024]{0} %slice.155, f32[1024]{0} %param.191, f32[1024]{0} %param.57, f32[1024]{0} %param.188, f16[1024]{0} %slice.131, /*index=25*/f32[1024]{0} %param.187, f32[1024]{0} %param.58, f32[1024]{0} %param.186, f32[1024]{0} %slice.156, f32[1024]{0} %param.185, /*index=30*/f32[1024]{0} %param.59, f32[1024]{0} %param.184, f32[1024]{0} %slice.157, f32[1024]{0} %param.183), kind=kLoop, calls=%fused_computation.46, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %get-tuple-element.946 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.46), index=21
  %get-tuple-element.947 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.46), index=18
  %get-tuple-element.948 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.46), index=15
  %get-tuple-element.949 = f32[512,1024]{1,0} get-tuple-element((f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=5*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=10*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=15*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}) %fusion.200), index=9
  %get-tuple-element.950 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.46), index=12
  %get-tuple-element.951 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.46), index=9
  %get-tuple-element.952 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.46), index=6
  %get-tuple-element.953 = f32[128,1024]{1,0} get-tuple-element((f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=5*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=10*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=15*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=20*/f32[128,1024]{1,0}) %fusion.245), index=6
  %get-tuple-element.954 = f32[384]{0} get-tuple-element((f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=5*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=10*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=15*/f32[384]{0}, f32[384]{0}, f32[384]{0}) %fusion.223), index=6
  %get-tuple-element.955 = f32[1024,384]{1,0} get-tuple-element((f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=5*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=10*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=15*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}) %fusion.220), index=6
  %get-tuple-element.956 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=5*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=10*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=15*/f32[512]{0}, f32[512]{0}, f32[512]{0}) %fusion.217), index=6
  %get-tuple-element.957 = f32[1024,512]{1,0} get-tuple-element((f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=5*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=10*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=15*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}) %fusion.214), index=6
  %get-tuple-element.958 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.46), index=3
  %get-tuple-element.959 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.46), index=0
  %get-tuple-element.960 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.258), index=21
  %get-tuple-element.961 = f32[512,1024]{1,0} get-tuple-element((f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=5*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=10*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=15*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}) %fusion.200), index=6
  %get-tuple-element.962 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.258), index=18
  %get-tuple-element.963 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.258), index=15
  %get-tuple-element.964 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.258), index=12
  %get-tuple-element.965 = f32[128,1024]{1,0} get-tuple-element((f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=5*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=10*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=15*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=20*/f32[128,1024]{1,0}) %fusion.245), index=3
  %get-tuple-element.966 = f32[384]{0} get-tuple-element((f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=5*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=10*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=15*/f32[384]{0}, f32[384]{0}, f32[384]{0}) %fusion.223), index=3
  %get-tuple-element.967 = f32[1024,384]{1,0} get-tuple-element((f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=5*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=10*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=15*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}) %fusion.220), index=3
  %get-tuple-element.968 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=5*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=10*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=15*/f32[512]{0}, f32[512]{0}, f32[512]{0}) %fusion.217), index=3
  %get-tuple-element.969 = f32[1024,512]{1,0} get-tuple-element((f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=5*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=10*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=15*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}) %fusion.214), index=3
  %get-tuple-element.970 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.258), index=9
  %get-tuple-element.971 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.258), index=6
  %get-tuple-element.972 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.258), index=3
  %get-tuple-element.973 = f32[512,1024]{1,0} get-tuple-element((f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=5*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=10*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=15*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}) %fusion.200), index=3
  %get-tuple-element.974 = s32[] get-tuple-element((f32[], f32[], s32[]) %fusion.528), index=2
  %get-tuple-element.975 = f32[6400]{0} get-tuple-element((f32[6400]{0}, f32[6400]{0}, f32[6400]{0}) %fusion.527), index=2
  %get-tuple-element.976 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.258), index=2
  %get-tuple-element.977 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.206), index=17
  %get-tuple-element.978 = f32[128,1024]{1,0} get-tuple-element((f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=5*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=10*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=15*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=20*/f32[128,1024]{1,0}) %fusion.245), index=2
  %get-tuple-element.979 = f32[6400,1024]{1,0} get-tuple-element((f32[6400,1024]{1,0}, f32[6400,1024]{1,0}, f32[6400,1024]{1,0}) %fusion.240), index=2
  %get-tuple-element.980 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.206), index=14
  %get-tuple-element.981 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.206), index=11
  %get-tuple-element.982 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.206), index=8
  %get-tuple-element.983 = f32[128,1024]{1,0} get-tuple-element((f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=5*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=10*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=15*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=20*/f32[128,1024]{1,0}) %fusion.245), index=20
  %get-tuple-element.984 = f32[384]{0} get-tuple-element((f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=5*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=10*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=15*/f32[384]{0}, f32[384]{0}, f32[384]{0}) %fusion.223), index=2
  %get-tuple-element.985 = f32[1024,384]{1,0} get-tuple-element((f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=5*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=10*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=15*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}) %fusion.220), index=2
  %get-tuple-element.986 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=5*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=10*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=15*/f32[512]{0}, f32[512]{0}, f32[512]{0}) %fusion.217), index=2
  %get-tuple-element.987 = f32[1024,512]{1,0} get-tuple-element((f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=5*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=10*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=15*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}) %fusion.214), index=2
  %get-tuple-element.988 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.206), index=5
  %get-tuple-element.989 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.206), index=2
  %get-tuple-element.990 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.156), index=23
  %get-tuple-element.991 = f32[512,1024]{1,0} get-tuple-element((f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=5*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=10*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=15*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}) %fusion.200), index=2
  %get-tuple-element.992 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.156), index=20
  %get-tuple-element.993 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.156), index=17
  %get-tuple-element.994 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.156), index=14
  %get-tuple-element.995 = f32[128,1024]{1,0} get-tuple-element((f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=5*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=10*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=15*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=20*/f32[128,1024]{1,0}) %fusion.245), index=17
  %get-tuple-element.996 = f32[384]{0} get-tuple-element((f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=5*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=10*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=15*/f32[384]{0}, f32[384]{0}, f32[384]{0}) %fusion.223), index=17
  %get-tuple-element.997 = f32[1024,384]{1,0} get-tuple-element((f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=5*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=10*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=15*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}) %fusion.220), index=17
  %get-tuple-element.998 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=5*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=10*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=15*/f32[512]{0}, f32[512]{0}, f32[512]{0}) %fusion.217), index=17
  %get-tuple-element.999 = f32[1024,512]{1,0} get-tuple-element((f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=5*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=10*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=15*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}) %fusion.214), index=17
  %get-tuple-element.1000 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.156), index=11
  %get-tuple-element.1001 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.156), index=8
  %get-tuple-element.1002 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.156), index=5
  %get-tuple-element.1003 = f32[512,1024]{1,0} get-tuple-element((f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=5*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=10*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=15*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}) %fusion.200), index=17
  %get-tuple-element.1004 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.156), index=2
  %get-tuple-element.1005 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.109), index=23
  %get-tuple-element.1006 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.109), index=20
  %get-tuple-element.1007 = f32[128,1024]{1,0} get-tuple-element((f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=5*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=10*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=15*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=20*/f32[128,1024]{1,0}) %fusion.245), index=14
  %get-tuple-element.1008 = f32[384]{0} get-tuple-element((f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=5*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=10*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=15*/f32[384]{0}, f32[384]{0}, f32[384]{0}) %fusion.223), index=14
  %get-tuple-element.1009 = f32[1024,384]{1,0} get-tuple-element((f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=5*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=10*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=15*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}) %fusion.220), index=14
  %get-tuple-element.1010 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=5*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=10*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=15*/f32[512]{0}, f32[512]{0}, f32[512]{0}) %fusion.217), index=14
  %get-tuple-element.1011 = f32[1024,512]{1,0} get-tuple-element((f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=5*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=10*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=15*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}) %fusion.214), index=14
  %get-tuple-element.1012 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.109), index=17
  %get-tuple-element.1013 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.109), index=14
  %get-tuple-element.1014 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.109), index=11
  %get-tuple-element.1015 = f32[512,1024]{1,0} get-tuple-element((f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=5*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=10*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=15*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}) %fusion.200), index=14
  %get-tuple-element.1016 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.109), index=8
  %get-tuple-element.1017 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.109), index=5
  %get-tuple-element.1018 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.109), index=2
  %get-tuple-element.1019 = f32[128,1024]{1,0} get-tuple-element((f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=5*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=10*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=15*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=20*/f32[128,1024]{1,0}) %fusion.245), index=11
  %get-tuple-element.1020 = f32[384]{0} get-tuple-element((f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=5*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=10*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=15*/f32[384]{0}, f32[384]{0}, f32[384]{0}) %fusion.223), index=11
  %get-tuple-element.1021 = f32[1024,384]{1,0} get-tuple-element((f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=5*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=10*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=15*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}) %fusion.220), index=11
  %get-tuple-element.1022 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=5*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=10*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=15*/f32[512]{0}, f32[512]{0}, f32[512]{0}) %fusion.217), index=11
  %get-tuple-element.1023 = f32[1024,512]{1,0} get-tuple-element((f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=5*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=10*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=15*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}) %fusion.214), index=11
  %get-tuple-element.1024 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.46), index=23
  %get-tuple-element.1025 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.46), index=20
  %get-tuple-element.1026 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.46), index=17
  %get-tuple-element.1027 = f32[512,1024]{1,0} get-tuple-element((f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=5*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=10*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=15*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}) %fusion.200), index=11
  %get-tuple-element.1028 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.46), index=14
  %get-tuple-element.1029 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.46), index=11
  %get-tuple-element.1030 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.46), index=8
  %get-tuple-element.1031 = f32[128,1024]{1,0} get-tuple-element((f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=5*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=10*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=15*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=20*/f32[128,1024]{1,0}) %fusion.245), index=8
  %get-tuple-element.1032 = f32[384]{0} get-tuple-element((f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=5*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=10*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=15*/f32[384]{0}, f32[384]{0}, f32[384]{0}) %fusion.223), index=8
  %get-tuple-element.1033 = f32[1024,384]{1,0} get-tuple-element((f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=5*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=10*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=15*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}) %fusion.220), index=8
  %get-tuple-element.1034 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=5*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=10*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=15*/f32[512]{0}, f32[512]{0}, f32[512]{0}) %fusion.217), index=8
  %get-tuple-element.1035 = f32[1024,512]{1,0} get-tuple-element((f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=5*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=10*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=15*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}) %fusion.214), index=8
  %get-tuple-element.1036 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.46), index=5
  %get-tuple-element.1037 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.46), index=2
  %get-tuple-element.1038 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.258), index=23
  %get-tuple-element.1039 = f32[512,1024]{1,0} get-tuple-element((f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=5*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=10*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=15*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}) %fusion.200), index=8
  %get-tuple-element.1040 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.258), index=20
  %get-tuple-element.1041 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.258), index=17
  %get-tuple-element.1042 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.258), index=14
  %get-tuple-element.1043 = f32[128,1024]{1,0} get-tuple-element((f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=5*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=10*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=15*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=20*/f32[128,1024]{1,0}) %fusion.245), index=5
  %get-tuple-element.1044 = f32[384]{0} get-tuple-element((f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=5*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=10*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=15*/f32[384]{0}, f32[384]{0}, f32[384]{0}) %fusion.223), index=5
  %get-tuple-element.1045 = f32[1024,384]{1,0} get-tuple-element((f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=5*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=10*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=15*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}) %fusion.220), index=5
  %get-tuple-element.1046 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=5*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=10*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=15*/f32[512]{0}, f32[512]{0}, f32[512]{0}) %fusion.217), index=5
  %get-tuple-element.1047 = f32[1024,512]{1,0} get-tuple-element((f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=5*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=10*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=15*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}) %fusion.214), index=5
  %get-tuple-element.1048 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.258), index=11
  %get-tuple-element.1049 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.258), index=8
  %get-tuple-element.1050 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.258), index=5
  %get-tuple-element.1051 = f32[512,1024]{1,0} get-tuple-element((f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=5*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=10*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=15*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}) %fusion.200), index=5
  %get-tuple-element.1052 = f32[6400]{0} get-tuple-element((f32[6400]{0}, f32[6400]{0}, f32[6400]{0}) %fusion.527), index=1
  %get-tuple-element.1053 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.258), index=1
  %get-tuple-element.1054 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.206), index=16
  %get-tuple-element.1055 = f32[128,1024]{1,0} get-tuple-element((f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=5*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=10*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=15*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=20*/f32[128,1024]{1,0}) %fusion.245), index=1
  %get-tuple-element.1056 = f32[6400,1024]{1,0} get-tuple-element((f32[6400,1024]{1,0}, f32[6400,1024]{1,0}, f32[6400,1024]{1,0}) %fusion.240), index=1
  %get-tuple-element.1057 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.206), index=13
  %get-tuple-element.1058 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.206), index=10
  %get-tuple-element.1059 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.206), index=7
  %get-tuple-element.1060 = f32[128,1024]{1,0} get-tuple-element((f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=5*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=10*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=15*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=20*/f32[128,1024]{1,0}) %fusion.245), index=19
  %get-tuple-element.1061 = f32[384]{0} get-tuple-element((f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=5*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=10*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=15*/f32[384]{0}, f32[384]{0}, f32[384]{0}) %fusion.223), index=1
  %get-tuple-element.1062 = f32[1024,384]{1,0} get-tuple-element((f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=5*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=10*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=15*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}) %fusion.220), index=1
  %get-tuple-element.1063 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=5*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=10*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=15*/f32[512]{0}, f32[512]{0}, f32[512]{0}) %fusion.217), index=1
  %get-tuple-element.1064 = f32[1024,512]{1,0} get-tuple-element((f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=5*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=10*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=15*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}) %fusion.214), index=1
  %get-tuple-element.1065 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.206), index=4
  %get-tuple-element.1066 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.206), index=1
  %get-tuple-element.1067 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.156), index=22
  %get-tuple-element.1068 = f32[512,1024]{1,0} get-tuple-element((f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=5*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=10*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=15*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}) %fusion.200), index=1
  %get-tuple-element.1069 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.156), index=19
  %get-tuple-element.1070 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.156), index=16
  %get-tuple-element.1071 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.156), index=13
  %get-tuple-element.1072 = f32[128,1024]{1,0} get-tuple-element((f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=5*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=10*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=15*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=20*/f32[128,1024]{1,0}) %fusion.245), index=16
  %get-tuple-element.1073 = f32[384]{0} get-tuple-element((f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=5*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=10*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=15*/f32[384]{0}, f32[384]{0}, f32[384]{0}) %fusion.223), index=16
  %get-tuple-element.1074 = f32[1024,384]{1,0} get-tuple-element((f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=5*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=10*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=15*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}) %fusion.220), index=16
  %get-tuple-element.1075 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=5*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=10*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=15*/f32[512]{0}, f32[512]{0}, f32[512]{0}) %fusion.217), index=16
  %get-tuple-element.1076 = f32[1024,512]{1,0} get-tuple-element((f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=5*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=10*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=15*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}) %fusion.214), index=16
  %get-tuple-element.1077 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.156), index=10
  %get-tuple-element.1078 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.156), index=7
  %get-tuple-element.1079 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.156), index=4
  %get-tuple-element.1080 = f32[512,1024]{1,0} get-tuple-element((f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=5*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=10*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=15*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}) %fusion.200), index=16
  %get-tuple-element.1081 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.156), index=1
  %get-tuple-element.1082 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.109), index=22
  %get-tuple-element.1083 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.109), index=19
  %get-tuple-element.1084 = f32[128,1024]{1,0} get-tuple-element((f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=5*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=10*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=15*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=20*/f32[128,1024]{1,0}) %fusion.245), index=13
  %get-tuple-element.1085 = f32[384]{0} get-tuple-element((f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=5*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=10*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=15*/f32[384]{0}, f32[384]{0}, f32[384]{0}) %fusion.223), index=13
  %get-tuple-element.1086 = f32[1024,384]{1,0} get-tuple-element((f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=5*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=10*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=15*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}) %fusion.220), index=13
  %get-tuple-element.1087 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=5*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=10*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=15*/f32[512]{0}, f32[512]{0}, f32[512]{0}) %fusion.217), index=13
  %get-tuple-element.1088 = f32[1024,512]{1,0} get-tuple-element((f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=5*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=10*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=15*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}) %fusion.214), index=13
  %get-tuple-element.1089 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.109), index=16
  %get-tuple-element.1090 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.109), index=13
  %get-tuple-element.1091 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.109), index=10
  %get-tuple-element.1092 = f32[512,1024]{1,0} get-tuple-element((f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=5*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=10*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=15*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}) %fusion.200), index=13
  %get-tuple-element.1093 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.109), index=7
  %get-tuple-element.1094 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.109), index=4
  %get-tuple-element.1095 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.109), index=1
  %get-tuple-element.1096 = f32[128,1024]{1,0} get-tuple-element((f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=5*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=10*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=15*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=20*/f32[128,1024]{1,0}) %fusion.245), index=10
  %get-tuple-element.1097 = f32[384]{0} get-tuple-element((f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=5*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=10*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=15*/f32[384]{0}, f32[384]{0}, f32[384]{0}) %fusion.223), index=10
  %get-tuple-element.1098 = f32[1024,384]{1,0} get-tuple-element((f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=5*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=10*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=15*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}) %fusion.220), index=10
  %get-tuple-element.1099 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=5*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=10*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=15*/f32[512]{0}, f32[512]{0}, f32[512]{0}) %fusion.217), index=10
  %get-tuple-element.1100 = f32[1024,512]{1,0} get-tuple-element((f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=5*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=10*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=15*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}) %fusion.214), index=10
  %get-tuple-element.1101 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.46), index=22
  %get-tuple-element.1102 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.46), index=19
  %get-tuple-element.1103 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.46), index=16
  %get-tuple-element.1104 = f32[512,1024]{1,0} get-tuple-element((f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=5*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=10*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=15*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}) %fusion.200), index=10
  %get-tuple-element.1105 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.46), index=13
  %get-tuple-element.1106 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.46), index=10
  %get-tuple-element.1107 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.46), index=7
  %get-tuple-element.1108 = f32[128,1024]{1,0} get-tuple-element((f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=5*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=10*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=15*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=20*/f32[128,1024]{1,0}) %fusion.245), index=7
  %get-tuple-element.1109 = f32[384]{0} get-tuple-element((f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=5*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=10*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=15*/f32[384]{0}, f32[384]{0}, f32[384]{0}) %fusion.223), index=7
  %get-tuple-element.1110 = f32[1024,384]{1,0} get-tuple-element((f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=5*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=10*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=15*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}) %fusion.220), index=7
  %get-tuple-element.1111 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=5*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=10*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=15*/f32[512]{0}, f32[512]{0}, f32[512]{0}) %fusion.217), index=7
  %get-tuple-element.1112 = f32[1024,512]{1,0} get-tuple-element((f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=5*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=10*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=15*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}) %fusion.214), index=7
  %get-tuple-element.1113 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.46), index=4
  %get-tuple-element.1114 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.46), index=1
  %get-tuple-element.1115 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.258), index=22
  %get-tuple-element.1116 = f32[512,1024]{1,0} get-tuple-element((f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=5*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=10*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=15*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}) %fusion.200), index=7
  %get-tuple-element.1117 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.258), index=19
  %get-tuple-element.1118 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.258), index=16
  %get-tuple-element.1119 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.258), index=13
  %get-tuple-element.1120 = f32[128,1024]{1,0} get-tuple-element((f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=5*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=10*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=15*/f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, f32[128,1024]{1,0}, /*index=20*/f32[128,1024]{1,0}) %fusion.245), index=4
  %get-tuple-element.1121 = f32[384]{0} get-tuple-element((f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=5*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=10*/f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, f32[384]{0}, /*index=15*/f32[384]{0}, f32[384]{0}, f32[384]{0}) %fusion.223), index=4
  %get-tuple-element.1122 = f32[1024,384]{1,0} get-tuple-element((f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=5*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=10*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}, /*index=15*/f32[1024,384]{1,0}, f32[1024,384]{1,0}, f32[1024,384]{1,0}) %fusion.220), index=4
  %get-tuple-element.1123 = f32[512]{0} get-tuple-element((f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=5*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=10*/f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, f32[512]{0}, /*index=15*/f32[512]{0}, f32[512]{0}, f32[512]{0}) %fusion.217), index=4
  %get-tuple-element.1124 = f32[1024,512]{1,0} get-tuple-element((f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=5*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=10*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}, /*index=15*/f32[1024,512]{1,0}, f32[1024,512]{1,0}, f32[1024,512]{1,0}) %fusion.214), index=4
  %get-tuple-element.1125 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.258), index=10
  %get-tuple-element.1126 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.258), index=7
  %get-tuple-element.1127 = f32[1024]{0} get-tuple-element((f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=5*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=10*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}) %fusion.258), index=4
  %get-tuple-element.1128 = f32[512,1024]{1,0} get-tuple-element((f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=5*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=10*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}, /*index=15*/f32[512,1024]{1,0}, f32[512,1024]{1,0}, f32[512,1024]{1,0}) %fusion.200), index=4
  ROOT %tuple.319 = (s32[], f32[6400]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, /*index=5*/f32[6400,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, /*index=10*/f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, f32[1024,512]{1,0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=25*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, /*index=30*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, /*index=35*/f32[1024,384]{1,0}, f32[512]{0}, f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=40*/f32[1024]{0}, f32[512,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=45*/f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, f32[1024,512]{1,0}, /*index=50*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f32[1024]{0}, /*index=55*/f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, /*index=60*/f32[512]{0}, f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=65*/f32[512,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, /*index=70*/f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, f32[1024,512]{1,0}, f32[1024]{0}, /*index=75*/f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, s32[], f32[6400]{0}, /*index=80*/f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, f32[6400,1024]{1,0}, f32[1024]{0}, /*index=85*/f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, /*index=90*/f32[512]{0}, f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=95*/f32[512,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, /*index=100*/f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, f32[1024,512]{1,0}, f32[1024]{0}, /*index=105*/f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=110*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=115*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, /*index=120*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, /*index=125*/f32[1024,384]{1,0}, f32[512]{0}, f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=130*/f32[1024]{0}, f32[512,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=135*/f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, f32[1024,512]{1,0}, /*index=140*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f32[1024]{0}, /*index=145*/f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, /*index=150*/f32[512]{0}, f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=155*/f32[512,1024]{1,0}, f32[6400]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, /*index=160*/f32[6400,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, /*index=165*/f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, f32[1024,512]{1,0}, f32[1024]{0}, /*index=170*/f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=175*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=180*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, /*index=185*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, /*index=190*/f32[1024,384]{1,0}, f32[512]{0}, f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=195*/f32[1024]{0}, f32[512,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=200*/f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, f32[1024,512]{1,0}, /*index=205*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f32[1024]{0}, /*index=210*/f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, /*index=215*/f32[512]{0}, f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=220*/f32[512,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, /*index=225*/f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, f32[1024,512]{1,0}, f32[1024]{0}, /*index=230*/f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}) tuple(s32[] %add.92, f32[6400]{0} %get-tuple-element.897, f32[1024]{0} %get-tuple-element.898, f32[1024]{0} %get-tuple-element.899, f32[128,1024]{1,0} %get-tuple-element.900, /*index=5*/f32[6400,1024]{1,0} %get-tuple-element.901, f32[1024]{0} %get-tuple-element.902, f32[1024]{0} %get-tuple-element.903, f32[1024]{0} %get-tuple-element.904, f32[128,1024]{1,0} %get-tuple-element.905, /*index=10*/f32[384]{0} %get-tuple-element.906, f32[1024,384]{1,0} %get-tuple-element.907, f32[512]{0} %get-tuple-element.908, f32[1024,512]{1,0} %get-tuple-element.909, f32[1024]{0} %get-tuple-element.910, /*index=15*/f32[1024]{0} %get-tuple-element.911, f32[1024]{0} %get-tuple-element.912, f32[512,1024]{1,0} %get-tuple-element.913, f32[1024]{0} %get-tuple-element.914, f32[1024]{0} %get-tuple-element.915, /*index=20*/f32[1024]{0} %get-tuple-element.916, f32[128,1024]{1,0} %get-tuple-element.917, f32[384]{0} %get-tuple-element.918, f32[1024,384]{1,0} %get-tuple-element.919, f32[512]{0} %get-tuple-element.920, /*index=25*/f32[1024,512]{1,0} %get-tuple-element.921, f32[1024]{0} %get-tuple-element.922, f32[1024]{0} %get-tuple-element.923, f32[1024]{0} %get-tuple-element.924, f32[512,1024]{1,0} %get-tuple-element.925, /*index=30*/f32[1024]{0} %get-tuple-element.926, f32[1024]{0} %get-tuple-element.927, f32[1024]{0} %get-tuple-element.928, f32[128,1024]{1,0} %get-tuple-element.929, f32[384]{0} %get-tuple-element.930, /*index=35*/f32[1024,384]{1,0} %get-tuple-element.931, f32[512]{0} %get-tuple-element.932, f32[1024,512]{1,0} %get-tuple-element.933, f32[1024]{0} %get-tuple-element.934, f32[1024]{0} %get-tuple-element.935, /*index=40*/f32[1024]{0} %get-tuple-element.936, f32[512,1024]{1,0} %get-tuple-element.937, f32[1024]{0} %get-tuple-element.938, f32[1024]{0} %get-tuple-element.939, f32[1024]{0} %get-tuple-element.940, /*index=45*/f32[128,1024]{1,0} %get-tuple-element.941, f32[384]{0} %get-tuple-element.942, f32[1024,384]{1,0} %get-tuple-element.943, f32[512]{0} %get-tuple-element.944, f32[1024,512]{1,0} %get-tuple-element.945, /*index=50*/f32[1024]{0} %get-tuple-element.946, f32[1024]{0} %get-tuple-element.947, f32[1024]{0} %get-tuple-element.948, f32[512,1024]{1,0} %get-tuple-element.949, f32[1024]{0} %get-tuple-element.950, /*index=55*/f32[1024]{0} %get-tuple-element.951, f32[1024]{0} %get-tuple-element.952, f32[128,1024]{1,0} %get-tuple-element.953, f32[384]{0} %get-tuple-element.954, f32[1024,384]{1,0} %get-tuple-element.955, /*index=60*/f32[512]{0} %get-tuple-element.956, f32[1024,512]{1,0} %get-tuple-element.957, f32[1024]{0} %get-tuple-element.958, f32[1024]{0} %get-tuple-element.959, f32[1024]{0} %get-tuple-element.960, /*index=65*/f32[512,1024]{1,0} %get-tuple-element.961, f32[1024]{0} %get-tuple-element.962, f32[1024]{0} %get-tuple-element.963, f32[1024]{0} %get-tuple-element.964, f32[128,1024]{1,0} %get-tuple-element.965, /*index=70*/f32[384]{0} %get-tuple-element.966, f32[1024,384]{1,0} %get-tuple-element.967, f32[512]{0} %get-tuple-element.968, f32[1024,512]{1,0} %get-tuple-element.969, f32[1024]{0} %get-tuple-element.970, /*index=75*/f32[1024]{0} %get-tuple-element.971, f32[1024]{0} %get-tuple-element.972, f32[512,1024]{1,0} %get-tuple-element.973, s32[] %get-tuple-element.974, f32[6400]{0} %get-tuple-element.975, /*index=80*/f32[1024]{0} %get-tuple-element.976, f32[1024]{0} %get-tuple-element.977, f32[128,1024]{1,0} %get-tuple-element.978, f32[6400,1024]{1,0} %get-tuple-element.979, f32[1024]{0} %get-tuple-element.980, /*index=85*/f32[1024]{0} %get-tuple-element.981, f32[1024]{0} %get-tuple-element.982, f32[128,1024]{1,0} %get-tuple-element.983, f32[384]{0} %get-tuple-element.984, f32[1024,384]{1,0} %get-tuple-element.985, /*index=90*/f32[512]{0} %get-tuple-element.986, f32[1024,512]{1,0} %get-tuple-element.987, f32[1024]{0} %get-tuple-element.988, f32[1024]{0} %get-tuple-element.989, f32[1024]{0} %get-tuple-element.990, /*index=95*/f32[512,1024]{1,0} %get-tuple-element.991, f32[1024]{0} %get-tuple-element.992, f32[1024]{0} %get-tuple-element.993, f32[1024]{0} %get-tuple-element.994, f32[128,1024]{1,0} %get-tuple-element.995, /*index=100*/f32[384]{0} %get-tuple-element.996, f32[1024,384]{1,0} %get-tuple-element.997, f32[512]{0} %get-tuple-element.998, f32[1024,512]{1,0} %get-tuple-element.999, f32[1024]{0} %get-tuple-element.1000, /*index=105*/f32[1024]{0} %get-tuple-element.1001, f32[1024]{0} %get-tuple-element.1002, f32[512,1024]{1,0} %get-tuple-element.1003, f32[1024]{0} %get-tuple-element.1004, f32[1024]{0} %get-tuple-element.1005, /*index=110*/f32[1024]{0} %get-tuple-element.1006, f32[128,1024]{1,0} %get-tuple-element.1007, f32[384]{0} %get-tuple-element.1008, f32[1024,384]{1,0} %get-tuple-element.1009, f32[512]{0} %get-tuple-element.1010, /*index=115*/f32[1024,512]{1,0} %get-tuple-element.1011, f32[1024]{0} %get-tuple-element.1012, f32[1024]{0} %get-tuple-element.1013, f32[1024]{0} %get-tuple-element.1014, f32[512,1024]{1,0} %get-tuple-element.1015, /*index=120*/f32[1024]{0} %get-tuple-element.1016, f32[1024]{0} %get-tuple-element.1017, f32[1024]{0} %get-tuple-element.1018, f32[128,1024]{1,0} %get-tuple-element.1019, f32[384]{0} %get-tuple-element.1020, /*index=125*/f32[1024,384]{1,0} %get-tuple-element.1021, f32[512]{0} %get-tuple-element.1022, f32[1024,512]{1,0} %get-tuple-element.1023, f32[1024]{0} %get-tuple-element.1024, f32[1024]{0} %get-tuple-element.1025, /*index=130*/f32[1024]{0} %get-tuple-element.1026, f32[512,1024]{1,0} %get-tuple-element.1027, f32[1024]{0} %get-tuple-element.1028, f32[1024]{0} %get-tuple-element.1029, f32[1024]{0} %get-tuple-element.1030, /*index=135*/f32[128,1024]{1,0} %get-tuple-element.1031, f32[384]{0} %get-tuple-element.1032, f32[1024,384]{1,0} %get-tuple-element.1033, f32[512]{0} %get-tuple-element.1034, f32[1024,512]{1,0} %get-tuple-element.1035, /*index=140*/f32[1024]{0} %get-tuple-element.1036, f32[1024]{0} %get-tuple-element.1037, f32[1024]{0} %get-tuple-element.1038, f32[512,1024]{1,0} %get-tuple-element.1039, f32[1024]{0} %get-tuple-element.1040, /*index=145*/f32[1024]{0} %get-tuple-element.1041, f32[1024]{0} %get-tuple-element.1042, f32[128,1024]{1,0} %get-tuple-element.1043, f32[384]{0} %get-tuple-element.1044, f32[1024,384]{1,0} %get-tuple-element.1045, /*index=150*/f32[512]{0} %get-tuple-element.1046, f32[1024,512]{1,0} %get-tuple-element.1047, f32[1024]{0} %get-tuple-element.1048, f32[1024]{0} %get-tuple-element.1049, f32[1024]{0} %get-tuple-element.1050, /*index=155*/f32[512,1024]{1,0} %get-tuple-element.1051, f32[6400]{0} %get-tuple-element.1052, f32[1024]{0} %get-tuple-element.1053, f32[1024]{0} %get-tuple-element.1054, f32[128,1024]{1,0} %get-tuple-element.1055, /*index=160*/f32[6400,1024]{1,0} %get-tuple-element.1056, f32[1024]{0} %get-tuple-element.1057, f32[1024]{0} %get-tuple-element.1058, f32[1024]{0} %get-tuple-element.1059, f32[128,1024]{1,0} %get-tuple-element.1060, /*index=165*/f32[384]{0} %get-tuple-element.1061, f32[1024,384]{1,0} %get-tuple-element.1062, f32[512]{0} %get-tuple-element.1063, f32[1024,512]{1,0} %get-tuple-element.1064, f32[1024]{0} %get-tuple-element.1065, /*index=170*/f32[1024]{0} %get-tuple-element.1066, f32[1024]{0} %get-tuple-element.1067, f32[512,1024]{1,0} %get-tuple-element.1068, f32[1024]{0} %get-tuple-element.1069, f32[1024]{0} %get-tuple-element.1070, /*index=175*/f32[1024]{0} %get-tuple-element.1071, f32[128,1024]{1,0} %get-tuple-element.1072, f32[384]{0} %get-tuple-element.1073, f32[1024,384]{1,0} %get-tuple-element.1074, f32[512]{0} %get-tuple-element.1075, /*index=180*/f32[1024,512]{1,0} %get-tuple-element.1076, f32[1024]{0} %get-tuple-element.1077, f32[1024]{0} %get-tuple-element.1078, f32[1024]{0} %get-tuple-element.1079, f32[512,1024]{1,0} %get-tuple-element.1080, /*index=185*/f32[1024]{0} %get-tuple-element.1081, f32[1024]{0} %get-tuple-element.1082, f32[1024]{0} %get-tuple-element.1083, f32[128,1024]{1,0} %get-tuple-element.1084, f32[384]{0} %get-tuple-element.1085, /*index=190*/f32[1024,384]{1,0} %get-tuple-element.1086, f32[512]{0} %get-tuple-element.1087, f32[1024,512]{1,0} %get-tuple-element.1088, f32[1024]{0} %get-tuple-element.1089, f32[1024]{0} %get-tuple-element.1090, /*index=195*/f32[1024]{0} %get-tuple-element.1091, f32[512,1024]{1,0} %get-tuple-element.1092, f32[1024]{0} %get-tuple-element.1093, f32[1024]{0} %get-tuple-element.1094, f32[1024]{0} %get-tuple-element.1095, /*index=200*/f32[128,1024]{1,0} %get-tuple-element.1096, f32[384]{0} %get-tuple-element.1097, f32[1024,384]{1,0} %get-tuple-element.1098, f32[512]{0} %get-tuple-element.1099, f32[1024,512]{1,0} %get-tuple-element.1100, /*index=205*/f32[1024]{0} %get-tuple-element.1101, f32[1024]{0} %get-tuple-element.1102, f32[1024]{0} %get-tuple-element.1103, f32[512,1024]{1,0} %get-tuple-element.1104, f32[1024]{0} %get-tuple-element.1105, /*index=210*/f32[1024]{0} %get-tuple-element.1106, f32[1024]{0} %get-tuple-element.1107, f32[128,1024]{1,0} %get-tuple-element.1108, f32[384]{0} %get-tuple-element.1109, f32[1024,384]{1,0} %get-tuple-element.1110, /*index=215*/f32[512]{0} %get-tuple-element.1111, f32[1024,512]{1,0} %get-tuple-element.1112, f32[1024]{0} %get-tuple-element.1113, f32[1024]{0} %get-tuple-element.1114, f32[1024]{0} %get-tuple-element.1115, /*index=220*/f32[512,1024]{1,0} %get-tuple-element.1116, f32[1024]{0} %get-tuple-element.1117, f32[1024]{0} %get-tuple-element.1118, f32[1024]{0} %get-tuple-element.1119, f32[128,1024]{1,0} %get-tuple-element.1120, /*index=225*/f32[384]{0} %get-tuple-element.1121, f32[1024,384]{1,0} %get-tuple-element.1122, f32[512]{0} %get-tuple-element.1123, f32[1024,512]{1,0} %get-tuple-element.1124, f32[1024]{0} %get-tuple-element.1125, /*index=230*/f32[1024]{0} %get-tuple-element.1126, f32[1024]{0} %get-tuple-element.1127, f32[512,1024]{1,0} %get-tuple-element.1128)
}

