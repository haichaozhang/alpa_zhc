HloModule train_step_shard_parallel.6, input_output_alias={ {0}: (0, {}, may-alias), {1}: (1, {}, may-alias), {2}: (2, {}, may-alias), {3}: (3, {}, may-alias), {4}: (4, {}, may-alias), {5}: (5, {}, may-alias), {6}: (6, {}, may-alias), {7}: (7, {}, may-alias), {8}: (8, {}, may-alias), {9}: (9, {}, may-alias), {10}: (10, {}, may-alias), {11}: (11, {}, may-alias), {12}: (12, {}, may-alias), {13}: (13, {}, may-alias), {14}: (14, {}, may-alias), {15}: (15, {}, may-alias), {16}: (16, {}, may-alias), {17}: (17, {}, may-alias), {18}: (18, {}, may-alias), {19}: (19, {}, may-alias), {20}: (20, {}, may-alias), {21}: (21, {}, may-alias), {22}: (22, {}, may-alias), {23}: (23, {}, may-alias), {24}: (24, {}, may-alias), {25}: (25, {}, may-alias), {26}: (26, {}, may-alias), {27}: (27, {}, may-alias), {28}: (28, {}, may-alias), {29}: (29, {}, may-alias), {30}: (30, {}, may-alias), {31}: (31, {}, may-alias), {32}: (32, {}, may-alias), {33}: (33, {}, may-alias), {34}: (34, {}, may-alias), {35}: (35, {}, may-alias), {36}: (36, {}, may-alias), {37}: (37, {}, may-alias), {38}: (38, {}, may-alias), {39}: (39, {}, may-alias), {40}: (40, {}, may-alias), {41}: (41, {}, may-alias), {42}: (42, {}, may-alias), {43}: (43, {}, may-alias), {44}: (44, {}, may-alias), {45}: (45, {}, may-alias), {46}: (46, {}, may-alias), {47}: (47, {}, may-alias), {48}: (48, {}, may-alias), {49}: (49, {}, may-alias), {50}: (50, {}, may-alias), {51}: (51, {}, may-alias), {52}: (52, {}, may-alias), {53}: (53, {}, may-alias), {54}: (54, {}, may-alias), {55}: (55, {}, may-alias), {56}: (56, {}, may-alias), {57}: (57, {}, may-alias), {58}: (58, {}, may-alias), {59}: (59, {}, may-alias), {60}: (60, {}, may-alias), {61}: (61, {}, may-alias), {62}: (62, {}, may-alias), {63}: (63, {}, may-alias), {64}: (64, {}, may-alias), {65}: (65, {}, may-alias), {66}: (66, {}, may-alias), {67}: (67, {}, may-alias), {68}: (68, {}, may-alias), {69}: (69, {}, may-alias), {70}: (70, {}, may-alias), {71}: (71, {}, may-alias), {72}: (72, {}, may-alias), {73}: (73, {}, may-alias), {74}: (74, {}, may-alias), {75}: (75, {}, may-alias), {76}: (76, {}, may-alias), {77}: (77, {}, may-alias), {78}: (78, {}, may-alias), {79}: (79, {}, may-alias), {80}: (80, {}, may-alias), {81}: (81, {}, may-alias), {82}: (82, {}, may-alias), {83}: (83, {}, may-alias), {84}: (84, {}, may-alias), {85}: (85, {}, may-alias), {86}: (86, {}, may-alias), {87}: (87, {}, may-alias), {88}: (88, {}, may-alias), {89}: (89, {}, may-alias), {90}: (90, {}, may-alias), {91}: (91, {}, may-alias), {92}: (92, {}, may-alias), {93}: (93, {}, may-alias), {94}: (94, {}, may-alias), {95}: (95, {}, may-alias), {96}: (96, {}, may-alias), {97}: (97, {}, may-alias), {98}: (98, {}, may-alias), {99}: (99, {}, may-alias), {100}: (100, {}, may-alias), {101}: (101, {}, may-alias), {102}: (102, {}, may-alias), {103}: (103, {}, may-alias), {104}: (104, {}, may-alias), {105}: (105, {}, may-alias), {106}: (106, {}, may-alias), {107}: (107, {}, may-alias), {108}: (108, {}, may-alias), {109}: (109, {}, may-alias), {110}: (110, {}, may-alias), {111}: (111, {}, may-alias), {112}: (112, {}, may-alias), {113}: (113, {}, may-alias), {114}: (114, {}, may-alias), {115}: (115, {}, may-alias), {116}: (116, {}, may-alias), {117}: (117, {}, may-alias), {118}: (118, {}, may-alias), {119}: (119, {}, may-alias), {120}: (120, {}, may-alias), {121}: (121, {}, may-alias), {122}: (122, {}, may-alias), {123}: (123, {}, may-alias), {124}: (124, {}, may-alias), {125}: (125, {}, may-alias), {126}: (126, {}, may-alias), {127}: (127, {}, may-alias), {128}: (128, {}, may-alias), {129}: (129, {}, may-alias), {130}: (130, {}, may-alias), {131}: (131, {}, may-alias), {132}: (132, {}, may-alias), {133}: (133, {}, may-alias), {134}: (134, {}, may-alias), {135}: (135, {}, may-alias), {136}: (136, {}, may-alias), {137}: (137, {}, may-alias), {138}: (138, {}, may-alias), {139}: (139, {}, may-alias), {140}: (140, {}, may-alias), {141}: (141, {}, may-alias), {142}: (142, {}, may-alias), {143}: (143, {}, may-alias), {144}: (144, {}, may-alias), {145}: (145, {}, may-alias), {146}: (146, {}, may-alias), {147}: (147, {}, may-alias), {148}: (148, {}, may-alias), {149}: (149, {}, may-alias), {150}: (150, {}, may-alias), {151}: (151, {}, may-alias), {152}: (152, {}, may-alias), {153}: (153, {}, may-alias), {154}: (154, {}, may-alias), {155}: (155, {}, may-alias), {156}: (156, {}, may-alias), {157}: (157, {}, may-alias), {158}: (158, {}, may-alias), {159}: (159, {}, may-alias), {160}: (160, {}, may-alias), {161}: (161, {}, may-alias), {162}: (162, {}, may-alias), {163}: (163, {}, may-alias), {164}: (164, {}, may-alias), {165}: (165, {}, may-alias), {166}: (166, {}, may-alias), {167}: (167, {}, may-alias), {168}: (168, {}, may-alias), {169}: (169, {}, may-alias), {170}: (170, {}, may-alias), {171}: (171, {}, may-alias), {172}: (172, {}, may-alias), {173}: (173, {}, may-alias), {174}: (174, {}, may-alias), {175}: (175, {}, may-alias), {176}: (176, {}, may-alias), {177}: (177, {}, may-alias), {178}: (178, {}, may-alias), {179}: (179, {}, may-alias), {180}: (180, {}, may-alias), {181}: (181, {}, may-alias), {182}: (182, {}, may-alias), {183}: (183, {}, may-alias), {184}: (184, {}, may-alias), {185}: (185, {}, may-alias), {186}: (186, {}, may-alias), {187}: (187, {}, may-alias), {188}: (188, {}, may-alias), {189}: (189, {}, may-alias), {190}: (190, {}, may-alias), {191}: (191, {}, may-alias), {192}: (192, {}, may-alias), {193}: (193, {}, may-alias), {194}: (194, {}, may-alias), {195}: (195, {}, may-alias), {196}: (196, {}, may-alias), {197}: (197, {}, may-alias), {198}: (198, {}, may-alias), {199}: (199, {}, may-alias), {200}: (200, {}, may-alias), {201}: (201, {}, may-alias), {202}: (202, {}, may-alias), {203}: (203, {}, may-alias), {204}: (204, {}, may-alias), {205}: (205, {}, may-alias), {206}: (206, {}, may-alias), {207}: (207, {}, may-alias), {208}: (208, {}, may-alias), {209}: (209, {}, may-alias), {210}: (210, {}, may-alias), {211}: (211, {}, may-alias), {212}: (212, {}, may-alias), {213}: (213, {}, may-alias), {214}: (214, {}, may-alias), {215}: (215, {}, may-alias), {216}: (216, {}, may-alias), {217}: (217, {}, may-alias), {218}: (218, {}, may-alias), {219}: (219, {}, may-alias), {220}: (220, {}, may-alias), {221}: (221, {}, may-alias), {222}: (222, {}, may-alias), {223}: (223, {}, may-alias), {224}: (224, {}, may-alias), {225}: (225, {}, may-alias), {226}: (226, {}, may-alias), {227}: (227, {}, may-alias), {228}: (228, {}, may-alias), {229}: (229, {}, may-alias), {230}: (230, {}, may-alias), {231}: (231, {}, may-alias), {232}: (232, {}, may-alias) }, entry_computation_layout={(s32[],f32[6400]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[6400,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},s32[],f32[6400]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[6400,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},f32[6400]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[6400,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[128,1024]{1,0},f32[384]{0},f32[1024,384]{1,0},f32[512]{0},f32[1024,512]{1,0},f32[1024]{0},f32[1024]{0},f32[1024]{0},f32[512,1024]{1,0},s32[4,1024]{1,0},s32[4,1024]{1,0},s32[4,1024]{1,0},s32[4,1024]{1,0},s32[4,1024]{1,0},u32[1]{0})->(s32[], f32[6400]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, /*index=5*/f32[6400,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, /*index=10*/f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, f32[1024,512]{1,0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=25*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, /*index=30*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, /*index=35*/f32[1024,384]{1,0}, f32[512]{0}, f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=40*/f32[1024]{0}, f32[512,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=45*/f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, f32[1024,512]{1,0}, /*index=50*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f32[1024]{0}, /*index=55*/f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, /*index=60*/f32[512]{0}, f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=65*/f32[512,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, /*index=70*/f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, f32[1024,512]{1,0}, f32[1024]{0}, /*index=75*/f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, s32[], f32[6400]{0}, /*index=80*/f32[1024]{0}, f32[1024]{0}, f32[128,1024]{0,1}, f32[6400,1024]{1,0}, f32[1024]{0}, /*index=85*/f32[1024]{0}, f32[1024]{0}, f32[128,1024]{0,1}, f32[384]{0}, f32[1024,384]{0,1}, /*index=90*/f32[512]{0}, f32[1024,512]{0,1}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=95*/f32[512,1024]{0,1}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{0,1}, /*index=100*/f32[384]{0}, f32[1024,384]{0,1}, f32[512]{0}, f32[1024,512]{0,1}, f32[1024]{0}, /*index=105*/f32[1024]{0}, f32[1024]{0}, f32[512,1024]{0,1}, f32[1024]{0}, f32[1024]{0}, /*index=110*/f32[1024]{0}, f32[128,1024]{0,1}, f32[384]{0}, f32[1024,384]{0,1}, f32[512]{0}, /*index=115*/f32[1024,512]{0,1}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{0,1}, /*index=120*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{0,1}, f32[384]{0}, /*index=125*/f32[1024,384]{0,1}, f32[512]{0}, f32[1024,512]{0,1}, f32[1024]{0}, f32[1024]{0}, /*index=130*/f32[1024]{0}, f32[512,1024]{0,1}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=135*/f32[128,1024]{0,1}, f32[384]{0}, f32[1024,384]{0,1}, f32[512]{0}, f32[1024,512]{0,1}, /*index=140*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{0,1}, f32[1024]{0}, /*index=145*/f32[1024]{0}, f32[1024]{0}, f32[128,1024]{0,1}, f32[384]{0}, f32[1024,384]{0,1}, /*index=150*/f32[512]{0}, f32[1024,512]{0,1}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=155*/f32[512,1024]{0,1}, f32[6400]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{0,1}, /*index=160*/f32[6400,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{0,1}, /*index=165*/f32[384]{0}, f32[1024,384]{0,1}, f32[512]{0}, f32[1024,512]{0,1}, f32[1024]{0}, /*index=170*/f32[1024]{0}, f32[1024]{0}, f32[512,1024]{0,1}, f32[1024]{0}, f32[1024]{0}, /*index=175*/f32[1024]{0}, f32[128,1024]{0,1}, f32[384]{0}, f32[1024,384]{0,1}, f32[512]{0}, /*index=180*/f32[1024,512]{0,1}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{0,1}, /*index=185*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{0,1}, f32[384]{0}, /*index=190*/f32[1024,384]{0,1}, f32[512]{0}, f32[1024,512]{0,1}, f32[1024]{0}, f32[1024]{0}, /*index=195*/f32[1024]{0}, f32[512,1024]{0,1}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=200*/f32[128,1024]{0,1}, f32[384]{0}, f32[1024,384]{0,1}, f32[512]{0}, f32[1024,512]{0,1}, /*index=205*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{0,1}, f32[1024]{0}, /*index=210*/f32[1024]{0}, f32[1024]{0}, f32[128,1024]{0,1}, f32[384]{0}, f32[1024,384]{0,1}, /*index=215*/f32[512]{0}, f32[1024,512]{0,1}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=220*/f32[512,1024]{0,1}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{0,1}, /*index=225*/f32[384]{0}, f32[1024,384]{0,1}, f32[512]{0}, f32[1024,512]{0,1}, f32[1024]{0}, /*index=230*/f32[1024]{0}, f32[1024]{0}, f32[512,1024]{0,1})}

%add.1 (x.1: f16[], y.1: f16[]) -> f16[] {
  %x.1 = f16[] parameter(0)
  %y.1 = f16[] parameter(1)
  ROOT %add.27 = f16[] add(f16[] %x.1, f16[] %y.1)
}

%region_40.1707 (Arg_0.1708: f32[], Arg_1.1709: f32[]) -> f32[] {
  %Arg_0.1708 = f32[] parameter(0)
  %Arg_1.1709 = f32[] parameter(1)
  ROOT %add.1710 = f32[] add(f32[] %Arg_0.1708, f32[] %Arg_1.1709), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(0, 1)]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=90}
}

%region_41.1720 (Arg_0.1721: f16[], Arg_1.1722: f16[]) -> f16[] {
  %Arg_0.1721 = f16[] parameter(0)
  %Arg_1.1722 = f16[] parameter(1)
  ROOT %add.1723 = f16[] add(f16[] %Arg_0.1721, f16[] %Arg_1.1722), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(2,)]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
}

%region_0.466 (Arg_0.467: f32[], Arg_1.468: f32[]) -> f32[] {
  %Arg_0.467 = f32[] parameter(0)
  %Arg_1.468 = f32[] parameter(1)
  ROOT %add.469 = f32[] add(f32[] %Arg_0.467, f32[] %Arg_1.468), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_1.474 (Arg_0.475: f32[], Arg_1.476: f32[]) -> f32[] {
  %Arg_0.475 = f32[] parameter(0)
  %Arg_1.476 = f32[] parameter(1)
  ROOT %add.477 = f32[] add(f32[] %Arg_0.475, f32[] %Arg_1.476), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%region_2.539 (Arg_0.540: f16[], Arg_1.541: f16[]) -> f16[] {
  %Arg_0.540 = f16[] parameter(0)
  %Arg_1.541 = f16[] parameter(1)
  ROOT %maximum.542 = f16[] maximum(f16[] %Arg_0.540, f16[] %Arg_1.541), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%region_3.551 (Arg_0.552: f32[], Arg_1.553: f32[]) -> f32[] {
  %Arg_0.552 = f32[] parameter(0)
  %Arg_1.553 = f32[] parameter(1)
  ROOT %add.554 = f32[] add(f32[] %Arg_0.552, f32[] %Arg_1.553), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%add.2 (x.2: f16[], y.2: f16[]) -> f16[] {
  %x.2 = f16[] parameter(0)
  %y.2 = f16[] parameter(1)
  ROOT %add.28 = f16[] add(f16[] %x.2, f16[] %y.2)
}

%region_4.576 (Arg_0.577: f32[], Arg_1.578: f32[]) -> f32[] {
  %Arg_0.577 = f32[] parameter(0)
  %Arg_1.578 = f32[] parameter(1)
  ROOT %add.579 = f32[] add(f32[] %Arg_0.577, f32[] %Arg_1.578), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_5.588 (Arg_0.589: f32[], Arg_1.590: f32[]) -> f32[] {
  %Arg_0.589 = f32[] parameter(0)
  %Arg_1.590 = f32[] parameter(1)
  ROOT %add.591 = f32[] add(f32[] %Arg_0.589, f32[] %Arg_1.590), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%add.3 (x.3: f16[], y.3: f16[]) -> f16[] {
  %x.3 = f16[] parameter(0)
  %y.3 = f16[] parameter(1)
  ROOT %add.29 = f16[] add(f16[] %x.3, f16[] %y.3)
}

%region_6.668 (Arg_0.669: f32[], Arg_1.670: f32[]) -> f32[] {
  %Arg_0.669 = f32[] parameter(0)
  %Arg_1.670 = f32[] parameter(1)
  ROOT %add.671 = f32[] add(f32[] %Arg_0.669, f32[] %Arg_1.670), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_7.680 (Arg_0.681: f32[], Arg_1.682: f32[]) -> f32[] {
  %Arg_0.681 = f32[] parameter(0)
  %Arg_1.682 = f32[] parameter(1)
  ROOT %add.683 = f32[] add(f32[] %Arg_0.681, f32[] %Arg_1.682), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%region_8.731 (Arg_0.732: f16[], Arg_1.733: f16[]) -> f16[] {
  %Arg_0.732 = f16[] parameter(0)
  %Arg_1.733 = f16[] parameter(1)
  ROOT %maximum.734 = f16[] maximum(f16[] %Arg_0.732, f16[] %Arg_1.733), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%region_9.743 (Arg_0.744: f32[], Arg_1.745: f32[]) -> f32[] {
  %Arg_0.744 = f32[] parameter(0)
  %Arg_1.745 = f32[] parameter(1)
  ROOT %add.746 = f32[] add(f32[] %Arg_0.744, f32[] %Arg_1.745), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%add.4 (x.4: f16[], y.4: f16[]) -> f16[] {
  %x.4 = f16[] parameter(0)
  %y.4 = f16[] parameter(1)
  ROOT %add.30 = f16[] add(f16[] %x.4, f16[] %y.4)
}

%region_10.768 (Arg_0.769: f32[], Arg_1.770: f32[]) -> f32[] {
  %Arg_0.769 = f32[] parameter(0)
  %Arg_1.770 = f32[] parameter(1)
  ROOT %add.771 = f32[] add(f32[] %Arg_0.769, f32[] %Arg_1.770), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_11.780 (Arg_0.781: f32[], Arg_1.782: f32[]) -> f32[] {
  %Arg_0.781 = f32[] parameter(0)
  %Arg_1.782 = f32[] parameter(1)
  ROOT %add.783 = f32[] add(f32[] %Arg_0.781, f32[] %Arg_1.782), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%add.5 (x.5: f16[], y.5: f16[]) -> f16[] {
  %x.5 = f16[] parameter(0)
  %y.5 = f16[] parameter(1)
  ROOT %add.31 = f16[] add(f16[] %x.5, f16[] %y.5)
}

%region_12.860 (Arg_0.861: f32[], Arg_1.862: f32[]) -> f32[] {
  %Arg_0.861 = f32[] parameter(0)
  %Arg_1.862 = f32[] parameter(1)
  ROOT %add.863 = f32[] add(f32[] %Arg_0.861, f32[] %Arg_1.862), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_13.872 (Arg_0.873: f32[], Arg_1.874: f32[]) -> f32[] {
  %Arg_0.873 = f32[] parameter(0)
  %Arg_1.874 = f32[] parameter(1)
  ROOT %add.875 = f32[] add(f32[] %Arg_0.873, f32[] %Arg_1.874), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%region_14.923 (Arg_0.924: f16[], Arg_1.925: f16[]) -> f16[] {
  %Arg_0.924 = f16[] parameter(0)
  %Arg_1.925 = f16[] parameter(1)
  ROOT %maximum.926 = f16[] maximum(f16[] %Arg_0.924, f16[] %Arg_1.925), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%region_15.935 (Arg_0.936: f32[], Arg_1.937: f32[]) -> f32[] {
  %Arg_0.936 = f32[] parameter(0)
  %Arg_1.937 = f32[] parameter(1)
  ROOT %add.938 = f32[] add(f32[] %Arg_0.936, f32[] %Arg_1.937), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%add.6 (x.6: f16[], y.6: f16[]) -> f16[] {
  %x.6 = f16[] parameter(0)
  %y.6 = f16[] parameter(1)
  ROOT %add.32 = f16[] add(f16[] %x.6, f16[] %y.6)
}

%region_16.960 (Arg_0.961: f32[], Arg_1.962: f32[]) -> f32[] {
  %Arg_0.961 = f32[] parameter(0)
  %Arg_1.962 = f32[] parameter(1)
  ROOT %add.963 = f32[] add(f32[] %Arg_0.961, f32[] %Arg_1.962), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_17.972 (Arg_0.973: f32[], Arg_1.974: f32[]) -> f32[] {
  %Arg_0.973 = f32[] parameter(0)
  %Arg_1.974 = f32[] parameter(1)
  ROOT %add.975 = f32[] add(f32[] %Arg_0.973, f32[] %Arg_1.974), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%add.7 (x.7: f16[], y.7: f16[]) -> f16[] {
  %x.7 = f16[] parameter(0)
  %y.7 = f16[] parameter(1)
  ROOT %add.33 = f16[] add(f16[] %x.7, f16[] %y.7)
}

%region_18.1052 (Arg_0.1053: f32[], Arg_1.1054: f32[]) -> f32[] {
  %Arg_0.1053 = f32[] parameter(0)
  %Arg_1.1054 = f32[] parameter(1)
  ROOT %add.1055 = f32[] add(f32[] %Arg_0.1053, f32[] %Arg_1.1054), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_19.1064 (Arg_0.1065: f32[], Arg_1.1066: f32[]) -> f32[] {
  %Arg_0.1065 = f32[] parameter(0)
  %Arg_1.1066 = f32[] parameter(1)
  ROOT %add.1067 = f32[] add(f32[] %Arg_0.1065, f32[] %Arg_1.1066), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%region_20.1115 (Arg_0.1116: f16[], Arg_1.1117: f16[]) -> f16[] {
  %Arg_0.1116 = f16[] parameter(0)
  %Arg_1.1117 = f16[] parameter(1)
  ROOT %maximum.1118 = f16[] maximum(f16[] %Arg_0.1116, f16[] %Arg_1.1117), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%region_21.1127 (Arg_0.1128: f32[], Arg_1.1129: f32[]) -> f32[] {
  %Arg_0.1128 = f32[] parameter(0)
  %Arg_1.1129 = f32[] parameter(1)
  ROOT %add.1130 = f32[] add(f32[] %Arg_0.1128, f32[] %Arg_1.1129), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%add.8 (x.8: f16[], y.8: f16[]) -> f16[] {
  %x.8 = f16[] parameter(0)
  %y.8 = f16[] parameter(1)
  ROOT %add.34 = f16[] add(f16[] %x.8, f16[] %y.8)
}

%region_22.1152 (Arg_0.1153: f32[], Arg_1.1154: f32[]) -> f32[] {
  %Arg_0.1153 = f32[] parameter(0)
  %Arg_1.1154 = f32[] parameter(1)
  ROOT %add.1155 = f32[] add(f32[] %Arg_0.1153, f32[] %Arg_1.1154), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_23.1164 (Arg_0.1165: f32[], Arg_1.1166: f32[]) -> f32[] {
  %Arg_0.1165 = f32[] parameter(0)
  %Arg_1.1166 = f32[] parameter(1)
  ROOT %add.1167 = f32[] add(f32[] %Arg_0.1165, f32[] %Arg_1.1166), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%add.9 (x.9: f16[], y.9: f16[]) -> f16[] {
  %x.9 = f16[] parameter(0)
  %y.9 = f16[] parameter(1)
  ROOT %add.35 = f16[] add(f16[] %x.9, f16[] %y.9)
}

%region_24.1244 (Arg_0.1245: f32[], Arg_1.1246: f32[]) -> f32[] {
  %Arg_0.1245 = f32[] parameter(0)
  %Arg_1.1246 = f32[] parameter(1)
  ROOT %add.1247 = f32[] add(f32[] %Arg_0.1245, f32[] %Arg_1.1246), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_25.1256 (Arg_0.1257: f32[], Arg_1.1258: f32[]) -> f32[] {
  %Arg_0.1257 = f32[] parameter(0)
  %Arg_1.1258 = f32[] parameter(1)
  ROOT %add.1259 = f32[] add(f32[] %Arg_0.1257, f32[] %Arg_1.1258), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%region_26.1307 (Arg_0.1308: f16[], Arg_1.1309: f16[]) -> f16[] {
  %Arg_0.1308 = f16[] parameter(0)
  %Arg_1.1309 = f16[] parameter(1)
  ROOT %maximum.1310 = f16[] maximum(f16[] %Arg_0.1308, f16[] %Arg_1.1309), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%region_27.1319 (Arg_0.1320: f32[], Arg_1.1321: f32[]) -> f32[] {
  %Arg_0.1320 = f32[] parameter(0)
  %Arg_1.1321 = f32[] parameter(1)
  ROOT %add.1322 = f32[] add(f32[] %Arg_0.1320, f32[] %Arg_1.1321), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%add.10 (x.10: f16[], y.10: f16[]) -> f16[] {
  %x.10 = f16[] parameter(0)
  %y.10 = f16[] parameter(1)
  ROOT %add.36 = f16[] add(f16[] %x.10, f16[] %y.10)
}

%region_28.1344 (Arg_0.1345: f32[], Arg_1.1346: f32[]) -> f32[] {
  %Arg_0.1345 = f32[] parameter(0)
  %Arg_1.1346 = f32[] parameter(1)
  ROOT %add.1347 = f32[] add(f32[] %Arg_0.1345, f32[] %Arg_1.1346), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_29.1356 (Arg_0.1357: f32[], Arg_1.1358: f32[]) -> f32[] {
  %Arg_0.1357 = f32[] parameter(0)
  %Arg_1.1358 = f32[] parameter(1)
  ROOT %add.1359 = f32[] add(f32[] %Arg_0.1357, f32[] %Arg_1.1358), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%add.11 (x.11: f16[], y.11: f16[]) -> f16[] {
  %x.11 = f16[] parameter(0)
  %y.11 = f16[] parameter(1)
  ROOT %add.37 = f16[] add(f16[] %x.11, f16[] %y.11)
}

%region_30.1436 (Arg_0.1437: f32[], Arg_1.1438: f32[]) -> f32[] {
  %Arg_0.1437 = f32[] parameter(0)
  %Arg_1.1438 = f32[] parameter(1)
  ROOT %add.1439 = f32[] add(f32[] %Arg_0.1437, f32[] %Arg_1.1438), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_31.1448 (Arg_0.1449: f32[], Arg_1.1450: f32[]) -> f32[] {
  %Arg_0.1449 = f32[] parameter(0)
  %Arg_1.1450 = f32[] parameter(1)
  ROOT %add.1451 = f32[] add(f32[] %Arg_0.1449, f32[] %Arg_1.1450), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%region_32.1499 (Arg_0.1500: f16[], Arg_1.1501: f16[]) -> f16[] {
  %Arg_0.1500 = f16[] parameter(0)
  %Arg_1.1501 = f16[] parameter(1)
  ROOT %maximum.1502 = f16[] maximum(f16[] %Arg_0.1500, f16[] %Arg_1.1501), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%region_33.1511 (Arg_0.1512: f32[], Arg_1.1513: f32[]) -> f32[] {
  %Arg_0.1512 = f32[] parameter(0)
  %Arg_1.1513 = f32[] parameter(1)
  ROOT %add.1514 = f32[] add(f32[] %Arg_0.1512, f32[] %Arg_1.1513), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%add.12 (x.12: f16[], y.12: f16[]) -> f16[] {
  %x.12 = f16[] parameter(0)
  %y.12 = f16[] parameter(1)
  ROOT %add.38 = f16[] add(f16[] %x.12, f16[] %y.12)
}

%region_34.1536 (Arg_0.1537: f32[], Arg_1.1538: f32[]) -> f32[] {
  %Arg_0.1537 = f32[] parameter(0)
  %Arg_1.1538 = f32[] parameter(1)
  ROOT %add.1539 = f32[] add(f32[] %Arg_0.1537, f32[] %Arg_1.1538), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_35.1548 (Arg_0.1549: f32[], Arg_1.1550: f32[]) -> f32[] {
  %Arg_0.1549 = f32[] parameter(0)
  %Arg_1.1550 = f32[] parameter(1)
  ROOT %add.1551 = f32[] add(f32[] %Arg_0.1549, f32[] %Arg_1.1550), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%add.13 (x.13: f16[], y.13: f16[]) -> f16[] {
  %x.13 = f16[] parameter(0)
  %y.13 = f16[] parameter(1)
  ROOT %add.39 = f16[] add(f16[] %x.13, f16[] %y.13)
}

%region_36.1628 (Arg_0.1629: f32[], Arg_1.1630: f32[]) -> f32[] {
  %Arg_0.1629 = f32[] parameter(0)
  %Arg_1.1630 = f32[] parameter(1)
  ROOT %add.1631 = f32[] add(f32[] %Arg_0.1629, f32[] %Arg_1.1630), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_37.1640 (Arg_0.1641: f32[], Arg_1.1642: f32[]) -> f32[] {
  %Arg_0.1641 = f32[] parameter(0)
  %Arg_1.1642 = f32[] parameter(1)
  ROOT %add.1643 = f32[] add(f32[] %Arg_0.1641, f32[] %Arg_1.1642), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%region_38.1688 (Arg_0.1689: f16[], Arg_1.1690: f16[]) -> f16[] {
  %Arg_0.1689 = f16[] parameter(0)
  %Arg_1.1690 = f16[] parameter(1)
  ROOT %maximum.1691 = f16[] maximum(f16[] %Arg_0.1689, f16[] %Arg_1.1690), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/reduce_max[axes=(2,)]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
}

%region_39.1700 (Arg_0.1701: f32[], Arg_1.1702: f32[]) -> f32[] {
  %Arg_0.1701 = f32[] parameter(0)
  %Arg_1.1702 = f32[] parameter(1)
  ROOT %add.1703 = f32[] add(f32[] %Arg_0.1701, f32[] %Arg_1.1702), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(2,)]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
}

%region_43.1737 (Arg_0.1738: f16[], Arg_1.1739: f16[]) -> f16[] {
  %Arg_0.1738 = f16[] parameter(0)
  %Arg_1.1739 = f16[] parameter(1)
  ROOT %add.1740 = f16[] add(f16[] %Arg_0.1738, f16[] %Arg_1.1739), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/reduce_sum[axes=(0, 1)]" source_file="/code/alpa/alpa/model/gpt_model.py" source_line=75}
}

%add.14 (x.14: f16[], y.14: f16[]) -> f16[] {
  %x.14 = f16[] parameter(0)
  %y.14 = f16[] parameter(1)
  ROOT %add.40 = f16[] add(f16[] %x.14, f16[] %y.14)
}

%region_45.1817 (Arg_0.1818: f16[], Arg_1.1819: f16[]) -> f16[] {
  %Arg_0.1818 = f16[] parameter(0)
  %Arg_1.1819 = f16[] parameter(1)
  ROOT %maximum.1820 = f16[] maximum(f16[] %Arg_0.1818, f16[] %Arg_1.1819), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%region_46.1829 (Arg_0.1830: f32[], Arg_1.1831: f32[]) -> f32[] {
  %Arg_0.1830 = f32[] parameter(0)
  %Arg_1.1831 = f32[] parameter(1)
  ROOT %add.1832 = f32[] add(f32[] %Arg_0.1830, f32[] %Arg_1.1831), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%add.15 (x.15: f16[], y.15: f16[]) -> f16[] {
  %x.15 = f16[] parameter(0)
  %y.15 = f16[] parameter(1)
  ROOT %add.41 = f16[] add(f16[] %x.15, f16[] %y.15)
}

%region_47.1854 (Arg_0.1855: f32[], Arg_1.1856: f32[]) -> f32[] {
  %Arg_0.1855 = f32[] parameter(0)
  %Arg_1.1856 = f32[] parameter(1)
  ROOT %add.1857 = f32[] add(f32[] %Arg_0.1855, f32[] %Arg_1.1856), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_48.1867 (Arg_0.1868: f32[], Arg_1.1869: f32[]) -> f32[] {
  %Arg_0.1868 = f32[] parameter(0)
  %Arg_1.1869 = f32[] parameter(1)
  ROOT %add.1870 = f32[] add(f32[] %Arg_0.1868, f32[] %Arg_1.1869), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%add.16 (x.16: f16[], y.16: f16[]) -> f16[] {
  %x.16 = f16[] parameter(0)
  %y.16 = f16[] parameter(1)
  ROOT %add.42 = f16[] add(f16[] %x.16, f16[] %y.16)
}

%region_49.1953 (Arg_0.1954: f32[], Arg_1.1955: f32[]) -> f32[] {
  %Arg_0.1954 = f32[] parameter(0)
  %Arg_1.1955 = f32[] parameter(1)
  ROOT %add.1956 = f32[] add(f32[] %Arg_0.1954, f32[] %Arg_1.1955), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_53.2034 (Arg_0.2035: f32[], Arg_1.2036: f32[]) -> f32[] {
  %Arg_0.2035 = f32[] parameter(0)
  %Arg_1.2036 = f32[] parameter(1)
  ROOT %add.2037 = f32[] add(f32[] %Arg_0.2035, f32[] %Arg_1.2036), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_50.1966 (Arg_0.1967: f32[], Arg_1.1968: f32[]) -> f32[] {
  %Arg_0.1967 = f32[] parameter(0)
  %Arg_1.1968 = f32[] parameter(1)
  ROOT %add.1969 = f32[] add(f32[] %Arg_0.1967, f32[] %Arg_1.1968), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%region_54.2051 (Arg_0.2052: f32[], Arg_1.2053: f32[]) -> f32[] {
  %Arg_0.2052 = f32[] parameter(0)
  %Arg_1.2053 = f32[] parameter(1)
  ROOT %add.2054 = f32[] add(f32[] %Arg_0.2052, f32[] %Arg_1.2053), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%add.17 (x.17: f16[], y.17: f16[]) -> f16[] {
  %x.17 = f16[] parameter(0)
  %y.17 = f16[] parameter(1)
  ROOT %add.43 = f16[] add(f16[] %x.17, f16[] %y.17)
}

%region_59.2114 (Arg_0.2115: f32[], Arg_1.2116: f32[]) -> f32[] {
  %Arg_0.2115 = f32[] parameter(0)
  %Arg_1.2116 = f32[] parameter(1)
  ROOT %add.2117 = f32[] add(f32[] %Arg_0.2115, f32[] %Arg_1.2116), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_60.2131 (Arg_0.2132: f32[], Arg_1.2133: f32[]) -> f32[] {
  %Arg_0.2132 = f32[] parameter(0)
  %Arg_1.2133 = f32[] parameter(1)
  ROOT %add.2134 = f32[] add(f32[] %Arg_0.2132, f32[] %Arg_1.2133), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%region_62.2162 (Arg_0.2163: f16[], Arg_1.2164: f16[]) -> f16[] {
  %Arg_0.2163 = f16[] parameter(0)
  %Arg_1.2164 = f16[] parameter(1)
  ROOT %add.2165 = f16[] add(f16[] %Arg_0.2163, f16[] %Arg_1.2164), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%add.18 (x.18: f16[], y.18: f16[]) -> f16[] {
  %x.18 = f16[] parameter(0)
  %y.18 = f16[] parameter(1)
  ROOT %add.44 = f16[] add(f16[] %x.18, f16[] %y.18)
}

%region_65.2276 (Arg_0.2277: f16[], Arg_1.2278: f16[]) -> f16[] {
  %Arg_0.2277 = f16[] parameter(0)
  %Arg_1.2278 = f16[] parameter(1)
  ROOT %maximum.2279 = f16[] maximum(f16[] %Arg_0.2277, f16[] %Arg_1.2278), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%region_66.2288 (Arg_0.2289: f32[], Arg_1.2290: f32[]) -> f32[] {
  %Arg_0.2289 = f32[] parameter(0)
  %Arg_1.2290 = f32[] parameter(1)
  ROOT %add.2291 = f32[] add(f32[] %Arg_0.2289, f32[] %Arg_1.2290), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%add.19 (x.19: f16[], y.19: f16[]) -> f16[] {
  %x.19 = f16[] parameter(0)
  %y.19 = f16[] parameter(1)
  ROOT %add.45 = f16[] add(f16[] %x.19, f16[] %y.19)
}

%region_67.2313 (Arg_0.2314: f32[], Arg_1.2315: f32[]) -> f32[] {
  %Arg_0.2314 = f32[] parameter(0)
  %Arg_1.2315 = f32[] parameter(1)
  ROOT %add.2316 = f32[] add(f32[] %Arg_0.2314, f32[] %Arg_1.2315), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_68.2326 (Arg_0.2327: f32[], Arg_1.2328: f32[]) -> f32[] {
  %Arg_0.2327 = f32[] parameter(0)
  %Arg_1.2328 = f32[] parameter(1)
  ROOT %add.2329 = f32[] add(f32[] %Arg_0.2327, f32[] %Arg_1.2328), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%add.20 (x.20: f16[], y.20: f16[]) -> f16[] {
  %x.20 = f16[] parameter(0)
  %y.20 = f16[] parameter(1)
  ROOT %add.46 = f16[] add(f16[] %x.20, f16[] %y.20)
}

%region_69.2412 (Arg_0.2413: f32[], Arg_1.2414: f32[]) -> f32[] {
  %Arg_0.2413 = f32[] parameter(0)
  %Arg_1.2414 = f32[] parameter(1)
  ROOT %add.2415 = f32[] add(f32[] %Arg_0.2413, f32[] %Arg_1.2414), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_73.2493 (Arg_0.2494: f32[], Arg_1.2495: f32[]) -> f32[] {
  %Arg_0.2494 = f32[] parameter(0)
  %Arg_1.2495 = f32[] parameter(1)
  ROOT %add.2496 = f32[] add(f32[] %Arg_0.2494, f32[] %Arg_1.2495), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_70.2425 (Arg_0.2426: f32[], Arg_1.2427: f32[]) -> f32[] {
  %Arg_0.2426 = f32[] parameter(0)
  %Arg_1.2427 = f32[] parameter(1)
  ROOT %add.2428 = f32[] add(f32[] %Arg_0.2426, f32[] %Arg_1.2427), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%region_74.2510 (Arg_0.2511: f32[], Arg_1.2512: f32[]) -> f32[] {
  %Arg_0.2511 = f32[] parameter(0)
  %Arg_1.2512 = f32[] parameter(1)
  ROOT %add.2513 = f32[] add(f32[] %Arg_0.2511, f32[] %Arg_1.2512), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%add.21 (x.21: f16[], y.21: f16[]) -> f16[] {
  %x.21 = f16[] parameter(0)
  %y.21 = f16[] parameter(1)
  ROOT %add.47 = f16[] add(f16[] %x.21, f16[] %y.21)
}

%region_79.2573 (Arg_0.2574: f32[], Arg_1.2575: f32[]) -> f32[] {
  %Arg_0.2574 = f32[] parameter(0)
  %Arg_1.2575 = f32[] parameter(1)
  ROOT %add.2576 = f32[] add(f32[] %Arg_0.2574, f32[] %Arg_1.2575), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_80.2590 (Arg_0.2591: f32[], Arg_1.2592: f32[]) -> f32[] {
  %Arg_0.2591 = f32[] parameter(0)
  %Arg_1.2592 = f32[] parameter(1)
  ROOT %add.2593 = f32[] add(f32[] %Arg_0.2591, f32[] %Arg_1.2592), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%region_82.2621 (Arg_0.2622: f16[], Arg_1.2623: f16[]) -> f16[] {
  %Arg_0.2622 = f16[] parameter(0)
  %Arg_1.2623 = f16[] parameter(1)
  ROOT %add.2624 = f16[] add(f16[] %Arg_0.2622, f16[] %Arg_1.2623), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%add.22 (x.22: f16[], y.22: f16[]) -> f16[] {
  %x.22 = f16[] parameter(0)
  %y.22 = f16[] parameter(1)
  ROOT %add.48 = f16[] add(f16[] %x.22, f16[] %y.22)
}

%region_85.2735 (Arg_0.2736: f16[], Arg_1.2737: f16[]) -> f16[] {
  %Arg_0.2736 = f16[] parameter(0)
  %Arg_1.2737 = f16[] parameter(1)
  ROOT %maximum.2738 = f16[] maximum(f16[] %Arg_0.2736, f16[] %Arg_1.2737), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%region_86.2747 (Arg_0.2748: f32[], Arg_1.2749: f32[]) -> f32[] {
  %Arg_0.2748 = f32[] parameter(0)
  %Arg_1.2749 = f32[] parameter(1)
  ROOT %add.2750 = f32[] add(f32[] %Arg_0.2748, f32[] %Arg_1.2749), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%add.23 (x.23: f16[], y.23: f16[]) -> f16[] {
  %x.23 = f16[] parameter(0)
  %y.23 = f16[] parameter(1)
  ROOT %add.49 = f16[] add(f16[] %x.23, f16[] %y.23)
}

%region_87.2772 (Arg_0.2773: f32[], Arg_1.2774: f32[]) -> f32[] {
  %Arg_0.2773 = f32[] parameter(0)
  %Arg_1.2774 = f32[] parameter(1)
  ROOT %add.2775 = f32[] add(f32[] %Arg_0.2773, f32[] %Arg_1.2774), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_88.2785 (Arg_0.2786: f32[], Arg_1.2787: f32[]) -> f32[] {
  %Arg_0.2786 = f32[] parameter(0)
  %Arg_1.2787 = f32[] parameter(1)
  ROOT %add.2788 = f32[] add(f32[] %Arg_0.2786, f32[] %Arg_1.2787), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%add.24 (x.24: f16[], y.24: f16[]) -> f16[] {
  %x.24 = f16[] parameter(0)
  %y.24 = f16[] parameter(1)
  ROOT %add.50 = f16[] add(f16[] %x.24, f16[] %y.24)
}

%region_89.2871 (Arg_0.2872: f32[], Arg_1.2873: f32[]) -> f32[] {
  %Arg_0.2872 = f32[] parameter(0)
  %Arg_1.2873 = f32[] parameter(1)
  ROOT %add.2874 = f32[] add(f32[] %Arg_0.2872, f32[] %Arg_1.2873), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_93.2952 (Arg_0.2953: f32[], Arg_1.2954: f32[]) -> f32[] {
  %Arg_0.2953 = f32[] parameter(0)
  %Arg_1.2954 = f32[] parameter(1)
  ROOT %add.2955 = f32[] add(f32[] %Arg_0.2953, f32[] %Arg_1.2954), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_90.2884 (Arg_0.2885: f32[], Arg_1.2886: f32[]) -> f32[] {
  %Arg_0.2885 = f32[] parameter(0)
  %Arg_1.2886 = f32[] parameter(1)
  ROOT %add.2887 = f32[] add(f32[] %Arg_0.2885, f32[] %Arg_1.2886), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%region_94.2969 (Arg_0.2970: f32[], Arg_1.2971: f32[]) -> f32[] {
  %Arg_0.2970 = f32[] parameter(0)
  %Arg_1.2971 = f32[] parameter(1)
  ROOT %add.2972 = f32[] add(f32[] %Arg_0.2970, f32[] %Arg_1.2971), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%add.25 (x.25: f16[], y.25: f16[]) -> f16[] {
  %x.25 = f16[] parameter(0)
  %y.25 = f16[] parameter(1)
  ROOT %add.51 = f16[] add(f16[] %x.25, f16[] %y.25)
}

%region_99.3032 (Arg_0.3033: f32[], Arg_1.3034: f32[]) -> f32[] {
  %Arg_0.3033 = f32[] parameter(0)
  %Arg_1.3034 = f32[] parameter(1)
  ROOT %add.3035 = f32[] add(f32[] %Arg_0.3033, f32[] %Arg_1.3034), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_100.3049 (Arg_0.3050: f32[], Arg_1.3051: f32[]) -> f32[] {
  %Arg_0.3050 = f32[] parameter(0)
  %Arg_1.3051 = f32[] parameter(1)
  ROOT %add.3052 = f32[] add(f32[] %Arg_0.3050, f32[] %Arg_1.3051), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%region_102.3080 (Arg_0.3081: f16[], Arg_1.3082: f16[]) -> f16[] {
  %Arg_0.3081 = f16[] parameter(0)
  %Arg_1.3082 = f16[] parameter(1)
  ROOT %add.3083 = f16[] add(f16[] %Arg_0.3081, f16[] %Arg_1.3082), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%add.26 (x.26: f16[], y.26: f16[]) -> f16[] {
  %x.26 = f16[] parameter(0)
  %y.26 = f16[] parameter(1)
  ROOT %add.52 = f16[] add(f16[] %x.26, f16[] %y.26)
}

%region_105.3194 (Arg_0.3195: f16[], Arg_1.3196: f16[]) -> f16[] {
  %Arg_0.3195 = f16[] parameter(0)
  %Arg_1.3196 = f16[] parameter(1)
  ROOT %maximum.3197 = f16[] maximum(f16[] %Arg_0.3195, f16[] %Arg_1.3196), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%region_106.3206 (Arg_0.3207: f32[], Arg_1.3208: f32[]) -> f32[] {
  %Arg_0.3207 = f32[] parameter(0)
  %Arg_1.3208 = f32[] parameter(1)
  ROOT %add.3209 = f32[] add(f32[] %Arg_0.3207, f32[] %Arg_1.3208), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%add.27 (x.27: f16[], y.27: f16[]) -> f16[] {
  %x.27 = f16[] parameter(0)
  %y.27 = f16[] parameter(1)
  ROOT %add.53 = f16[] add(f16[] %x.27, f16[] %y.27)
}

%region_107.3231 (Arg_0.3232: f32[], Arg_1.3233: f32[]) -> f32[] {
  %Arg_0.3232 = f32[] parameter(0)
  %Arg_1.3233 = f32[] parameter(1)
  ROOT %add.3234 = f32[] add(f32[] %Arg_0.3232, f32[] %Arg_1.3233), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_108.3244 (Arg_0.3245: f32[], Arg_1.3246: f32[]) -> f32[] {
  %Arg_0.3245 = f32[] parameter(0)
  %Arg_1.3246 = f32[] parameter(1)
  ROOT %add.3247 = f32[] add(f32[] %Arg_0.3245, f32[] %Arg_1.3246), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%add.28 (x.28: f16[], y.28: f16[]) -> f16[] {
  %x.28 = f16[] parameter(0)
  %y.28 = f16[] parameter(1)
  ROOT %add.54 = f16[] add(f16[] %x.28, f16[] %y.28)
}

%region_109.3330 (Arg_0.3331: f32[], Arg_1.3332: f32[]) -> f32[] {
  %Arg_0.3331 = f32[] parameter(0)
  %Arg_1.3332 = f32[] parameter(1)
  ROOT %add.3333 = f32[] add(f32[] %Arg_0.3331, f32[] %Arg_1.3332), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_113.3411 (Arg_0.3412: f32[], Arg_1.3413: f32[]) -> f32[] {
  %Arg_0.3412 = f32[] parameter(0)
  %Arg_1.3413 = f32[] parameter(1)
  ROOT %add.3414 = f32[] add(f32[] %Arg_0.3412, f32[] %Arg_1.3413), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_110.3343 (Arg_0.3344: f32[], Arg_1.3345: f32[]) -> f32[] {
  %Arg_0.3344 = f32[] parameter(0)
  %Arg_1.3345 = f32[] parameter(1)
  ROOT %add.3346 = f32[] add(f32[] %Arg_0.3344, f32[] %Arg_1.3345), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%region_114.3428 (Arg_0.3429: f32[], Arg_1.3430: f32[]) -> f32[] {
  %Arg_0.3429 = f32[] parameter(0)
  %Arg_1.3430 = f32[] parameter(1)
  ROOT %add.3431 = f32[] add(f32[] %Arg_0.3429, f32[] %Arg_1.3430), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%add.29 (x.29: f16[], y.29: f16[]) -> f16[] {
  %x.29 = f16[] parameter(0)
  %y.29 = f16[] parameter(1)
  ROOT %add.55 = f16[] add(f16[] %x.29, f16[] %y.29)
}

%region_119.3491 (Arg_0.3492: f32[], Arg_1.3493: f32[]) -> f32[] {
  %Arg_0.3492 = f32[] parameter(0)
  %Arg_1.3493 = f32[] parameter(1)
  ROOT %add.3494 = f32[] add(f32[] %Arg_0.3492, f32[] %Arg_1.3493), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_120.3508 (Arg_0.3509: f32[], Arg_1.3510: f32[]) -> f32[] {
  %Arg_0.3509 = f32[] parameter(0)
  %Arg_1.3510 = f32[] parameter(1)
  ROOT %add.3511 = f32[] add(f32[] %Arg_0.3509, f32[] %Arg_1.3510), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%region_122.3539 (Arg_0.3540: f16[], Arg_1.3541: f16[]) -> f16[] {
  %Arg_0.3540 = f16[] parameter(0)
  %Arg_1.3541 = f16[] parameter(1)
  ROOT %add.3542 = f16[] add(f16[] %Arg_0.3540, f16[] %Arg_1.3541), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%add.30 (x.30: f16[], y.30: f16[]) -> f16[] {
  %x.30 = f16[] parameter(0)
  %y.30 = f16[] parameter(1)
  ROOT %add.56 = f16[] add(f16[] %x.30, f16[] %y.30)
}

%region_125.3653 (Arg_0.3654: f16[], Arg_1.3655: f16[]) -> f16[] {
  %Arg_0.3654 = f16[] parameter(0)
  %Arg_1.3655 = f16[] parameter(1)
  ROOT %maximum.3656 = f16[] maximum(f16[] %Arg_0.3654, f16[] %Arg_1.3655), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%region_126.3665 (Arg_0.3666: f32[], Arg_1.3667: f32[]) -> f32[] {
  %Arg_0.3666 = f32[] parameter(0)
  %Arg_1.3667 = f32[] parameter(1)
  ROOT %add.3668 = f32[] add(f32[] %Arg_0.3666, f32[] %Arg_1.3667), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%add.31 (x.31: f16[], y.31: f16[]) -> f16[] {
  %x.31 = f16[] parameter(0)
  %y.31 = f16[] parameter(1)
  ROOT %add.57 = f16[] add(f16[] %x.31, f16[] %y.31)
}

%region_127.3690 (Arg_0.3691: f32[], Arg_1.3692: f32[]) -> f32[] {
  %Arg_0.3691 = f32[] parameter(0)
  %Arg_1.3692 = f32[] parameter(1)
  ROOT %add.3693 = f32[] add(f32[] %Arg_0.3691, f32[] %Arg_1.3692), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_128.3703 (Arg_0.3704: f32[], Arg_1.3705: f32[]) -> f32[] {
  %Arg_0.3704 = f32[] parameter(0)
  %Arg_1.3705 = f32[] parameter(1)
  ROOT %add.3706 = f32[] add(f32[] %Arg_0.3704, f32[] %Arg_1.3705), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%add.32 (x.32: f16[], y.32: f16[]) -> f16[] {
  %x.32 = f16[] parameter(0)
  %y.32 = f16[] parameter(1)
  ROOT %add.58 = f16[] add(f16[] %x.32, f16[] %y.32)
}

%region_129.3789 (Arg_0.3790: f32[], Arg_1.3791: f32[]) -> f32[] {
  %Arg_0.3790 = f32[] parameter(0)
  %Arg_1.3791 = f32[] parameter(1)
  ROOT %add.3792 = f32[] add(f32[] %Arg_0.3790, f32[] %Arg_1.3791), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_133.3870 (Arg_0.3871: f32[], Arg_1.3872: f32[]) -> f32[] {
  %Arg_0.3871 = f32[] parameter(0)
  %Arg_1.3872 = f32[] parameter(1)
  ROOT %add.3873 = f32[] add(f32[] %Arg_0.3871, f32[] %Arg_1.3872), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_130.3802 (Arg_0.3803: f32[], Arg_1.3804: f32[]) -> f32[] {
  %Arg_0.3803 = f32[] parameter(0)
  %Arg_1.3804 = f32[] parameter(1)
  ROOT %add.3805 = f32[] add(f32[] %Arg_0.3803, f32[] %Arg_1.3804), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%region_134.3887 (Arg_0.3888: f32[], Arg_1.3889: f32[]) -> f32[] {
  %Arg_0.3888 = f32[] parameter(0)
  %Arg_1.3889 = f32[] parameter(1)
  ROOT %add.3890 = f32[] add(f32[] %Arg_0.3888, f32[] %Arg_1.3889), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%add.33 (x.33: f16[], y.33: f16[]) -> f16[] {
  %x.33 = f16[] parameter(0)
  %y.33 = f16[] parameter(1)
  ROOT %add.59 = f16[] add(f16[] %x.33, f16[] %y.33)
}

%region_139.3950 (Arg_0.3951: f32[], Arg_1.3952: f32[]) -> f32[] {
  %Arg_0.3951 = f32[] parameter(0)
  %Arg_1.3952 = f32[] parameter(1)
  ROOT %add.3953 = f32[] add(f32[] %Arg_0.3951, f32[] %Arg_1.3952), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_140.3967 (Arg_0.3968: f32[], Arg_1.3969: f32[]) -> f32[] {
  %Arg_0.3968 = f32[] parameter(0)
  %Arg_1.3969 = f32[] parameter(1)
  ROOT %add.3970 = f32[] add(f32[] %Arg_0.3968, f32[] %Arg_1.3969), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%region_142.3998 (Arg_0.3999: f16[], Arg_1.4000: f16[]) -> f16[] {
  %Arg_0.3999 = f16[] parameter(0)
  %Arg_1.4000 = f16[] parameter(1)
  ROOT %add.4001 = f16[] add(f16[] %Arg_0.3999, f16[] %Arg_1.4000), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%add.34 (x.34: f16[], y.34: f16[]) -> f16[] {
  %x.34 = f16[] parameter(0)
  %y.34 = f16[] parameter(1)
  ROOT %add.60 = f16[] add(f16[] %x.34, f16[] %y.34)
}

%region_145.4112 (Arg_0.4113: f16[], Arg_1.4114: f16[]) -> f16[] {
  %Arg_0.4113 = f16[] parameter(0)
  %Arg_1.4114 = f16[] parameter(1)
  ROOT %maximum.4115 = f16[] maximum(f16[] %Arg_0.4113, f16[] %Arg_1.4114), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%region_146.4124 (Arg_0.4125: f32[], Arg_1.4126: f32[]) -> f32[] {
  %Arg_0.4125 = f32[] parameter(0)
  %Arg_1.4126 = f32[] parameter(1)
  ROOT %add.4127 = f32[] add(f32[] %Arg_0.4125, f32[] %Arg_1.4126), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%add.35 (x.35: f16[], y.35: f16[]) -> f16[] {
  %x.35 = f16[] parameter(0)
  %y.35 = f16[] parameter(1)
  ROOT %add.61 = f16[] add(f16[] %x.35, f16[] %y.35)
}

%region_147.4149 (Arg_0.4150: f32[], Arg_1.4151: f32[]) -> f32[] {
  %Arg_0.4150 = f32[] parameter(0)
  %Arg_1.4151 = f32[] parameter(1)
  ROOT %add.4152 = f32[] add(f32[] %Arg_0.4150, f32[] %Arg_1.4151), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_148.4162 (Arg_0.4163: f32[], Arg_1.4164: f32[]) -> f32[] {
  %Arg_0.4163 = f32[] parameter(0)
  %Arg_1.4164 = f32[] parameter(1)
  ROOT %add.4165 = f32[] add(f32[] %Arg_0.4163, f32[] %Arg_1.4164), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%add.36 (x.36: f16[], y.36: f16[]) -> f16[] {
  %x.36 = f16[] parameter(0)
  %y.36 = f16[] parameter(1)
  ROOT %add.62 = f16[] add(f16[] %x.36, f16[] %y.36)
}

%region_149.4248 (Arg_0.4249: f32[], Arg_1.4250: f32[]) -> f32[] {
  %Arg_0.4249 = f32[] parameter(0)
  %Arg_1.4250 = f32[] parameter(1)
  ROOT %add.4251 = f32[] add(f32[] %Arg_0.4249, f32[] %Arg_1.4250), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
}

%region_153.4329 (Arg_0.4330: f32[], Arg_1.4331: f32[]) -> f32[] {
  %Arg_0.4330 = f32[] parameter(0)
  %Arg_1.4331 = f32[] parameter(1)
  ROOT %add.4332 = f32[] add(f32[] %Arg_0.4330, f32[] %Arg_1.4331), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_150.4261 (Arg_0.4262: f32[], Arg_1.4263: f32[]) -> f32[] {
  %Arg_0.4262 = f32[] parameter(0)
  %Arg_1.4263 = f32[] parameter(1)
  ROOT %add.4264 = f32[] add(f32[] %Arg_0.4262, f32[] %Arg_1.4263), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
}

%region_154.4346 (Arg_0.4347: f32[], Arg_1.4348: f32[]) -> f32[] {
  %Arg_0.4347 = f32[] parameter(0)
  %Arg_1.4348 = f32[] parameter(1)
  ROOT %add.4349 = f32[] add(f32[] %Arg_0.4347, f32[] %Arg_1.4348), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%add.37 (x.37: f16[], y.37: f16[]) -> f16[] {
  %x.37 = f16[] parameter(0)
  %y.37 = f16[] parameter(1)
  ROOT %add.63 = f16[] add(f16[] %x.37, f16[] %y.37)
}

%region_159.4409 (Arg_0.4410: f32[], Arg_1.4411: f32[]) -> f32[] {
  %Arg_0.4410 = f32[] parameter(0)
  %Arg_1.4411 = f32[] parameter(1)
  ROOT %add.4412 = f32[] add(f32[] %Arg_0.4410, f32[] %Arg_1.4411), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_160.4426 (Arg_0.4427: f32[], Arg_1.4428: f32[]) -> f32[] {
  %Arg_0.4427 = f32[] parameter(0)
  %Arg_1.4428 = f32[] parameter(1)
  ROOT %add.4429 = f32[] add(f32[] %Arg_0.4427, f32[] %Arg_1.4428), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%region_162.4457 (Arg_0.4458: f16[], Arg_1.4459: f16[]) -> f16[] {
  %Arg_0.4458 = f16[] parameter(0)
  %Arg_1.4459 = f16[] parameter(1)
  ROOT %add.4460 = f16[] add(f16[] %Arg_0.4458, f16[] %Arg_1.4459), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
}

%add.38 (x.38: f16[], y.38: f16[]) -> f16[] {
  %x.38 = f16[] parameter(0)
  %y.38 = f16[] parameter(1)
  ROOT %add.64 = f16[] add(f16[] %x.38, f16[] %y.38)
}

%region_165.4507 (Arg_0.4508: f32[], Arg_1.4509: f32[]) -> f32[] {
  %Arg_0.4508 = f32[] parameter(0)
  %Arg_1.4509 = f32[] parameter(1)
  ROOT %add.4510 = f32[] add(f32[] %Arg_0.4508, f32[] %Arg_1.4509), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
}

%region_166.4517 (Arg_0.4518: f32[], Arg_1.4519: f32[]) -> f32[] {
  %Arg_0.4518 = f32[] parameter(0)
  %Arg_1.4519 = f32[] parameter(1)
  ROOT %add.4520 = f32[] add(f32[] %Arg_0.4518, f32[] %Arg_1.4519), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_167.4526 (Arg_0.4527: f32[], Arg_1.4528: f32[]) -> f32[] {
  %Arg_0.4527 = f32[] parameter(0)
  %Arg_1.4528 = f32[] parameter(1)
  ROOT %add.4529 = f32[] add(f32[] %Arg_0.4527, f32[] %Arg_1.4528), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_168.4542 (Arg_0.4543: f32[], Arg_1.4544: f32[]) -> f32[] {
  %Arg_0.4543 = f32[] parameter(0)
  %Arg_1.4544 = f32[] parameter(1)
  ROOT %add.4545 = f32[] add(f32[] %Arg_0.4543, f32[] %Arg_1.4544), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
}

%add.39 (x.39: f16[], y.39: f16[]) -> f16[] {
  %x.39 = f16[] parameter(0)
  %y.39 = f16[] parameter(1)
  ROOT %add.65 = f16[] add(f16[] %x.39, f16[] %y.39)
}

%add.40 (x.40: f16[], y.40: f16[]) -> f16[] {
  %x.40 = f16[] parameter(0)
  %y.40 = f16[] parameter(1)
  ROOT %add.66 = f16[] add(f16[] %x.40, f16[] %y.40)
}

%add.41 (x.41: f16[], y.41: f16[]) -> f16[] {
  %x.41 = f16[] parameter(0)
  %y.41 = f16[] parameter(1)
  ROOT %add.67 = f16[] add(f16[] %x.41, f16[] %y.41)
}

%region_157.4390 (Arg_0.4391: f32[], Arg_1.4392: f32[]) -> f32[] {
  %Arg_0.4391 = f32[] parameter(0)
  %Arg_1.4392 = f32[] parameter(1)
  ROOT %add.4393 = f32[] add(f32[] %Arg_0.4391, f32[] %Arg_1.4392), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
}

%region_158.4400 (Arg_0.4401: f32[], Arg_1.4402: f32[]) -> f32[] {
  %Arg_0.4401 = f32[] parameter(0)
  %Arg_1.4402 = f32[] parameter(1)
  ROOT %add.4403 = f32[] add(f32[] %Arg_0.4401, f32[] %Arg_1.4402), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_161.4439 (Arg_0.4440: f16[], Arg_1.4441: f16[]) -> f16[] {
  %Arg_0.4440 = f16[] parameter(0)
  %Arg_1.4441 = f16[] parameter(1)
  ROOT %add.4442 = f16[] add(f16[] %Arg_0.4440, f16[] %Arg_1.4441), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%add.42 (x.42: f16[], y.42: f16[]) -> f16[] {
  %x.42 = f16[] parameter(0)
  %y.42 = f16[] parameter(1)
  ROOT %add.68 = f16[] add(f16[] %x.42, f16[] %y.42)
}

%region_164.4495 (Arg_0.4496: f16[], Arg_1.4497: f16[]) -> f16[] {
  %Arg_0.4496 = f16[] parameter(0)
  %Arg_1.4497 = f16[] parameter(1)
  ROOT %add.4498 = f16[] add(f16[] %Arg_0.4496, f16[] %Arg_1.4497), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%add.43 (x.43: f16[], y.43: f16[]) -> f16[] {
  %x.43 = f16[] parameter(0)
  %y.43 = f16[] parameter(1)
  ROOT %add.69 = f16[] add(f16[] %x.43, f16[] %y.43)
}

%region_156.4378 (Arg_0.4379: f16[], Arg_1.4380: f16[]) -> f16[] {
  %Arg_0.4379 = f16[] parameter(0)
  %Arg_1.4380 = f16[] parameter(1)
  ROOT %add.4381 = f16[] add(f16[] %Arg_0.4379, f16[] %Arg_1.4380), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%add.44 (x.44: f16[], y.44: f16[]) -> f16[] {
  %x.44 = f16[] parameter(0)
  %y.44 = f16[] parameter(1)
  ROOT %add.70 = f16[] add(f16[] %x.44, f16[] %y.44)
}

%region_151.4310 (Arg_0.4311: f32[], Arg_1.4312: f32[]) -> f32[] {
  %Arg_0.4311 = f32[] parameter(0)
  %Arg_1.4312 = f32[] parameter(1)
  ROOT %add.4313 = f32[] add(f32[] %Arg_0.4311, f32[] %Arg_1.4312), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
}

%region_152.4320 (Arg_0.4321: f32[], Arg_1.4322: f32[]) -> f32[] {
  %Arg_0.4321 = f32[] parameter(0)
  %Arg_1.4322 = f32[] parameter(1)
  ROOT %add.4323 = f32[] add(f32[] %Arg_0.4321, f32[] %Arg_1.4322), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_155.4359 (Arg_0.4360: f16[], Arg_1.4361: f16[]) -> f16[] {
  %Arg_0.4360 = f16[] parameter(0)
  %Arg_1.4361 = f16[] parameter(1)
  ROOT %add.4362 = f16[] add(f16[] %Arg_0.4360, f16[] %Arg_1.4361), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%add.45 (x.45: f16[], y.45: f16[]) -> f16[] {
  %x.45 = f16[] parameter(0)
  %y.45 = f16[] parameter(1)
  ROOT %add.71 = f16[] add(f16[] %x.45, f16[] %y.45)
}

%region_137.3931 (Arg_0.3932: f32[], Arg_1.3933: f32[]) -> f32[] {
  %Arg_0.3932 = f32[] parameter(0)
  %Arg_1.3933 = f32[] parameter(1)
  ROOT %add.3934 = f32[] add(f32[] %Arg_0.3932, f32[] %Arg_1.3933), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
}

%region_138.3941 (Arg_0.3942: f32[], Arg_1.3943: f32[]) -> f32[] {
  %Arg_0.3942 = f32[] parameter(0)
  %Arg_1.3943 = f32[] parameter(1)
  ROOT %add.3944 = f32[] add(f32[] %Arg_0.3942, f32[] %Arg_1.3943), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_141.3980 (Arg_0.3981: f16[], Arg_1.3982: f16[]) -> f16[] {
  %Arg_0.3981 = f16[] parameter(0)
  %Arg_1.3982 = f16[] parameter(1)
  ROOT %add.3983 = f16[] add(f16[] %Arg_0.3981, f16[] %Arg_1.3982), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%add.46 (x.46: f16[], y.46: f16[]) -> f16[] {
  %x.46 = f16[] parameter(0)
  %y.46 = f16[] parameter(1)
  ROOT %add.72 = f16[] add(f16[] %x.46, f16[] %y.46)
}

%region_144.4036 (Arg_0.4037: f16[], Arg_1.4038: f16[]) -> f16[] {
  %Arg_0.4037 = f16[] parameter(0)
  %Arg_1.4038 = f16[] parameter(1)
  ROOT %add.4039 = f16[] add(f16[] %Arg_0.4037, f16[] %Arg_1.4038), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%add.47 (x.47: f16[], y.47: f16[]) -> f16[] {
  %x.47 = f16[] parameter(0)
  %y.47 = f16[] parameter(1)
  ROOT %add.73 = f16[] add(f16[] %x.47, f16[] %y.47)
}

%region_136.3919 (Arg_0.3920: f16[], Arg_1.3921: f16[]) -> f16[] {
  %Arg_0.3920 = f16[] parameter(0)
  %Arg_1.3921 = f16[] parameter(1)
  ROOT %add.3922 = f16[] add(f16[] %Arg_0.3920, f16[] %Arg_1.3921), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%add.48 (x.48: f16[], y.48: f16[]) -> f16[] {
  %x.48 = f16[] parameter(0)
  %y.48 = f16[] parameter(1)
  ROOT %add.74 = f16[] add(f16[] %x.48, f16[] %y.48)
}

%region_131.3851 (Arg_0.3852: f32[], Arg_1.3853: f32[]) -> f32[] {
  %Arg_0.3852 = f32[] parameter(0)
  %Arg_1.3853 = f32[] parameter(1)
  ROOT %add.3854 = f32[] add(f32[] %Arg_0.3852, f32[] %Arg_1.3853), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
}

%region_132.3861 (Arg_0.3862: f32[], Arg_1.3863: f32[]) -> f32[] {
  %Arg_0.3862 = f32[] parameter(0)
  %Arg_1.3863 = f32[] parameter(1)
  ROOT %add.3864 = f32[] add(f32[] %Arg_0.3862, f32[] %Arg_1.3863), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_135.3900 (Arg_0.3901: f16[], Arg_1.3902: f16[]) -> f16[] {
  %Arg_0.3901 = f16[] parameter(0)
  %Arg_1.3902 = f16[] parameter(1)
  ROOT %add.3903 = f16[] add(f16[] %Arg_0.3901, f16[] %Arg_1.3902), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%add.49 (x.49: f16[], y.49: f16[]) -> f16[] {
  %x.49 = f16[] parameter(0)
  %y.49 = f16[] parameter(1)
  ROOT %add.75 = f16[] add(f16[] %x.49, f16[] %y.49)
}

%region_117.3472 (Arg_0.3473: f32[], Arg_1.3474: f32[]) -> f32[] {
  %Arg_0.3473 = f32[] parameter(0)
  %Arg_1.3474 = f32[] parameter(1)
  ROOT %add.3475 = f32[] add(f32[] %Arg_0.3473, f32[] %Arg_1.3474), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
}

%region_118.3482 (Arg_0.3483: f32[], Arg_1.3484: f32[]) -> f32[] {
  %Arg_0.3483 = f32[] parameter(0)
  %Arg_1.3484 = f32[] parameter(1)
  ROOT %add.3485 = f32[] add(f32[] %Arg_0.3483, f32[] %Arg_1.3484), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_121.3521 (Arg_0.3522: f16[], Arg_1.3523: f16[]) -> f16[] {
  %Arg_0.3522 = f16[] parameter(0)
  %Arg_1.3523 = f16[] parameter(1)
  ROOT %add.3524 = f16[] add(f16[] %Arg_0.3522, f16[] %Arg_1.3523), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%add.50 (x.50: f16[], y.50: f16[]) -> f16[] {
  %x.50 = f16[] parameter(0)
  %y.50 = f16[] parameter(1)
  ROOT %add.76 = f16[] add(f16[] %x.50, f16[] %y.50)
}

%region_124.3577 (Arg_0.3578: f16[], Arg_1.3579: f16[]) -> f16[] {
  %Arg_0.3578 = f16[] parameter(0)
  %Arg_1.3579 = f16[] parameter(1)
  ROOT %add.3580 = f16[] add(f16[] %Arg_0.3578, f16[] %Arg_1.3579), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%add.51 (x.51: f16[], y.51: f16[]) -> f16[] {
  %x.51 = f16[] parameter(0)
  %y.51 = f16[] parameter(1)
  ROOT %add.77 = f16[] add(f16[] %x.51, f16[] %y.51)
}

%region_116.3460 (Arg_0.3461: f16[], Arg_1.3462: f16[]) -> f16[] {
  %Arg_0.3461 = f16[] parameter(0)
  %Arg_1.3462 = f16[] parameter(1)
  ROOT %add.3463 = f16[] add(f16[] %Arg_0.3461, f16[] %Arg_1.3462), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%add.52 (x.52: f16[], y.52: f16[]) -> f16[] {
  %x.52 = f16[] parameter(0)
  %y.52 = f16[] parameter(1)
  ROOT %add.78 = f16[] add(f16[] %x.52, f16[] %y.52)
}

%region_111.3392 (Arg_0.3393: f32[], Arg_1.3394: f32[]) -> f32[] {
  %Arg_0.3393 = f32[] parameter(0)
  %Arg_1.3394 = f32[] parameter(1)
  ROOT %add.3395 = f32[] add(f32[] %Arg_0.3393, f32[] %Arg_1.3394), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
}

%region_112.3402 (Arg_0.3403: f32[], Arg_1.3404: f32[]) -> f32[] {
  %Arg_0.3403 = f32[] parameter(0)
  %Arg_1.3404 = f32[] parameter(1)
  ROOT %add.3405 = f32[] add(f32[] %Arg_0.3403, f32[] %Arg_1.3404), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_115.3441 (Arg_0.3442: f16[], Arg_1.3443: f16[]) -> f16[] {
  %Arg_0.3442 = f16[] parameter(0)
  %Arg_1.3443 = f16[] parameter(1)
  ROOT %add.3444 = f16[] add(f16[] %Arg_0.3442, f16[] %Arg_1.3443), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%add.53 (x.53: f16[], y.53: f16[]) -> f16[] {
  %x.53 = f16[] parameter(0)
  %y.53 = f16[] parameter(1)
  ROOT %add.79 = f16[] add(f16[] %x.53, f16[] %y.53)
}

%region_97.3013 (Arg_0.3014: f32[], Arg_1.3015: f32[]) -> f32[] {
  %Arg_0.3014 = f32[] parameter(0)
  %Arg_1.3015 = f32[] parameter(1)
  ROOT %add.3016 = f32[] add(f32[] %Arg_0.3014, f32[] %Arg_1.3015), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
}

%region_98.3023 (Arg_0.3024: f32[], Arg_1.3025: f32[]) -> f32[] {
  %Arg_0.3024 = f32[] parameter(0)
  %Arg_1.3025 = f32[] parameter(1)
  ROOT %add.3026 = f32[] add(f32[] %Arg_0.3024, f32[] %Arg_1.3025), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_101.3062 (Arg_0.3063: f16[], Arg_1.3064: f16[]) -> f16[] {
  %Arg_0.3063 = f16[] parameter(0)
  %Arg_1.3064 = f16[] parameter(1)
  ROOT %add.3065 = f16[] add(f16[] %Arg_0.3063, f16[] %Arg_1.3064), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%add.54 (x.54: f16[], y.54: f16[]) -> f16[] {
  %x.54 = f16[] parameter(0)
  %y.54 = f16[] parameter(1)
  ROOT %add.80 = f16[] add(f16[] %x.54, f16[] %y.54)
}

%region_104.3118 (Arg_0.3119: f16[], Arg_1.3120: f16[]) -> f16[] {
  %Arg_0.3119 = f16[] parameter(0)
  %Arg_1.3120 = f16[] parameter(1)
  ROOT %add.3121 = f16[] add(f16[] %Arg_0.3119, f16[] %Arg_1.3120), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%add.55 (x.55: f16[], y.55: f16[]) -> f16[] {
  %x.55 = f16[] parameter(0)
  %y.55 = f16[] parameter(1)
  ROOT %add.81 = f16[] add(f16[] %x.55, f16[] %y.55)
}

%region_96.3001 (Arg_0.3002: f16[], Arg_1.3003: f16[]) -> f16[] {
  %Arg_0.3002 = f16[] parameter(0)
  %Arg_1.3003 = f16[] parameter(1)
  ROOT %add.3004 = f16[] add(f16[] %Arg_0.3002, f16[] %Arg_1.3003), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%add.56 (x.56: f16[], y.56: f16[]) -> f16[] {
  %x.56 = f16[] parameter(0)
  %y.56 = f16[] parameter(1)
  ROOT %add.82 = f16[] add(f16[] %x.56, f16[] %y.56)
}

%region_91.2933 (Arg_0.2934: f32[], Arg_1.2935: f32[]) -> f32[] {
  %Arg_0.2934 = f32[] parameter(0)
  %Arg_1.2935 = f32[] parameter(1)
  ROOT %add.2936 = f32[] add(f32[] %Arg_0.2934, f32[] %Arg_1.2935), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
}

%region_92.2943 (Arg_0.2944: f32[], Arg_1.2945: f32[]) -> f32[] {
  %Arg_0.2944 = f32[] parameter(0)
  %Arg_1.2945 = f32[] parameter(1)
  ROOT %add.2946 = f32[] add(f32[] %Arg_0.2944, f32[] %Arg_1.2945), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_95.2982 (Arg_0.2983: f16[], Arg_1.2984: f16[]) -> f16[] {
  %Arg_0.2983 = f16[] parameter(0)
  %Arg_1.2984 = f16[] parameter(1)
  ROOT %add.2985 = f16[] add(f16[] %Arg_0.2983, f16[] %Arg_1.2984), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%add.57 (x.57: f16[], y.57: f16[]) -> f16[] {
  %x.57 = f16[] parameter(0)
  %y.57 = f16[] parameter(1)
  ROOT %add.83 = f16[] add(f16[] %x.57, f16[] %y.57)
}

%region_77.2554 (Arg_0.2555: f32[], Arg_1.2556: f32[]) -> f32[] {
  %Arg_0.2555 = f32[] parameter(0)
  %Arg_1.2556 = f32[] parameter(1)
  ROOT %add.2557 = f32[] add(f32[] %Arg_0.2555, f32[] %Arg_1.2556), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
}

%region_78.2564 (Arg_0.2565: f32[], Arg_1.2566: f32[]) -> f32[] {
  %Arg_0.2565 = f32[] parameter(0)
  %Arg_1.2566 = f32[] parameter(1)
  ROOT %add.2567 = f32[] add(f32[] %Arg_0.2565, f32[] %Arg_1.2566), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_81.2603 (Arg_0.2604: f16[], Arg_1.2605: f16[]) -> f16[] {
  %Arg_0.2604 = f16[] parameter(0)
  %Arg_1.2605 = f16[] parameter(1)
  ROOT %add.2606 = f16[] add(f16[] %Arg_0.2604, f16[] %Arg_1.2605), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%add.58 (x.58: f16[], y.58: f16[]) -> f16[] {
  %x.58 = f16[] parameter(0)
  %y.58 = f16[] parameter(1)
  ROOT %add.84 = f16[] add(f16[] %x.58, f16[] %y.58)
}

%region_84.2659 (Arg_0.2660: f16[], Arg_1.2661: f16[]) -> f16[] {
  %Arg_0.2660 = f16[] parameter(0)
  %Arg_1.2661 = f16[] parameter(1)
  ROOT %add.2662 = f16[] add(f16[] %Arg_0.2660, f16[] %Arg_1.2661), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%add.59 (x.59: f16[], y.59: f16[]) -> f16[] {
  %x.59 = f16[] parameter(0)
  %y.59 = f16[] parameter(1)
  ROOT %add.85 = f16[] add(f16[] %x.59, f16[] %y.59)
}

%region_76.2542 (Arg_0.2543: f16[], Arg_1.2544: f16[]) -> f16[] {
  %Arg_0.2543 = f16[] parameter(0)
  %Arg_1.2544 = f16[] parameter(1)
  ROOT %add.2545 = f16[] add(f16[] %Arg_0.2543, f16[] %Arg_1.2544), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%add.60 (x.60: f16[], y.60: f16[]) -> f16[] {
  %x.60 = f16[] parameter(0)
  %y.60 = f16[] parameter(1)
  ROOT %add.86 = f16[] add(f16[] %x.60, f16[] %y.60)
}

%region_71.2474 (Arg_0.2475: f32[], Arg_1.2476: f32[]) -> f32[] {
  %Arg_0.2475 = f32[] parameter(0)
  %Arg_1.2476 = f32[] parameter(1)
  ROOT %add.2477 = f32[] add(f32[] %Arg_0.2475, f32[] %Arg_1.2476), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
}

%region_72.2484 (Arg_0.2485: f32[], Arg_1.2486: f32[]) -> f32[] {
  %Arg_0.2485 = f32[] parameter(0)
  %Arg_1.2486 = f32[] parameter(1)
  ROOT %add.2487 = f32[] add(f32[] %Arg_0.2485, f32[] %Arg_1.2486), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_75.2523 (Arg_0.2524: f16[], Arg_1.2525: f16[]) -> f16[] {
  %Arg_0.2524 = f16[] parameter(0)
  %Arg_1.2525 = f16[] parameter(1)
  ROOT %add.2526 = f16[] add(f16[] %Arg_0.2524, f16[] %Arg_1.2525), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%add.61 (x.61: f16[], y.61: f16[]) -> f16[] {
  %x.61 = f16[] parameter(0)
  %y.61 = f16[] parameter(1)
  ROOT %add.87 = f16[] add(f16[] %x.61, f16[] %y.61)
}

%region_57.2095 (Arg_0.2096: f32[], Arg_1.2097: f32[]) -> f32[] {
  %Arg_0.2096 = f32[] parameter(0)
  %Arg_1.2097 = f32[] parameter(1)
  ROOT %add.2098 = f32[] add(f32[] %Arg_0.2096, f32[] %Arg_1.2097), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
}

%region_58.2105 (Arg_0.2106: f32[], Arg_1.2107: f32[]) -> f32[] {
  %Arg_0.2106 = f32[] parameter(0)
  %Arg_1.2107 = f32[] parameter(1)
  ROOT %add.2108 = f32[] add(f32[] %Arg_0.2106, f32[] %Arg_1.2107), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_61.2144 (Arg_0.2145: f16[], Arg_1.2146: f16[]) -> f16[] {
  %Arg_0.2145 = f16[] parameter(0)
  %Arg_1.2146 = f16[] parameter(1)
  ROOT %add.2147 = f16[] add(f16[] %Arg_0.2145, f16[] %Arg_1.2146), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%add.62 (x.62: f16[], y.62: f16[]) -> f16[] {
  %x.62 = f16[] parameter(0)
  %y.62 = f16[] parameter(1)
  ROOT %add.88 = f16[] add(f16[] %x.62, f16[] %y.62)
}

%region_64.2200 (Arg_0.2201: f16[], Arg_1.2202: f16[]) -> f16[] {
  %Arg_0.2201 = f16[] parameter(0)
  %Arg_1.2202 = f16[] parameter(1)
  ROOT %add.2203 = f16[] add(f16[] %Arg_0.2201, f16[] %Arg_1.2202), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%add.63 (x.63: f16[], y.63: f16[]) -> f16[] {
  %x.63 = f16[] parameter(0)
  %y.63 = f16[] parameter(1)
  ROOT %add.89 = f16[] add(f16[] %x.63, f16[] %y.63)
}

%region_56.2083 (Arg_0.2084: f16[], Arg_1.2085: f16[]) -> f16[] {
  %Arg_0.2084 = f16[] parameter(0)
  %Arg_1.2085 = f16[] parameter(1)
  ROOT %add.2086 = f16[] add(f16[] %Arg_0.2084, f16[] %Arg_1.2085), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%add.64 (x.64: f16[], y.64: f16[]) -> f16[] {
  %x.64 = f16[] parameter(0)
  %y.64 = f16[] parameter(1)
  ROOT %add.90 = f16[] add(f16[] %x.64, f16[] %y.64)
}

%region_51.2015 (Arg_0.2016: f32[], Arg_1.2017: f32[]) -> f32[] {
  %Arg_0.2016 = f32[] parameter(0)
  %Arg_1.2017 = f32[] parameter(1)
  ROOT %add.2018 = f32[] add(f32[] %Arg_0.2016, f32[] %Arg_1.2017), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
}

%region_52.2025 (Arg_0.2026: f32[], Arg_1.2027: f32[]) -> f32[] {
  %Arg_0.2026 = f32[] parameter(0)
  %Arg_1.2027 = f32[] parameter(1)
  ROOT %add.2028 = f32[] add(f32[] %Arg_0.2026, f32[] %Arg_1.2027), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
}

%region_55.2064 (Arg_0.2065: f16[], Arg_1.2066: f16[]) -> f16[] {
  %Arg_0.2065 = f16[] parameter(0)
  %Arg_1.2066 = f16[] parameter(1)
  ROOT %add.2067 = f16[] add(f16[] %Arg_0.2065, f16[] %Arg_1.2066), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
}

%add.65 (x.65: f16[], y.65: f16[]) -> f16[] {
  %x.65 = f16[] parameter(0)
  %y.65 = f16[] parameter(1)
  ROOT %add.91 = f16[] add(f16[] %x.65, f16[] %y.65)
}

%add (x: f16[], y: f16[]) -> f16[] {
  %x = f16[] parameter(0)
  %y = f16[] parameter(1)
  ROOT %add.26 = f16[] add(f16[] %x, f16[] %y)
}

ENTRY %main.5888_spmd (param.2: s32[], param.3: f32[6400], param.10: f32[1024], param.9: f32[1024], param.8: f32[128,1024], param.6: f32[6400,1024], param.17: f32[1024], param.16: f32[1024], param.15: f32[1024], param.14: f32[128,1024], param.12: f32[384], param.11: f32[1024,384], param.19: f32[512], param.18: f32[1024,512], param.23: f32[1024], param.22: f32[1024], param.21: f32[1024], param.20: f32[512,1024], param.29: f32[1024], param.28: f32[1024], param.27: f32[1024], param.26: f32[128,1024], param.25: f32[384], param.24: f32[1024,384], param.31: f32[512], param.30: f32[1024,512], param.35: f32[1024], param.34: f32[1024], param.33: f32[1024], param.32: f32[512,1024], param.41: f32[1024], param.40: f32[1024], param.39: f32[1024], param.38: f32[128,1024], param.37: f32[384], param.36: f32[1024,384], param.43: f32[512], param.42: f32[1024,512], param.47: f32[1024], param.46: f32[1024], param.45: f32[1024], param.44: f32[512,1024], param.53: f32[1024], param.52: f32[1024], param.51: f32[1024], param.50: f32[128,1024], param.49: f32[384], param.48: f32[1024,384], param.55: f32[512], param.54: f32[1024,512], param.59: f32[1024], param.58: f32[1024], param.57: f32[1024], param.56: f32[512,1024], param.65: f32[1024], param.64: f32[1024], param.63: f32[1024], param.62: f32[128,1024], param.61: f32[384], param.60: f32[1024,384], param.67: f32[512], param.66: f32[1024,512], param.71: f32[1024], param.70: f32[1024], param.69: f32[1024], param.68: f32[512,1024], param.77: f32[1024], param.76: f32[1024], param.75: f32[1024], param.74: f32[128,1024], param.73: f32[384], param.72: f32[1024,384], param.79: f32[512], param.78: f32[1024,512], param.83: f32[1024], param.82: f32[1024], param.81: f32[1024], param.80: f32[512,1024], param.85: s32[], param.84: f32[6400], param.87: f32[1024], param.89: f32[1024], param.91: f32[128,1024], param.93: f32[6400,1024], param.95: f32[1024], param.97: f32[1024], param.99: f32[1024], param.101: f32[128,1024], param.103: f32[384], param.105: f32[1024,384], param.107: f32[512], param.109: f32[1024,512], param.111: f32[1024], param.113: f32[1024], param.115: f32[1024], param.117: f32[512,1024], param.119: f32[1024], param.121: f32[1024], param.123: f32[1024], param.125: f32[128,1024], param.127: f32[384], param.129: f32[1024,384], param.131: f32[512], param.133: f32[1024,512], param.135: f32[1024], param.137: f32[1024], param.139: f32[1024], param.141: f32[512,1024], param.143: f32[1024], param.145: f32[1024], param.147: f32[1024], param.149: f32[128,1024], param.151: f32[384], param.153: f32[1024,384], param.155: f32[512], param.157: f32[1024,512], param.159: f32[1024], param.161: f32[1024], param.163: f32[1024], param.165: f32[512,1024], param.167: f32[1024], param.169: f32[1024], param.171: f32[1024], param.173: f32[128,1024], param.175: f32[384], param.177: f32[1024,384], param.179: f32[512], param.181: f32[1024,512], param.183: f32[1024], param.185: f32[1024], param.187: f32[1024], param.189: f32[512,1024], param.191: f32[1024], param.193: f32[1024], param.195: f32[1024], param.197: f32[128,1024], param.199: f32[384], param.201: f32[1024,384], param.203: f32[512], param.205: f32[1024,512], param.207: f32[1024], param.209: f32[1024], param.211: f32[1024], param.213: f32[512,1024], param.215: f32[1024], param.217: f32[1024], param.219: f32[1024], param.221: f32[128,1024], param.223: f32[384], param.225: f32[1024,384], param.227: f32[512], param.229: f32[1024,512], param.231: f32[1024], param.233: f32[1024], param.235: f32[1024], param.237: f32[512,1024], param.86: f32[6400], param.88: f32[1024], param.90: f32[1024], param.92: f32[128,1024], param.94: f32[6400,1024], param.96: f32[1024], param.98: f32[1024], param.100: f32[1024], param.102: f32[128,1024], param.104: f32[384], param.106: f32[1024,384], param.108: f32[512], param.110: f32[1024,512], param.112: f32[1024], param.114: f32[1024], param.116: f32[1024], param.118: f32[512,1024], param.120: f32[1024], param.122: f32[1024], param.124: f32[1024], param.126: f32[128,1024], param.128: f32[384], param.130: f32[1024,384], param.132: f32[512], param.134: f32[1024,512], param.136: f32[1024], param.138: f32[1024], param.140: f32[1024], param.142: f32[512,1024], param.144: f32[1024], param.146: f32[1024], param.148: f32[1024], param.150: f32[128,1024], param.152: f32[384], param.154: f32[1024,384], param.156: f32[512], param.158: f32[1024,512], param.160: f32[1024], param.162: f32[1024], param.164: f32[1024], param.166: f32[512,1024], param.168: f32[1024], param.170: f32[1024], param.172: f32[1024], param.174: f32[128,1024], param.176: f32[384], param.178: f32[1024,384], param.180: f32[512], param.182: f32[1024,512], param.184: f32[1024], param.186: f32[1024], param.188: f32[1024], param.190: f32[512,1024], param.192: f32[1024], param.194: f32[1024], param.196: f32[1024], param.198: f32[128,1024], param.200: f32[384], param.202: f32[1024,384], param.204: f32[512], param.206: f32[1024,512], param.208: f32[1024], param.210: f32[1024], param.212: f32[1024], param.214: f32[512,1024], param.216: f32[1024], param.218: f32[1024], param.220: f32[1024], param.222: f32[128,1024], param.224: f32[384], param.226: f32[1024,384], param.228: f32[512], param.230: f32[1024,512], param.232: f32[1024], param.234: f32[1024], param.236: f32[1024], param.238: f32[512,1024], param.13: s32[4,1024], param.5: s32[4,1024], param.4: s32[4,1024], param.7: s32[4,1024], param: s32[4,1024], param.1: u32[1]) -> (s32[], f32[6400], f32[1024], f32[1024], f32[128,1024], /*index=5*/f32[6400,1024], f32[1024], f32[1024], f32[1024], f32[128,1024], /*index=10*/f32[384], f32[1024,384], f32[512], f32[1024,512], f32[1024], /*index=15*/f32[1024], f32[1024], f32[512,1024], f32[1024], f32[1024], /*index=20*/f32[1024], f32[128,1024], f32[384], f32[1024,384], f32[512], /*index=25*/f32[1024,512], f32[1024], f32[1024], f32[1024], f32[512,1024], /*index=30*/f32[1024], f32[1024], f32[1024], f32[128,1024], f32[384], /*index=35*/f32[1024,384], f32[512], f32[1024,512], f32[1024], f32[1024], /*index=40*/f32[1024], f32[512,1024], f32[1024], f32[1024], f32[1024], /*index=45*/f32[128,1024], f32[384], f32[1024,384], f32[512], f32[1024,512], /*index=50*/f32[1024], f32[1024], f32[1024], f32[512,1024], f32[1024], /*index=55*/f32[1024], f32[1024], f32[128,1024], f32[384], f32[1024,384], /*index=60*/f32[512], f32[1024,512], f32[1024], f32[1024], f32[1024], /*index=65*/f32[512,1024], f32[1024], f32[1024], f32[1024], f32[128,1024], /*index=70*/f32[384], f32[1024,384], f32[512], f32[1024,512], f32[1024], /*index=75*/f32[1024], f32[1024], f32[512,1024], s32[], f32[6400], /*index=80*/f32[1024], f32[1024], f32[128,1024], f32[6400,1024], f32[1024], /*index=85*/f32[1024], f32[1024], f32[128,1024], f32[384], f32[1024,384], /*index=90*/f32[512], f32[1024,512], f32[1024], f32[1024], f32[1024], /*index=95*/f32[512,1024], f32[1024], f32[1024], f32[1024], f32[128,1024], /*index=100*/f32[384], f32[1024,384], f32[512], f32[1024,512], f32[1024], /*index=105*/f32[1024], f32[1024], f32[512,1024], f32[1024], f32[1024], /*index=110*/f32[1024], f32[128,1024], f32[384], f32[1024,384], f32[512], /*index=115*/f32[1024,512], f32[1024], f32[1024], f32[1024], f32[512,1024], /*index=120*/f32[1024], f32[1024], f32[1024], f32[128,1024], f32[384], /*index=125*/f32[1024,384], f32[512], f32[1024,512], f32[1024], f32[1024], /*index=130*/f32[1024], f32[512,1024], f32[1024], f32[1024], f32[1024], /*index=135*/f32[128,1024], f32[384], f32[1024,384], f32[512], f32[1024,512], /*index=140*/f32[1024], f32[1024], f32[1024], f32[512,1024], f32[1024], /*index=145*/f32[1024], f32[1024], f32[128,1024], f32[384], f32[1024,384], /*index=150*/f32[512], f32[1024,512], f32[1024], f32[1024], f32[1024], /*index=155*/f32[512,1024], f32[6400], f32[1024], f32[1024], f32[128,1024], /*index=160*/f32[6400,1024], f32[1024], f32[1024], f32[1024], f32[128,1024], /*index=165*/f32[384], f32[1024,384], f32[512], f32[1024,512], f32[1024], /*index=170*/f32[1024], f32[1024], f32[512,1024], f32[1024], f32[1024], /*index=175*/f32[1024], f32[128,1024], f32[384], f32[1024,384], f32[512], /*index=180*/f32[1024,512], f32[1024], f32[1024], f32[1024], f32[512,1024], /*index=185*/f32[1024], f32[1024], f32[1024], f32[128,1024], f32[384], /*index=190*/f32[1024,384], f32[512], f32[1024,512], f32[1024], f32[1024], /*index=195*/f32[1024], f32[512,1024], f32[1024], f32[1024], f32[1024], /*index=200*/f32[128,1024], f32[384], f32[1024,384], f32[512], f32[1024,512], /*index=205*/f32[1024], f32[1024], f32[1024], f32[512,1024], f32[1024], /*index=210*/f32[1024], f32[1024], f32[128,1024], f32[384], f32[1024,384], /*index=215*/f32[512], f32[1024,512], f32[1024], f32[1024], f32[1024], /*index=220*/f32[512,1024], f32[1024], f32[1024], f32[1024], f32[128,1024], /*index=225*/f32[384], f32[1024,384], f32[512], f32[1024,512], f32[1024], /*index=230*/f32[1024], f32[1024], f32[512,1024]) {
  %param = s32[4,1024]{1,0} parameter(237), sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}
  %param.1 = u32[1]{0} parameter(238), sharding={devices=[2,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}
  %param.2 = s32[] parameter(0), sharding={replicated}
  %constant.49 = s32[] constant(1)
  %add.92 = s32[] add(s32[] %param.2, s32[] %constant.49), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/code/alpa/alpa/model/model_util.py" source_line=301}
  %param.3 = f32[6400]{0} parameter(1), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %param.4 = s32[4,1024]{1,0} parameter(235), sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}
  %broadcast.926 = s32[4,1024,6400]{2,1,0} broadcast(s32[4,1024]{1,0} %param.4), dimensions={0,1}
  %iota.9 = s32[4,1024,6400]{2,1,0} iota(), iota_dimension=2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %constant.52 = s32[16]{0} constant({...}), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %partition-id = u32[] partition-id()
  %dynamic-slice.1 = s32[1]{0} dynamic-slice(s32[16]{0} %constant.52, u32[] %partition-id), dynamic_slice_sizes={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %reshape.1418 = s32[] reshape(s32[1]{0} %dynamic-slice.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %constant.53 = s32[] constant(6400), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %multiply.171 = s32[] multiply(s32[] %reshape.1418, s32[] %constant.53), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %broadcast.415 = s32[4,1024,6400]{2,1,0} broadcast(s32[] %multiply.171), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %add.93 = s32[4,1024,6400]{2,1,0} add(s32[4,1024,6400]{2,1,0} %iota.9, s32[4,1024,6400]{2,1,0} %broadcast.415), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %compare.12 = pred[4,1024,6400]{2,1,0} compare(s32[4,1024,6400]{2,1,0} %broadcast.926, s32[4,1024,6400]{2,1,0} %add.93), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %constant.59 = s32[] constant(0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %broadcast.927 = s32[4,1024]{1,0} broadcast(s32[] %constant.59), dimensions={}
  %compare.13 = pred[4,1024]{1,0} compare(s32[4,1024]{1,0} %param.4, s32[4,1024]{1,0} %broadcast.927), direction=GT, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/gt" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=86}
  %constant.68 = f32[] constant(1)
  %broadcast.928 = f32[4,1024]{1,0} broadcast(f32[] %constant.68), dimensions={}
  %constant.69 = f32[] constant(0)
  %broadcast.930 = f32[4,1024]{1,0} broadcast(f32[] %constant.69), dimensions={}
  %select.2 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.13, f32[4,1024]{1,0} %broadcast.928, f32[4,1024]{1,0} %broadcast.930), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/select_n" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=86}
  %reduce = f32[] reduce(f32[4,1024]{1,0} %select.2, f32[] %constant.69), dimensions={0,1}, to_apply=%region_40.1707, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(0, 1)]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=90}
  %all-reduce = f32[] all-reduce(f32[] %reduce), channel_id=1, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_40.1707, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(0, 1)]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=90}
  %divide.102 = f32[] divide(f32[] %constant.68, f32[] %all-reduce), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=90}
  %broadcast.423 = f32[4,1024]{1,0} broadcast(f32[] %divide.102), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/broadcast_in_dim[shape=(8, 1024) broadcast_dimensions=()]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=90}
  %multiply.172 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %select.2, f32[4,1024]{1,0} %broadcast.423), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=90}
  %negate.6 = f32[4,1024]{1,0} negate(f32[4,1024]{1,0} %multiply.172), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/neg" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %broadcast.932 = f32[4,1024,6400]{2,1,0} broadcast(f32[4,1024]{1,0} %negate.6), dimensions={0,1}
  %broadcast.933 = f32[4,1024,6400]{2,1,0} broadcast(f32[] %constant.69), dimensions={}
  %select.3 = f32[4,1024,6400]{2,1,0} select(pred[4,1024,6400]{2,1,0} %compare.12, f32[4,1024,6400]{2,1,0} %broadcast.932, f32[4,1024,6400]{2,1,0} %broadcast.933), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %convert.42 = f16[4,1024,6400]{2,1,0} convert(f32[4,1024,6400]{2,1,0} %select.3), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %negate.7 = f16[4,1024,6400]{2,1,0} negate(f16[4,1024,6400]{2,1,0} %convert.42), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/neg" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %constant.89 = f16[] constant(0)
  %reduce.1 = f16[4,1024]{1,0} reduce(f16[4,1024,6400]{2,1,0} %negate.7, f16[] %constant.89), dimensions={2}, to_apply=%region_41.1720, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(2,)]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %all-reduce.1 = f16[4,1024]{1,0} all-reduce(f16[4,1024]{1,0} %reduce.1), channel_id=2, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%region_41.1720, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(2,)]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %param.5 = s32[4,1024]{1,0} parameter(234), sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}
  %broadcast.934 = s32[4,1024,6400]{2,1,0} broadcast(s32[4,1024]{1,0} %param.5), dimensions={0,1}
  %compare.14 = pred[4,1024,6400]{2,1,0} compare(s32[4,1024,6400]{2,1,0} %broadcast.934, s32[4,1024,6400]{2,1,0} %add.93), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/word_embeddings.embed_call_one_hot/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %convert.43 = f16[4,1024,6400]{2,1,0} convert(pred[4,1024,6400]{2,1,0} %compare.14), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/word_embeddings.embed_call_one_hot/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %reshape.1441 = f16[4096,6400]{1,0} reshape(f16[4,1024,6400]{2,1,0} %convert.43)
  %param.6 = f32[6400,1024]{1,0} parameter(5), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.44 = f16[6400,1024]{1,0} convert(f32[6400,1024]{1,0} %param.6), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/word_embeddings.embed_call_one_hot/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=237}
  %dot.214 = f16[4096,1024]{1,0} dot(f16[4096,6400]{1,0} %reshape.1441, f16[6400,1024]{1,0} %convert.44), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/word_embeddings.embed_call_one_hot/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=237}
  %param.7 = s32[4,1024]{1,0} parameter(236), sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}
  %broadcast.935 = s32[4,1024,128]{2,1,0} broadcast(s32[4,1024]{1,0} %param.7), dimensions={0,1}
  %iota.10 = s32[4,1024,128]{2,1,0} iota(), iota_dimension=2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/position_embeddings.embed_call_one_hot/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %constant.106 = s32[] constant(128), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/position_embeddings.embed_call_one_hot/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %multiply.173 = s32[] multiply(s32[] %reshape.1418, s32[] %constant.106), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/position_embeddings.embed_call_one_hot/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %broadcast.430 = s32[4,1024,128]{2,1,0} broadcast(s32[] %multiply.173), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/position_embeddings.embed_call_one_hot/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %add.94 = s32[4,1024,128]{2,1,0} add(s32[4,1024,128]{2,1,0} %iota.10, s32[4,1024,128]{2,1,0} %broadcast.430), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/position_embeddings.embed_call_one_hot/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %compare.15 = pred[4,1024,128]{2,1,0} compare(s32[4,1024,128]{2,1,0} %broadcast.935, s32[4,1024,128]{2,1,0} %add.94), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/position_embeddings.embed_call_one_hot/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %convert.45 = f16[4,1024,128]{2,1,0} convert(pred[4,1024,128]{2,1,0} %compare.15), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/position_embeddings.embed_call_one_hot/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %reshape.1451 = f16[4096,128]{1,0} reshape(f16[4,1024,128]{2,1,0} %convert.45)
  %param.8 = f32[128,1024]{1,0} parameter(4), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.46 = f16[128,1024]{1,0} convert(f32[128,1024]{1,0} %param.8), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/position_embeddings.embed_call_one_hot/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=237}
  %dot.215 = f16[4096,1024]{1,0} dot(f16[4096,128]{1,0} %reshape.1451, f16[128,1024]{1,0} %convert.46), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/position_embeddings.embed_call_one_hot/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=237}
  %add.850 = f16[4096,1024]{1,0} add(f16[4096,1024]{1,0} %dot.214, f16[4096,1024]{1,0} %dot.215), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=130}
  %all-reduce.121 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %add.850), channel_id=122, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/word_embeddings.embed_call_one_hot/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=237}
  %convert.47 = f32[4096,1024]{1,0} convert(f16[4096,1024]{1,0} %all-reduce.121), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reshape.1453 = f32[4,1024,1024]{2,1,0} reshape(f32[4096,1024]{1,0} %convert.47), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reduce.2 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %reshape.1453, f32[] %constant.69), dimensions={2}, to_apply=%region_0.466, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %constant.120 = f32[] constant(0.0009765625)
  %broadcast.936 = f32[4,1024]{1,0} broadcast(f32[] %constant.120), dimensions={}
  %multiply.174 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reduce.2, f32[4,1024]{1,0} %broadcast.936), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.432 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.174), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.12 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %reshape.1453, f32[4,1024,1024]{2,1,0} %broadcast.432), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.175 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %reshape.1453, f32[4,1024,1024]{2,1,0} %reshape.1453), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.3 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.175, f32[] %constant.69), dimensions={2}, to_apply=%region_1.474, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.176 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reduce.3, f32[4,1024]{1,0} %broadcast.936), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.177 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %multiply.174, f32[4,1024]{1,0} %multiply.174), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.13 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %multiply.176, f32[4,1024]{1,0} %multiply.177), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.0 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %subtract.13, f32[4,1024]{1,0} %broadcast.930), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant.124 = f32[] constant(1e-12)
  %broadcast.937 = f32[4,1024]{1,0} broadcast(f32[] %constant.124), dimensions={}
  %add.96 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.0, f32[4,1024]{1,0} %broadcast.937), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.1459 = f32[4,1024,1]{2,1,0} reshape(f32[4,1024]{1,0} %add.96), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.12 = f32[4,1024,1]{2,1,0} rsqrt(f32[4,1024,1]{2,1,0} %reshape.1459), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.1461 = f32[4,1024]{1,0} reshape(f32[4,1024,1]{2,1,0} %rsqrt.12), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.434 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %reshape.1461), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param.9 = f32[1024]{0} parameter(3), sharding={replicated}
  %broadcast.938 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param.9), dimensions={2}
  %multiply.178 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.434, f32[4,1024,1024]{2,1,0} %broadcast.938), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.179 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.12, f32[4,1024,1024]{2,1,0} %multiply.178), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param.10 = f32[1024]{0} parameter(2), sharding={replicated}
  %broadcast.939 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param.10), dimensions={2}
  %add.97 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.179, f32[4,1024,1024]{2,1,0} %broadcast.939), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %convert.48 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.97), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %reshape.1468 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %convert.48)
  %param.11 = f32[1024,384]{1,0} parameter(11), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.49 = f16[1024,384]{1,0} convert(f32[1024,384]{1,0} %param.11), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.216 = f16[4096,384]{1,0} dot(f16[4096,1024]{1,0} %reshape.1468, f16[1024,384]{1,0} %convert.49), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.1472 = f16[4,1024,384]{2,1,0} reshape(f16[4096,384]{1,0} %dot.216), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.12 = f32[384]{0} parameter(10), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.50 = f16[384]{0} convert(f32[384]{0} %param.12), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.940 = f16[4,1024,384]{2,1,0} broadcast(f16[384]{0} %convert.50), dimensions={2}
  %add.98 = f16[4,1024,384]{2,1,0} add(f16[4,1024,384]{2,1,0} %reshape.1472, f16[4,1024,384]{2,1,0} %broadcast.940), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %reshape.1477 = f16[4,1024,128,3]{3,2,1,0} reshape(f16[4,1024,384]{2,1,0} %add.98), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/reshape[new_sizes=(8, 1024, 1024, 3) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=161}
  %slice = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.1477), slice={[0:4], [0:1024], [0:128], [1:2]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/slice[start_indices=(0, 0, 0, 1) limit_indices=(8, 1024, 1024, 2) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %reshape.1482 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %slice), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/reshape[new_sizes=(8, 1024, 32, 32) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=170}
  %transpose.303 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.1482), dimensions={0,2,3,1}
  %slice.1 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.1477), slice={[0:4], [0:1024], [0:128], [0:1]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/slice[start_indices=(0, 0, 0, 0) limit_indices=(8, 1024, 1024, 1) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %constant.165 = f16[] constant(0.17676)
  %broadcast.941 = f16[4,1024,128,1]{3,2,1,0} broadcast(f16[] %constant.165), dimensions={}
  %multiply.180 = f16[4,1024,128,1]{3,2,1,0} multiply(f16[4,1024,128,1]{3,2,1,0} %slice.1, f16[4,1024,128,1]{3,2,1,0} %broadcast.941), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %reshape.1491 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %multiply.180), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %transpose.304 = f16[4,4,1024,32]{3,1,2,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.1491), dimensions={0,2,1,3}
  %slice.2 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.1477), slice={[0:4], [0:1024], [0:128], [2:3]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/slice[start_indices=(0, 0, 0, 2) limit_indices=(8, 1024, 1024, 3) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %reshape.1495 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %slice.2), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/reshape[new_sizes=(8, 1024, 32, 32) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=173}
  %transpose.305 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.1495), dimensions={0,2,3,1}
  %dot.217 = f16[4,4,1024,1024]{3,2,1,0} dot(f16[4,4,1024,32]{3,1,2,0} %transpose.304, f16[4,4,32,1024]{2,1,3,0} %transpose.305), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/dot_general[dimension_numbers=(((3,), (3,)), ((0, 2), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %param.13 = s32[4,1024]{1,0} parameter(233), sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}
  %compare.16 = pred[4,1024]{1,0} compare(s32[4,1024]{1,0} %param.13, s32[4,1024]{1,0} %broadcast.927), direction=GT, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %broadcast.943 = f16[4,1024]{1,0} broadcast(f16[] %constant.89), dimensions={}
  %constant.179 = f16[] constant(-inf)
  %broadcast.945 = f16[4,1024]{1,0} broadcast(f16[] %constant.179), dimensions={}
  %select.4 = f16[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.16, f16[4,1024]{1,0} %broadcast.943, f16[4,1024]{1,0} %broadcast.945), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %broadcast.946 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,1024]{1,0} %select.4), dimensions={0,3}
  %add.99 = f16[4,4,1024,1024]{3,2,1,0} add(f16[4,4,1024,1024]{3,2,1,0} %dot.217, f16[4,4,1024,1024]{3,2,1,0} %broadcast.946), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=100}
  %reduce.4 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %add.99, f16[] %constant.179), dimensions={3}, to_apply=%region_2.539, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.445 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %reduce.4), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.21 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %add.99, f16[4,4,1024,1024]{3,2,1,0} %broadcast.445), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.0 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.21), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.51 = f32[4,4,1024,1024]{3,2,1,0} convert(f16[4,4,1024,1024]{3,2,1,0} %exponential.0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %reduce.5 = f32[4,4,1024]{2,1,0} reduce(f32[4,4,1024,1024]{3,2,1,0} %convert.51, f32[] %constant.69), dimensions={3}, to_apply=%region_3.551, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.52 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %reduce.5), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.447 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %convert.52), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.103 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %exponential.0, f16[4,4,1024,1024]{3,2,1,0} %broadcast.447), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %dot.218 = f16[4,4,32,1024]{3,2,1,0} dot(f16[4,4,32,1024]{2,1,3,0} %transpose.303, f16[4,4,1024,1024]{3,2,1,0} %divide.103), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/dot_general[dimension_numbers=(((1,), (3,)), ((0, 2), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %transpose.306 = f16[4,1024,4,32]{1,3,2,0} transpose(f16[4,4,32,1024]{3,2,1,0} %dot.218), dimensions={0,3,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %reshape.1507 = f16[4096,128]{1,0} reshape(f16[4,1024,4,32]{1,3,2,0} %transpose.306)
  %param.14 = f32[128,1024]{1,0} parameter(9), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.53 = f16[128,1024]{1,0} convert(f32[128,1024]{1,0} %param.14), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.219 = f16[4096,1024]{1,0} dot(f16[4096,128]{1,0} %reshape.1507, f16[128,1024]{1,0} %convert.53), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.4 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.219), channel_id=5, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.2, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.1509 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.4), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.15 = f32[1024]{0} parameter(8), sharding={replicated}
  %convert.54 = f16[1024]{0} convert(f32[1024]{0} %param.15), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.947 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.54), dimensions={2}
  %add.100 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %reshape.1509, f16[4,1024,1024]{2,1,0} %broadcast.947), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %add.101 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.100, f16[4,1024,1024]{2,1,0} %convert.48), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=233}
  %convert.55 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.101), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reduce.6 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.55, f32[] %constant.69), dimensions={2}, to_apply=%region_4.576, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.181 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reduce.6, f32[4,1024]{1,0} %broadcast.936), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.449 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.181), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.22 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.55, f32[4,1024,1024]{2,1,0} %broadcast.449), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.182 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.55, f32[4,1024,1024]{2,1,0} %convert.55), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.7 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.182, f32[] %constant.69), dimensions={2}, to_apply=%region_5.588, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.183 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reduce.7, f32[4,1024]{1,0} %broadcast.936), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.184 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %multiply.181, f32[4,1024]{1,0} %multiply.181), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.23 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %multiply.183, f32[4,1024]{1,0} %multiply.184), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.1 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %subtract.23, f32[4,1024]{1,0} %broadcast.930), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %add.102 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.1, f32[4,1024]{1,0} %broadcast.937), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.13 = f32[4,1024]{1,0} rsqrt(f32[4,1024]{1,0} %add.102), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %broadcast.450 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %rsqrt.13), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param.16 = f32[1024]{0} parameter(7), sharding={replicated}
  %broadcast.948 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param.16), dimensions={2}
  %multiply.185 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.450, f32[4,1024,1024]{2,1,0} %broadcast.948), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.186 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.22, f32[4,1024,1024]{2,1,0} %multiply.185), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param.17 = f32[1024]{0} parameter(6), sharding={replicated}
  %broadcast.949 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param.17), dimensions={2}
  %add.103 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.186, f32[4,1024,1024]{2,1,0} %broadcast.949), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %convert.56 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.103), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %reshape.1517 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %convert.56)
  %param.18 = f32[1024,512]{1,0} parameter(13), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.57 = f16[1024,512]{1,0} convert(f32[1024,512]{1,0} %param.18), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.220 = f16[4096,512]{1,0} dot(f16[4096,1024]{1,0} %reshape.1517, f16[1024,512]{1,0} %convert.57), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.1520 = f16[4,1024,512]{2,1,0} reshape(f16[4096,512]{1,0} %dot.220), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.19 = f32[512]{0} parameter(12), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.58 = f16[512]{0} convert(f32[512]{0} %param.19), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.950 = f16[4,1024,512]{2,1,0} broadcast(f16[512]{0} %convert.58), dimensions={2}
  %add.104 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %reshape.1520, f16[4,1024,512]{2,1,0} %broadcast.950), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %constant.225 = f32[] constant(-4)
  %broadcast.952 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant.225), dimensions={}
  %constant.226 = f16[] constant(0.70703)
  %broadcast.951 = f16[4,1024,512]{2,1,0} broadcast(f16[] %constant.226), dimensions={}
  %multiply.187 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %add.104, f16[4,1024,512]{2,1,0} %broadcast.951), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.59 = f32[4,1024,512]{2,1,0} convert(f16[4,1024,512]{2,1,0} %multiply.187), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant.230 = f32[] constant(4)
  %broadcast.954 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant.230), dimensions={}
  %clamp.0 = f32[4,1024,512]{2,1,0} clamp(f32[4,1024,512]{2,1,0} %broadcast.952, f32[4,1024,512]{2,1,0} %convert.59, f32[4,1024,512]{2,1,0} %broadcast.954), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.188 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.0, f32[4,1024,512]{2,1,0} %clamp.0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %broadcast.956 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant.69), dimensions={}
  %multiply.189 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %multiply.188, f32[4,1024,512]{2,1,0} %broadcast.956), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant.241 = f32[] constant(-2.72614237e-10)
  %broadcast.957 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant.241), dimensions={}
  %add.105 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.189, f32[4,1024,512]{2,1,0} %broadcast.957), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.190 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.105, f32[4,1024,512]{2,1,0} %multiply.188), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant.249 = f32[] constant(2.77068146e-08)
  %broadcast.958 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant.249), dimensions={}
  %add.106 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.190, f32[4,1024,512]{2,1,0} %broadcast.958), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.191 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.106, f32[4,1024,512]{2,1,0} %multiply.188), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant.257 = f32[] constant(-2.10102394e-06)
  %broadcast.959 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant.257), dimensions={}
  %add.107 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.191, f32[4,1024,512]{2,1,0} %broadcast.959), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.192 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.107, f32[4,1024,512]{2,1,0} %multiply.188), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant.265 = f32[] constant(-5.69250624e-05)
  %broadcast.960 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant.265), dimensions={}
  %add.108 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.192, f32[4,1024,512]{2,1,0} %broadcast.960), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.193 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.108, f32[4,1024,512]{2,1,0} %multiply.188), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant.273 = f32[] constant(-0.000734990637)
  %broadcast.961 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant.273), dimensions={}
  %add.109 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.193, f32[4,1024,512]{2,1,0} %broadcast.961), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.194 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.109, f32[4,1024,512]{2,1,0} %multiply.188), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant.281 = f32[] constant(-0.0029546)
  %broadcast.962 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant.281), dimensions={}
  %add.110 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.194, f32[4,1024,512]{2,1,0} %broadcast.962), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.195 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.110, f32[4,1024,512]{2,1,0} %multiply.188), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant.289 = f32[] constant(-0.0160960332)
  %broadcast.963 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant.289), dimensions={}
  %add.111 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.195, f32[4,1024,512]{2,1,0} %broadcast.963), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.196 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.0, f32[4,1024,512]{2,1,0} %add.111), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant.297 = f32[] constant(-1.45660715e-05)
  %broadcast.964 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant.297), dimensions={}
  %add.112 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.189, f32[4,1024,512]{2,1,0} %broadcast.964), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.197 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.112, f32[4,1024,512]{2,1,0} %multiply.188), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant.305 = f32[] constant(-0.000213374049)
  %broadcast.965 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant.305), dimensions={}
  %add.113 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.197, f32[4,1024,512]{2,1,0} %broadcast.965), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.198 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.113, f32[4,1024,512]{2,1,0} %multiply.188), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant.313 = f32[] constant(-0.00168282702)
  %broadcast.966 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant.313), dimensions={}
  %add.114 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.198, f32[4,1024,512]{2,1,0} %broadcast.966), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.199 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.114, f32[4,1024,512]{2,1,0} %multiply.188), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant.321 = f32[] constant(-0.00737332925)
  %broadcast.968 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant.321), dimensions={}
  %add.115 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.199, f32[4,1024,512]{2,1,0} %broadcast.968), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.200 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.115, f32[4,1024,512]{2,1,0} %multiply.188), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant.329 = f32[] constant(-0.0142647391)
  %broadcast.970 = f32[4,1024,512]{2,1,0} broadcast(f32[] %constant.329), dimensions={}
  %add.116 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.200, f32[4,1024,512]{2,1,0} %broadcast.970), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.104 = f32[4,1024,512]{2,1,0} divide(f32[4,1024,512]{2,1,0} %multiply.196, f32[4,1024,512]{2,1,0} %add.116), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.60 = f16[4,1024,512]{2,1,0} convert(f32[4,1024,512]{2,1,0} %divide.104), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant.337 = f16[] constant(1)
  %broadcast.971 = f16[4,1024,512]{2,1,0} broadcast(f16[] %constant.337), dimensions={}
  %add.117 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %convert.60, f16[4,1024,512]{2,1,0} %broadcast.971), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.201 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %add.104, f16[4,1024,512]{2,1,0} %add.117), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant.345 = f16[] constant(0.5)
  %broadcast.972 = f16[4,1024,512]{2,1,0} broadcast(f16[] %constant.345), dimensions={}
  %multiply.202 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.201, f16[4,1024,512]{2,1,0} %broadcast.972), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %reshape.1572 = f16[4096,512]{1,0} reshape(f16[4,1024,512]{2,1,0} %multiply.202)
  %param.20 = f32[512,1024]{1,0} parameter(17), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.61 = f16[512,1024]{1,0} convert(f32[512,1024]{1,0} %param.20), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.221 = f16[4096,1024]{1,0} dot(f16[4096,512]{1,0} %reshape.1572, f16[512,1024]{1,0} %convert.61), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.5 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.221), channel_id=6, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.3, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.1574 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.5), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.21 = f32[1024]{0} parameter(16), sharding={replicated}
  %convert.62 = f16[1024]{0} convert(f32[1024]{0} %param.21), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.973 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.62), dimensions={2}
  %add.118 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %reshape.1574, f16[4,1024,1024]{2,1,0} %broadcast.973), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %add.119 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.118, f16[4,1024,1024]{2,1,0} %convert.56), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=310}
  %convert.63 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.119), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reduce.8 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.63, f32[] %constant.69), dimensions={2}, to_apply=%region_6.668, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.203 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reduce.8, f32[4,1024]{1,0} %broadcast.936), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.477 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.203), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.27 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.63, f32[4,1024,1024]{2,1,0} %broadcast.477), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.204 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.63, f32[4,1024,1024]{2,1,0} %convert.63), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.9 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.204, f32[] %constant.69), dimensions={2}, to_apply=%region_7.680, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.205 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reduce.9, f32[4,1024]{1,0} %broadcast.936), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.206 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %multiply.203, f32[4,1024]{1,0} %multiply.203), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.28 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %multiply.205, f32[4,1024]{1,0} %multiply.206), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.2 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %subtract.28, f32[4,1024]{1,0} %broadcast.930), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %add.120 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.2, f32[4,1024]{1,0} %broadcast.937), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.14 = f32[4,1024]{1,0} rsqrt(f32[4,1024]{1,0} %add.120), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %broadcast.478 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %rsqrt.14), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param.22 = f32[1024]{0} parameter(15), sharding={replicated}
  %broadcast.974 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param.22), dimensions={2}
  %multiply.207 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.478, f32[4,1024,1024]{2,1,0} %broadcast.974), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.208 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.27, f32[4,1024,1024]{2,1,0} %multiply.207), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param.23 = f32[1024]{0} parameter(14), sharding={replicated}
  %broadcast.975 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param.23), dimensions={2}
  %add.121 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.208, f32[4,1024,1024]{2,1,0} %broadcast.975), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %convert.64 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.121), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/0/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %reshape.1583 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %convert.64)
  %param.24 = f32[1024,384]{1,0} parameter(23), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.65 = f16[1024,384]{1,0} convert(f32[1024,384]{1,0} %param.24), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.222 = f16[4096,384]{1,0} dot(f16[4096,1024]{1,0} %reshape.1583, f16[1024,384]{1,0} %convert.65), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.1585 = f16[4,1024,384]{2,1,0} reshape(f16[4096,384]{1,0} %dot.222), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.25 = f32[384]{0} parameter(22), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.66 = f16[384]{0} convert(f32[384]{0} %param.25), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.976 = f16[4,1024,384]{2,1,0} broadcast(f16[384]{0} %convert.66), dimensions={2}
  %add.122 = f16[4,1024,384]{2,1,0} add(f16[4,1024,384]{2,1,0} %reshape.1585, f16[4,1024,384]{2,1,0} %broadcast.976), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %reshape.1590 = f16[4,1024,128,3]{3,2,1,0} reshape(f16[4,1024,384]{2,1,0} %add.122), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/reshape[new_sizes=(8, 1024, 1024, 3) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=161}
  %slice.3 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.1590), slice={[0:4], [0:1024], [0:128], [1:2]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/slice[start_indices=(0, 0, 0, 1) limit_indices=(8, 1024, 1024, 2) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %reshape.1593 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %slice.3), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/reshape[new_sizes=(8, 1024, 32, 32) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=170}
  %transpose.307 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.1593), dimensions={0,2,3,1}
  %slice.4 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.1590), slice={[0:4], [0:1024], [0:128], [0:1]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/slice[start_indices=(0, 0, 0, 0) limit_indices=(8, 1024, 1024, 1) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %multiply.209 = f16[4,1024,128,1]{3,2,1,0} multiply(f16[4,1024,128,1]{3,2,1,0} %slice.4, f16[4,1024,128,1]{3,2,1,0} %broadcast.941), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %reshape.1596 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %multiply.209), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %transpose.308 = f16[4,4,1024,32]{3,1,2,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.1596), dimensions={0,2,1,3}
  %slice.5 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.1590), slice={[0:4], [0:1024], [0:128], [2:3]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/slice[start_indices=(0, 0, 0, 2) limit_indices=(8, 1024, 1024, 3) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %reshape.1599 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %slice.5), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/reshape[new_sizes=(8, 1024, 32, 32) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=173}
  %transpose.309 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.1599), dimensions={0,2,3,1}
  %dot.223 = f16[4,4,1024,1024]{3,2,1,0} dot(f16[4,4,1024,32]{3,1,2,0} %transpose.308, f16[4,4,32,1024]{2,1,3,0} %transpose.309), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/dot_general[dimension_numbers=(((3,), (3,)), ((0, 2), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %add.123 = f16[4,4,1024,1024]{3,2,1,0} add(f16[4,4,1024,1024]{3,2,1,0} %dot.223, f16[4,4,1024,1024]{3,2,1,0} %broadcast.946), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=100}
  %reduce.10 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %add.123, f16[] %constant.179), dimensions={3}, to_apply=%region_8.731, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.482 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %reduce.10), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.32 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %add.123, f16[4,4,1024,1024]{3,2,1,0} %broadcast.482), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.1 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.32), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.67 = f32[4,4,1024,1024]{3,2,1,0} convert(f16[4,4,1024,1024]{3,2,1,0} %exponential.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %reduce.11 = f32[4,4,1024]{2,1,0} reduce(f32[4,4,1024,1024]{3,2,1,0} %convert.67, f32[] %constant.69), dimensions={3}, to_apply=%region_9.743, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.68 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %reduce.11), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.483 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %convert.68), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.105 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %exponential.1, f16[4,4,1024,1024]{3,2,1,0} %broadcast.483), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %dot.224 = f16[4,4,32,1024]{3,2,1,0} dot(f16[4,4,32,1024]{2,1,3,0} %transpose.307, f16[4,4,1024,1024]{3,2,1,0} %divide.105), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/dot_general[dimension_numbers=(((1,), (3,)), ((0, 2), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %transpose.310 = f16[4,1024,4,32]{1,3,2,0} transpose(f16[4,4,32,1024]{3,2,1,0} %dot.224), dimensions={0,3,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %reshape.1600 = f16[4096,128]{1,0} reshape(f16[4,1024,4,32]{1,3,2,0} %transpose.310)
  %param.26 = f32[128,1024]{1,0} parameter(21), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.69 = f16[128,1024]{1,0} convert(f32[128,1024]{1,0} %param.26), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.225 = f16[4096,1024]{1,0} dot(f16[4096,128]{1,0} %reshape.1600, f16[128,1024]{1,0} %convert.69), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.6 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.225), channel_id=7, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.4, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.1602 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.6), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.27 = f32[1024]{0} parameter(20), sharding={replicated}
  %convert.70 = f16[1024]{0} convert(f32[1024]{0} %param.27), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.977 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.70), dimensions={2}
  %add.124 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %reshape.1602, f16[4,1024,1024]{2,1,0} %broadcast.977), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %add.125 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.124, f16[4,1024,1024]{2,1,0} %convert.64), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=233}
  %convert.71 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.125), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reduce.12 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.71, f32[] %constant.69), dimensions={2}, to_apply=%region_10.768, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.210 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reduce.12, f32[4,1024]{1,0} %broadcast.936), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.485 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.210), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.33 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.71, f32[4,1024,1024]{2,1,0} %broadcast.485), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.211 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.71, f32[4,1024,1024]{2,1,0} %convert.71), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.13 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.211, f32[] %constant.69), dimensions={2}, to_apply=%region_11.780, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.212 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reduce.13, f32[4,1024]{1,0} %broadcast.936), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.213 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %multiply.210, f32[4,1024]{1,0} %multiply.210), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.34 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %multiply.212, f32[4,1024]{1,0} %multiply.213), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.3 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %subtract.34, f32[4,1024]{1,0} %broadcast.930), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %add.126 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.3, f32[4,1024]{1,0} %broadcast.937), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.15 = f32[4,1024]{1,0} rsqrt(f32[4,1024]{1,0} %add.126), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %broadcast.486 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %rsqrt.15), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param.28 = f32[1024]{0} parameter(19), sharding={replicated}
  %broadcast.978 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param.28), dimensions={2}
  %multiply.214 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.486, f32[4,1024,1024]{2,1,0} %broadcast.978), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.215 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.33, f32[4,1024,1024]{2,1,0} %multiply.214), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param.29 = f32[1024]{0} parameter(18), sharding={replicated}
  %broadcast.979 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param.29), dimensions={2}
  %add.127 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.215, f32[4,1024,1024]{2,1,0} %broadcast.979), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %convert.72 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.127), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %reshape.1609 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %convert.72)
  %param.30 = f32[1024,512]{1,0} parameter(25), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.73 = f16[1024,512]{1,0} convert(f32[1024,512]{1,0} %param.30), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.226 = f16[4096,512]{1,0} dot(f16[4096,1024]{1,0} %reshape.1609, f16[1024,512]{1,0} %convert.73), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.1611 = f16[4,1024,512]{2,1,0} reshape(f16[4096,512]{1,0} %dot.226), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.31 = f32[512]{0} parameter(24), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.74 = f16[512]{0} convert(f32[512]{0} %param.31), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.980 = f16[4,1024,512]{2,1,0} broadcast(f16[512]{0} %convert.74), dimensions={2}
  %add.128 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %reshape.1611, f16[4,1024,512]{2,1,0} %broadcast.980), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %multiply.216 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %add.128, f16[4,1024,512]{2,1,0} %broadcast.951), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.75 = f32[4,1024,512]{2,1,0} convert(f16[4,1024,512]{2,1,0} %multiply.216), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %clamp.1 = f32[4,1024,512]{2,1,0} clamp(f32[4,1024,512]{2,1,0} %broadcast.952, f32[4,1024,512]{2,1,0} %convert.75, f32[4,1024,512]{2,1,0} %broadcast.954), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.217 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.1, f32[4,1024,512]{2,1,0} %clamp.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.218 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %multiply.217, f32[4,1024,512]{2,1,0} %broadcast.956), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.129 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.218, f32[4,1024,512]{2,1,0} %broadcast.957), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.219 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.129, f32[4,1024,512]{2,1,0} %multiply.217), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.130 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.219, f32[4,1024,512]{2,1,0} %broadcast.958), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.220 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.130, f32[4,1024,512]{2,1,0} %multiply.217), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.131 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.220, f32[4,1024,512]{2,1,0} %broadcast.959), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.221 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.131, f32[4,1024,512]{2,1,0} %multiply.217), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.132 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.221, f32[4,1024,512]{2,1,0} %broadcast.960), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.222 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.132, f32[4,1024,512]{2,1,0} %multiply.217), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.133 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.222, f32[4,1024,512]{2,1,0} %broadcast.961), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.223 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.133, f32[4,1024,512]{2,1,0} %multiply.217), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.134 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.223, f32[4,1024,512]{2,1,0} %broadcast.962), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.224 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.134, f32[4,1024,512]{2,1,0} %multiply.217), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.135 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.224, f32[4,1024,512]{2,1,0} %broadcast.963), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.225 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.1, f32[4,1024,512]{2,1,0} %add.135), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.136 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.218, f32[4,1024,512]{2,1,0} %broadcast.964), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.226 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.136, f32[4,1024,512]{2,1,0} %multiply.217), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.137 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.226, f32[4,1024,512]{2,1,0} %broadcast.965), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.227 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.137, f32[4,1024,512]{2,1,0} %multiply.217), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.138 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.227, f32[4,1024,512]{2,1,0} %broadcast.966), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.228 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.138, f32[4,1024,512]{2,1,0} %multiply.217), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.139 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.228, f32[4,1024,512]{2,1,0} %broadcast.968), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.229 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.139, f32[4,1024,512]{2,1,0} %multiply.217), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.140 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.229, f32[4,1024,512]{2,1,0} %broadcast.970), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.106 = f32[4,1024,512]{2,1,0} divide(f32[4,1024,512]{2,1,0} %multiply.225, f32[4,1024,512]{2,1,0} %add.140), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.76 = f16[4,1024,512]{2,1,0} convert(f32[4,1024,512]{2,1,0} %divide.106), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.141 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %convert.76, f16[4,1024,512]{2,1,0} %broadcast.971), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.230 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %add.128, f16[4,1024,512]{2,1,0} %add.141), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.231 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.230, f16[4,1024,512]{2,1,0} %broadcast.972), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %reshape.1616 = f16[4096,512]{1,0} reshape(f16[4,1024,512]{2,1,0} %multiply.231)
  %param.32 = f32[512,1024]{1,0} parameter(29), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.77 = f16[512,1024]{1,0} convert(f32[512,1024]{1,0} %param.32), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.227 = f16[4096,1024]{1,0} dot(f16[4096,512]{1,0} %reshape.1616, f16[512,1024]{1,0} %convert.77), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.7 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.227), channel_id=8, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.5, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.1618 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.7), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.33 = f32[1024]{0} parameter(28), sharding={replicated}
  %convert.78 = f16[1024]{0} convert(f32[1024]{0} %param.33), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.981 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.78), dimensions={2}
  %add.142 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %reshape.1618, f16[4,1024,1024]{2,1,0} %broadcast.981), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %add.143 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.142, f16[4,1024,1024]{2,1,0} %convert.72), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=310}
  %convert.79 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.143), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reduce.14 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.79, f32[] %constant.69), dimensions={2}, to_apply=%region_12.860, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.232 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reduce.14, f32[4,1024]{1,0} %broadcast.936), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.491 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.232), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.38 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.79, f32[4,1024,1024]{2,1,0} %broadcast.491), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.233 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.79, f32[4,1024,1024]{2,1,0} %convert.79), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.15 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.233, f32[] %constant.69), dimensions={2}, to_apply=%region_13.872, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.234 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reduce.15, f32[4,1024]{1,0} %broadcast.936), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.235 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %multiply.232, f32[4,1024]{1,0} %multiply.232), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.39 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %multiply.234, f32[4,1024]{1,0} %multiply.235), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.4 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %subtract.39, f32[4,1024]{1,0} %broadcast.930), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %add.144 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.4, f32[4,1024]{1,0} %broadcast.937), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.16 = f32[4,1024]{1,0} rsqrt(f32[4,1024]{1,0} %add.144), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %broadcast.493 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %rsqrt.16), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param.34 = f32[1024]{0} parameter(27), sharding={replicated}
  %broadcast.982 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param.34), dimensions={2}
  %multiply.236 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.493, f32[4,1024,1024]{2,1,0} %broadcast.982), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.237 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.38, f32[4,1024,1024]{2,1,0} %multiply.236), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param.35 = f32[1024]{0} parameter(26), sharding={replicated}
  %broadcast.983 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param.35), dimensions={2}
  %add.145 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.237, f32[4,1024,1024]{2,1,0} %broadcast.983), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %convert.80 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.145), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/1/1/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %reshape.1627 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %convert.80)
  %param.36 = f32[1024,384]{1,0} parameter(35), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.81 = f16[1024,384]{1,0} convert(f32[1024,384]{1,0} %param.36), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.228 = f16[4096,384]{1,0} dot(f16[4096,1024]{1,0} %reshape.1627, f16[1024,384]{1,0} %convert.81), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.1629 = f16[4,1024,384]{2,1,0} reshape(f16[4096,384]{1,0} %dot.228), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.37 = f32[384]{0} parameter(34), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.82 = f16[384]{0} convert(f32[384]{0} %param.37), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.984 = f16[4,1024,384]{2,1,0} broadcast(f16[384]{0} %convert.82), dimensions={2}
  %add.146 = f16[4,1024,384]{2,1,0} add(f16[4,1024,384]{2,1,0} %reshape.1629, f16[4,1024,384]{2,1,0} %broadcast.984), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %reshape.1635 = f16[4,1024,128,3]{3,2,1,0} reshape(f16[4,1024,384]{2,1,0} %add.146), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/reshape[new_sizes=(8, 1024, 1024, 3) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=161}
  %slice.6 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.1635), slice={[0:4], [0:1024], [0:128], [1:2]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/slice[start_indices=(0, 0, 0, 1) limit_indices=(8, 1024, 1024, 2) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %reshape.1639 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %slice.6), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/reshape[new_sizes=(8, 1024, 32, 32) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=170}
  %transpose.311 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.1639), dimensions={0,2,3,1}
  %slice.7 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.1635), slice={[0:4], [0:1024], [0:128], [0:1]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/slice[start_indices=(0, 0, 0, 0) limit_indices=(8, 1024, 1024, 1) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %multiply.238 = f16[4,1024,128,1]{3,2,1,0} multiply(f16[4,1024,128,1]{3,2,1,0} %slice.7, f16[4,1024,128,1]{3,2,1,0} %broadcast.941), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %reshape.1642 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %multiply.238), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %transpose.312 = f16[4,4,1024,32]{3,1,2,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.1642), dimensions={0,2,1,3}
  %slice.8 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.1635), slice={[0:4], [0:1024], [0:128], [2:3]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/slice[start_indices=(0, 0, 0, 2) limit_indices=(8, 1024, 1024, 3) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %reshape.1645 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %slice.8), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/reshape[new_sizes=(8, 1024, 32, 32) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=173}
  %transpose.313 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.1645), dimensions={0,2,3,1}
  %dot.229 = f16[4,4,1024,1024]{3,2,1,0} dot(f16[4,4,1024,32]{3,1,2,0} %transpose.312, f16[4,4,32,1024]{2,1,3,0} %transpose.313), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/dot_general[dimension_numbers=(((3,), (3,)), ((0, 2), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %add.147 = f16[4,4,1024,1024]{3,2,1,0} add(f16[4,4,1024,1024]{3,2,1,0} %dot.229, f16[4,4,1024,1024]{3,2,1,0} %broadcast.946), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=100}
  %reduce.16 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %add.147, f16[] %constant.179), dimensions={3}, to_apply=%region_14.923, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.498 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %reduce.16), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.43 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %add.147, f16[4,4,1024,1024]{3,2,1,0} %broadcast.498), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.2 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.43), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.83 = f32[4,4,1024,1024]{3,2,1,0} convert(f16[4,4,1024,1024]{3,2,1,0} %exponential.2), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %reduce.17 = f32[4,4,1024]{2,1,0} reduce(f32[4,4,1024,1024]{3,2,1,0} %convert.83, f32[] %constant.69), dimensions={3}, to_apply=%region_15.935, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.84 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %reduce.17), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.499 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %convert.84), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.107 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %exponential.2, f16[4,4,1024,1024]{3,2,1,0} %broadcast.499), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %dot.230 = f16[4,4,32,1024]{3,2,1,0} dot(f16[4,4,32,1024]{2,1,3,0} %transpose.311, f16[4,4,1024,1024]{3,2,1,0} %divide.107), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/dot_general[dimension_numbers=(((1,), (3,)), ((0, 2), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %transpose.314 = f16[4,1024,4,32]{1,3,2,0} transpose(f16[4,4,32,1024]{3,2,1,0} %dot.230), dimensions={0,3,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %reshape.1646 = f16[4096,128]{1,0} reshape(f16[4,1024,4,32]{1,3,2,0} %transpose.314)
  %param.38 = f32[128,1024]{1,0} parameter(33), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.85 = f16[128,1024]{1,0} convert(f32[128,1024]{1,0} %param.38), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.231 = f16[4096,1024]{1,0} dot(f16[4096,128]{1,0} %reshape.1646, f16[128,1024]{1,0} %convert.85), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.8 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.231), channel_id=9, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.6, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.1648 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.8), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.39 = f32[1024]{0} parameter(32), sharding={replicated}
  %convert.86 = f16[1024]{0} convert(f32[1024]{0} %param.39), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.986 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.86), dimensions={2}
  %add.148 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %reshape.1648, f16[4,1024,1024]{2,1,0} %broadcast.986), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %add.149 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.148, f16[4,1024,1024]{2,1,0} %convert.80), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=233}
  %convert.87 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.149), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reduce.18 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.87, f32[] %constant.69), dimensions={2}, to_apply=%region_16.960, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.239 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reduce.18, f32[4,1024]{1,0} %broadcast.936), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.502 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.239), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.44 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.87, f32[4,1024,1024]{2,1,0} %broadcast.502), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.240 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.87, f32[4,1024,1024]{2,1,0} %convert.87), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.19 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.240, f32[] %constant.69), dimensions={2}, to_apply=%region_17.972, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.241 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reduce.19, f32[4,1024]{1,0} %broadcast.936), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.242 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %multiply.239, f32[4,1024]{1,0} %multiply.239), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.45 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %multiply.241, f32[4,1024]{1,0} %multiply.242), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.5 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %subtract.45, f32[4,1024]{1,0} %broadcast.930), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %add.150 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.5, f32[4,1024]{1,0} %broadcast.937), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.17 = f32[4,1024]{1,0} rsqrt(f32[4,1024]{1,0} %add.150), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %broadcast.505 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %rsqrt.17), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param.40 = f32[1024]{0} parameter(31), sharding={replicated}
  %broadcast.989 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param.40), dimensions={2}
  %multiply.243 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.505, f32[4,1024,1024]{2,1,0} %broadcast.989), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.244 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.44, f32[4,1024,1024]{2,1,0} %multiply.243), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param.41 = f32[1024]{0} parameter(30), sharding={replicated}
  %broadcast.991 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param.41), dimensions={2}
  %add.151 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.244, f32[4,1024,1024]{2,1,0} %broadcast.991), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %convert.88 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.151), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %reshape.1659 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %convert.88)
  %param.42 = f32[1024,512]{1,0} parameter(37), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.89 = f16[1024,512]{1,0} convert(f32[1024,512]{1,0} %param.42), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.232 = f16[4096,512]{1,0} dot(f16[4096,1024]{1,0} %reshape.1659, f16[1024,512]{1,0} %convert.89), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.1662 = f16[4,1024,512]{2,1,0} reshape(f16[4096,512]{1,0} %dot.232), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.43 = f32[512]{0} parameter(36), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.90 = f16[512]{0} convert(f32[512]{0} %param.43), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.992 = f16[4,1024,512]{2,1,0} broadcast(f16[512]{0} %convert.90), dimensions={2}
  %add.152 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %reshape.1662, f16[4,1024,512]{2,1,0} %broadcast.992), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %multiply.245 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %add.152, f16[4,1024,512]{2,1,0} %broadcast.951), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.91 = f32[4,1024,512]{2,1,0} convert(f16[4,1024,512]{2,1,0} %multiply.245), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %clamp.2 = f32[4,1024,512]{2,1,0} clamp(f32[4,1024,512]{2,1,0} %broadcast.952, f32[4,1024,512]{2,1,0} %convert.91, f32[4,1024,512]{2,1,0} %broadcast.954), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.246 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.2, f32[4,1024,512]{2,1,0} %clamp.2), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.247 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %multiply.246, f32[4,1024,512]{2,1,0} %broadcast.956), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.153 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.247, f32[4,1024,512]{2,1,0} %broadcast.957), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.248 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.153, f32[4,1024,512]{2,1,0} %multiply.246), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.154 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.248, f32[4,1024,512]{2,1,0} %broadcast.958), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.249 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.154, f32[4,1024,512]{2,1,0} %multiply.246), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.155 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.249, f32[4,1024,512]{2,1,0} %broadcast.959), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.250 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.155, f32[4,1024,512]{2,1,0} %multiply.246), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.156 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.250, f32[4,1024,512]{2,1,0} %broadcast.960), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.251 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.156, f32[4,1024,512]{2,1,0} %multiply.246), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.157 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.251, f32[4,1024,512]{2,1,0} %broadcast.961), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.252 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.157, f32[4,1024,512]{2,1,0} %multiply.246), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.158 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.252, f32[4,1024,512]{2,1,0} %broadcast.962), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.253 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.158, f32[4,1024,512]{2,1,0} %multiply.246), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.159 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.253, f32[4,1024,512]{2,1,0} %broadcast.963), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.254 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.2, f32[4,1024,512]{2,1,0} %add.159), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.160 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.247, f32[4,1024,512]{2,1,0} %broadcast.964), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.255 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.160, f32[4,1024,512]{2,1,0} %multiply.246), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.161 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.255, f32[4,1024,512]{2,1,0} %broadcast.965), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.256 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.161, f32[4,1024,512]{2,1,0} %multiply.246), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.162 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.256, f32[4,1024,512]{2,1,0} %broadcast.966), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.257 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.162, f32[4,1024,512]{2,1,0} %multiply.246), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.163 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.257, f32[4,1024,512]{2,1,0} %broadcast.968), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.258 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.163, f32[4,1024,512]{2,1,0} %multiply.246), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.164 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.258, f32[4,1024,512]{2,1,0} %broadcast.970), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.108 = f32[4,1024,512]{2,1,0} divide(f32[4,1024,512]{2,1,0} %multiply.254, f32[4,1024,512]{2,1,0} %add.164), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.92 = f16[4,1024,512]{2,1,0} convert(f32[4,1024,512]{2,1,0} %divide.108), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.165 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %convert.92, f16[4,1024,512]{2,1,0} %broadcast.971), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.259 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %add.152, f16[4,1024,512]{2,1,0} %add.165), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.260 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.259, f16[4,1024,512]{2,1,0} %broadcast.972), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %reshape.1668 = f16[4096,512]{1,0} reshape(f16[4,1024,512]{2,1,0} %multiply.260)
  %param.44 = f32[512,1024]{1,0} parameter(41), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.93 = f16[512,1024]{1,0} convert(f32[512,1024]{1,0} %param.44), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.233 = f16[4096,1024]{1,0} dot(f16[4096,512]{1,0} %reshape.1668, f16[512,1024]{1,0} %convert.93), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.9 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.233), channel_id=10, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.7, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.1671 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.9), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.45 = f32[1024]{0} parameter(40), sharding={replicated}
  %convert.94 = f16[1024]{0} convert(f32[1024]{0} %param.45), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.993 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.94), dimensions={2}
  %add.166 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %reshape.1671, f16[4,1024,1024]{2,1,0} %broadcast.993), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %add.167 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.166, f16[4,1024,1024]{2,1,0} %convert.88), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=310}
  %convert.95 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.167), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reduce.20 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.95, f32[] %constant.69), dimensions={2}, to_apply=%region_18.1052, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.261 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reduce.20, f32[4,1024]{1,0} %broadcast.936), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.513 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.261), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.49 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.95, f32[4,1024,1024]{2,1,0} %broadcast.513), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.262 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.95, f32[4,1024,1024]{2,1,0} %convert.95), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.21 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.262, f32[] %constant.69), dimensions={2}, to_apply=%region_19.1064, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.263 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reduce.21, f32[4,1024]{1,0} %broadcast.936), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.264 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %multiply.261, f32[4,1024]{1,0} %multiply.261), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.50 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %multiply.263, f32[4,1024]{1,0} %multiply.264), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.6 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %subtract.50, f32[4,1024]{1,0} %broadcast.930), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %add.168 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.6, f32[4,1024]{1,0} %broadcast.937), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.18 = f32[4,1024]{1,0} rsqrt(f32[4,1024]{1,0} %add.168), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %broadcast.514 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %rsqrt.18), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param.46 = f32[1024]{0} parameter(39), sharding={replicated}
  %broadcast.995 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param.46), dimensions={2}
  %multiply.265 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.514, f32[4,1024,1024]{2,1,0} %broadcast.995), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.266 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.49, f32[4,1024,1024]{2,1,0} %multiply.265), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param.47 = f32[1024]{0} parameter(38), sharding={replicated}
  %broadcast.997 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param.47), dimensions={2}
  %add.169 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.266, f32[4,1024,1024]{2,1,0} %broadcast.997), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %convert.96 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.169), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/2/2/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %reshape.1682 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %convert.96)
  %param.48 = f32[1024,384]{1,0} parameter(47), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.97 = f16[1024,384]{1,0} convert(f32[1024,384]{1,0} %param.48), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.234 = f16[4096,384]{1,0} dot(f16[4096,1024]{1,0} %reshape.1682, f16[1024,384]{1,0} %convert.97), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.1685 = f16[4,1024,384]{2,1,0} reshape(f16[4096,384]{1,0} %dot.234), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.49 = f32[384]{0} parameter(46), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.98 = f16[384]{0} convert(f32[384]{0} %param.49), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.998 = f16[4,1024,384]{2,1,0} broadcast(f16[384]{0} %convert.98), dimensions={2}
  %add.170 = f16[4,1024,384]{2,1,0} add(f16[4,1024,384]{2,1,0} %reshape.1685, f16[4,1024,384]{2,1,0} %broadcast.998), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %reshape.1690 = f16[4,1024,128,3]{3,2,1,0} reshape(f16[4,1024,384]{2,1,0} %add.170), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/reshape[new_sizes=(8, 1024, 1024, 3) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=161}
  %slice.9 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.1690), slice={[0:4], [0:1024], [0:128], [1:2]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/slice[start_indices=(0, 0, 0, 1) limit_indices=(8, 1024, 1024, 2) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %reshape.1694 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %slice.9), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/reshape[new_sizes=(8, 1024, 32, 32) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=170}
  %transpose.315 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.1694), dimensions={0,2,3,1}
  %slice.10 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.1690), slice={[0:4], [0:1024], [0:128], [0:1]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/slice[start_indices=(0, 0, 0, 0) limit_indices=(8, 1024, 1024, 1) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %multiply.267 = f16[4,1024,128,1]{3,2,1,0} multiply(f16[4,1024,128,1]{3,2,1,0} %slice.10, f16[4,1024,128,1]{3,2,1,0} %broadcast.941), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %reshape.1698 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %multiply.267), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %transpose.316 = f16[4,4,1024,32]{3,1,2,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.1698), dimensions={0,2,1,3}
  %slice.11 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.1690), slice={[0:4], [0:1024], [0:128], [2:3]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/slice[start_indices=(0, 0, 0, 2) limit_indices=(8, 1024, 1024, 3) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %reshape.1701 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %slice.11), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/reshape[new_sizes=(8, 1024, 32, 32) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=173}
  %transpose.317 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.1701), dimensions={0,2,3,1}
  %dot.235 = f16[4,4,1024,1024]{3,2,1,0} dot(f16[4,4,1024,32]{3,1,2,0} %transpose.316, f16[4,4,32,1024]{2,1,3,0} %transpose.317), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/dot_general[dimension_numbers=(((3,), (3,)), ((0, 2), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %add.171 = f16[4,4,1024,1024]{3,2,1,0} add(f16[4,4,1024,1024]{3,2,1,0} %dot.235, f16[4,4,1024,1024]{3,2,1,0} %broadcast.946), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=100}
  %reduce.22 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %add.171, f16[] %constant.179), dimensions={3}, to_apply=%region_20.1115, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.518 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %reduce.22), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.54 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %add.171, f16[4,4,1024,1024]{3,2,1,0} %broadcast.518), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.3 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.54), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.99 = f32[4,4,1024,1024]{3,2,1,0} convert(f16[4,4,1024,1024]{3,2,1,0} %exponential.3), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %reduce.23 = f32[4,4,1024]{2,1,0} reduce(f32[4,4,1024,1024]{3,2,1,0} %convert.99, f32[] %constant.69), dimensions={3}, to_apply=%region_21.1127, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.100 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %reduce.23), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.520 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %convert.100), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.109 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %exponential.3, f16[4,4,1024,1024]{3,2,1,0} %broadcast.520), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %dot.236 = f16[4,4,32,1024]{3,2,1,0} dot(f16[4,4,32,1024]{2,1,3,0} %transpose.315, f16[4,4,1024,1024]{3,2,1,0} %divide.109), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/dot_general[dimension_numbers=(((1,), (3,)), ((0, 2), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %transpose.318 = f16[4,1024,4,32]{1,3,2,0} transpose(f16[4,4,32,1024]{3,2,1,0} %dot.236), dimensions={0,3,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %reshape.1702 = f16[4096,128]{1,0} reshape(f16[4,1024,4,32]{1,3,2,0} %transpose.318)
  %param.50 = f32[128,1024]{1,0} parameter(45), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.101 = f16[128,1024]{1,0} convert(f32[128,1024]{1,0} %param.50), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.237 = f16[4096,1024]{1,0} dot(f16[4096,128]{1,0} %reshape.1702, f16[128,1024]{1,0} %convert.101), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.10 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.237), channel_id=11, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.8, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.1704 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.10), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.51 = f32[1024]{0} parameter(44), sharding={replicated}
  %convert.102 = f16[1024]{0} convert(f32[1024]{0} %param.51), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.999 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.102), dimensions={2}
  %add.172 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %reshape.1704, f16[4,1024,1024]{2,1,0} %broadcast.999), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %add.173 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.172, f16[4,1024,1024]{2,1,0} %convert.96), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=233}
  %convert.103 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.173), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reduce.24 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.103, f32[] %constant.69), dimensions={2}, to_apply=%region_22.1152, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.268 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reduce.24, f32[4,1024]{1,0} %broadcast.936), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.523 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.268), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.55 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.103, f32[4,1024,1024]{2,1,0} %broadcast.523), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.269 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.103, f32[4,1024,1024]{2,1,0} %convert.103), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.25 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.269, f32[] %constant.69), dimensions={2}, to_apply=%region_23.1164, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.270 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reduce.25, f32[4,1024]{1,0} %broadcast.936), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.271 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %multiply.268, f32[4,1024]{1,0} %multiply.268), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.56 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %multiply.270, f32[4,1024]{1,0} %multiply.271), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.7 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %subtract.56, f32[4,1024]{1,0} %broadcast.930), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %add.174 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.7, f32[4,1024]{1,0} %broadcast.937), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.19 = f32[4,1024]{1,0} rsqrt(f32[4,1024]{1,0} %add.174), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %broadcast.524 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %rsqrt.19), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param.52 = f32[1024]{0} parameter(43), sharding={replicated}
  %broadcast.1000 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param.52), dimensions={2}
  %multiply.272 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.524, f32[4,1024,1024]{2,1,0} %broadcast.1000), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.273 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.55, f32[4,1024,1024]{2,1,0} %multiply.272), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param.53 = f32[1024]{0} parameter(42), sharding={replicated}
  %broadcast.1001 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param.53), dimensions={2}
  %add.175 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.273, f32[4,1024,1024]{2,1,0} %broadcast.1001), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %convert.104 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.175), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %reshape.1712 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %convert.104)
  %param.54 = f32[1024,512]{1,0} parameter(49), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.105 = f16[1024,512]{1,0} convert(f32[1024,512]{1,0} %param.54), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.238 = f16[4096,512]{1,0} dot(f16[4096,1024]{1,0} %reshape.1712, f16[1024,512]{1,0} %convert.105), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.1714 = f16[4,1024,512]{2,1,0} reshape(f16[4096,512]{1,0} %dot.238), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.55 = f32[512]{0} parameter(48), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.106 = f16[512]{0} convert(f32[512]{0} %param.55), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1002 = f16[4,1024,512]{2,1,0} broadcast(f16[512]{0} %convert.106), dimensions={2}
  %add.176 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %reshape.1714, f16[4,1024,512]{2,1,0} %broadcast.1002), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %multiply.274 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %add.176, f16[4,1024,512]{2,1,0} %broadcast.951), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.107 = f32[4,1024,512]{2,1,0} convert(f16[4,1024,512]{2,1,0} %multiply.274), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %clamp.3 = f32[4,1024,512]{2,1,0} clamp(f32[4,1024,512]{2,1,0} %broadcast.952, f32[4,1024,512]{2,1,0} %convert.107, f32[4,1024,512]{2,1,0} %broadcast.954), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.275 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.3, f32[4,1024,512]{2,1,0} %clamp.3), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.276 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %multiply.275, f32[4,1024,512]{2,1,0} %broadcast.956), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.177 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.276, f32[4,1024,512]{2,1,0} %broadcast.957), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.277 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.177, f32[4,1024,512]{2,1,0} %multiply.275), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.178 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.277, f32[4,1024,512]{2,1,0} %broadcast.958), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.278 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.178, f32[4,1024,512]{2,1,0} %multiply.275), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.179 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.278, f32[4,1024,512]{2,1,0} %broadcast.959), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.279 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.179, f32[4,1024,512]{2,1,0} %multiply.275), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.180 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.279, f32[4,1024,512]{2,1,0} %broadcast.960), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.280 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.180, f32[4,1024,512]{2,1,0} %multiply.275), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.181 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.280, f32[4,1024,512]{2,1,0} %broadcast.961), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.281 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.181, f32[4,1024,512]{2,1,0} %multiply.275), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.182 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.281, f32[4,1024,512]{2,1,0} %broadcast.962), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.282 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.182, f32[4,1024,512]{2,1,0} %multiply.275), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.183 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.282, f32[4,1024,512]{2,1,0} %broadcast.963), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.283 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.3, f32[4,1024,512]{2,1,0} %add.183), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.184 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.276, f32[4,1024,512]{2,1,0} %broadcast.964), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.284 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.184, f32[4,1024,512]{2,1,0} %multiply.275), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.185 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.284, f32[4,1024,512]{2,1,0} %broadcast.965), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.285 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.185, f32[4,1024,512]{2,1,0} %multiply.275), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.186 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.285, f32[4,1024,512]{2,1,0} %broadcast.966), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.286 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.186, f32[4,1024,512]{2,1,0} %multiply.275), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.187 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.286, f32[4,1024,512]{2,1,0} %broadcast.968), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.287 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.187, f32[4,1024,512]{2,1,0} %multiply.275), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.188 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.287, f32[4,1024,512]{2,1,0} %broadcast.970), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.110 = f32[4,1024,512]{2,1,0} divide(f32[4,1024,512]{2,1,0} %multiply.283, f32[4,1024,512]{2,1,0} %add.188), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.108 = f16[4,1024,512]{2,1,0} convert(f32[4,1024,512]{2,1,0} %divide.110), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.189 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %convert.108, f16[4,1024,512]{2,1,0} %broadcast.971), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.288 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %add.176, f16[4,1024,512]{2,1,0} %add.189), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.289 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.288, f16[4,1024,512]{2,1,0} %broadcast.972), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %reshape.1719 = f16[4096,512]{1,0} reshape(f16[4,1024,512]{2,1,0} %multiply.289)
  %param.56 = f32[512,1024]{1,0} parameter(53), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.109 = f16[512,1024]{1,0} convert(f32[512,1024]{1,0} %param.56), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.239 = f16[4096,1024]{1,0} dot(f16[4096,512]{1,0} %reshape.1719, f16[512,1024]{1,0} %convert.109), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.11 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.239), channel_id=12, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.9, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.1721 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.11), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.57 = f32[1024]{0} parameter(52), sharding={replicated}
  %convert.110 = f16[1024]{0} convert(f32[1024]{0} %param.57), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1004 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.110), dimensions={2}
  %add.190 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %reshape.1721, f16[4,1024,1024]{2,1,0} %broadcast.1004), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %add.191 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.190, f16[4,1024,1024]{2,1,0} %convert.104), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=310}
  %convert.111 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.191), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reduce.26 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.111, f32[] %constant.69), dimensions={2}, to_apply=%region_24.1244, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.290 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reduce.26, f32[4,1024]{1,0} %broadcast.936), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.529 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.290), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.60 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.111, f32[4,1024,1024]{2,1,0} %broadcast.529), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.291 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.111, f32[4,1024,1024]{2,1,0} %convert.111), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.27 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.291, f32[] %constant.69), dimensions={2}, to_apply=%region_25.1256, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.292 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reduce.27, f32[4,1024]{1,0} %broadcast.936), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.293 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %multiply.290, f32[4,1024]{1,0} %multiply.290), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.61 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %multiply.292, f32[4,1024]{1,0} %multiply.293), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.8 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %subtract.61, f32[4,1024]{1,0} %broadcast.930), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %add.192 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.8, f32[4,1024]{1,0} %broadcast.937), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.20 = f32[4,1024]{1,0} rsqrt(f32[4,1024]{1,0} %add.192), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %broadcast.530 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %rsqrt.20), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param.58 = f32[1024]{0} parameter(51), sharding={replicated}
  %broadcast.1006 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param.58), dimensions={2}
  %multiply.294 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.530, f32[4,1024,1024]{2,1,0} %broadcast.1006), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.295 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.60, f32[4,1024,1024]{2,1,0} %multiply.294), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param.59 = f32[1024]{0} parameter(50), sharding={replicated}
  %broadcast.1007 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param.59), dimensions={2}
  %add.193 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.295, f32[4,1024,1024]{2,1,0} %broadcast.1007), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %convert.112 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.193), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/3/3/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %reshape.1729 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %convert.112)
  %param.60 = f32[1024,384]{1,0} parameter(59), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.113 = f16[1024,384]{1,0} convert(f32[1024,384]{1,0} %param.60), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.240 = f16[4096,384]{1,0} dot(f16[4096,1024]{1,0} %reshape.1729, f16[1024,384]{1,0} %convert.113), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.1731 = f16[4,1024,384]{2,1,0} reshape(f16[4096,384]{1,0} %dot.240), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.61 = f32[384]{0} parameter(58), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.114 = f16[384]{0} convert(f32[384]{0} %param.61), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1008 = f16[4,1024,384]{2,1,0} broadcast(f16[384]{0} %convert.114), dimensions={2}
  %add.194 = f16[4,1024,384]{2,1,0} add(f16[4,1024,384]{2,1,0} %reshape.1731, f16[4,1024,384]{2,1,0} %broadcast.1008), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %reshape.1736 = f16[4,1024,128,3]{3,2,1,0} reshape(f16[4,1024,384]{2,1,0} %add.194), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/reshape[new_sizes=(8, 1024, 1024, 3) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=161}
  %slice.12 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.1736), slice={[0:4], [0:1024], [0:128], [1:2]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/slice[start_indices=(0, 0, 0, 1) limit_indices=(8, 1024, 1024, 2) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %reshape.1739 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %slice.12), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/reshape[new_sizes=(8, 1024, 32, 32) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=170}
  %transpose.319 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.1739), dimensions={0,2,3,1}
  %slice.13 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.1736), slice={[0:4], [0:1024], [0:128], [0:1]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/slice[start_indices=(0, 0, 0, 0) limit_indices=(8, 1024, 1024, 1) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %multiply.296 = f16[4,1024,128,1]{3,2,1,0} multiply(f16[4,1024,128,1]{3,2,1,0} %slice.13, f16[4,1024,128,1]{3,2,1,0} %broadcast.941), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %reshape.1743 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %multiply.296), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %transpose.320 = f16[4,4,1024,32]{3,1,2,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.1743), dimensions={0,2,1,3}
  %slice.14 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.1736), slice={[0:4], [0:1024], [0:128], [2:3]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/slice[start_indices=(0, 0, 0, 2) limit_indices=(8, 1024, 1024, 3) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %reshape.1746 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %slice.14), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/reshape[new_sizes=(8, 1024, 32, 32) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=173}
  %transpose.321 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.1746), dimensions={0,2,3,1}
  %dot.241 = f16[4,4,1024,1024]{3,2,1,0} dot(f16[4,4,1024,32]{3,1,2,0} %transpose.320, f16[4,4,32,1024]{2,1,3,0} %transpose.321), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/dot_general[dimension_numbers=(((3,), (3,)), ((0, 2), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %add.195 = f16[4,4,1024,1024]{3,2,1,0} add(f16[4,4,1024,1024]{3,2,1,0} %dot.241, f16[4,4,1024,1024]{3,2,1,0} %broadcast.946), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=100}
  %reduce.28 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %add.195, f16[] %constant.179), dimensions={3}, to_apply=%region_26.1307, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.534 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %reduce.28), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.65 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %add.195, f16[4,4,1024,1024]{3,2,1,0} %broadcast.534), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.4 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.65), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.115 = f32[4,4,1024,1024]{3,2,1,0} convert(f16[4,4,1024,1024]{3,2,1,0} %exponential.4), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %reduce.29 = f32[4,4,1024]{2,1,0} reduce(f32[4,4,1024,1024]{3,2,1,0} %convert.115, f32[] %constant.69), dimensions={3}, to_apply=%region_27.1319, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.116 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %reduce.29), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.536 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %convert.116), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.111 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %exponential.4, f16[4,4,1024,1024]{3,2,1,0} %broadcast.536), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %dot.242 = f16[4,4,32,1024]{3,2,1,0} dot(f16[4,4,32,1024]{2,1,3,0} %transpose.319, f16[4,4,1024,1024]{3,2,1,0} %divide.111), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/dot_general[dimension_numbers=(((1,), (3,)), ((0, 2), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %transpose.322 = f16[4,1024,4,32]{1,3,2,0} transpose(f16[4,4,32,1024]{3,2,1,0} %dot.242), dimensions={0,3,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %reshape.1747 = f16[4096,128]{1,0} reshape(f16[4,1024,4,32]{1,3,2,0} %transpose.322)
  %param.62 = f32[128,1024]{1,0} parameter(57), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.117 = f16[128,1024]{1,0} convert(f32[128,1024]{1,0} %param.62), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.243 = f16[4096,1024]{1,0} dot(f16[4096,128]{1,0} %reshape.1747, f16[128,1024]{1,0} %convert.117), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.12 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.243), channel_id=13, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.10, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.1749 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.12), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.63 = f32[1024]{0} parameter(56), sharding={replicated}
  %convert.118 = f16[1024]{0} convert(f32[1024]{0} %param.63), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1009 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.118), dimensions={2}
  %add.196 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %reshape.1749, f16[4,1024,1024]{2,1,0} %broadcast.1009), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %add.197 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.196, f16[4,1024,1024]{2,1,0} %convert.112), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=233}
  %convert.119 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.197), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reduce.30 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.119, f32[] %constant.69), dimensions={2}, to_apply=%region_28.1344, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.297 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reduce.30, f32[4,1024]{1,0} %broadcast.936), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.539 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.297), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.66 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.119, f32[4,1024,1024]{2,1,0} %broadcast.539), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.298 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.119, f32[4,1024,1024]{2,1,0} %convert.119), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.31 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.298, f32[] %constant.69), dimensions={2}, to_apply=%region_29.1356, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.299 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reduce.31, f32[4,1024]{1,0} %broadcast.936), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.300 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %multiply.297, f32[4,1024]{1,0} %multiply.297), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.67 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %multiply.299, f32[4,1024]{1,0} %multiply.300), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.9 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %subtract.67, f32[4,1024]{1,0} %broadcast.930), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %add.198 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.9, f32[4,1024]{1,0} %broadcast.937), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.21 = f32[4,1024]{1,0} rsqrt(f32[4,1024]{1,0} %add.198), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %broadcast.540 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %rsqrt.21), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param.64 = f32[1024]{0} parameter(55), sharding={replicated}
  %broadcast.1010 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param.64), dimensions={2}
  %multiply.301 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.540, f32[4,1024,1024]{2,1,0} %broadcast.1010), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.302 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.66, f32[4,1024,1024]{2,1,0} %multiply.301), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param.65 = f32[1024]{0} parameter(54), sharding={replicated}
  %broadcast.1011 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param.65), dimensions={2}
  %add.199 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.302, f32[4,1024,1024]{2,1,0} %broadcast.1011), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %convert.120 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.199), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %reshape.1756 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %convert.120)
  %param.66 = f32[1024,512]{1,0} parameter(61), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.121 = f16[1024,512]{1,0} convert(f32[1024,512]{1,0} %param.66), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.244 = f16[4096,512]{1,0} dot(f16[4096,1024]{1,0} %reshape.1756, f16[1024,512]{1,0} %convert.121), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.1758 = f16[4,1024,512]{2,1,0} reshape(f16[4096,512]{1,0} %dot.244), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.67 = f32[512]{0} parameter(60), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.122 = f16[512]{0} convert(f32[512]{0} %param.67), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1012 = f16[4,1024,512]{2,1,0} broadcast(f16[512]{0} %convert.122), dimensions={2}
  %add.200 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %reshape.1758, f16[4,1024,512]{2,1,0} %broadcast.1012), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %multiply.303 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %add.200, f16[4,1024,512]{2,1,0} %broadcast.951), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.123 = f32[4,1024,512]{2,1,0} convert(f16[4,1024,512]{2,1,0} %multiply.303), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %clamp.4 = f32[4,1024,512]{2,1,0} clamp(f32[4,1024,512]{2,1,0} %broadcast.952, f32[4,1024,512]{2,1,0} %convert.123, f32[4,1024,512]{2,1,0} %broadcast.954), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.304 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.4, f32[4,1024,512]{2,1,0} %clamp.4), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.305 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %multiply.304, f32[4,1024,512]{2,1,0} %broadcast.956), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.201 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.305, f32[4,1024,512]{2,1,0} %broadcast.957), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.306 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.201, f32[4,1024,512]{2,1,0} %multiply.304), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.202 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.306, f32[4,1024,512]{2,1,0} %broadcast.958), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.307 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.202, f32[4,1024,512]{2,1,0} %multiply.304), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.203 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.307, f32[4,1024,512]{2,1,0} %broadcast.959), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.308 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.203, f32[4,1024,512]{2,1,0} %multiply.304), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.204 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.308, f32[4,1024,512]{2,1,0} %broadcast.960), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.309 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.204, f32[4,1024,512]{2,1,0} %multiply.304), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.205 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.309, f32[4,1024,512]{2,1,0} %broadcast.961), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.310 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.205, f32[4,1024,512]{2,1,0} %multiply.304), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.206 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.310, f32[4,1024,512]{2,1,0} %broadcast.962), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.311 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.206, f32[4,1024,512]{2,1,0} %multiply.304), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.207 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.311, f32[4,1024,512]{2,1,0} %broadcast.963), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.312 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.4, f32[4,1024,512]{2,1,0} %add.207), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.208 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.305, f32[4,1024,512]{2,1,0} %broadcast.964), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.313 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.208, f32[4,1024,512]{2,1,0} %multiply.304), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.209 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.313, f32[4,1024,512]{2,1,0} %broadcast.965), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.314 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.209, f32[4,1024,512]{2,1,0} %multiply.304), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.210 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.314, f32[4,1024,512]{2,1,0} %broadcast.966), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.315 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.210, f32[4,1024,512]{2,1,0} %multiply.304), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.211 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.315, f32[4,1024,512]{2,1,0} %broadcast.968), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.316 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.211, f32[4,1024,512]{2,1,0} %multiply.304), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.212 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.316, f32[4,1024,512]{2,1,0} %broadcast.970), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.112 = f32[4,1024,512]{2,1,0} divide(f32[4,1024,512]{2,1,0} %multiply.312, f32[4,1024,512]{2,1,0} %add.212), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.124 = f16[4,1024,512]{2,1,0} convert(f32[4,1024,512]{2,1,0} %divide.112), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.213 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %convert.124, f16[4,1024,512]{2,1,0} %broadcast.971), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.317 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %add.200, f16[4,1024,512]{2,1,0} %add.213), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.318 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.317, f16[4,1024,512]{2,1,0} %broadcast.972), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %reshape.1763 = f16[4096,512]{1,0} reshape(f16[4,1024,512]{2,1,0} %multiply.318)
  %param.68 = f32[512,1024]{1,0} parameter(65), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.125 = f16[512,1024]{1,0} convert(f32[512,1024]{1,0} %param.68), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.245 = f16[4096,1024]{1,0} dot(f16[4096,512]{1,0} %reshape.1763, f16[512,1024]{1,0} %convert.125), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.13 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.245), channel_id=14, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.11, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.1765 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.13), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.69 = f32[1024]{0} parameter(64), sharding={replicated}
  %convert.126 = f16[1024]{0} convert(f32[1024]{0} %param.69), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1013 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.126), dimensions={2}
  %add.214 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %reshape.1765, f16[4,1024,1024]{2,1,0} %broadcast.1013), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %add.215 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.214, f16[4,1024,1024]{2,1,0} %convert.120), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=310}
  %convert.127 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.215), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reduce.32 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.127, f32[] %constant.69), dimensions={2}, to_apply=%region_30.1436, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.319 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reduce.32, f32[4,1024]{1,0} %broadcast.936), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.546 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.319), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.71 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.127, f32[4,1024,1024]{2,1,0} %broadcast.546), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.320 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.127, f32[4,1024,1024]{2,1,0} %convert.127), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.33 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.320, f32[] %constant.69), dimensions={2}, to_apply=%region_31.1448, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.321 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reduce.33, f32[4,1024]{1,0} %broadcast.936), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.322 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %multiply.319, f32[4,1024]{1,0} %multiply.319), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.72 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %multiply.321, f32[4,1024]{1,0} %multiply.322), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.10 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %subtract.72, f32[4,1024]{1,0} %broadcast.930), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %add.216 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.10, f32[4,1024]{1,0} %broadcast.937), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.22 = f32[4,1024]{1,0} rsqrt(f32[4,1024]{1,0} %add.216), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %broadcast.548 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %rsqrt.22), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param.70 = f32[1024]{0} parameter(63), sharding={replicated}
  %broadcast.1014 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param.70), dimensions={2}
  %multiply.323 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.548, f32[4,1024,1024]{2,1,0} %broadcast.1014), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.324 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.71, f32[4,1024,1024]{2,1,0} %multiply.323), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param.71 = f32[1024]{0} parameter(62), sharding={replicated}
  %broadcast.1015 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param.71), dimensions={2}
  %add.217 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.324, f32[4,1024,1024]{2,1,0} %broadcast.1015), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %convert.128 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.217), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/4/4/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %reshape.1772 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %convert.128)
  %param.72 = f32[1024,384]{1,0} parameter(71), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.129 = f16[1024,384]{1,0} convert(f32[1024,384]{1,0} %param.72), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.246 = f16[4096,384]{1,0} dot(f16[4096,1024]{1,0} %reshape.1772, f16[1024,384]{1,0} %convert.129), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.1774 = f16[4,1024,384]{2,1,0} reshape(f16[4096,384]{1,0} %dot.246), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.73 = f32[384]{0} parameter(70), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.130 = f16[384]{0} convert(f32[384]{0} %param.73), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1016 = f16[4,1024,384]{2,1,0} broadcast(f16[384]{0} %convert.130), dimensions={2}
  %add.218 = f16[4,1024,384]{2,1,0} add(f16[4,1024,384]{2,1,0} %reshape.1774, f16[4,1024,384]{2,1,0} %broadcast.1016), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %reshape.1779 = f16[4,1024,128,3]{3,2,1,0} reshape(f16[4,1024,384]{2,1,0} %add.218), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/reshape[new_sizes=(8, 1024, 1024, 3) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=161}
  %slice.15 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.1779), slice={[0:4], [0:1024], [0:128], [1:2]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/slice[start_indices=(0, 0, 0, 1) limit_indices=(8, 1024, 1024, 2) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %reshape.1782 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %slice.15), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/reshape[new_sizes=(8, 1024, 32, 32) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=170}
  %transpose.323 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.1782), dimensions={0,2,3,1}
  %slice.16 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.1779), slice={[0:4], [0:1024], [0:128], [0:1]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/slice[start_indices=(0, 0, 0, 0) limit_indices=(8, 1024, 1024, 1) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %multiply.325 = f16[4,1024,128,1]{3,2,1,0} multiply(f16[4,1024,128,1]{3,2,1,0} %slice.16, f16[4,1024,128,1]{3,2,1,0} %broadcast.941), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %reshape.1785 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %multiply.325), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %transpose.324 = f16[4,4,1024,32]{3,1,2,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.1785), dimensions={0,2,1,3}
  %slice.17 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.1779), slice={[0:4], [0:1024], [0:128], [2:3]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/slice[start_indices=(0, 0, 0, 2) limit_indices=(8, 1024, 1024, 3) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %reshape.1788 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %slice.17), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/reshape[new_sizes=(8, 1024, 32, 32) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=173}
  %transpose.325 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.1788), dimensions={0,2,3,1}
  %dot.247 = f16[4,4,1024,1024]{3,2,1,0} dot(f16[4,4,1024,32]{3,1,2,0} %transpose.324, f16[4,4,32,1024]{2,1,3,0} %transpose.325), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/dot_general[dimension_numbers=(((3,), (3,)), ((0, 2), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %add.219 = f16[4,4,1024,1024]{3,2,1,0} add(f16[4,4,1024,1024]{3,2,1,0} %dot.247, f16[4,4,1024,1024]{3,2,1,0} %broadcast.946), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=100}
  %reduce.34 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %add.219, f16[] %constant.179), dimensions={3}, to_apply=%region_32.1499, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.552 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %reduce.34), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.76 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %add.219, f16[4,4,1024,1024]{3,2,1,0} %broadcast.552), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.5 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.76), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.131 = f32[4,4,1024,1024]{3,2,1,0} convert(f16[4,4,1024,1024]{3,2,1,0} %exponential.5), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %reduce.35 = f32[4,4,1024]{2,1,0} reduce(f32[4,4,1024,1024]{3,2,1,0} %convert.131, f32[] %constant.69), dimensions={3}, to_apply=%region_33.1511, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.132 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %reduce.35), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.553 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %convert.132), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.113 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %exponential.5, f16[4,4,1024,1024]{3,2,1,0} %broadcast.553), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %dot.248 = f16[4,4,32,1024]{3,2,1,0} dot(f16[4,4,32,1024]{2,1,3,0} %transpose.323, f16[4,4,1024,1024]{3,2,1,0} %divide.113), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/dot_general[dimension_numbers=(((1,), (3,)), ((0, 2), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %transpose.326 = f16[4,1024,4,32]{1,3,2,0} transpose(f16[4,4,32,1024]{3,2,1,0} %dot.248), dimensions={0,3,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %reshape.1789 = f16[4096,128]{1,0} reshape(f16[4,1024,4,32]{1,3,2,0} %transpose.326)
  %param.74 = f32[128,1024]{1,0} parameter(69), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.133 = f16[128,1024]{1,0} convert(f32[128,1024]{1,0} %param.74), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.249 = f16[4096,1024]{1,0} dot(f16[4096,128]{1,0} %reshape.1789, f16[128,1024]{1,0} %convert.133), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.14 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.249), channel_id=15, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.12, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.1791 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.14), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.75 = f32[1024]{0} parameter(68), sharding={replicated}
  %convert.134 = f16[1024]{0} convert(f32[1024]{0} %param.75), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1017 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.134), dimensions={2}
  %add.220 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %reshape.1791, f16[4,1024,1024]{2,1,0} %broadcast.1017), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %add.221 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.220, f16[4,1024,1024]{2,1,0} %convert.128), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=233}
  %convert.135 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.221), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reduce.36 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.135, f32[] %constant.69), dimensions={2}, to_apply=%region_34.1536, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.326 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reduce.36, f32[4,1024]{1,0} %broadcast.936), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.555 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.326), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.77 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.135, f32[4,1024,1024]{2,1,0} %broadcast.555), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.327 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.135, f32[4,1024,1024]{2,1,0} %convert.135), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.37 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.327, f32[] %constant.69), dimensions={2}, to_apply=%region_35.1548, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.328 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reduce.37, f32[4,1024]{1,0} %broadcast.936), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.329 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %multiply.326, f32[4,1024]{1,0} %multiply.326), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.78 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %multiply.328, f32[4,1024]{1,0} %multiply.329), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.11 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %subtract.78, f32[4,1024]{1,0} %broadcast.930), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %add.222 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.11, f32[4,1024]{1,0} %broadcast.937), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.23 = f32[4,1024]{1,0} rsqrt(f32[4,1024]{1,0} %add.222), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %broadcast.556 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %rsqrt.23), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param.76 = f32[1024]{0} parameter(67), sharding={replicated}
  %broadcast.1018 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param.76), dimensions={2}
  %multiply.330 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.556, f32[4,1024,1024]{2,1,0} %broadcast.1018), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.331 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.77, f32[4,1024,1024]{2,1,0} %multiply.330), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param.77 = f32[1024]{0} parameter(66), sharding={replicated}
  %broadcast.1019 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param.77), dimensions={2}
  %add.223 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.331, f32[4,1024,1024]{2,1,0} %broadcast.1019), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %convert.136 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.223), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %reshape.1802 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %convert.136)
  %param.78 = f32[1024,512]{1,0} parameter(73), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.137 = f16[1024,512]{1,0} convert(f32[1024,512]{1,0} %param.78), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.250 = f16[4096,512]{1,0} dot(f16[4096,1024]{1,0} %reshape.1802, f16[1024,512]{1,0} %convert.137), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.1805 = f16[4,1024,512]{2,1,0} reshape(f16[4096,512]{1,0} %dot.250), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.79 = f32[512]{0} parameter(72), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.138 = f16[512]{0} convert(f32[512]{0} %param.79), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1020 = f16[4,1024,512]{2,1,0} broadcast(f16[512]{0} %convert.138), dimensions={2}
  %add.224 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %reshape.1805, f16[4,1024,512]{2,1,0} %broadcast.1020), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %multiply.332 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %add.224, f16[4,1024,512]{2,1,0} %broadcast.951), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.139 = f32[4,1024,512]{2,1,0} convert(f16[4,1024,512]{2,1,0} %multiply.332), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %clamp.5 = f32[4,1024,512]{2,1,0} clamp(f32[4,1024,512]{2,1,0} %broadcast.952, f32[4,1024,512]{2,1,0} %convert.139, f32[4,1024,512]{2,1,0} %broadcast.954), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.333 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.5, f32[4,1024,512]{2,1,0} %clamp.5), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.334 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %multiply.333, f32[4,1024,512]{2,1,0} %broadcast.956), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.225 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.334, f32[4,1024,512]{2,1,0} %broadcast.957), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.335 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.225, f32[4,1024,512]{2,1,0} %multiply.333), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.226 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.335, f32[4,1024,512]{2,1,0} %broadcast.958), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.336 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.226, f32[4,1024,512]{2,1,0} %multiply.333), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.227 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.336, f32[4,1024,512]{2,1,0} %broadcast.959), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.337 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.227, f32[4,1024,512]{2,1,0} %multiply.333), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.228 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.337, f32[4,1024,512]{2,1,0} %broadcast.960), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.338 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.228, f32[4,1024,512]{2,1,0} %multiply.333), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.229 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.338, f32[4,1024,512]{2,1,0} %broadcast.961), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.339 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.229, f32[4,1024,512]{2,1,0} %multiply.333), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.230 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.339, f32[4,1024,512]{2,1,0} %broadcast.962), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.340 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.230, f32[4,1024,512]{2,1,0} %multiply.333), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.231 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.340, f32[4,1024,512]{2,1,0} %broadcast.963), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.341 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.5, f32[4,1024,512]{2,1,0} %add.231), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.232 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.334, f32[4,1024,512]{2,1,0} %broadcast.964), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.342 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.232, f32[4,1024,512]{2,1,0} %multiply.333), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.233 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.342, f32[4,1024,512]{2,1,0} %broadcast.965), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.343 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.233, f32[4,1024,512]{2,1,0} %multiply.333), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.234 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.343, f32[4,1024,512]{2,1,0} %broadcast.966), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.344 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.234, f32[4,1024,512]{2,1,0} %multiply.333), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.235 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.344, f32[4,1024,512]{2,1,0} %broadcast.968), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.345 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.235, f32[4,1024,512]{2,1,0} %multiply.333), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.236 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.345, f32[4,1024,512]{2,1,0} %broadcast.970), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.114 = f32[4,1024,512]{2,1,0} divide(f32[4,1024,512]{2,1,0} %multiply.341, f32[4,1024,512]{2,1,0} %add.236), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.140 = f16[4,1024,512]{2,1,0} convert(f32[4,1024,512]{2,1,0} %divide.114), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.237 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %convert.140, f16[4,1024,512]{2,1,0} %broadcast.971), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.346 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %add.224, f16[4,1024,512]{2,1,0} %add.237), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.347 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.346, f16[4,1024,512]{2,1,0} %broadcast.972), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %reshape.1812 = f16[4096,512]{1,0} reshape(f16[4,1024,512]{2,1,0} %multiply.347)
  %param.80 = f32[512,1024]{1,0} parameter(77), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %convert.141 = f16[512,1024]{1,0} convert(f32[512,1024]{1,0} %param.80), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.251 = f16[4096,1024]{1,0} dot(f16[4096,512]{1,0} %reshape.1812, f16[512,1024]{1,0} %convert.141), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.15 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.251), channel_id=16, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.13, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.1815 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.15), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %param.81 = f32[1024]{0} parameter(76), sharding={replicated}
  %convert.142 = f16[1024]{0} convert(f32[1024]{0} %param.81), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1021 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.142), dimensions={2}
  %add.238 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %reshape.1815, f16[4,1024,1024]{2,1,0} %broadcast.1021), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %add.239 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.238, f16[4,1024,1024]{2,1,0} %convert.136), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=310}
  %convert.143 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.239), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reduce.38 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.143, f32[] %constant.69), dimensions={2}, to_apply=%region_36.1628, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.348 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reduce.38, f32[4,1024]{1,0} %broadcast.936), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.563 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.348), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.82 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.143, f32[4,1024,1024]{2,1,0} %broadcast.563), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %multiply.349 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.143, f32[4,1024,1024]{2,1,0} %convert.143), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.39 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.349, f32[] %constant.69), dimensions={2}, to_apply=%region_37.1640, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.350 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reduce.39, f32[4,1024]{1,0} %broadcast.936), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.351 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %multiply.348, f32[4,1024]{1,0} %multiply.348), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.83 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %multiply.350, f32[4,1024]{1,0} %multiply.351), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.12 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %subtract.83, f32[4,1024]{1,0} %broadcast.930), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %add.240 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.12, f32[4,1024]{1,0} %broadcast.937), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.24 = f32[4,1024]{1,0} rsqrt(f32[4,1024]{1,0} %add.240), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %broadcast.564 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %rsqrt.24), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %param.82 = f32[1024]{0} parameter(75), sharding={replicated}
  %broadcast.1022 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param.82), dimensions={2}
  %multiply.352 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.564, f32[4,1024,1024]{2,1,0} %broadcast.1022), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.353 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.82, f32[4,1024,1024]{2,1,0} %multiply.352), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %param.83 = f32[1024]{0} parameter(74), sharding={replicated}
  %broadcast.1023 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %param.83), dimensions={2}
  %add.241 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.353, f32[4,1024,1024]{2,1,0} %broadcast.1023), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %convert.144 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.241), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/5/5/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %reshape.1823 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %convert.144)
  %dot.252 = f16[4096,6400]{1,0} dot(f16[4096,1024]{1,0} %reshape.1823, f16[6400,1024]{1,0} %convert.44), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/gpt_model.py" source_line=70}
  %reshape.1826 = f16[4,1024,6400]{2,1,0} reshape(f16[4096,6400]{1,0} %dot.252), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/gpt_model.py" source_line=70}
  %convert.145 = f16[6400]{0} convert(f32[6400]{0} %param.3), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/code/alpa/alpa/model/gpt_model.py" source_line=75}
  %broadcast.1024 = f16[4,1024,6400]{2,1,0} broadcast(f16[6400]{0} %convert.145), dimensions={2}
  %add.242 = f16[4,1024,6400]{2,1,0} add(f16[4,1024,6400]{2,1,0} %reshape.1826, f16[4,1024,6400]{2,1,0} %broadcast.1024), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/add" source_file="/code/alpa/alpa/model/gpt_model.py" source_line=75}
  %reduce.40 = f16[4,1024]{1,0} reduce(f16[4,1024,6400]{2,1,0} %add.242, f16[] %constant.179), dimensions={2}, to_apply=%region_38.1688, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/reduce_max[axes=(2,)]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %all-reduce.16 = f16[4,1024]{1,0} all-reduce(f16[4,1024]{1,0} %reduce.40), channel_id=17, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%region_38.1688, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/reduce_max[axes=(2,)]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %broadcast.1025 = f16[4,1024,6400]{2,1,0} broadcast(f16[4,1024]{1,0} %all-reduce.16), dimensions={0,1}
  %subtract.90 = f16[4,1024,6400]{2,1,0} subtract(f16[4,1024,6400]{2,1,0} %add.242, f16[4,1024,6400]{2,1,0} %broadcast.1025), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sub" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %exponential.6 = f16[4,1024,6400]{2,1,0} exponential(f16[4,1024,6400]{2,1,0} %subtract.90), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/exp" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %convert.146 = f32[4,1024,6400]{2,1,0} convert(f16[4,1024,6400]{2,1,0} %exponential.6), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %reduce.41 = f32[4,1024]{1,0} reduce(f32[4,1024,6400]{2,1,0} %convert.146, f32[] %constant.69), dimensions={2}, to_apply=%region_39.1700, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(2,)]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %all-reduce.17 = f32[4,1024]{1,0} all-reduce(f32[4,1024]{1,0} %reduce.41), channel_id=18, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%region_39.1700, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/reduce_sum[axes=(2,)]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %convert.147 = f16[4,1024]{1,0} convert(f32[4,1024]{1,0} %all-reduce.17), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %divide.115 = f16[4,1024]{1,0} divide(f16[4,1024]{1,0} %all-reduce.1, f16[4,1024]{1,0} %convert.147), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %broadcast.1026 = f16[4,1024,6400]{2,1,0} broadcast(f16[4,1024]{1,0} %divide.115), dimensions={0,1}
  %multiply.354 = f16[4,1024,6400]{2,1,0} multiply(f16[4,1024,6400]{2,1,0} %broadcast.1026, f16[4,1024,6400]{2,1,0} %exponential.6), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %add.243 = f16[4,1024,6400]{2,1,0} add(f16[4,1024,6400]{2,1,0} %convert.42, f16[4,1024,6400]{2,1,0} %multiply.354), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add_any" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=88}
  %reduce.42 = f16[6400]{0} reduce(f16[4,1024,6400]{2,1,0} %add.243, f16[] %constant.89), dimensions={0,1}, to_apply=%region_43.1737, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/reduce_sum[axes=(0, 1)]" source_file="/code/alpa/alpa/model/gpt_model.py" source_line=75}
  %all-reduce.18 = f16[6400]{0} all-reduce(f16[6400]{0} %reduce.42), channel_id=19, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_43.1737, sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/reduce_sum[axes=(0, 1)]" source_file="/code/alpa/alpa/model/gpt_model.py" source_line=75}
  %convert.148 = f32[6400]{0} convert(f16[6400]{0} %all-reduce.18), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/code/alpa/alpa/model/gpt_model.py" source_line=75}
  %constant.843 = f32[] constant(0.1)
  %broadcast.1027 = f32[6400]{0} broadcast(f32[] %constant.843), dimensions={}
  %multiply.355 = f32[6400]{0} multiply(f32[6400]{0} %convert.148, f32[6400]{0} %broadcast.1027), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.84 = f32[6400]{0} parameter(79), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %constant.846 = f32[] constant(0.9)
  %broadcast.1028 = f32[6400]{0} broadcast(f32[] %constant.846), dimensions={}
  %multiply.356 = f32[6400]{0} multiply(f32[6400]{0} %param.84, f32[6400]{0} %broadcast.1028), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.244 = f32[6400]{0} add(f32[6400]{0} %multiply.355, f32[6400]{0} %multiply.356), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.85 = s32[] parameter(78), sharding={replicated}
  %constant.849 = s32[] constant(2147483647)
  %compare.17 = pred[] compare(s32[] %param.85, s32[] %constant.849), direction=LT, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/lt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/numerics.py" source_line=118}
  %add.245 = s32[] add(s32[] %param.85, s32[] %constant.49), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/numerics.py" source_line=118}
  %select.5 = s32[] select(pred[] %compare.17, s32[] %add.245, s32[] %constant.849), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/numerics.py" source_line=118}
  %convert.149 = f32[] convert(s32[] %select.5), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/convert_element_type[new_dtype=float32 weak_type=True]" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=112}
  %power.0 = f32[] power(f32[] %constant.846, f32[] %convert.149), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/pow" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=112}
  %subtract.94 = f32[] subtract(f32[] %constant.68, f32[] %power.0), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sub" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=112}
  %broadcast.574 = f32[6400]{0} broadcast(f32[] %subtract.94), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %multiply.357 = f32[6400]{0} multiply(f32[6400]{0} %convert.148, f32[6400]{0} %convert.148), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %constant.850 = f32[] constant(0.001)
  %broadcast.1029 = f32[6400]{0} broadcast(f32[] %constant.850), dimensions={}
  %multiply.358 = f32[6400]{0} multiply(f32[6400]{0} %multiply.357, f32[6400]{0} %broadcast.1029), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.86 = f32[6400]{0} parameter(156), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %constant.853 = f32[] constant(0.999)
  %broadcast.1030 = f32[6400]{0} broadcast(f32[] %constant.853), dimensions={}
  %multiply.359 = f32[6400]{0} multiply(f32[6400]{0} %param.86, f32[6400]{0} %broadcast.1030), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.246 = f32[6400]{0} add(f32[6400]{0} %multiply.358, f32[6400]{0} %multiply.359), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %power.1 = f32[] power(f32[] %constant.853, f32[] %convert.149), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/pow" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=112}
  %subtract.95 = f32[] subtract(f32[] %constant.68, f32[] %power.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sub" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=112}
  %broadcast.577 = f32[6400]{0} broadcast(f32[] %subtract.95), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %divide.116 = f32[6400]{0} divide(f32[6400]{0} %add.246, f32[6400]{0} %broadcast.577), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.0 = f32[6400]{0} sqrt(f32[6400]{0} %divide.116), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %constant.856 = f32[] constant(1e-08)
  %broadcast.1031 = f32[6400]{0} broadcast(f32[] %constant.856), dimensions={}
  %add.247 = f32[6400]{0} add(f32[6400]{0} %sqrt.0, f32[6400]{0} %broadcast.1031), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.360 = f32[6400]{0} multiply(f32[6400]{0} %broadcast.574, f32[6400]{0} %add.247)
  %divide.117 = f32[6400]{0} divide(f32[6400]{0} %add.244, f32[6400]{0} %multiply.360), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %constant.859 = f32[] constant(-0.01)
  %broadcast.1032 = f32[6400]{0} broadcast(f32[] %constant.859), dimensions={}
  %multiply.361 = f32[6400]{0} multiply(f32[6400]{0} %divide.117, f32[6400]{0} %broadcast.1032), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.248 = f32[6400]{0} add(f32[6400]{0} %param.3, f32[6400]{0} %multiply.361), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %constant.862 = f16[] constant(5.6562)
  %broadcast.580 = f16[8,1,1,1024]{3,2,1,0} broadcast(f16[] %constant.179), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=184}
  %broadcast.581 = f16[8,1,1,1024]{3,2,1,0} broadcast(f16[] %constant.89), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=183}
  %constant.863 = f32[] constant(1024)
  %constant.864 = f16[] constant(1.4141)
  %constant.865 = f16[] constant(2)
  %broadcast.421 = f32[8,1024]{1,0} broadcast(f32[] %constant.69), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %broadcast.419 = f32[8,1024]{1,0} broadcast(f32[] %constant.68), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/broadcast_in_dim[shape=(8, 1024) broadcast_dimensions=()]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant.866 = f32[] constant(2)
  %broadcast.582 = f32[8,1024]{1,0} broadcast(f32[] %constant.866), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reshape.1856 = f16[4096,6400]{1,0} reshape(f16[4,1024,6400]{2,1,0} %add.243)
  %dot.253 = f16[4096,1024]{1,0} dot(f16[4096,6400]{1,0} %reshape.1856, f16[6400,1024]{1,0} %convert.44), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/gpt_model.py" source_line=70}
  %all-reduce.19 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.253), channel_id=20, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.14, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/gpt_model.py" source_line=70}
  %reshape.1858 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.19), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/gpt_model.py" source_line=70}
  %tuple = (f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) tuple(f16[] %constant.862, s32[] %constant.59, f16[8,1,1,1024]{3,2,1,0} %broadcast.580, f16[8,1,1,1024]{3,2,1,0} %broadcast.581, f32[] %constant.863, /*index=5*/f32[] %constant.69, f32[] %constant.863, f32[] %constant.124, f16[] %constant.864, f16[] %constant.337, /*index=10*/f16[] %constant.865, f32[] %constant.863, f32[] %constant.69, f32[] %constant.863, f32[] %constant.124, /*index=15*/f32[8,1024]{1,0} %broadcast.421, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.582, f32[8,1024]{1,0} %broadcast.421, /*index=20*/f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.582, f32[1024]{0} %param.77, f32[1024]{0} %param.76, /*index=25*/f32[1024]{0} %param.75, f32[128,1024]{1,0} %param.74, f32[384]{0} %param.73, f32[1024,384]{1,0} %param.72, f32[512]{0} %param.79, /*index=30*/f32[1024,512]{1,0} %param.78, f32[1024]{0} %param.82, f32[1024]{0} %param.81, f32[512,1024]{1,0} %param.80, f16[4,1024,1024]{2,1,0} %convert.128, /*index=35*/s32[4,1024]{1,0} %param.13, f16[4,1024,1024]{2,1,0} %reshape.1858), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %opt-barrier.0 = (f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) opt-barrier((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %tuple), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element = f16[4,1024,1024]{2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=34, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %reshape.1859 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %get-tuple-element)
  %get-tuple-element.1 = f32[1024,384]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=28, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.150 = f16[1024,384]{1,0} convert(f32[1024,384]{1,0} %get-tuple-element.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.254 = f16[4096,384]{1,0} dot(f16[4096,1024]{1,0} %reshape.1859, f16[1024,384]{1,0} %convert.150), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.1862 = f16[4,1024,384]{2,1,0} reshape(f16[4096,384]{1,0} %dot.254), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %get-tuple-element.2 = f32[384]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=27, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.151 = f16[384]{0} convert(f32[384]{0} %get-tuple-element.2), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1033 = f16[4,1024,384]{2,1,0} broadcast(f16[384]{0} %convert.151), dimensions={2}
  %add.249 = f16[4,1024,384]{2,1,0} add(f16[4,1024,384]{2,1,0} %reshape.1862, f16[4,1024,384]{2,1,0} %broadcast.1033), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %reshape.1868 = f16[4,1024,128,3]{3,2,1,0} reshape(f16[4,1024,384]{2,1,0} %add.249), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/reshape[new_sizes=(8, 1024, 1024, 3) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=161}
  %slice.18 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.1868), slice={[0:4], [0:1024], [0:128], [1:2]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/slice[start_indices=(0, 0, 0, 1) limit_indices=(8, 1024, 1024, 2) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %reshape.1871 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %slice.18), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/reshape[new_sizes=(8, 1024, 32, 32) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=170}
  %transpose.327 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.1871), dimensions={0,2,3,1}
  %slice.19 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.1868), slice={[0:4], [0:1024], [0:128], [0:1]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/slice[start_indices=(0, 0, 0, 0) limit_indices=(8, 1024, 1024, 1) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %get-tuple-element.3 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=0, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1034 = f16[4,1024,128,1]{3,2,1,0} broadcast(f16[] %get-tuple-element.3), dimensions={}
  %divide.118 = f16[4,1024,128,1]{3,2,1,0} divide(f16[4,1024,128,1]{3,2,1,0} %slice.19, f16[4,1024,128,1]{3,2,1,0} %broadcast.1034), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %reshape.1879 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %divide.118), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %transpose.328 = f16[4,4,1024,32]{3,1,2,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.1879), dimensions={0,2,1,3}
  %slice.20 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.1868), slice={[0:4], [0:1024], [0:128], [2:3]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/slice[start_indices=(0, 0, 0, 2) limit_indices=(8, 1024, 1024, 3) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %reshape.1883 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %slice.20), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/reshape[new_sizes=(8, 1024, 32, 32) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=173}
  %transpose.329 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.1883), dimensions={0,2,3,1}
  %dot.255 = f16[4,4,1024,1024]{3,2,1,0} dot(f16[4,4,1024,32]{3,1,2,0} %transpose.328, f16[4,4,32,1024]{2,1,3,0} %transpose.329), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/dot_general[dimension_numbers=(((3,), (3,)), ((0, 2), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %get-tuple-element.4 = s32[4,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=35, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.5 = s32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=1, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.587 = s32[4,1024]{1,0} broadcast(s32[] %get-tuple-element.5), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %compare.18 = pred[4,1024]{1,0} compare(s32[4,1024]{1,0} %get-tuple-element.4, s32[4,1024]{1,0} %broadcast.587), direction=GT, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %reshape.1885 = pred[4,1,1,1024]{3,2,1,0} reshape(pred[4,1024]{1,0} %compare.18), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %get-tuple-element.6 = f16[8,1,1,1024]{3,2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=3, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %constant.66 = s32[2]{0} constant({0, 4}), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/gt" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=86}
  %constant.65 = u32[16]{0} constant({...}), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/gt" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=86}
  %dynamic-slice.7 = u32[1]{0} dynamic-slice(u32[16]{0} %constant.65, u32[] %partition-id), dynamic_slice_sizes={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/gt" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=86}
  %reshape.1423 = u32[] reshape(u32[1]{0} %dynamic-slice.7), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/gt" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=86}
  %dynamic-slice.8 = s32[1]{0} dynamic-slice(s32[2]{0} %constant.66, u32[] %reshape.1423), dynamic_slice_sizes={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/gt" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=86}
  %reshape.1424 = s32[] reshape(s32[1]{0} %dynamic-slice.8), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/gt" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=86}
  %dynamic-slice.400 = f16[4,1,1,1024]{3,2,1,0} dynamic-slice(f16[8,1,1,1024]{3,2,1,0} %get-tuple-element.6, s32[] %reshape.1424, s32[] %constant.59, s32[] %constant.59, s32[] %constant.59), dynamic_slice_sizes={4,1,1,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %get-tuple-element.7 = f16[8,1,1,1024]{3,2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.403 = f16[4,1,1,1024]{3,2,1,0} dynamic-slice(f16[8,1,1,1024]{3,2,1,0} %get-tuple-element.7, s32[] %reshape.1424, s32[] %constant.59, s32[] %constant.59, s32[] %constant.59), dynamic_slice_sizes={4,1,1,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %select.6 = f16[4,1,1,1024]{3,2,1,0} select(pred[4,1,1,1024]{3,2,1,0} %reshape.1885, f16[4,1,1,1024]{3,2,1,0} %dynamic-slice.400, f16[4,1,1,1024]{3,2,1,0} %dynamic-slice.403), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %reshape.1892 = f16[4,1024]{1,0} reshape(f16[4,1,1,1024]{3,2,1,0} %select.6), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=100}
  %broadcast.1035 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,1024]{1,0} %reshape.1892), dimensions={0,3}
  %add.250 = f16[4,4,1024,1024]{3,2,1,0} add(f16[4,4,1024,1024]{3,2,1,0} %dot.255, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1035), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=100}
  %reduce.43 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %add.250, f16[] %constant.179), dimensions={3}, to_apply=%region_45.1817, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.589 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %reduce.43), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.107 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %add.250, f16[4,4,1024,1024]{3,2,1,0} %broadcast.589), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.7 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.107), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.152 = f32[4,4,1024,1024]{3,2,1,0} convert(f16[4,4,1024,1024]{3,2,1,0} %exponential.7), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %reduce.44 = f32[4,4,1024]{2,1,0} reduce(f32[4,4,1024,1024]{3,2,1,0} %convert.152, f32[] %constant.69), dimensions={3}, to_apply=%region_46.1829, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.153 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %reduce.44), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.590 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %convert.153), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.119 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %exponential.7, f16[4,4,1024,1024]{3,2,1,0} %broadcast.590), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %dot.256 = f16[4,4,32,1024]{3,2,1,0} dot(f16[4,4,32,1024]{2,1,3,0} %transpose.327, f16[4,4,1024,1024]{3,2,1,0} %divide.119), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/dot_general[dimension_numbers=(((1,), (3,)), ((0, 2), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %transpose.330 = f16[4,1024,4,32]{1,3,2,0} transpose(f16[4,4,32,1024]{3,2,1,0} %dot.256), dimensions={0,3,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %reshape.1898 = f16[4096,128]{1,0} reshape(f16[4,1024,4,32]{1,3,2,0} %transpose.330)
  %get-tuple-element.8 = f32[128,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=26, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.154 = f16[128,1024]{1,0} convert(f32[128,1024]{1,0} %get-tuple-element.8), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.257 = f16[4096,1024]{1,0} dot(f16[4096,128]{1,0} %reshape.1898, f16[128,1024]{1,0} %convert.154), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.20 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.257), channel_id=21, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.15, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.1901 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.20), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %get-tuple-element.9 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=25, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.155 = f16[1024]{0} convert(f32[1024]{0} %get-tuple-element.9), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1036 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.155), dimensions={2}
  %add.251 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %reshape.1901, f16[4,1024,1024]{2,1,0} %broadcast.1036), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %add.252 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.251, f16[4,1024,1024]{2,1,0} %get-tuple-element), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=233}
  %convert.156 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.252), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %reduce.45 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.156, f32[] %constant.69), dimensions={2}, to_apply=%region_47.1854, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.10 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=4, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.592 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.10), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.120 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %reduce.45, f32[4,1024]{1,0} %broadcast.592), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.593 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.120), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.108 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.156, f32[4,1024,1024]{2,1,0} %broadcast.593), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %get-tuple-element.11 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=5, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.594 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.11), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.362 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.156, f32[4,1024,1024]{2,1,0} %convert.156), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.46 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.362, f32[] %constant.69), dimensions={2}, to_apply=%region_48.1867, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %get-tuple-element.12 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=6, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.595 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.12), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.121 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %reduce.46, f32[4,1024]{1,0} %broadcast.595), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.363 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.120, f32[4,1024]{1,0} %divide.120), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.109 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.121, f32[4,1024]{1,0} %multiply.363), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.13 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.594, f32[4,1024]{1,0} %subtract.109), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.13 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=7, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.596 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.13), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.253 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.13, f32[4,1024]{1,0} %broadcast.596), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.1905 = f32[4,1024,1]{2,1,0} reshape(f32[4,1024]{1,0} %add.253), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.25 = f32[4,1024,1]{2,1,0} rsqrt(f32[4,1024,1]{2,1,0} %reshape.1905), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.1906 = f32[4,1024]{1,0} reshape(f32[4,1024,1]{2,1,0} %rsqrt.25), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.597 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %reshape.1906), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %get-tuple-element.14 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=24, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1037 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %get-tuple-element.14), dimensions={2}
  %multiply.364 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.597, f32[4,1024,1024]{2,1,0} %broadcast.1037), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.365 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.108, f32[4,1024,1024]{2,1,0} %multiply.364), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %get-tuple-element.15 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=23, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1038 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %get-tuple-element.15), dimensions={2}
  %add.254 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.365, f32[4,1024,1024]{2,1,0} %broadcast.1038), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %convert.157 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.254), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %reshape.1911 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %convert.157)
  %get-tuple-element.16 = f32[1024,512]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=30, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.158 = f16[1024,512]{1,0} convert(f32[1024,512]{1,0} %get-tuple-element.16), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.258 = f16[4096,512]{1,0} dot(f16[4096,1024]{1,0} %reshape.1911, f16[1024,512]{1,0} %convert.158), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.1913 = f16[4,1024,512]{2,1,0} reshape(f16[4096,512]{1,0} %dot.258), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %get-tuple-element.17 = f32[512]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=29, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.159 = f16[512]{0} convert(f32[512]{0} %get-tuple-element.17), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1039 = f16[4,1024,512]{2,1,0} broadcast(f16[512]{0} %convert.159), dimensions={2}
  %add.255 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %reshape.1913, f16[4,1024,512]{2,1,0} %broadcast.1039), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %get-tuple-element.18 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=8, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1040 = f16[4,1024,512]{2,1,0} broadcast(f16[] %get-tuple-element.18), dimensions={}
  %divide.122 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %add.255, f16[4,1024,512]{2,1,0} %broadcast.1040), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.160 = f32[4,1024,512]{2,1,0} convert(f16[4,1024,512]{2,1,0} %divide.122), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %clamp.6 = f32[4,1024,512]{2,1,0} clamp(f32[4,1024,512]{2,1,0} %broadcast.952, f32[4,1024,512]{2,1,0} %convert.160, f32[4,1024,512]{2,1,0} %broadcast.954), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.366 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.6, f32[4,1024,512]{2,1,0} %clamp.6), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.367 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %multiply.366, f32[4,1024,512]{2,1,0} %broadcast.956), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.256 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.367, f32[4,1024,512]{2,1,0} %broadcast.957), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.368 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.256, f32[4,1024,512]{2,1,0} %multiply.366), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.257 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.368, f32[4,1024,512]{2,1,0} %broadcast.958), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.369 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.257, f32[4,1024,512]{2,1,0} %multiply.366), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.258 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.369, f32[4,1024,512]{2,1,0} %broadcast.959), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.370 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.258, f32[4,1024,512]{2,1,0} %multiply.366), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.259 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.370, f32[4,1024,512]{2,1,0} %broadcast.960), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.371 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.259, f32[4,1024,512]{2,1,0} %multiply.366), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.260 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.371, f32[4,1024,512]{2,1,0} %broadcast.961), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.372 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.260, f32[4,1024,512]{2,1,0} %multiply.366), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.261 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.372, f32[4,1024,512]{2,1,0} %broadcast.962), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.373 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.261, f32[4,1024,512]{2,1,0} %multiply.366), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.262 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.373, f32[4,1024,512]{2,1,0} %broadcast.963), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.374 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.6, f32[4,1024,512]{2,1,0} %add.262), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.263 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.367, f32[4,1024,512]{2,1,0} %broadcast.964), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.375 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.263, f32[4,1024,512]{2,1,0} %multiply.366), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.264 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.375, f32[4,1024,512]{2,1,0} %broadcast.965), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.376 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.264, f32[4,1024,512]{2,1,0} %multiply.366), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.265 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.376, f32[4,1024,512]{2,1,0} %broadcast.966), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.377 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.265, f32[4,1024,512]{2,1,0} %multiply.366), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.266 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.377, f32[4,1024,512]{2,1,0} %broadcast.968), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.378 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.266, f32[4,1024,512]{2,1,0} %multiply.366), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.267 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.378, f32[4,1024,512]{2,1,0} %broadcast.970), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.123 = f32[4,1024,512]{2,1,0} divide(f32[4,1024,512]{2,1,0} %multiply.374, f32[4,1024,512]{2,1,0} %add.267), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.161 = f16[4,1024,512]{2,1,0} convert(f32[4,1024,512]{2,1,0} %divide.123), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %get-tuple-element.19 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=9, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1041 = f16[4,1024,512]{2,1,0} broadcast(f16[] %get-tuple-element.19), dimensions={}
  %add.268 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %convert.161, f16[4,1024,512]{2,1,0} %broadcast.1041), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.379 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %add.255, f16[4,1024,512]{2,1,0} %add.268), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %get-tuple-element.20 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=10, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1042 = f16[4,1024,512]{2,1,0} broadcast(f16[] %get-tuple-element.20), dimensions={}
  %divide.124 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %multiply.379, f16[4,1024,512]{2,1,0} %broadcast.1042), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %reshape.1930 = f16[4096,512]{1,0} reshape(f16[4,1024,512]{2,1,0} %divide.124)
  %get-tuple-element.21 = f32[512,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=33, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.162 = f16[512,1024]{1,0} convert(f32[512,1024]{1,0} %get-tuple-element.21), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.259 = f16[4096,1024]{1,0} dot(f16[4096,512]{1,0} %reshape.1930, f16[512,1024]{1,0} %convert.162), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.21 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.259), channel_id=22, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.16, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.1932 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.21), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %get-tuple-element.22 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=32, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.163 = f16[1024]{0} convert(f32[1024]{0} %get-tuple-element.22), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1043 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.163), dimensions={2}
  %add.269 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %reshape.1932, f16[4,1024,1024]{2,1,0} %broadcast.1043), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %add.270 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.269, f16[4,1024,1024]{2,1,0} %convert.157), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=310}
  %convert.164 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.270), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %reduce.47 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.164, f32[] %constant.69), dimensions={2}, to_apply=%region_49.1953, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.23 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=11, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.609 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.23), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.125 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %reduce.47, f32[4,1024]{1,0} %broadcast.609), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.611 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.125), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.122 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.164, f32[4,1024,1024]{2,1,0} %broadcast.611), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %get-tuple-element.24 = f16[4,1024,1024]{2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=36, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.165 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %get-tuple-element.24), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %multiply.380 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.122, f32[4,1024,1024]{2,1,0} %convert.165), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %get-tuple-element.25 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=31, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1044 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %get-tuple-element.25), dimensions={2}
  %multiply.381 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %multiply.380, f32[4,1024,1024]{2,1,0} %broadcast.1044), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.48 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.381, f32[] %constant.69), dimensions={2}, to_apply=%region_53.2034, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reshape.1937 = f32[4,1024,1]{2,1,0} reshape(f32[4,1024]{1,0} %reduce.48), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reshape[new_sizes=(8, 1024, 1) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %get-tuple-element.26 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=12, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.614 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.26), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.382 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.164, f32[4,1024,1024]{2,1,0} %convert.164), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.49 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.382, f32[] %constant.69), dimensions={2}, to_apply=%region_50.1966, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %get-tuple-element.27 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=13, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.615 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.27), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.126 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %reduce.49, f32[4,1024]{1,0} %broadcast.615), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.383 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.125, f32[4,1024]{1,0} %divide.125), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.123 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.126, f32[4,1024]{1,0} %multiply.383), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.14 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.614, f32[4,1024]{1,0} %subtract.123), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.28 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=14, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.616 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.28), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.271 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.14, f32[4,1024]{1,0} %broadcast.616), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.1938 = f32[4,1024,1]{2,1,0} reshape(f32[4,1024]{1,0} %add.271), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.26 = f32[4,1024,1]{2,1,0} rsqrt(f32[4,1024,1]{2,1,0} %reshape.1938), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %divide.127 = f32[4,1024,1]{2,1,0} divide(f32[4,1024,1]{2,1,0} %rsqrt.26, f32[4,1024,1]{2,1,0} %reshape.1938), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %constant.1011 = f32[] constant(-0.5)
  %broadcast.1046 = f32[4,1024,1]{2,1,0} broadcast(f32[] %constant.1011), dimensions={}
  %multiply.384 = f32[4,1024,1]{2,1,0} multiply(f32[4,1024,1]{2,1,0} %divide.127, f32[4,1024,1]{2,1,0} %broadcast.1046), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.385 = f32[4,1024,1]{2,1,0} multiply(f32[4,1024,1]{2,1,0} %reshape.1937, f32[4,1024,1]{2,1,0} %multiply.384), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.1941 = f32[4,1024]{1,0} reshape(f32[4,1024,1]{2,1,0} %multiply.385), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reshape[new_sizes=(8, 1024) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=132}
  %compare.19 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %subtract.123, f32[4,1024]{1,0} %maximum.14), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.29 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=20, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.452 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.29, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.30 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=19, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.455 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.30, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.7 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.19, f32[4,1024]{1,0} %dynamic-slice.452, f32[4,1024]{1,0} %dynamic-slice.455), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %compare.20 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %broadcast.614, f32[4,1024]{1,0} %maximum.14), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.31 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=22, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.458 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.31, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.32 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=21, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.461 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.32, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.8 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.20, f32[4,1024]{1,0} %dynamic-slice.458, f32[4,1024]{1,0} %dynamic-slice.461), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.128 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %select.7, f32[4,1024]{1,0} %select.8), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.386 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reshape.1941, f32[4,1024]{1,0} %divide.128), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.129 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %multiply.386, f32[4,1024]{1,0} %broadcast.615), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %broadcast.1048 = f32[4,1024]{1,0} broadcast(f32[] %constant.866), dimensions={}
  %multiply.387 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.129, f32[4,1024]{1,0} %broadcast.1048), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %broadcast.618 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.387), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.388 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.164, f32[4,1024,1024]{2,1,0} %broadcast.618), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %negate.8 = f32[4,1024]{1,0} negate(f32[4,1024]{1,0} %multiply.386), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.389 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.125, f32[4,1024]{1,0} %broadcast.1048), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.390 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %negate.8, f32[4,1024]{1,0} %multiply.389), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reshape.1954 = f32[4,1024]{1,0} reshape(f32[4,1024,1]{2,1,0} %rsqrt.26), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.620 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %reshape.1954), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.391 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.620, f32[4,1024,1024]{2,1,0} %broadcast.1044), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.392 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.165, f32[4,1024,1024]{2,1,0} %multiply.391), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %negate.9 = f32[4,1024,1024]{2,1,0} negate(f32[4,1024,1024]{2,1,0} %multiply.392), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reduce.50 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %negate.9, f32[] %constant.69), dimensions={2}, to_apply=%region_54.2051, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %add.272 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %multiply.390, f32[4,1024]{1,0} %reduce.50), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=131}
  %divide.130 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %add.272, f32[4,1024]{1,0} %broadcast.609), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.622 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.130), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/broadcast_in_dim[shape=(8, 1024, 1024) broadcast_dimensions=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %add.273 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.388, f32[4,1024,1024]{2,1,0} %broadcast.622), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %convert.166 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.273), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %convert.167 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %multiply.392), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %add.274 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %convert.166, f16[4,1024,1024]{2,1,0} %convert.167), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reshape.1955 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %add.274)
  %dot.260 = f16[4096,512]{1,0} dot(f16[4096,1024]{1,0} %reshape.1955, f16[512,1024]{1,0} %convert.162), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %broadcast.1049 = f16[4096,512]{1,0} broadcast(f16[] %get-tuple-element.20), dimensions={}
  %divide.131 = f16[4096,512]{1,0} divide(f16[4096,512]{1,0} %dot.260, f16[4096,512]{1,0} %broadcast.1049), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %reshape.1963 = f16[4,1024,512]{2,1,0} reshape(f16[4096,512]{1,0} %divide.131), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.393 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %add.255, f16[4,1024,512]{2,1,0} %reshape.1963), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant.1039 = f16[] constant(1.1279)
  %broadcast.1050 = f16[4,1024,512]{2,1,0} broadcast(f16[] %constant.1039), dimensions={}
  %multiply.394 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.393, f16[4,1024,512]{2,1,0} %broadcast.1050), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.395 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %divide.122, f16[4,1024,512]{2,1,0} %divide.122), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %negate.10 = f16[4,1024,512]{2,1,0} negate(f16[4,1024,512]{2,1,0} %multiply.395), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/neg" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %exponential.8 = f16[4,1024,512]{2,1,0} exponential(f16[4,1024,512]{2,1,0} %negate.10), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/exp" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.396 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.394, f16[4,1024,512]{2,1,0} %exponential.8), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.132 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %multiply.396, f16[4,1024,512]{2,1,0} %broadcast.1040), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.397 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %reshape.1963, f16[4,1024,512]{2,1,0} %add.268), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.275 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %divide.132, f16[4,1024,512]{2,1,0} %multiply.397), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %reshape.1966 = f16[4096,512]{1,0} reshape(f16[4,1024,512]{2,1,0} %add.275)
  %dot.261 = f16[4096,1024]{1,0} dot(f16[4096,512]{1,0} %reshape.1966, f16[1024,512]{1,0} %convert.158), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.22 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.261), channel_id=23, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.17, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.1968 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.22), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.276 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.274, f16[4,1024,1024]{2,1,0} %reshape.1968), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.168 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.276), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %multiply.398 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.108, f32[4,1024,1024]{2,1,0} %convert.168), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.399 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %multiply.398, f32[4,1024,1024]{2,1,0} %broadcast.1037), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.51 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.399, f32[] %constant.69), dimensions={2}, to_apply=%region_59.2114, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reshape.1969 = f32[4,1024,1]{2,1,0} reshape(f32[4,1024]{1,0} %reduce.51), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reshape[new_sizes=(8, 1024, 1) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %divide.133 = f32[4,1024,1]{2,1,0} divide(f32[4,1024,1]{2,1,0} %rsqrt.25, f32[4,1024,1]{2,1,0} %reshape.1905), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.400 = f32[4,1024,1]{2,1,0} multiply(f32[4,1024,1]{2,1,0} %divide.133, f32[4,1024,1]{2,1,0} %broadcast.1046), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.401 = f32[4,1024,1]{2,1,0} multiply(f32[4,1024,1]{2,1,0} %reshape.1969, f32[4,1024,1]{2,1,0} %multiply.400), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.1970 = f32[4,1024]{1,0} reshape(f32[4,1024,1]{2,1,0} %multiply.401), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reshape[new_sizes=(8, 1024) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=132}
  %compare.21 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %subtract.109, f32[4,1024]{1,0} %maximum.13), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.33 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=16, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.477 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.33, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.34 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=15, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.480 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.34, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.9 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.21, f32[4,1024]{1,0} %dynamic-slice.477, f32[4,1024]{1,0} %dynamic-slice.480), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %compare.22 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %broadcast.594, f32[4,1024]{1,0} %maximum.13), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.35 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=18, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.483 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.35, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.36 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.0), index=17, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.486 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.36, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.10 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.22, f32[4,1024]{1,0} %dynamic-slice.483, f32[4,1024]{1,0} %dynamic-slice.486), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.134 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %select.9, f32[4,1024]{1,0} %select.10), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.402 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reshape.1970, f32[4,1024]{1,0} %divide.134), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.135 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %multiply.402, f32[4,1024]{1,0} %broadcast.595), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.403 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.135, f32[4,1024]{1,0} %broadcast.1048), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %broadcast.625 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.403), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.404 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.156, f32[4,1024,1024]{2,1,0} %broadcast.625), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %negate.11 = f32[4,1024]{1,0} negate(f32[4,1024]{1,0} %multiply.402), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.405 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.120, f32[4,1024]{1,0} %broadcast.1048), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.406 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %negate.11, f32[4,1024]{1,0} %multiply.405), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.407 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.168, f32[4,1024,1024]{2,1,0} %multiply.364), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %negate.12 = f32[4,1024,1024]{2,1,0} negate(f32[4,1024,1024]{2,1,0} %multiply.407), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reduce.52 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %negate.12, f32[] %constant.69), dimensions={2}, to_apply=%region_60.2131, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %add.277 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %multiply.406, f32[4,1024]{1,0} %reduce.52), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=131}
  %divide.136 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %add.277, f32[4,1024]{1,0} %broadcast.592), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.626 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.136), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/broadcast_in_dim[shape=(8, 1024, 1024) broadcast_dimensions=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %add.278 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.404, f32[4,1024,1024]{2,1,0} %broadcast.626), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %convert.169 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.278), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %convert.170 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %multiply.407), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %add.279 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %convert.169, f16[4,1024,1024]{2,1,0} %convert.170), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reshape.1980 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %add.279)
  %dot.262 = f16[4096,128]{1,0} dot(f16[4096,1024]{1,0} %reshape.1980, f16[128,1024]{1,0} %convert.154), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.1984 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4096,128]{1,0} %dot.262), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/reshape[new_sizes=(8, 1024, 32, 32) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=208}
  %transpose.331 = f16[4,4,1024,32]{3,1,2,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.1984), dimensions={0,2,1,3}
  %dot.263 = f16[4,4,1024,1024]{3,2,1,0} dot(f16[4,4,1024,32]{3,1,2,0} %transpose.331, f16[4,4,32,1024]{2,1,3,0} %transpose.327), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/dot_general[dimension_numbers=(((2,), (3,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %broadcast.1051 = f16[4,4,1024]{2,1,0} broadcast(f16[] %constant.337), dimensions={}
  %multiply.408 = f16[4,4,1024]{2,1,0} multiply(f16[4,4,1024]{2,1,0} %convert.153, f16[4,4,1024]{2,1,0} %convert.153), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.137 = f16[4,4,1024]{2,1,0} divide(f16[4,4,1024]{2,1,0} %broadcast.1051, f16[4,4,1024]{2,1,0} %multiply.408), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.628 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %divide.137), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.409 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %dot.263, f16[4,4,1024,1024]{3,2,1,0} %broadcast.628), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.410 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %multiply.409, f16[4,4,1024,1024]{3,2,1,0} %exponential.7), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %reduce.53 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %multiply.410, f16[] %constant.89), dimensions={3}, to_apply=%region_62.2162, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %negate.13 = f16[4,4,1024]{2,1,0} negate(f16[4,4,1024]{2,1,0} %reduce.53), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.629 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %negate.13), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/broadcast_in_dim[shape=(8, 32, 1024, 1024) broadcast_dimensions=(0, 1, 2)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.138 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %dot.263, f16[4,4,1024,1024]{3,2,1,0} %broadcast.590), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %add.280 = f16[4,4,1024,1024]{3,2,1,0} add(f16[4,4,1024,1024]{3,2,1,0} %broadcast.629, f16[4,4,1024,1024]{3,2,1,0} %divide.138), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.411 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %add.280, f16[4,4,1024,1024]{3,2,1,0} %exponential.7), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %dot.264 = f16[4,4,1024,32]{3,2,1,0} dot(f16[4,4,1024,1024]{3,2,1,0} %multiply.411, f16[4,4,1024,32]{3,1,2,0} %transpose.328), lhs_batch_dims={0,1}, lhs_contracting_dims={2}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/dot_general[dimension_numbers=(((2,), (1,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %transpose.332 = f16[4,1024,4,32]{3,1,2,0} transpose(f16[4,4,1024,32]{3,2,1,0} %dot.264), dimensions={0,2,1,3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/transpose[permutation=(0, 2, 1, 3)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %reshape.1988 = f16[4,1024,128,1]{3,2,1,0} reshape(f16[4,1024,4,32]{3,1,2,0} %transpose.332), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=173}
  %pad = f16[4,1024,128,3]{3,2,1,0} pad(f16[4,1024,128,1]{3,2,1,0} %reshape.1988, f16[] %constant.89), padding=0_0x0_0x0_0x2_0, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (2, 0, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.333 = f16[4,4,1024,32]{3,1,2,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.1883), dimensions={0,2,1,3}
  %dot.265 = f16[4,4,1024,32]{3,2,1,0} dot(f16[4,4,1024,1024]{3,2,1,0} %multiply.411, f16[4,4,1024,32]{3,1,2,0} %transpose.333), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/dot_general[dimension_numbers=(((3,), (1,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %broadcast.1052 = f16[4,4,1024,32]{3,2,1,0} broadcast(f16[] %get-tuple-element.3), dimensions={}
  %divide.139 = f16[4,4,1024,32]{3,2,1,0} divide(f16[4,4,1024,32]{3,2,1,0} %dot.265, f16[4,4,1024,32]{3,2,1,0} %broadcast.1052), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %transpose.334 = f16[4,1024,4,32]{3,1,2,0} transpose(f16[4,4,1024,32]{3,2,1,0} %divide.139), dimensions={0,2,1,3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %reshape.1995 = f16[4,1024,128,1]{3,2,1,0} reshape(f16[4,1024,4,32]{3,1,2,0} %transpose.334), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=167}
  %pad.1 = f16[4,1024,128,3]{3,2,1,0} pad(f16[4,1024,128,1]{3,2,1,0} %reshape.1995, f16[] %constant.89), padding=0_0x0_0x0_0x0_2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 2, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %add.281 = f16[4,1024,128,3]{3,2,1,0} add(f16[4,1024,128,3]{3,2,1,0} %pad, f16[4,1024,128,3]{3,2,1,0} %pad.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.335 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.1984), dimensions={0,2,3,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/transpose[permutation=(0, 2, 3, 1)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %dot.266 = f16[4,4,32,1024]{3,2,1,0} dot(f16[4,4,32,1024]{2,1,3,0} %transpose.335, f16[4,4,1024,1024]{3,2,1,0} %divide.119), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/dot_general[dimension_numbers=(((3,), (2,)), ((0, 1), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %transpose.336 = f16[4,1024,4,32]{1,3,2,0} transpose(f16[4,4,32,1024]{3,2,1,0} %dot.266), dimensions={0,3,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %reshape.1998 = f16[4,1024,128,1]{3,2,1,0} reshape(f16[4,1024,4,32]{1,3,2,0} %transpose.336), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=170}
  %pad.2 = f16[4,1024,128,3]{3,2,1,0} pad(f16[4,1024,128,1]{3,2,1,0} %reshape.1998, f16[] %constant.89), padding=0_0x0_0x0_0x1_1, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (1, 1, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %add.282 = f16[4,1024,128,3]{3,2,1,0} add(f16[4,1024,128,3]{3,2,1,0} %add.281, f16[4,1024,128,3]{3,2,1,0} %pad.2), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %reshape.2001 = f16[4096,384]{1,0} reshape(f16[4,1024,128,3]{3,2,1,0} %add.282)
  %dot.267 = f16[4096,1024]{1,0} dot(f16[4096,384]{1,0} %reshape.2001, f16[1024,384]{1,0} %convert.150), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.23 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.267), channel_id=24, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.18, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2003 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.23), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.283 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.279, f16[4,1024,1024]{2,1,0} %reshape.2003), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %tuple.1 = (f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) tuple(f16[] %constant.862, s32[] %constant.59, f16[8,1,1,1024]{3,2,1,0} %broadcast.580, f16[8,1,1,1024]{3,2,1,0} %broadcast.581, f32[] %constant.863, /*index=5*/f32[] %constant.69, f32[] %constant.863, f32[] %constant.124, f16[] %constant.864, f16[] %constant.337, /*index=10*/f16[] %constant.865, f32[] %constant.863, f32[] %constant.69, f32[] %constant.863, f32[] %constant.124, /*index=15*/f32[8,1024]{1,0} %broadcast.421, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.582, f32[8,1024]{1,0} %broadcast.421, /*index=20*/f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.582, f32[1024]{0} %param.65, f32[1024]{0} %param.64, /*index=25*/f32[1024]{0} %param.63, f32[128,1024]{1,0} %param.62, f32[384]{0} %param.61, f32[1024,384]{1,0} %param.60, f32[512]{0} %param.67, /*index=30*/f32[1024,512]{1,0} %param.66, f32[1024]{0} %param.70, f32[1024]{0} %param.69, f32[512,1024]{1,0} %param.68, f16[4,1024,1024]{2,1,0} %convert.112, /*index=35*/s32[4,1024]{1,0} %param.13, f16[4,1024,1024]{2,1,0} %add.283), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %opt-barrier.1 = (f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) opt-barrier((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %tuple.1), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.37 = f16[4,1024,1024]{2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=34, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %reshape.2004 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %get-tuple-element.37)
  %get-tuple-element.38 = f32[1024,384]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=28, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.171 = f16[1024,384]{1,0} convert(f32[1024,384]{1,0} %get-tuple-element.38), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.268 = f16[4096,384]{1,0} dot(f16[4096,1024]{1,0} %reshape.2004, f16[1024,384]{1,0} %convert.171), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2006 = f16[4,1024,384]{2,1,0} reshape(f16[4096,384]{1,0} %dot.268), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %get-tuple-element.39 = f32[384]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=27, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.172 = f16[384]{0} convert(f32[384]{0} %get-tuple-element.39), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1053 = f16[4,1024,384]{2,1,0} broadcast(f16[384]{0} %convert.172), dimensions={2}
  %add.284 = f16[4,1024,384]{2,1,0} add(f16[4,1024,384]{2,1,0} %reshape.2006, f16[4,1024,384]{2,1,0} %broadcast.1053), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %reshape.2011 = f16[4,1024,128,3]{3,2,1,0} reshape(f16[4,1024,384]{2,1,0} %add.284), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/reshape[new_sizes=(8, 1024, 1024, 3) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=161}
  %slice.21 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.2011), slice={[0:4], [0:1024], [0:128], [1:2]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/slice[start_indices=(0, 0, 0, 1) limit_indices=(8, 1024, 1024, 2) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %reshape.2014 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %slice.21), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/reshape[new_sizes=(8, 1024, 32, 32) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=170}
  %transpose.337 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.2014), dimensions={0,2,3,1}
  %slice.22 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.2011), slice={[0:4], [0:1024], [0:128], [0:1]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/slice[start_indices=(0, 0, 0, 0) limit_indices=(8, 1024, 1024, 1) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %get-tuple-element.40 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=0, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1054 = f16[4,1024,128,1]{3,2,1,0} broadcast(f16[] %get-tuple-element.40), dimensions={}
  %divide.140 = f16[4,1024,128,1]{3,2,1,0} divide(f16[4,1024,128,1]{3,2,1,0} %slice.22, f16[4,1024,128,1]{3,2,1,0} %broadcast.1054), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %reshape.2021 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %divide.140), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %transpose.338 = f16[4,4,1024,32]{3,1,2,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.2021), dimensions={0,2,1,3}
  %slice.23 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.2011), slice={[0:4], [0:1024], [0:128], [2:3]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/slice[start_indices=(0, 0, 0, 2) limit_indices=(8, 1024, 1024, 3) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %reshape.2025 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %slice.23), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/reshape[new_sizes=(8, 1024, 32, 32) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=173}
  %transpose.339 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.2025), dimensions={0,2,3,1}
  %dot.269 = f16[4,4,1024,1024]{3,2,1,0} dot(f16[4,4,1024,32]{3,1,2,0} %transpose.338, f16[4,4,32,1024]{2,1,3,0} %transpose.339), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/dot_general[dimension_numbers=(((3,), (3,)), ((0, 2), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %get-tuple-element.41 = s32[4,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=35, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.42 = s32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=1, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.633 = s32[4,1024]{1,0} broadcast(s32[] %get-tuple-element.42), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %compare.23 = pred[4,1024]{1,0} compare(s32[4,1024]{1,0} %get-tuple-element.41, s32[4,1024]{1,0} %broadcast.633), direction=GT, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %reshape.2026 = pred[4,1,1,1024]{3,2,1,0} reshape(pred[4,1024]{1,0} %compare.23), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %get-tuple-element.43 = f16[8,1,1,1024]{3,2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=3, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.522 = f16[4,1,1,1024]{3,2,1,0} dynamic-slice(f16[8,1,1,1024]{3,2,1,0} %get-tuple-element.43, s32[] %reshape.1424, s32[] %constant.59, s32[] %constant.59, s32[] %constant.59), dynamic_slice_sizes={4,1,1,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %get-tuple-element.44 = f16[8,1,1,1024]{3,2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.525 = f16[4,1,1,1024]{3,2,1,0} dynamic-slice(f16[8,1,1,1024]{3,2,1,0} %get-tuple-element.44, s32[] %reshape.1424, s32[] %constant.59, s32[] %constant.59, s32[] %constant.59), dynamic_slice_sizes={4,1,1,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %select.11 = f16[4,1,1,1024]{3,2,1,0} select(pred[4,1,1,1024]{3,2,1,0} %reshape.2026, f16[4,1,1,1024]{3,2,1,0} %dynamic-slice.522, f16[4,1,1,1024]{3,2,1,0} %dynamic-slice.525), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %reshape.2032 = f16[4,1024]{1,0} reshape(f16[4,1,1,1024]{3,2,1,0} %select.11), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=100}
  %broadcast.1055 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,1024]{1,0} %reshape.2032), dimensions={0,3}
  %add.285 = f16[4,4,1024,1024]{3,2,1,0} add(f16[4,4,1024,1024]{3,2,1,0} %dot.269, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1055), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=100}
  %reduce.54 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %add.285, f16[] %constant.179), dimensions={3}, to_apply=%region_65.2276, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.635 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %reduce.54), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.141 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %add.285, f16[4,4,1024,1024]{3,2,1,0} %broadcast.635), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.9 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.141), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.173 = f32[4,4,1024,1024]{3,2,1,0} convert(f16[4,4,1024,1024]{3,2,1,0} %exponential.9), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %reduce.55 = f32[4,4,1024]{2,1,0} reduce(f32[4,4,1024,1024]{3,2,1,0} %convert.173, f32[] %constant.69), dimensions={3}, to_apply=%region_66.2288, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.174 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %reduce.55), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.636 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %convert.174), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.141 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %exponential.9, f16[4,4,1024,1024]{3,2,1,0} %broadcast.636), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %dot.270 = f16[4,4,32,1024]{3,2,1,0} dot(f16[4,4,32,1024]{2,1,3,0} %transpose.337, f16[4,4,1024,1024]{3,2,1,0} %divide.141), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/dot_general[dimension_numbers=(((1,), (3,)), ((0, 2), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %transpose.340 = f16[4,1024,4,32]{1,3,2,0} transpose(f16[4,4,32,1024]{3,2,1,0} %dot.270), dimensions={0,3,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %reshape.2037 = f16[4096,128]{1,0} reshape(f16[4,1024,4,32]{1,3,2,0} %transpose.340)
  %get-tuple-element.45 = f32[128,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=26, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.175 = f16[128,1024]{1,0} convert(f32[128,1024]{1,0} %get-tuple-element.45), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.271 = f16[4096,1024]{1,0} dot(f16[4096,128]{1,0} %reshape.2037, f16[128,1024]{1,0} %convert.175), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.24 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.271), channel_id=25, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.19, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2040 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.24), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %get-tuple-element.46 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=25, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.176 = f16[1024]{0} convert(f32[1024]{0} %get-tuple-element.46), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1056 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.176), dimensions={2}
  %add.286 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %reshape.2040, f16[4,1024,1024]{2,1,0} %broadcast.1056), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %add.287 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.286, f16[4,1024,1024]{2,1,0} %get-tuple-element.37), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=233}
  %convert.177 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.287), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %reduce.56 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.177, f32[] %constant.69), dimensions={2}, to_apply=%region_67.2313, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.47 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=4, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.638 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.47), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.142 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %reduce.56, f32[4,1024]{1,0} %broadcast.638), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.639 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.142), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.142 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.177, f32[4,1024,1024]{2,1,0} %broadcast.639), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %get-tuple-element.48 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=5, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.640 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.48), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.412 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.177, f32[4,1024,1024]{2,1,0} %convert.177), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.57 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.412, f32[] %constant.69), dimensions={2}, to_apply=%region_68.2326, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %get-tuple-element.49 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=6, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.641 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.49), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.143 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %reduce.57, f32[4,1024]{1,0} %broadcast.641), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.413 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.142, f32[4,1024]{1,0} %divide.142), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.143 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.143, f32[4,1024]{1,0} %multiply.413), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.15 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.640, f32[4,1024]{1,0} %subtract.143), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.50 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=7, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.642 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.50), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.288 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.15, f32[4,1024]{1,0} %broadcast.642), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.2044 = f32[4,1024,1]{2,1,0} reshape(f32[4,1024]{1,0} %add.288), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.27 = f32[4,1024,1]{2,1,0} rsqrt(f32[4,1024,1]{2,1,0} %reshape.2044), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.2045 = f32[4,1024]{1,0} reshape(f32[4,1024,1]{2,1,0} %rsqrt.27), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.643 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %reshape.2045), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %get-tuple-element.51 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=24, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1057 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %get-tuple-element.51), dimensions={2}
  %multiply.414 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.643, f32[4,1024,1024]{2,1,0} %broadcast.1057), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.415 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.142, f32[4,1024,1024]{2,1,0} %multiply.414), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %get-tuple-element.52 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=23, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1058 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %get-tuple-element.52), dimensions={2}
  %add.289 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.415, f32[4,1024,1024]{2,1,0} %broadcast.1058), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %convert.178 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.289), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %reshape.2050 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %convert.178)
  %get-tuple-element.53 = f32[1024,512]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=30, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.179 = f16[1024,512]{1,0} convert(f32[1024,512]{1,0} %get-tuple-element.53), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.272 = f16[4096,512]{1,0} dot(f16[4096,1024]{1,0} %reshape.2050, f16[1024,512]{1,0} %convert.179), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2052 = f16[4,1024,512]{2,1,0} reshape(f16[4096,512]{1,0} %dot.272), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %get-tuple-element.54 = f32[512]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=29, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.180 = f16[512]{0} convert(f32[512]{0} %get-tuple-element.54), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1060 = f16[4,1024,512]{2,1,0} broadcast(f16[512]{0} %convert.180), dimensions={2}
  %add.290 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %reshape.2052, f16[4,1024,512]{2,1,0} %broadcast.1060), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %get-tuple-element.55 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=8, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1062 = f16[4,1024,512]{2,1,0} broadcast(f16[] %get-tuple-element.55), dimensions={}
  %divide.144 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %add.290, f16[4,1024,512]{2,1,0} %broadcast.1062), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.181 = f32[4,1024,512]{2,1,0} convert(f16[4,1024,512]{2,1,0} %divide.144), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %clamp.7 = f32[4,1024,512]{2,1,0} clamp(f32[4,1024,512]{2,1,0} %broadcast.952, f32[4,1024,512]{2,1,0} %convert.181, f32[4,1024,512]{2,1,0} %broadcast.954), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.416 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.7, f32[4,1024,512]{2,1,0} %clamp.7), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.417 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %multiply.416, f32[4,1024,512]{2,1,0} %broadcast.956), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.291 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.417, f32[4,1024,512]{2,1,0} %broadcast.957), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.418 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.291, f32[4,1024,512]{2,1,0} %multiply.416), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.292 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.418, f32[4,1024,512]{2,1,0} %broadcast.958), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.419 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.292, f32[4,1024,512]{2,1,0} %multiply.416), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.293 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.419, f32[4,1024,512]{2,1,0} %broadcast.959), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.420 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.293, f32[4,1024,512]{2,1,0} %multiply.416), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.294 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.420, f32[4,1024,512]{2,1,0} %broadcast.960), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.421 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.294, f32[4,1024,512]{2,1,0} %multiply.416), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.295 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.421, f32[4,1024,512]{2,1,0} %broadcast.961), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.422 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.295, f32[4,1024,512]{2,1,0} %multiply.416), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.296 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.422, f32[4,1024,512]{2,1,0} %broadcast.962), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.423 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.296, f32[4,1024,512]{2,1,0} %multiply.416), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.297 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.423, f32[4,1024,512]{2,1,0} %broadcast.963), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.424 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.7, f32[4,1024,512]{2,1,0} %add.297), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.298 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.417, f32[4,1024,512]{2,1,0} %broadcast.964), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.425 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.298, f32[4,1024,512]{2,1,0} %multiply.416), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.299 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.425, f32[4,1024,512]{2,1,0} %broadcast.965), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.426 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.299, f32[4,1024,512]{2,1,0} %multiply.416), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.300 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.426, f32[4,1024,512]{2,1,0} %broadcast.966), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.427 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.300, f32[4,1024,512]{2,1,0} %multiply.416), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.301 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.427, f32[4,1024,512]{2,1,0} %broadcast.968), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.428 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.301, f32[4,1024,512]{2,1,0} %multiply.416), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.302 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.428, f32[4,1024,512]{2,1,0} %broadcast.970), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.145 = f32[4,1024,512]{2,1,0} divide(f32[4,1024,512]{2,1,0} %multiply.424, f32[4,1024,512]{2,1,0} %add.302), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.182 = f16[4,1024,512]{2,1,0} convert(f32[4,1024,512]{2,1,0} %divide.145), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %get-tuple-element.56 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=9, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1063 = f16[4,1024,512]{2,1,0} broadcast(f16[] %get-tuple-element.56), dimensions={}
  %add.303 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %convert.182, f16[4,1024,512]{2,1,0} %broadcast.1063), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.429 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %add.290, f16[4,1024,512]{2,1,0} %add.303), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %get-tuple-element.57 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=10, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1064 = f16[4,1024,512]{2,1,0} broadcast(f16[] %get-tuple-element.57), dimensions={}
  %divide.146 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %multiply.429, f16[4,1024,512]{2,1,0} %broadcast.1064), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %reshape.2069 = f16[4096,512]{1,0} reshape(f16[4,1024,512]{2,1,0} %divide.146)
  %get-tuple-element.58 = f32[512,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=33, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.183 = f16[512,1024]{1,0} convert(f32[512,1024]{1,0} %get-tuple-element.58), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.273 = f16[4096,1024]{1,0} dot(f16[4096,512]{1,0} %reshape.2069, f16[512,1024]{1,0} %convert.183), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.25 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.273), channel_id=26, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.20, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2071 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.25), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %get-tuple-element.59 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=32, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.184 = f16[1024]{0} convert(f32[1024]{0} %get-tuple-element.59), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1065 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.184), dimensions={2}
  %add.304 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %reshape.2071, f16[4,1024,1024]{2,1,0} %broadcast.1065), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %add.305 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.304, f16[4,1024,1024]{2,1,0} %convert.178), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=310}
  %convert.185 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.305), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %reduce.58 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.185, f32[] %constant.69), dimensions={2}, to_apply=%region_69.2412, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.60 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=11, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.651 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.60), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.147 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %reduce.58, f32[4,1024]{1,0} %broadcast.651), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.652 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.147), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.156 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.185, f32[4,1024,1024]{2,1,0} %broadcast.652), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %get-tuple-element.61 = f16[4,1024,1024]{2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=36, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.186 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %get-tuple-element.61), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %multiply.430 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.156, f32[4,1024,1024]{2,1,0} %convert.186), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %get-tuple-element.62 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=31, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1066 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %get-tuple-element.62), dimensions={2}
  %multiply.431 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %multiply.430, f32[4,1024,1024]{2,1,0} %broadcast.1066), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.59 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.431, f32[] %constant.69), dimensions={2}, to_apply=%region_73.2493, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reshape.2076 = f32[4,1024,1]{2,1,0} reshape(f32[4,1024]{1,0} %reduce.59), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reshape[new_sizes=(8, 1024, 1) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %get-tuple-element.63 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=12, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.654 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.63), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.432 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.185, f32[4,1024,1024]{2,1,0} %convert.185), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.60 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.432, f32[] %constant.69), dimensions={2}, to_apply=%region_70.2425, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %get-tuple-element.64 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=13, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.655 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.64), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.148 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %reduce.60, f32[4,1024]{1,0} %broadcast.655), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.433 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.147, f32[4,1024]{1,0} %divide.147), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.157 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.148, f32[4,1024]{1,0} %multiply.433), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.16 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.654, f32[4,1024]{1,0} %subtract.157), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.65 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=14, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.656 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.65), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.306 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.16, f32[4,1024]{1,0} %broadcast.656), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.2077 = f32[4,1024,1]{2,1,0} reshape(f32[4,1024]{1,0} %add.306), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.28 = f32[4,1024,1]{2,1,0} rsqrt(f32[4,1024,1]{2,1,0} %reshape.2077), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %divide.149 = f32[4,1024,1]{2,1,0} divide(f32[4,1024,1]{2,1,0} %rsqrt.28, f32[4,1024,1]{2,1,0} %reshape.2077), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.434 = f32[4,1024,1]{2,1,0} multiply(f32[4,1024,1]{2,1,0} %divide.149, f32[4,1024,1]{2,1,0} %broadcast.1046), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.435 = f32[4,1024,1]{2,1,0} multiply(f32[4,1024,1]{2,1,0} %reshape.2076, f32[4,1024,1]{2,1,0} %multiply.434), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.2078 = f32[4,1024]{1,0} reshape(f32[4,1024,1]{2,1,0} %multiply.435), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reshape[new_sizes=(8, 1024) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=132}
  %compare.24 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %subtract.157, f32[4,1024]{1,0} %maximum.16), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.66 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=20, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.571 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.66, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.67 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=19, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.574 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.67, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.12 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.24, f32[4,1024]{1,0} %dynamic-slice.571, f32[4,1024]{1,0} %dynamic-slice.574), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %compare.25 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %broadcast.654, f32[4,1024]{1,0} %maximum.16), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.68 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=22, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.577 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.68, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.69 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=21, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.580 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.69, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.13 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.25, f32[4,1024]{1,0} %dynamic-slice.577, f32[4,1024]{1,0} %dynamic-slice.580), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.150 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %select.12, f32[4,1024]{1,0} %select.13), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.436 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reshape.2078, f32[4,1024]{1,0} %divide.150), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.151 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %multiply.436, f32[4,1024]{1,0} %broadcast.655), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.437 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.151, f32[4,1024]{1,0} %broadcast.1048), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %broadcast.657 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.437), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.438 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.185, f32[4,1024,1024]{2,1,0} %broadcast.657), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %negate.14 = f32[4,1024]{1,0} negate(f32[4,1024]{1,0} %multiply.436), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.439 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.147, f32[4,1024]{1,0} %broadcast.1048), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.440 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %negate.14, f32[4,1024]{1,0} %multiply.439), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reshape.2087 = f32[4,1024]{1,0} reshape(f32[4,1024,1]{2,1,0} %rsqrt.28), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.658 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %reshape.2087), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.441 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.658, f32[4,1024,1024]{2,1,0} %broadcast.1066), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.442 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.186, f32[4,1024,1024]{2,1,0} %multiply.441), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %negate.15 = f32[4,1024,1024]{2,1,0} negate(f32[4,1024,1024]{2,1,0} %multiply.442), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reduce.61 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %negate.15, f32[] %constant.69), dimensions={2}, to_apply=%region_74.2510, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %add.307 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %multiply.440, f32[4,1024]{1,0} %reduce.61), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=131}
  %divide.152 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %add.307, f32[4,1024]{1,0} %broadcast.651), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.659 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.152), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/broadcast_in_dim[shape=(8, 1024, 1024) broadcast_dimensions=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %add.308 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.438, f32[4,1024,1024]{2,1,0} %broadcast.659), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %convert.187 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.308), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %convert.188 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %multiply.442), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %add.309 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %convert.187, f16[4,1024,1024]{2,1,0} %convert.188), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reshape.2088 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %add.309)
  %dot.274 = f16[4096,512]{1,0} dot(f16[4096,1024]{1,0} %reshape.2088, f16[512,1024]{1,0} %convert.183), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %broadcast.1067 = f16[4096,512]{1,0} broadcast(f16[] %get-tuple-element.57), dimensions={}
  %divide.153 = f16[4096,512]{1,0} divide(f16[4096,512]{1,0} %dot.274, f16[4096,512]{1,0} %broadcast.1067), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %reshape.2094 = f16[4,1024,512]{2,1,0} reshape(f16[4096,512]{1,0} %divide.153), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.443 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %add.290, f16[4,1024,512]{2,1,0} %reshape.2094), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.444 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.443, f16[4,1024,512]{2,1,0} %broadcast.1050), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.445 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %divide.144, f16[4,1024,512]{2,1,0} %divide.144), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %negate.16 = f16[4,1024,512]{2,1,0} negate(f16[4,1024,512]{2,1,0} %multiply.445), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/neg" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %exponential.10 = f16[4,1024,512]{2,1,0} exponential(f16[4,1024,512]{2,1,0} %negate.16), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/exp" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.446 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.444, f16[4,1024,512]{2,1,0} %exponential.10), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.154 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %multiply.446, f16[4,1024,512]{2,1,0} %broadcast.1062), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.447 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %reshape.2094, f16[4,1024,512]{2,1,0} %add.303), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.310 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %divide.154, f16[4,1024,512]{2,1,0} %multiply.447), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %reshape.2095 = f16[4096,512]{1,0} reshape(f16[4,1024,512]{2,1,0} %add.310)
  %dot.275 = f16[4096,1024]{1,0} dot(f16[4096,512]{1,0} %reshape.2095, f16[1024,512]{1,0} %convert.179), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.26 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.275), channel_id=27, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.21, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2097 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.26), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.311 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.309, f16[4,1024,1024]{2,1,0} %reshape.2097), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.189 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.311), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %multiply.448 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.142, f32[4,1024,1024]{2,1,0} %convert.189), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.449 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %multiply.448, f32[4,1024,1024]{2,1,0} %broadcast.1057), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.62 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.449, f32[] %constant.69), dimensions={2}, to_apply=%region_79.2573, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reshape.2098 = f32[4,1024,1]{2,1,0} reshape(f32[4,1024]{1,0} %reduce.62), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reshape[new_sizes=(8, 1024, 1) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %divide.155 = f32[4,1024,1]{2,1,0} divide(f32[4,1024,1]{2,1,0} %rsqrt.27, f32[4,1024,1]{2,1,0} %reshape.2044), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.450 = f32[4,1024,1]{2,1,0} multiply(f32[4,1024,1]{2,1,0} %divide.155, f32[4,1024,1]{2,1,0} %broadcast.1046), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.451 = f32[4,1024,1]{2,1,0} multiply(f32[4,1024,1]{2,1,0} %reshape.2098, f32[4,1024,1]{2,1,0} %multiply.450), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.2099 = f32[4,1024]{1,0} reshape(f32[4,1024,1]{2,1,0} %multiply.451), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reshape[new_sizes=(8, 1024) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=132}
  %compare.26 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %subtract.143, f32[4,1024]{1,0} %maximum.15), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.70 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=16, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.590 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.70, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.71 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=15, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.593 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.71, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.14 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.26, f32[4,1024]{1,0} %dynamic-slice.590, f32[4,1024]{1,0} %dynamic-slice.593), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %compare.27 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %broadcast.640, f32[4,1024]{1,0} %maximum.15), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.72 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=18, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.596 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.72, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.73 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.1), index=17, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.599 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.73, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.15 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.27, f32[4,1024]{1,0} %dynamic-slice.596, f32[4,1024]{1,0} %dynamic-slice.599), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.156 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %select.14, f32[4,1024]{1,0} %select.15), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.452 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reshape.2099, f32[4,1024]{1,0} %divide.156), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.157 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %multiply.452, f32[4,1024]{1,0} %broadcast.641), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.453 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.157, f32[4,1024]{1,0} %broadcast.1048), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %broadcast.662 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.453), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.454 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.177, f32[4,1024,1024]{2,1,0} %broadcast.662), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %negate.17 = f32[4,1024]{1,0} negate(f32[4,1024]{1,0} %multiply.452), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.455 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.142, f32[4,1024]{1,0} %broadcast.1048), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.456 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %negate.17, f32[4,1024]{1,0} %multiply.455), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.457 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.189, f32[4,1024,1024]{2,1,0} %multiply.414), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %negate.18 = f32[4,1024,1024]{2,1,0} negate(f32[4,1024,1024]{2,1,0} %multiply.457), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reduce.63 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %negate.18, f32[] %constant.69), dimensions={2}, to_apply=%region_80.2590, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %add.312 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %multiply.456, f32[4,1024]{1,0} %reduce.63), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=131}
  %divide.158 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %add.312, f32[4,1024]{1,0} %broadcast.638), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.664 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.158), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/broadcast_in_dim[shape=(8, 1024, 1024) broadcast_dimensions=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %add.313 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.454, f32[4,1024,1024]{2,1,0} %broadcast.664), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %convert.190 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.313), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %convert.191 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %multiply.457), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %add.314 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %convert.190, f16[4,1024,1024]{2,1,0} %convert.191), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reshape.2109 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %add.314)
  %dot.276 = f16[4096,128]{1,0} dot(f16[4096,1024]{1,0} %reshape.2109, f16[128,1024]{1,0} %convert.175), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2112 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4096,128]{1,0} %dot.276), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/reshape[new_sizes=(8, 1024, 32, 32) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=208}
  %transpose.341 = f16[4,4,1024,32]{3,1,2,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.2112), dimensions={0,2,1,3}
  %dot.277 = f16[4,4,1024,1024]{3,2,1,0} dot(f16[4,4,1024,32]{3,1,2,0} %transpose.341, f16[4,4,32,1024]{2,1,3,0} %transpose.337), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/dot_general[dimension_numbers=(((2,), (3,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %multiply.458 = f16[4,4,1024]{2,1,0} multiply(f16[4,4,1024]{2,1,0} %convert.174, f16[4,4,1024]{2,1,0} %convert.174), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.159 = f16[4,4,1024]{2,1,0} divide(f16[4,4,1024]{2,1,0} %broadcast.1051, f16[4,4,1024]{2,1,0} %multiply.458), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.665 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %divide.159), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.459 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %dot.277, f16[4,4,1024,1024]{3,2,1,0} %broadcast.665), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.460 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %multiply.459, f16[4,4,1024,1024]{3,2,1,0} %exponential.9), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %reduce.64 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %multiply.460, f16[] %constant.89), dimensions={3}, to_apply=%region_82.2621, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %negate.19 = f16[4,4,1024]{2,1,0} negate(f16[4,4,1024]{2,1,0} %reduce.64), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.666 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %negate.19), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/broadcast_in_dim[shape=(8, 32, 1024, 1024) broadcast_dimensions=(0, 1, 2)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.160 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %dot.277, f16[4,4,1024,1024]{3,2,1,0} %broadcast.636), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %add.315 = f16[4,4,1024,1024]{3,2,1,0} add(f16[4,4,1024,1024]{3,2,1,0} %broadcast.666, f16[4,4,1024,1024]{3,2,1,0} %divide.160), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.461 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %add.315, f16[4,4,1024,1024]{3,2,1,0} %exponential.9), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %dot.278 = f16[4,4,1024,32]{3,2,1,0} dot(f16[4,4,1024,1024]{3,2,1,0} %multiply.461, f16[4,4,1024,32]{3,1,2,0} %transpose.338), lhs_batch_dims={0,1}, lhs_contracting_dims={2}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/dot_general[dimension_numbers=(((2,), (1,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %transpose.342 = f16[4,1024,4,32]{3,1,2,0} transpose(f16[4,4,1024,32]{3,2,1,0} %dot.278), dimensions={0,2,1,3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/transpose[permutation=(0, 2, 1, 3)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %reshape.2113 = f16[4,1024,128,1]{3,2,1,0} reshape(f16[4,1024,4,32]{3,1,2,0} %transpose.342), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=173}
  %pad.3 = f16[4,1024,128,3]{3,2,1,0} pad(f16[4,1024,128,1]{3,2,1,0} %reshape.2113, f16[] %constant.89), padding=0_0x0_0x0_0x2_0, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (2, 0, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.343 = f16[4,4,1024,32]{3,1,2,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.2025), dimensions={0,2,1,3}
  %dot.279 = f16[4,4,1024,32]{3,2,1,0} dot(f16[4,4,1024,1024]{3,2,1,0} %multiply.461, f16[4,4,1024,32]{3,1,2,0} %transpose.343), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/dot_general[dimension_numbers=(((3,), (1,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %broadcast.1068 = f16[4,4,1024,32]{3,2,1,0} broadcast(f16[] %get-tuple-element.40), dimensions={}
  %divide.161 = f16[4,4,1024,32]{3,2,1,0} divide(f16[4,4,1024,32]{3,2,1,0} %dot.279, f16[4,4,1024,32]{3,2,1,0} %broadcast.1068), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %transpose.344 = f16[4,1024,4,32]{3,1,2,0} transpose(f16[4,4,1024,32]{3,2,1,0} %divide.161), dimensions={0,2,1,3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %reshape.2122 = f16[4,1024,128,1]{3,2,1,0} reshape(f16[4,1024,4,32]{3,1,2,0} %transpose.344), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=167}
  %pad.4 = f16[4,1024,128,3]{3,2,1,0} pad(f16[4,1024,128,1]{3,2,1,0} %reshape.2122, f16[] %constant.89), padding=0_0x0_0x0_0x0_2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 2, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %add.316 = f16[4,1024,128,3]{3,2,1,0} add(f16[4,1024,128,3]{3,2,1,0} %pad.3, f16[4,1024,128,3]{3,2,1,0} %pad.4), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.345 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.2112), dimensions={0,2,3,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/transpose[permutation=(0, 2, 3, 1)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %dot.280 = f16[4,4,32,1024]{3,2,1,0} dot(f16[4,4,32,1024]{2,1,3,0} %transpose.345, f16[4,4,1024,1024]{3,2,1,0} %divide.141), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/dot_general[dimension_numbers=(((3,), (2,)), ((0, 1), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %transpose.346 = f16[4,1024,4,32]{1,3,2,0} transpose(f16[4,4,32,1024]{3,2,1,0} %dot.280), dimensions={0,3,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %reshape.2125 = f16[4,1024,128,1]{3,2,1,0} reshape(f16[4,1024,4,32]{1,3,2,0} %transpose.346), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=170}
  %pad.5 = f16[4,1024,128,3]{3,2,1,0} pad(f16[4,1024,128,1]{3,2,1,0} %reshape.2125, f16[] %constant.89), padding=0_0x0_0x0_0x1_1, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (1, 1, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %add.317 = f16[4,1024,128,3]{3,2,1,0} add(f16[4,1024,128,3]{3,2,1,0} %add.316, f16[4,1024,128,3]{3,2,1,0} %pad.5), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %reshape.2128 = f16[4096,384]{1,0} reshape(f16[4,1024,128,3]{3,2,1,0} %add.317)
  %dot.281 = f16[4096,1024]{1,0} dot(f16[4096,384]{1,0} %reshape.2128, f16[1024,384]{1,0} %convert.171), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.27 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.281), channel_id=28, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.22, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2130 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.27), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.318 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.314, f16[4,1024,1024]{2,1,0} %reshape.2130), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %tuple.2 = (f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) tuple(f16[] %constant.862, s32[] %constant.59, f16[8,1,1,1024]{3,2,1,0} %broadcast.580, f16[8,1,1,1024]{3,2,1,0} %broadcast.581, f32[] %constant.863, /*index=5*/f32[] %constant.69, f32[] %constant.863, f32[] %constant.124, f16[] %constant.864, f16[] %constant.337, /*index=10*/f16[] %constant.865, f32[] %constant.863, f32[] %constant.69, f32[] %constant.863, f32[] %constant.124, /*index=15*/f32[8,1024]{1,0} %broadcast.421, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.582, f32[8,1024]{1,0} %broadcast.421, /*index=20*/f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.582, f32[1024]{0} %param.53, f32[1024]{0} %param.52, /*index=25*/f32[1024]{0} %param.51, f32[128,1024]{1,0} %param.50, f32[384]{0} %param.49, f32[1024,384]{1,0} %param.48, f32[512]{0} %param.55, /*index=30*/f32[1024,512]{1,0} %param.54, f32[1024]{0} %param.58, f32[1024]{0} %param.57, f32[512,1024]{1,0} %param.56, f16[4,1024,1024]{2,1,0} %convert.96, /*index=35*/s32[4,1024]{1,0} %param.13, f16[4,1024,1024]{2,1,0} %add.318), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %opt-barrier.2 = (f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) opt-barrier((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %tuple.2), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.74 = f16[4,1024,1024]{2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=34, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %reshape.2131 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %get-tuple-element.74)
  %get-tuple-element.75 = f32[1024,384]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=28, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.192 = f16[1024,384]{1,0} convert(f32[1024,384]{1,0} %get-tuple-element.75), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.282 = f16[4096,384]{1,0} dot(f16[4096,1024]{1,0} %reshape.2131, f16[1024,384]{1,0} %convert.192), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2133 = f16[4,1024,384]{2,1,0} reshape(f16[4096,384]{1,0} %dot.282), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %get-tuple-element.76 = f32[384]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=27, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.193 = f16[384]{0} convert(f32[384]{0} %get-tuple-element.76), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1069 = f16[4,1024,384]{2,1,0} broadcast(f16[384]{0} %convert.193), dimensions={2}
  %add.319 = f16[4,1024,384]{2,1,0} add(f16[4,1024,384]{2,1,0} %reshape.2133, f16[4,1024,384]{2,1,0} %broadcast.1069), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %reshape.2138 = f16[4,1024,128,3]{3,2,1,0} reshape(f16[4,1024,384]{2,1,0} %add.319), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/reshape[new_sizes=(8, 1024, 1024, 3) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=161}
  %slice.24 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.2138), slice={[0:4], [0:1024], [0:128], [1:2]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/slice[start_indices=(0, 0, 0, 1) limit_indices=(8, 1024, 1024, 2) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %reshape.2141 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %slice.24), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/reshape[new_sizes=(8, 1024, 32, 32) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=170}
  %transpose.347 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.2141), dimensions={0,2,3,1}
  %slice.25 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.2138), slice={[0:4], [0:1024], [0:128], [0:1]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/slice[start_indices=(0, 0, 0, 0) limit_indices=(8, 1024, 1024, 1) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %get-tuple-element.77 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=0, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1070 = f16[4,1024,128,1]{3,2,1,0} broadcast(f16[] %get-tuple-element.77), dimensions={}
  %divide.162 = f16[4,1024,128,1]{3,2,1,0} divide(f16[4,1024,128,1]{3,2,1,0} %slice.25, f16[4,1024,128,1]{3,2,1,0} %broadcast.1070), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %reshape.2148 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %divide.162), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %transpose.348 = f16[4,4,1024,32]{3,1,2,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.2148), dimensions={0,2,1,3}
  %slice.26 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.2138), slice={[0:4], [0:1024], [0:128], [2:3]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/slice[start_indices=(0, 0, 0, 2) limit_indices=(8, 1024, 1024, 3) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %reshape.2151 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %slice.26), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/reshape[new_sizes=(8, 1024, 32, 32) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=173}
  %transpose.349 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.2151), dimensions={0,2,3,1}
  %dot.283 = f16[4,4,1024,1024]{3,2,1,0} dot(f16[4,4,1024,32]{3,1,2,0} %transpose.348, f16[4,4,32,1024]{2,1,3,0} %transpose.349), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/dot_general[dimension_numbers=(((3,), (3,)), ((0, 2), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %get-tuple-element.78 = s32[4,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=35, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.79 = s32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=1, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.670 = s32[4,1024]{1,0} broadcast(s32[] %get-tuple-element.79), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %compare.28 = pred[4,1024]{1,0} compare(s32[4,1024]{1,0} %get-tuple-element.78, s32[4,1024]{1,0} %broadcast.670), direction=GT, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %reshape.2152 = pred[4,1,1,1024]{3,2,1,0} reshape(pred[4,1024]{1,0} %compare.28), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %get-tuple-element.80 = f16[8,1,1,1024]{3,2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=3, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.632 = f16[4,1,1,1024]{3,2,1,0} dynamic-slice(f16[8,1,1,1024]{3,2,1,0} %get-tuple-element.80, s32[] %reshape.1424, s32[] %constant.59, s32[] %constant.59, s32[] %constant.59), dynamic_slice_sizes={4,1,1,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %get-tuple-element.81 = f16[8,1,1,1024]{3,2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.635 = f16[4,1,1,1024]{3,2,1,0} dynamic-slice(f16[8,1,1,1024]{3,2,1,0} %get-tuple-element.81, s32[] %reshape.1424, s32[] %constant.59, s32[] %constant.59, s32[] %constant.59), dynamic_slice_sizes={4,1,1,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %select.16 = f16[4,1,1,1024]{3,2,1,0} select(pred[4,1,1,1024]{3,2,1,0} %reshape.2152, f16[4,1,1,1024]{3,2,1,0} %dynamic-slice.632, f16[4,1,1,1024]{3,2,1,0} %dynamic-slice.635), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %reshape.2159 = f16[4,1024]{1,0} reshape(f16[4,1,1,1024]{3,2,1,0} %select.16), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=100}
  %broadcast.1071 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,1024]{1,0} %reshape.2159), dimensions={0,3}
  %add.320 = f16[4,4,1024,1024]{3,2,1,0} add(f16[4,4,1024,1024]{3,2,1,0} %dot.283, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1071), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=100}
  %reduce.65 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %add.320, f16[] %constant.179), dimensions={3}, to_apply=%region_85.2735, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.672 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %reduce.65), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.175 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %add.320, f16[4,4,1024,1024]{3,2,1,0} %broadcast.672), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.11 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.175), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.194 = f32[4,4,1024,1024]{3,2,1,0} convert(f16[4,4,1024,1024]{3,2,1,0} %exponential.11), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %reduce.66 = f32[4,4,1024]{2,1,0} reduce(f32[4,4,1024,1024]{3,2,1,0} %convert.194, f32[] %constant.69), dimensions={3}, to_apply=%region_86.2747, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.195 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %reduce.66), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.673 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %convert.195), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.163 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %exponential.11, f16[4,4,1024,1024]{3,2,1,0} %broadcast.673), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %dot.284 = f16[4,4,32,1024]{3,2,1,0} dot(f16[4,4,32,1024]{2,1,3,0} %transpose.347, f16[4,4,1024,1024]{3,2,1,0} %divide.163), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/dot_general[dimension_numbers=(((1,), (3,)), ((0, 2), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %transpose.350 = f16[4,1024,4,32]{1,3,2,0} transpose(f16[4,4,32,1024]{3,2,1,0} %dot.284), dimensions={0,3,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %reshape.2164 = f16[4096,128]{1,0} reshape(f16[4,1024,4,32]{1,3,2,0} %transpose.350)
  %get-tuple-element.82 = f32[128,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=26, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.196 = f16[128,1024]{1,0} convert(f32[128,1024]{1,0} %get-tuple-element.82), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.285 = f16[4096,1024]{1,0} dot(f16[4096,128]{1,0} %reshape.2164, f16[128,1024]{1,0} %convert.196), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.28 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.285), channel_id=29, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.23, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2166 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.28), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %get-tuple-element.83 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=25, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.197 = f16[1024]{0} convert(f32[1024]{0} %get-tuple-element.83), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1072 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.197), dimensions={2}
  %add.321 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %reshape.2166, f16[4,1024,1024]{2,1,0} %broadcast.1072), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %add.322 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.321, f16[4,1024,1024]{2,1,0} %get-tuple-element.74), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=233}
  %convert.198 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.322), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %reduce.67 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.198, f32[] %constant.69), dimensions={2}, to_apply=%region_87.2772, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.84 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=4, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.676 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.84), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.164 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %reduce.67, f32[4,1024]{1,0} %broadcast.676), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.678 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.164), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.176 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.198, f32[4,1024,1024]{2,1,0} %broadcast.678), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %get-tuple-element.85 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=5, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.679 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.85), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.462 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.198, f32[4,1024,1024]{2,1,0} %convert.198), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.68 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.462, f32[] %constant.69), dimensions={2}, to_apply=%region_88.2785, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %get-tuple-element.86 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=6, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.680 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.86), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.165 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %reduce.68, f32[4,1024]{1,0} %broadcast.680), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.463 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.164, f32[4,1024]{1,0} %divide.164), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.177 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.165, f32[4,1024]{1,0} %multiply.463), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.17 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.679, f32[4,1024]{1,0} %subtract.177), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.87 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=7, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.681 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.87), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.323 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.17, f32[4,1024]{1,0} %broadcast.681), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.2170 = f32[4,1024,1]{2,1,0} reshape(f32[4,1024]{1,0} %add.323), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.29 = f32[4,1024,1]{2,1,0} rsqrt(f32[4,1024,1]{2,1,0} %reshape.2170), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.2171 = f32[4,1024]{1,0} reshape(f32[4,1024,1]{2,1,0} %rsqrt.29), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.682 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %reshape.2171), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %get-tuple-element.88 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=24, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1073 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %get-tuple-element.88), dimensions={2}
  %multiply.464 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.682, f32[4,1024,1024]{2,1,0} %broadcast.1073), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.465 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.176, f32[4,1024,1024]{2,1,0} %multiply.464), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %get-tuple-element.89 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=23, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1074 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %get-tuple-element.89), dimensions={2}
  %add.324 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.465, f32[4,1024,1024]{2,1,0} %broadcast.1074), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %convert.199 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.324), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %reshape.2176 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %convert.199)
  %get-tuple-element.90 = f32[1024,512]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=30, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.200 = f16[1024,512]{1,0} convert(f32[1024,512]{1,0} %get-tuple-element.90), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.286 = f16[4096,512]{1,0} dot(f16[4096,1024]{1,0} %reshape.2176, f16[1024,512]{1,0} %convert.200), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2179 = f16[4,1024,512]{2,1,0} reshape(f16[4096,512]{1,0} %dot.286), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %get-tuple-element.91 = f32[512]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=29, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.201 = f16[512]{0} convert(f32[512]{0} %get-tuple-element.91), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1075 = f16[4,1024,512]{2,1,0} broadcast(f16[512]{0} %convert.201), dimensions={2}
  %add.325 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %reshape.2179, f16[4,1024,512]{2,1,0} %broadcast.1075), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %get-tuple-element.92 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=8, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1076 = f16[4,1024,512]{2,1,0} broadcast(f16[] %get-tuple-element.92), dimensions={}
  %divide.166 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %add.325, f16[4,1024,512]{2,1,0} %broadcast.1076), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.202 = f32[4,1024,512]{2,1,0} convert(f16[4,1024,512]{2,1,0} %divide.166), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %clamp.8 = f32[4,1024,512]{2,1,0} clamp(f32[4,1024,512]{2,1,0} %broadcast.952, f32[4,1024,512]{2,1,0} %convert.202, f32[4,1024,512]{2,1,0} %broadcast.954), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.466 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.8, f32[4,1024,512]{2,1,0} %clamp.8), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.467 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %multiply.466, f32[4,1024,512]{2,1,0} %broadcast.956), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.326 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.467, f32[4,1024,512]{2,1,0} %broadcast.957), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.468 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.326, f32[4,1024,512]{2,1,0} %multiply.466), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.327 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.468, f32[4,1024,512]{2,1,0} %broadcast.958), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.469 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.327, f32[4,1024,512]{2,1,0} %multiply.466), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.328 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.469, f32[4,1024,512]{2,1,0} %broadcast.959), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.470 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.328, f32[4,1024,512]{2,1,0} %multiply.466), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.329 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.470, f32[4,1024,512]{2,1,0} %broadcast.960), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.471 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.329, f32[4,1024,512]{2,1,0} %multiply.466), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.330 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.471, f32[4,1024,512]{2,1,0} %broadcast.961), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.474 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.330, f32[4,1024,512]{2,1,0} %multiply.466), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.331 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.474, f32[4,1024,512]{2,1,0} %broadcast.962), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.475 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.331, f32[4,1024,512]{2,1,0} %multiply.466), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.332 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.475, f32[4,1024,512]{2,1,0} %broadcast.963), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.476 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.8, f32[4,1024,512]{2,1,0} %add.332), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.333 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.467, f32[4,1024,512]{2,1,0} %broadcast.964), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.477 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.333, f32[4,1024,512]{2,1,0} %multiply.466), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.334 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.477, f32[4,1024,512]{2,1,0} %broadcast.965), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.478 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.334, f32[4,1024,512]{2,1,0} %multiply.466), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.335 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.478, f32[4,1024,512]{2,1,0} %broadcast.966), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.479 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.335, f32[4,1024,512]{2,1,0} %multiply.466), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.336 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.479, f32[4,1024,512]{2,1,0} %broadcast.968), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.482 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.336, f32[4,1024,512]{2,1,0} %multiply.466), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.337 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.482, f32[4,1024,512]{2,1,0} %broadcast.970), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.167 = f32[4,1024,512]{2,1,0} divide(f32[4,1024,512]{2,1,0} %multiply.476, f32[4,1024,512]{2,1,0} %add.337), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.203 = f16[4,1024,512]{2,1,0} convert(f32[4,1024,512]{2,1,0} %divide.167), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %get-tuple-element.93 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=9, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1078 = f16[4,1024,512]{2,1,0} broadcast(f16[] %get-tuple-element.93), dimensions={}
  %add.338 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %convert.203, f16[4,1024,512]{2,1,0} %broadcast.1078), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.483 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %add.325, f16[4,1024,512]{2,1,0} %add.338), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %get-tuple-element.94 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=10, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1081 = f16[4,1024,512]{2,1,0} broadcast(f16[] %get-tuple-element.94), dimensions={}
  %divide.168 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %multiply.483, f16[4,1024,512]{2,1,0} %broadcast.1081), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %reshape.2200 = f16[4096,512]{1,0} reshape(f16[4,1024,512]{2,1,0} %divide.168)
  %get-tuple-element.95 = f32[512,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=33, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.204 = f16[512,1024]{1,0} convert(f32[512,1024]{1,0} %get-tuple-element.95), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.287 = f16[4096,1024]{1,0} dot(f16[4096,512]{1,0} %reshape.2200, f16[512,1024]{1,0} %convert.204), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.29 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.287), channel_id=30, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.24, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2202 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.29), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %get-tuple-element.96 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=32, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.205 = f16[1024]{0} convert(f32[1024]{0} %get-tuple-element.96), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1083 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.205), dimensions={2}
  %add.339 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %reshape.2202, f16[4,1024,1024]{2,1,0} %broadcast.1083), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %add.340 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.339, f16[4,1024,1024]{2,1,0} %convert.199), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=310}
  %convert.206 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.340), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %reduce.69 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.206, f32[] %constant.69), dimensions={2}, to_apply=%region_89.2871, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.97 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=11, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.690 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.97), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.169 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %reduce.69, f32[4,1024]{1,0} %broadcast.690), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.691 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.169), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.190 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.206, f32[4,1024,1024]{2,1,0} %broadcast.691), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %get-tuple-element.98 = f16[4,1024,1024]{2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=36, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.207 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %get-tuple-element.98), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %multiply.484 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.190, f32[4,1024,1024]{2,1,0} %convert.207), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %get-tuple-element.99 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=31, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1084 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %get-tuple-element.99), dimensions={2}
  %multiply.485 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %multiply.484, f32[4,1024,1024]{2,1,0} %broadcast.1084), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.70 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.485, f32[] %constant.69), dimensions={2}, to_apply=%region_93.2952, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reshape.2207 = f32[4,1024,1]{2,1,0} reshape(f32[4,1024]{1,0} %reduce.70), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reshape[new_sizes=(8, 1024, 1) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %get-tuple-element.100 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=12, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.694 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.100), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.486 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.206, f32[4,1024,1024]{2,1,0} %convert.206), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.71 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.486, f32[] %constant.69), dimensions={2}, to_apply=%region_90.2884, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %get-tuple-element.101 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=13, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.697 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.101), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.170 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %reduce.71, f32[4,1024]{1,0} %broadcast.697), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.487 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.169, f32[4,1024]{1,0} %divide.169), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.191 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.170, f32[4,1024]{1,0} %multiply.487), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.18 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.694, f32[4,1024]{1,0} %subtract.191), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.102 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=14, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.699 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.102), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.341 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.18, f32[4,1024]{1,0} %broadcast.699), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.2208 = f32[4,1024,1]{2,1,0} reshape(f32[4,1024]{1,0} %add.341), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.30 = f32[4,1024,1]{2,1,0} rsqrt(f32[4,1024,1]{2,1,0} %reshape.2208), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %divide.171 = f32[4,1024,1]{2,1,0} divide(f32[4,1024,1]{2,1,0} %rsqrt.30, f32[4,1024,1]{2,1,0} %reshape.2208), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.488 = f32[4,1024,1]{2,1,0} multiply(f32[4,1024,1]{2,1,0} %divide.171, f32[4,1024,1]{2,1,0} %broadcast.1046), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.489 = f32[4,1024,1]{2,1,0} multiply(f32[4,1024,1]{2,1,0} %reshape.2207, f32[4,1024,1]{2,1,0} %multiply.488), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.2209 = f32[4,1024]{1,0} reshape(f32[4,1024,1]{2,1,0} %multiply.489), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reshape[new_sizes=(8, 1024) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=132}
  %compare.29 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %subtract.191, f32[4,1024]{1,0} %maximum.18), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.103 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=20, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.681 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.103, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.104 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=19, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.684 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.104, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.17 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.29, f32[4,1024]{1,0} %dynamic-slice.681, f32[4,1024]{1,0} %dynamic-slice.684), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %compare.30 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %broadcast.694, f32[4,1024]{1,0} %maximum.18), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.105 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=22, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.687 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.105, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.106 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=21, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.690 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.106, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.18 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.30, f32[4,1024]{1,0} %dynamic-slice.687, f32[4,1024]{1,0} %dynamic-slice.690), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.172 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %select.17, f32[4,1024]{1,0} %select.18), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.490 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reshape.2209, f32[4,1024]{1,0} %divide.172), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.173 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %multiply.490, f32[4,1024]{1,0} %broadcast.697), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.491 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.173, f32[4,1024]{1,0} %broadcast.1048), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %broadcast.700 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.491), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.492 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.206, f32[4,1024,1024]{2,1,0} %broadcast.700), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %negate.20 = f32[4,1024]{1,0} negate(f32[4,1024]{1,0} %multiply.490), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.493 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.169, f32[4,1024]{1,0} %broadcast.1048), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.494 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %negate.20, f32[4,1024]{1,0} %multiply.493), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reshape.2218 = f32[4,1024]{1,0} reshape(f32[4,1024,1]{2,1,0} %rsqrt.30), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.701 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %reshape.2218), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.495 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.701, f32[4,1024,1024]{2,1,0} %broadcast.1084), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.496 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.207, f32[4,1024,1024]{2,1,0} %multiply.495), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %negate.21 = f32[4,1024,1024]{2,1,0} negate(f32[4,1024,1024]{2,1,0} %multiply.496), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reduce.72 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %negate.21, f32[] %constant.69), dimensions={2}, to_apply=%region_94.2969, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %add.342 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %multiply.494, f32[4,1024]{1,0} %reduce.72), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=131}
  %divide.174 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %add.342, f32[4,1024]{1,0} %broadcast.690), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.703 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.174), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/broadcast_in_dim[shape=(8, 1024, 1024) broadcast_dimensions=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %add.343 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.492, f32[4,1024,1024]{2,1,0} %broadcast.703), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %convert.208 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.343), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %convert.209 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %multiply.496), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %add.344 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %convert.208, f16[4,1024,1024]{2,1,0} %convert.209), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reshape.2219 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %add.344)
  %dot.288 = f16[4096,512]{1,0} dot(f16[4096,1024]{1,0} %reshape.2219, f16[512,1024]{1,0} %convert.204), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %broadcast.1085 = f16[4096,512]{1,0} broadcast(f16[] %get-tuple-element.94), dimensions={}
  %divide.175 = f16[4096,512]{1,0} divide(f16[4096,512]{1,0} %dot.288, f16[4096,512]{1,0} %broadcast.1085), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %reshape.2225 = f16[4,1024,512]{2,1,0} reshape(f16[4096,512]{1,0} %divide.175), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.497 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %add.325, f16[4,1024,512]{2,1,0} %reshape.2225), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.498 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.497, f16[4,1024,512]{2,1,0} %broadcast.1050), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.500 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %divide.166, f16[4,1024,512]{2,1,0} %divide.166), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %negate.22 = f16[4,1024,512]{2,1,0} negate(f16[4,1024,512]{2,1,0} %multiply.500), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/neg" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %exponential.12 = f16[4,1024,512]{2,1,0} exponential(f16[4,1024,512]{2,1,0} %negate.22), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/exp" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.501 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.498, f16[4,1024,512]{2,1,0} %exponential.12), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.176 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %multiply.501, f16[4,1024,512]{2,1,0} %broadcast.1076), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.502 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %reshape.2225, f16[4,1024,512]{2,1,0} %add.338), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.345 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %divide.176, f16[4,1024,512]{2,1,0} %multiply.502), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %reshape.2226 = f16[4096,512]{1,0} reshape(f16[4,1024,512]{2,1,0} %add.345)
  %dot.289 = f16[4096,1024]{1,0} dot(f16[4096,512]{1,0} %reshape.2226, f16[1024,512]{1,0} %convert.200), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.30 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.289), channel_id=31, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.25, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2228 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.30), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.346 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.344, f16[4,1024,1024]{2,1,0} %reshape.2228), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.210 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.346), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %multiply.503 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.176, f32[4,1024,1024]{2,1,0} %convert.210), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.504 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %multiply.503, f32[4,1024,1024]{2,1,0} %broadcast.1073), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.73 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.504, f32[] %constant.69), dimensions={2}, to_apply=%region_99.3032, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reshape.2229 = f32[4,1024,1]{2,1,0} reshape(f32[4,1024]{1,0} %reduce.73), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reshape[new_sizes=(8, 1024, 1) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %divide.177 = f32[4,1024,1]{2,1,0} divide(f32[4,1024,1]{2,1,0} %rsqrt.29, f32[4,1024,1]{2,1,0} %reshape.2170), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.505 = f32[4,1024,1]{2,1,0} multiply(f32[4,1024,1]{2,1,0} %divide.177, f32[4,1024,1]{2,1,0} %broadcast.1046), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.506 = f32[4,1024,1]{2,1,0} multiply(f32[4,1024,1]{2,1,0} %reshape.2229, f32[4,1024,1]{2,1,0} %multiply.505), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.2230 = f32[4,1024]{1,0} reshape(f32[4,1024,1]{2,1,0} %multiply.506), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reshape[new_sizes=(8, 1024) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=132}
  %compare.31 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %subtract.177, f32[4,1024]{1,0} %maximum.17), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.107 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=16, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.700 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.107, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.108 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=15, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.703 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.108, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.19 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.31, f32[4,1024]{1,0} %dynamic-slice.700, f32[4,1024]{1,0} %dynamic-slice.703), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %compare.32 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %broadcast.679, f32[4,1024]{1,0} %maximum.17), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.109 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=18, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.706 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.109, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.110 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.2), index=17, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.709 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.110, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.20 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.32, f32[4,1024]{1,0} %dynamic-slice.706, f32[4,1024]{1,0} %dynamic-slice.709), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.178 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %select.19, f32[4,1024]{1,0} %select.20), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.509 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reshape.2230, f32[4,1024]{1,0} %divide.178), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.179 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %multiply.509, f32[4,1024]{1,0} %broadcast.680), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.510 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.179, f32[4,1024]{1,0} %broadcast.1048), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %broadcast.706 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.510), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.511 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.198, f32[4,1024,1024]{2,1,0} %broadcast.706), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %negate.23 = f32[4,1024]{1,0} negate(f32[4,1024]{1,0} %multiply.509), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.512 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.164, f32[4,1024]{1,0} %broadcast.1048), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.513 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %negate.23, f32[4,1024]{1,0} %multiply.512), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.514 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.210, f32[4,1024,1024]{2,1,0} %multiply.464), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %negate.24 = f32[4,1024,1024]{2,1,0} negate(f32[4,1024,1024]{2,1,0} %multiply.514), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reduce.74 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %negate.24, f32[] %constant.69), dimensions={2}, to_apply=%region_100.3049, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %add.347 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %multiply.513, f32[4,1024]{1,0} %reduce.74), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=131}
  %divide.180 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %add.347, f32[4,1024]{1,0} %broadcast.676), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.707 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.180), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/broadcast_in_dim[shape=(8, 1024, 1024) broadcast_dimensions=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %add.348 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.511, f32[4,1024,1024]{2,1,0} %broadcast.707), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %convert.211 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.348), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %convert.212 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %multiply.514), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %add.349 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %convert.211, f16[4,1024,1024]{2,1,0} %convert.212), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reshape.2239 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %add.349)
  %dot.290 = f16[4096,128]{1,0} dot(f16[4096,1024]{1,0} %reshape.2239, f16[128,1024]{1,0} %convert.196), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2241 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4096,128]{1,0} %dot.290), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/reshape[new_sizes=(8, 1024, 32, 32) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=208}
  %transpose.351 = f16[4,4,1024,32]{3,1,2,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.2241), dimensions={0,2,1,3}
  %dot.291 = f16[4,4,1024,1024]{3,2,1,0} dot(f16[4,4,1024,32]{3,1,2,0} %transpose.351, f16[4,4,32,1024]{2,1,3,0} %transpose.347), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/dot_general[dimension_numbers=(((2,), (3,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %multiply.515 = f16[4,4,1024]{2,1,0} multiply(f16[4,4,1024]{2,1,0} %convert.195, f16[4,4,1024]{2,1,0} %convert.195), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.181 = f16[4,4,1024]{2,1,0} divide(f16[4,4,1024]{2,1,0} %broadcast.1051, f16[4,4,1024]{2,1,0} %multiply.515), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.708 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %divide.181), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.516 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %dot.291, f16[4,4,1024,1024]{3,2,1,0} %broadcast.708), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.517 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %multiply.516, f16[4,4,1024,1024]{3,2,1,0} %exponential.11), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %reduce.75 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %multiply.517, f16[] %constant.89), dimensions={3}, to_apply=%region_102.3080, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %negate.25 = f16[4,4,1024]{2,1,0} negate(f16[4,4,1024]{2,1,0} %reduce.75), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.709 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %negate.25), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/broadcast_in_dim[shape=(8, 32, 1024, 1024) broadcast_dimensions=(0, 1, 2)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.182 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %dot.291, f16[4,4,1024,1024]{3,2,1,0} %broadcast.673), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %add.350 = f16[4,4,1024,1024]{3,2,1,0} add(f16[4,4,1024,1024]{3,2,1,0} %broadcast.709, f16[4,4,1024,1024]{3,2,1,0} %divide.182), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.518 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %add.350, f16[4,4,1024,1024]{3,2,1,0} %exponential.11), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %dot.292 = f16[4,4,1024,32]{3,2,1,0} dot(f16[4,4,1024,1024]{3,2,1,0} %multiply.518, f16[4,4,1024,32]{3,1,2,0} %transpose.348), lhs_batch_dims={0,1}, lhs_contracting_dims={2}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/dot_general[dimension_numbers=(((2,), (1,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %transpose.352 = f16[4,1024,4,32]{3,1,2,0} transpose(f16[4,4,1024,32]{3,2,1,0} %dot.292), dimensions={0,2,1,3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/transpose[permutation=(0, 2, 1, 3)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %reshape.2242 = f16[4,1024,128,1]{3,2,1,0} reshape(f16[4,1024,4,32]{3,1,2,0} %transpose.352), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=173}
  %pad.6 = f16[4,1024,128,3]{3,2,1,0} pad(f16[4,1024,128,1]{3,2,1,0} %reshape.2242, f16[] %constant.89), padding=0_0x0_0x0_0x2_0, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (2, 0, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.353 = f16[4,4,1024,32]{3,1,2,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.2151), dimensions={0,2,1,3}
  %dot.293 = f16[4,4,1024,32]{3,2,1,0} dot(f16[4,4,1024,1024]{3,2,1,0} %multiply.518, f16[4,4,1024,32]{3,1,2,0} %transpose.353), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/dot_general[dimension_numbers=(((3,), (1,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %broadcast.1087 = f16[4,4,1024,32]{3,2,1,0} broadcast(f16[] %get-tuple-element.77), dimensions={}
  %divide.183 = f16[4,4,1024,32]{3,2,1,0} divide(f16[4,4,1024,32]{3,2,1,0} %dot.293, f16[4,4,1024,32]{3,2,1,0} %broadcast.1087), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %transpose.354 = f16[4,1024,4,32]{3,1,2,0} transpose(f16[4,4,1024,32]{3,2,1,0} %divide.183), dimensions={0,2,1,3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %reshape.2249 = f16[4,1024,128,1]{3,2,1,0} reshape(f16[4,1024,4,32]{3,1,2,0} %transpose.354), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=167}
  %pad.7 = f16[4,1024,128,3]{3,2,1,0} pad(f16[4,1024,128,1]{3,2,1,0} %reshape.2249, f16[] %constant.89), padding=0_0x0_0x0_0x0_2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 2, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %add.351 = f16[4,1024,128,3]{3,2,1,0} add(f16[4,1024,128,3]{3,2,1,0} %pad.6, f16[4,1024,128,3]{3,2,1,0} %pad.7), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.355 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.2241), dimensions={0,2,3,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/transpose[permutation=(0, 2, 3, 1)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %dot.294 = f16[4,4,32,1024]{3,2,1,0} dot(f16[4,4,32,1024]{2,1,3,0} %transpose.355, f16[4,4,1024,1024]{3,2,1,0} %divide.163), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/dot_general[dimension_numbers=(((3,), (2,)), ((0, 1), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %transpose.356 = f16[4,1024,4,32]{1,3,2,0} transpose(f16[4,4,32,1024]{3,2,1,0} %dot.294), dimensions={0,3,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %reshape.2252 = f16[4,1024,128,1]{3,2,1,0} reshape(f16[4,1024,4,32]{1,3,2,0} %transpose.356), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=170}
  %pad.8 = f16[4,1024,128,3]{3,2,1,0} pad(f16[4,1024,128,1]{3,2,1,0} %reshape.2252, f16[] %constant.89), padding=0_0x0_0x0_0x1_1, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (1, 1, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %add.352 = f16[4,1024,128,3]{3,2,1,0} add(f16[4,1024,128,3]{3,2,1,0} %add.351, f16[4,1024,128,3]{3,2,1,0} %pad.8), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %reshape.2257 = f16[4096,384]{1,0} reshape(f16[4,1024,128,3]{3,2,1,0} %add.352)
  %dot.295 = f16[4096,1024]{1,0} dot(f16[4096,384]{1,0} %reshape.2257, f16[1024,384]{1,0} %convert.192), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.31 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.295), channel_id=32, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.26, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2261 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.31), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.353 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.349, f16[4,1024,1024]{2,1,0} %reshape.2261), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %tuple.3 = (f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) tuple(f16[] %constant.862, s32[] %constant.59, f16[8,1,1,1024]{3,2,1,0} %broadcast.580, f16[8,1,1,1024]{3,2,1,0} %broadcast.581, f32[] %constant.863, /*index=5*/f32[] %constant.69, f32[] %constant.863, f32[] %constant.124, f16[] %constant.864, f16[] %constant.337, /*index=10*/f16[] %constant.865, f32[] %constant.863, f32[] %constant.69, f32[] %constant.863, f32[] %constant.124, /*index=15*/f32[8,1024]{1,0} %broadcast.421, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.582, f32[8,1024]{1,0} %broadcast.421, /*index=20*/f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.582, f32[1024]{0} %param.41, f32[1024]{0} %param.40, /*index=25*/f32[1024]{0} %param.39, f32[128,1024]{1,0} %param.38, f32[384]{0} %param.37, f32[1024,384]{1,0} %param.36, f32[512]{0} %param.43, /*index=30*/f32[1024,512]{1,0} %param.42, f32[1024]{0} %param.46, f32[1024]{0} %param.45, f32[512,1024]{1,0} %param.44, f16[4,1024,1024]{2,1,0} %convert.80, /*index=35*/s32[4,1024]{1,0} %param.13, f16[4,1024,1024]{2,1,0} %add.353), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %opt-barrier.3 = (f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) opt-barrier((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %tuple.3), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.111 = f16[4,1024,1024]{2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=34, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %reshape.2263 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %get-tuple-element.111)
  %get-tuple-element.112 = f32[1024,384]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=28, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.213 = f16[1024,384]{1,0} convert(f32[1024,384]{1,0} %get-tuple-element.112), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.296 = f16[4096,384]{1,0} dot(f16[4096,1024]{1,0} %reshape.2263, f16[1024,384]{1,0} %convert.213), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2265 = f16[4,1024,384]{2,1,0} reshape(f16[4096,384]{1,0} %dot.296), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %get-tuple-element.113 = f32[384]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=27, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.214 = f16[384]{0} convert(f32[384]{0} %get-tuple-element.113), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1089 = f16[4,1024,384]{2,1,0} broadcast(f16[384]{0} %convert.214), dimensions={2}
  %add.354 = f16[4,1024,384]{2,1,0} add(f16[4,1024,384]{2,1,0} %reshape.2265, f16[4,1024,384]{2,1,0} %broadcast.1089), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %reshape.2272 = f16[4,1024,128,3]{3,2,1,0} reshape(f16[4,1024,384]{2,1,0} %add.354), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/reshape[new_sizes=(8, 1024, 1024, 3) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=161}
  %slice.27 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.2272), slice={[0:4], [0:1024], [0:128], [1:2]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/slice[start_indices=(0, 0, 0, 1) limit_indices=(8, 1024, 1024, 2) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %reshape.2276 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %slice.27), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/reshape[new_sizes=(8, 1024, 32, 32) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=170}
  %transpose.357 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.2276), dimensions={0,2,3,1}
  %slice.28 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.2272), slice={[0:4], [0:1024], [0:128], [0:1]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/slice[start_indices=(0, 0, 0, 0) limit_indices=(8, 1024, 1024, 1) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %get-tuple-element.114 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=0, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1090 = f16[4,1024,128,1]{3,2,1,0} broadcast(f16[] %get-tuple-element.114), dimensions={}
  %divide.184 = f16[4,1024,128,1]{3,2,1,0} divide(f16[4,1024,128,1]{3,2,1,0} %slice.28, f16[4,1024,128,1]{3,2,1,0} %broadcast.1090), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %reshape.2285 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %divide.184), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %transpose.358 = f16[4,4,1024,32]{3,1,2,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.2285), dimensions={0,2,1,3}
  %slice.29 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.2272), slice={[0:4], [0:1024], [0:128], [2:3]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/slice[start_indices=(0, 0, 0, 2) limit_indices=(8, 1024, 1024, 3) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %reshape.2288 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %slice.29), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/reshape[new_sizes=(8, 1024, 32, 32) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=173}
  %transpose.359 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.2288), dimensions={0,2,3,1}
  %dot.297 = f16[4,4,1024,1024]{3,2,1,0} dot(f16[4,4,1024,32]{3,1,2,0} %transpose.358, f16[4,4,32,1024]{2,1,3,0} %transpose.359), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/dot_general[dimension_numbers=(((3,), (3,)), ((0, 2), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %get-tuple-element.115 = s32[4,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=35, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.116 = s32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=1, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.715 = s32[4,1024]{1,0} broadcast(s32[] %get-tuple-element.116), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %compare.33 = pred[4,1024]{1,0} compare(s32[4,1024]{1,0} %get-tuple-element.115, s32[4,1024]{1,0} %broadcast.715), direction=GT, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %reshape.2289 = pred[4,1,1,1024]{3,2,1,0} reshape(pred[4,1024]{1,0} %compare.33), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %get-tuple-element.117 = f16[8,1,1,1024]{3,2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=3, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.742 = f16[4,1,1,1024]{3,2,1,0} dynamic-slice(f16[8,1,1,1024]{3,2,1,0} %get-tuple-element.117, s32[] %reshape.1424, s32[] %constant.59, s32[] %constant.59, s32[] %constant.59), dynamic_slice_sizes={4,1,1,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %get-tuple-element.118 = f16[8,1,1,1024]{3,2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.745 = f16[4,1,1,1024]{3,2,1,0} dynamic-slice(f16[8,1,1,1024]{3,2,1,0} %get-tuple-element.118, s32[] %reshape.1424, s32[] %constant.59, s32[] %constant.59, s32[] %constant.59), dynamic_slice_sizes={4,1,1,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %select.21 = f16[4,1,1,1024]{3,2,1,0} select(pred[4,1,1,1024]{3,2,1,0} %reshape.2289, f16[4,1,1,1024]{3,2,1,0} %dynamic-slice.742, f16[4,1,1,1024]{3,2,1,0} %dynamic-slice.745), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %reshape.2295 = f16[4,1024]{1,0} reshape(f16[4,1,1,1024]{3,2,1,0} %select.21), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=100}
  %broadcast.1091 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,1024]{1,0} %reshape.2295), dimensions={0,3}
  %add.355 = f16[4,4,1024,1024]{3,2,1,0} add(f16[4,4,1024,1024]{3,2,1,0} %dot.297, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1091), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=100}
  %reduce.76 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %add.355, f16[] %constant.179), dimensions={3}, to_apply=%region_105.3194, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.717 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %reduce.76), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.209 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %add.355, f16[4,4,1024,1024]{3,2,1,0} %broadcast.717), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.13 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.209), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.215 = f32[4,4,1024,1024]{3,2,1,0} convert(f16[4,4,1024,1024]{3,2,1,0} %exponential.13), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %reduce.77 = f32[4,4,1024]{2,1,0} reduce(f32[4,4,1024,1024]{3,2,1,0} %convert.215, f32[] %constant.69), dimensions={3}, to_apply=%region_106.3206, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.216 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %reduce.77), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.718 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %convert.216), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.185 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %exponential.13, f16[4,4,1024,1024]{3,2,1,0} %broadcast.718), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %dot.298 = f16[4,4,32,1024]{3,2,1,0} dot(f16[4,4,32,1024]{2,1,3,0} %transpose.357, f16[4,4,1024,1024]{3,2,1,0} %divide.185), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/dot_general[dimension_numbers=(((1,), (3,)), ((0, 2), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %transpose.360 = f16[4,1024,4,32]{1,3,2,0} transpose(f16[4,4,32,1024]{3,2,1,0} %dot.298), dimensions={0,3,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %reshape.2302 = f16[4096,128]{1,0} reshape(f16[4,1024,4,32]{1,3,2,0} %transpose.360)
  %get-tuple-element.119 = f32[128,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=26, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.217 = f16[128,1024]{1,0} convert(f32[128,1024]{1,0} %get-tuple-element.119), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.299 = f16[4096,1024]{1,0} dot(f16[4096,128]{1,0} %reshape.2302, f16[128,1024]{1,0} %convert.217), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.32 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.299), channel_id=33, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.27, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2304 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.32), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %get-tuple-element.120 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=25, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.218 = f16[1024]{0} convert(f32[1024]{0} %get-tuple-element.120), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1092 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.218), dimensions={2}
  %add.356 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %reshape.2304, f16[4,1024,1024]{2,1,0} %broadcast.1092), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %add.357 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.356, f16[4,1024,1024]{2,1,0} %get-tuple-element.111), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=233}
  %convert.219 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.357), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %reduce.78 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.219, f32[] %constant.69), dimensions={2}, to_apply=%region_107.3231, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.121 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=4, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.720 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.121), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.186 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %reduce.78, f32[4,1024]{1,0} %broadcast.720), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.721 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.186), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.210 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.219, f32[4,1024,1024]{2,1,0} %broadcast.721), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %get-tuple-element.122 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=5, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.722 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.122), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.519 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.219, f32[4,1024,1024]{2,1,0} %convert.219), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.79 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.519, f32[] %constant.69), dimensions={2}, to_apply=%region_108.3244, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %get-tuple-element.123 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=6, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.723 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.123), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.187 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %reduce.79, f32[4,1024]{1,0} %broadcast.723), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.520 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.186, f32[4,1024]{1,0} %divide.186), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.211 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.187, f32[4,1024]{1,0} %multiply.520), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.19 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.722, f32[4,1024]{1,0} %subtract.211), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.124 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=7, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.724 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.124), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.358 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.19, f32[4,1024]{1,0} %broadcast.724), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.2309 = f32[4,1024,1]{2,1,0} reshape(f32[4,1024]{1,0} %add.358), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.31 = f32[4,1024,1]{2,1,0} rsqrt(f32[4,1024,1]{2,1,0} %reshape.2309), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.2310 = f32[4,1024]{1,0} reshape(f32[4,1024,1]{2,1,0} %rsqrt.31), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.725 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %reshape.2310), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %get-tuple-element.125 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=24, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1093 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %get-tuple-element.125), dimensions={2}
  %multiply.521 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.725, f32[4,1024,1024]{2,1,0} %broadcast.1093), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.522 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.210, f32[4,1024,1024]{2,1,0} %multiply.521), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %get-tuple-element.126 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=23, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1094 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %get-tuple-element.126), dimensions={2}
  %add.359 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.522, f32[4,1024,1024]{2,1,0} %broadcast.1094), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %convert.220 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.359), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %reshape.2315 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %convert.220)
  %get-tuple-element.127 = f32[1024,512]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=30, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.221 = f16[1024,512]{1,0} convert(f32[1024,512]{1,0} %get-tuple-element.127), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.300 = f16[4096,512]{1,0} dot(f16[4096,1024]{1,0} %reshape.2315, f16[1024,512]{1,0} %convert.221), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2317 = f16[4,1024,512]{2,1,0} reshape(f16[4096,512]{1,0} %dot.300), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %get-tuple-element.128 = f32[512]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=29, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.222 = f16[512]{0} convert(f32[512]{0} %get-tuple-element.128), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1096 = f16[4,1024,512]{2,1,0} broadcast(f16[512]{0} %convert.222), dimensions={2}
  %add.360 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %reshape.2317, f16[4,1024,512]{2,1,0} %broadcast.1096), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %get-tuple-element.129 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=8, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1098 = f16[4,1024,512]{2,1,0} broadcast(f16[] %get-tuple-element.129), dimensions={}
  %divide.188 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %add.360, f16[4,1024,512]{2,1,0} %broadcast.1098), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.223 = f32[4,1024,512]{2,1,0} convert(f16[4,1024,512]{2,1,0} %divide.188), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %clamp.9 = f32[4,1024,512]{2,1,0} clamp(f32[4,1024,512]{2,1,0} %broadcast.952, f32[4,1024,512]{2,1,0} %convert.223, f32[4,1024,512]{2,1,0} %broadcast.954), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.523 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.9, f32[4,1024,512]{2,1,0} %clamp.9), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.524 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %multiply.523, f32[4,1024,512]{2,1,0} %broadcast.956), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.361 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.524, f32[4,1024,512]{2,1,0} %broadcast.957), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.525 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.361, f32[4,1024,512]{2,1,0} %multiply.523), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.362 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.525, f32[4,1024,512]{2,1,0} %broadcast.958), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.526 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.362, f32[4,1024,512]{2,1,0} %multiply.523), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.363 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.526, f32[4,1024,512]{2,1,0} %broadcast.959), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.527 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.363, f32[4,1024,512]{2,1,0} %multiply.523), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.364 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.527, f32[4,1024,512]{2,1,0} %broadcast.960), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.528 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.364, f32[4,1024,512]{2,1,0} %multiply.523), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.365 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.528, f32[4,1024,512]{2,1,0} %broadcast.961), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.529 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.365, f32[4,1024,512]{2,1,0} %multiply.523), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.366 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.529, f32[4,1024,512]{2,1,0} %broadcast.962), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.530 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.366, f32[4,1024,512]{2,1,0} %multiply.523), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.367 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.530, f32[4,1024,512]{2,1,0} %broadcast.963), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.531 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.9, f32[4,1024,512]{2,1,0} %add.367), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.368 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.524, f32[4,1024,512]{2,1,0} %broadcast.964), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.532 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.368, f32[4,1024,512]{2,1,0} %multiply.523), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.369 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.532, f32[4,1024,512]{2,1,0} %broadcast.965), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.533 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.369, f32[4,1024,512]{2,1,0} %multiply.523), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.370 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.533, f32[4,1024,512]{2,1,0} %broadcast.966), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.534 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.370, f32[4,1024,512]{2,1,0} %multiply.523), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.371 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.534, f32[4,1024,512]{2,1,0} %broadcast.968), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.535 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.371, f32[4,1024,512]{2,1,0} %multiply.523), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.372 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.535, f32[4,1024,512]{2,1,0} %broadcast.970), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.189 = f32[4,1024,512]{2,1,0} divide(f32[4,1024,512]{2,1,0} %multiply.531, f32[4,1024,512]{2,1,0} %add.372), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.224 = f16[4,1024,512]{2,1,0} convert(f32[4,1024,512]{2,1,0} %divide.189), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %get-tuple-element.130 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=9, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1099 = f16[4,1024,512]{2,1,0} broadcast(f16[] %get-tuple-element.130), dimensions={}
  %add.373 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %convert.224, f16[4,1024,512]{2,1,0} %broadcast.1099), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.536 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %add.360, f16[4,1024,512]{2,1,0} %add.373), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %get-tuple-element.131 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=10, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1100 = f16[4,1024,512]{2,1,0} broadcast(f16[] %get-tuple-element.131), dimensions={}
  %divide.190 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %multiply.536, f16[4,1024,512]{2,1,0} %broadcast.1100), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %reshape.2336 = f16[4096,512]{1,0} reshape(f16[4,1024,512]{2,1,0} %divide.190)
  %get-tuple-element.132 = f32[512,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=33, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.225 = f16[512,1024]{1,0} convert(f32[512,1024]{1,0} %get-tuple-element.132), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.301 = f16[4096,1024]{1,0} dot(f16[4096,512]{1,0} %reshape.2336, f16[512,1024]{1,0} %convert.225), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.33 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.301), channel_id=34, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.28, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2339 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.33), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %get-tuple-element.133 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=32, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.226 = f16[1024]{0} convert(f32[1024]{0} %get-tuple-element.133), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1101 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.226), dimensions={2}
  %add.374 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %reshape.2339, f16[4,1024,1024]{2,1,0} %broadcast.1101), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %add.375 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.374, f16[4,1024,1024]{2,1,0} %convert.220), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=310}
  %convert.227 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.375), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %reduce.80 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.227, f32[] %constant.69), dimensions={2}, to_apply=%region_109.3330, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.134 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=11, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.735 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.134), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.191 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %reduce.80, f32[4,1024]{1,0} %broadcast.735), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.736 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.191), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.224 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.227, f32[4,1024,1024]{2,1,0} %broadcast.736), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %get-tuple-element.135 = f16[4,1024,1024]{2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=36, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.228 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %get-tuple-element.135), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %multiply.537 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.224, f32[4,1024,1024]{2,1,0} %convert.228), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %get-tuple-element.136 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=31, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1102 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %get-tuple-element.136), dimensions={2}
  %multiply.538 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %multiply.537, f32[4,1024,1024]{2,1,0} %broadcast.1102), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.81 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.538, f32[] %constant.69), dimensions={2}, to_apply=%region_113.3411, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reshape.2347 = f32[4,1024,1]{2,1,0} reshape(f32[4,1024]{1,0} %reduce.81), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reshape[new_sizes=(8, 1024, 1) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %get-tuple-element.137 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=12, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.740 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.137), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.539 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.227, f32[4,1024,1024]{2,1,0} %convert.227), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.82 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.539, f32[] %constant.69), dimensions={2}, to_apply=%region_110.3343, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %get-tuple-element.138 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=13, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.741 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.138), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.192 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %reduce.82, f32[4,1024]{1,0} %broadcast.741), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.540 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.191, f32[4,1024]{1,0} %divide.191), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.225 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.192, f32[4,1024]{1,0} %multiply.540), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.20 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.740, f32[4,1024]{1,0} %subtract.225), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.139 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=14, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.742 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.139), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.376 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.20, f32[4,1024]{1,0} %broadcast.742), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.2348 = f32[4,1024,1]{2,1,0} reshape(f32[4,1024]{1,0} %add.376), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.32 = f32[4,1024,1]{2,1,0} rsqrt(f32[4,1024,1]{2,1,0} %reshape.2348), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %divide.193 = f32[4,1024,1]{2,1,0} divide(f32[4,1024,1]{2,1,0} %rsqrt.32, f32[4,1024,1]{2,1,0} %reshape.2348), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.541 = f32[4,1024,1]{2,1,0} multiply(f32[4,1024,1]{2,1,0} %divide.193, f32[4,1024,1]{2,1,0} %broadcast.1046), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.542 = f32[4,1024,1]{2,1,0} multiply(f32[4,1024,1]{2,1,0} %reshape.2347, f32[4,1024,1]{2,1,0} %multiply.541), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.2349 = f32[4,1024]{1,0} reshape(f32[4,1024,1]{2,1,0} %multiply.542), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reshape[new_sizes=(8, 1024) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=132}
  %compare.34 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %subtract.225, f32[4,1024]{1,0} %maximum.20), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.140 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=20, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.791 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.140, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.141 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=19, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.794 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.141, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.22 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.34, f32[4,1024]{1,0} %dynamic-slice.791, f32[4,1024]{1,0} %dynamic-slice.794), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %compare.35 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %broadcast.740, f32[4,1024]{1,0} %maximum.20), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.142 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=22, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.797 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.142, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.143 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=21, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.800 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.143, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.23 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.35, f32[4,1024]{1,0} %dynamic-slice.797, f32[4,1024]{1,0} %dynamic-slice.800), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.194 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %select.22, f32[4,1024]{1,0} %select.23), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.543 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reshape.2349, f32[4,1024]{1,0} %divide.194), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.195 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %multiply.543, f32[4,1024]{1,0} %broadcast.741), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.544 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.195, f32[4,1024]{1,0} %broadcast.1048), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %broadcast.743 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.544), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.545 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.227, f32[4,1024,1024]{2,1,0} %broadcast.743), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %negate.26 = f32[4,1024]{1,0} negate(f32[4,1024]{1,0} %multiply.543), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.546 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.191, f32[4,1024]{1,0} %broadcast.1048), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.547 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %negate.26, f32[4,1024]{1,0} %multiply.546), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reshape.2362 = f32[4,1024]{1,0} reshape(f32[4,1024,1]{2,1,0} %rsqrt.32), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.744 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %reshape.2362), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.548 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.744, f32[4,1024,1024]{2,1,0} %broadcast.1102), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.549 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.228, f32[4,1024,1024]{2,1,0} %multiply.548), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %negate.27 = f32[4,1024,1024]{2,1,0} negate(f32[4,1024,1024]{2,1,0} %multiply.549), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reduce.83 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %negate.27, f32[] %constant.69), dimensions={2}, to_apply=%region_114.3428, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %add.377 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %multiply.547, f32[4,1024]{1,0} %reduce.83), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=131}
  %divide.196 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %add.377, f32[4,1024]{1,0} %broadcast.735), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.745 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.196), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/broadcast_in_dim[shape=(8, 1024, 1024) broadcast_dimensions=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %add.378 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.545, f32[4,1024,1024]{2,1,0} %broadcast.745), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %convert.229 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.378), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %convert.230 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %multiply.549), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %add.379 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %convert.229, f16[4,1024,1024]{2,1,0} %convert.230), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reshape.2363 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %add.379)
  %dot.302 = f16[4096,512]{1,0} dot(f16[4096,1024]{1,0} %reshape.2363, f16[512,1024]{1,0} %convert.225), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %broadcast.1103 = f16[4096,512]{1,0} broadcast(f16[] %get-tuple-element.131), dimensions={}
  %divide.197 = f16[4096,512]{1,0} divide(f16[4096,512]{1,0} %dot.302, f16[4096,512]{1,0} %broadcast.1103), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %reshape.2369 = f16[4,1024,512]{2,1,0} reshape(f16[4096,512]{1,0} %divide.197), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.550 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %add.360, f16[4,1024,512]{2,1,0} %reshape.2369), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.551 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.550, f16[4,1024,512]{2,1,0} %broadcast.1050), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.552 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %divide.188, f16[4,1024,512]{2,1,0} %divide.188), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %negate.28 = f16[4,1024,512]{2,1,0} negate(f16[4,1024,512]{2,1,0} %multiply.552), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/neg" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %exponential.14 = f16[4,1024,512]{2,1,0} exponential(f16[4,1024,512]{2,1,0} %negate.28), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/exp" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.553 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.551, f16[4,1024,512]{2,1,0} %exponential.14), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.198 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %multiply.553, f16[4,1024,512]{2,1,0} %broadcast.1098), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.554 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %reshape.2369, f16[4,1024,512]{2,1,0} %add.373), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.380 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %divide.198, f16[4,1024,512]{2,1,0} %multiply.554), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %reshape.2370 = f16[4096,512]{1,0} reshape(f16[4,1024,512]{2,1,0} %add.380)
  %dot.303 = f16[4096,1024]{1,0} dot(f16[4096,512]{1,0} %reshape.2370, f16[1024,512]{1,0} %convert.221), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.34 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.303), channel_id=35, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.29, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2372 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.34), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.381 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.379, f16[4,1024,1024]{2,1,0} %reshape.2372), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.231 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.381), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %multiply.555 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.210, f32[4,1024,1024]{2,1,0} %convert.231), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.556 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %multiply.555, f32[4,1024,1024]{2,1,0} %broadcast.1093), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.84 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.556, f32[] %constant.69), dimensions={2}, to_apply=%region_119.3491, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reshape.2373 = f32[4,1024,1]{2,1,0} reshape(f32[4,1024]{1,0} %reduce.84), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reshape[new_sizes=(8, 1024, 1) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %divide.199 = f32[4,1024,1]{2,1,0} divide(f32[4,1024,1]{2,1,0} %rsqrt.31, f32[4,1024,1]{2,1,0} %reshape.2309), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.557 = f32[4,1024,1]{2,1,0} multiply(f32[4,1024,1]{2,1,0} %divide.199, f32[4,1024,1]{2,1,0} %broadcast.1046), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.558 = f32[4,1024,1]{2,1,0} multiply(f32[4,1024,1]{2,1,0} %reshape.2373, f32[4,1024,1]{2,1,0} %multiply.557), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.2374 = f32[4,1024]{1,0} reshape(f32[4,1024,1]{2,1,0} %multiply.558), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reshape[new_sizes=(8, 1024) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=132}
  %compare.36 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %subtract.211, f32[4,1024]{1,0} %maximum.19), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.144 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=16, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.810 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.144, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.145 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=15, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.813 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.145, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.24 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.36, f32[4,1024]{1,0} %dynamic-slice.810, f32[4,1024]{1,0} %dynamic-slice.813), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %compare.37 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %broadcast.722, f32[4,1024]{1,0} %maximum.19), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.146 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=18, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.816 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.146, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.147 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.3), index=17, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.819 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.147, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.25 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.37, f32[4,1024]{1,0} %dynamic-slice.816, f32[4,1024]{1,0} %dynamic-slice.819), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.200 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %select.24, f32[4,1024]{1,0} %select.25), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.559 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reshape.2374, f32[4,1024]{1,0} %divide.200), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.201 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %multiply.559, f32[4,1024]{1,0} %broadcast.723), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.560 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.201, f32[4,1024]{1,0} %broadcast.1048), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %broadcast.747 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.560), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.561 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.219, f32[4,1024,1024]{2,1,0} %broadcast.747), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %negate.29 = f32[4,1024]{1,0} negate(f32[4,1024]{1,0} %multiply.559), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.562 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.186, f32[4,1024]{1,0} %broadcast.1048), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.563 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %negate.29, f32[4,1024]{1,0} %multiply.562), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.564 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.231, f32[4,1024,1024]{2,1,0} %multiply.521), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %negate.30 = f32[4,1024,1024]{2,1,0} negate(f32[4,1024,1024]{2,1,0} %multiply.564), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reduce.85 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %negate.30, f32[] %constant.69), dimensions={2}, to_apply=%region_120.3508, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %add.382 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %multiply.563, f32[4,1024]{1,0} %reduce.85), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=131}
  %divide.202 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %add.382, f32[4,1024]{1,0} %broadcast.720), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.748 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.202), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/broadcast_in_dim[shape=(8, 1024, 1024) broadcast_dimensions=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %add.383 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.561, f32[4,1024,1024]{2,1,0} %broadcast.748), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %convert.232 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.383), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %convert.233 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %multiply.564), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %add.384 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %convert.232, f16[4,1024,1024]{2,1,0} %convert.233), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reshape.2383 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %add.384)
  %dot.304 = f16[4096,128]{1,0} dot(f16[4096,1024]{1,0} %reshape.2383, f16[128,1024]{1,0} %convert.217), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2385 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4096,128]{1,0} %dot.304), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/reshape[new_sizes=(8, 1024, 32, 32) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=208}
  %transpose.361 = f16[4,4,1024,32]{3,1,2,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.2385), dimensions={0,2,1,3}
  %dot.305 = f16[4,4,1024,1024]{3,2,1,0} dot(f16[4,4,1024,32]{3,1,2,0} %transpose.361, f16[4,4,32,1024]{2,1,3,0} %transpose.357), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/dot_general[dimension_numbers=(((2,), (3,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %multiply.565 = f16[4,4,1024]{2,1,0} multiply(f16[4,4,1024]{2,1,0} %convert.216, f16[4,4,1024]{2,1,0} %convert.216), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.203 = f16[4,4,1024]{2,1,0} divide(f16[4,4,1024]{2,1,0} %broadcast.1051, f16[4,4,1024]{2,1,0} %multiply.565), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.749 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %divide.203), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.566 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %dot.305, f16[4,4,1024,1024]{3,2,1,0} %broadcast.749), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.567 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %multiply.566, f16[4,4,1024,1024]{3,2,1,0} %exponential.13), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %reduce.86 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %multiply.567, f16[] %constant.89), dimensions={3}, to_apply=%region_122.3539, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %negate.31 = f16[4,4,1024]{2,1,0} negate(f16[4,4,1024]{2,1,0} %reduce.86), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.751 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %negate.31), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/broadcast_in_dim[shape=(8, 32, 1024, 1024) broadcast_dimensions=(0, 1, 2)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.204 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %dot.305, f16[4,4,1024,1024]{3,2,1,0} %broadcast.718), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %add.385 = f16[4,4,1024,1024]{3,2,1,0} add(f16[4,4,1024,1024]{3,2,1,0} %broadcast.751, f16[4,4,1024,1024]{3,2,1,0} %divide.204), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.568 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %add.385, f16[4,4,1024,1024]{3,2,1,0} %exponential.13), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %dot.306 = f16[4,4,1024,32]{3,2,1,0} dot(f16[4,4,1024,1024]{3,2,1,0} %multiply.568, f16[4,4,1024,32]{3,1,2,0} %transpose.358), lhs_batch_dims={0,1}, lhs_contracting_dims={2}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/dot_general[dimension_numbers=(((2,), (1,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %transpose.362 = f16[4,1024,4,32]{3,1,2,0} transpose(f16[4,4,1024,32]{3,2,1,0} %dot.306), dimensions={0,2,1,3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/transpose[permutation=(0, 2, 1, 3)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %reshape.2386 = f16[4,1024,128,1]{3,2,1,0} reshape(f16[4,1024,4,32]{3,1,2,0} %transpose.362), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=173}
  %pad.9 = f16[4,1024,128,3]{3,2,1,0} pad(f16[4,1024,128,1]{3,2,1,0} %reshape.2386, f16[] %constant.89), padding=0_0x0_0x0_0x2_0, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (2, 0, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.363 = f16[4,4,1024,32]{3,1,2,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.2288), dimensions={0,2,1,3}
  %dot.307 = f16[4,4,1024,32]{3,2,1,0} dot(f16[4,4,1024,1024]{3,2,1,0} %multiply.568, f16[4,4,1024,32]{3,1,2,0} %transpose.363), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/dot_general[dimension_numbers=(((3,), (1,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %broadcast.1104 = f16[4,4,1024,32]{3,2,1,0} broadcast(f16[] %get-tuple-element.114), dimensions={}
  %divide.205 = f16[4,4,1024,32]{3,2,1,0} divide(f16[4,4,1024,32]{3,2,1,0} %dot.307, f16[4,4,1024,32]{3,2,1,0} %broadcast.1104), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %transpose.364 = f16[4,1024,4,32]{3,1,2,0} transpose(f16[4,4,1024,32]{3,2,1,0} %divide.205), dimensions={0,2,1,3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %reshape.2393 = f16[4,1024,128,1]{3,2,1,0} reshape(f16[4,1024,4,32]{3,1,2,0} %transpose.364), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=167}
  %pad.10 = f16[4,1024,128,3]{3,2,1,0} pad(f16[4,1024,128,1]{3,2,1,0} %reshape.2393, f16[] %constant.89), padding=0_0x0_0x0_0x0_2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 2, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %add.386 = f16[4,1024,128,3]{3,2,1,0} add(f16[4,1024,128,3]{3,2,1,0} %pad.9, f16[4,1024,128,3]{3,2,1,0} %pad.10), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.365 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.2385), dimensions={0,2,3,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/transpose[permutation=(0, 2, 3, 1)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %dot.308 = f16[4,4,32,1024]{3,2,1,0} dot(f16[4,4,32,1024]{2,1,3,0} %transpose.365, f16[4,4,1024,1024]{3,2,1,0} %divide.185), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/dot_general[dimension_numbers=(((3,), (2,)), ((0, 1), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %transpose.366 = f16[4,1024,4,32]{1,3,2,0} transpose(f16[4,4,32,1024]{3,2,1,0} %dot.308), dimensions={0,3,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %reshape.2396 = f16[4,1024,128,1]{3,2,1,0} reshape(f16[4,1024,4,32]{1,3,2,0} %transpose.366), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=170}
  %pad.11 = f16[4,1024,128,3]{3,2,1,0} pad(f16[4,1024,128,1]{3,2,1,0} %reshape.2396, f16[] %constant.89), padding=0_0x0_0x0_0x1_1, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (1, 1, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %add.387 = f16[4,1024,128,3]{3,2,1,0} add(f16[4,1024,128,3]{3,2,1,0} %add.386, f16[4,1024,128,3]{3,2,1,0} %pad.11), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %reshape.2399 = f16[4096,384]{1,0} reshape(f16[4,1024,128,3]{3,2,1,0} %add.387)
  %dot.309 = f16[4096,1024]{1,0} dot(f16[4096,384]{1,0} %reshape.2399, f16[1024,384]{1,0} %convert.213), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.35 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.309), channel_id=36, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.30, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2401 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.35), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.388 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.384, f16[4,1024,1024]{2,1,0} %reshape.2401), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %tuple.4 = (f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) tuple(f16[] %constant.862, s32[] %constant.59, f16[8,1,1,1024]{3,2,1,0} %broadcast.580, f16[8,1,1,1024]{3,2,1,0} %broadcast.581, f32[] %constant.863, /*index=5*/f32[] %constant.69, f32[] %constant.863, f32[] %constant.124, f16[] %constant.864, f16[] %constant.337, /*index=10*/f16[] %constant.865, f32[] %constant.863, f32[] %constant.69, f32[] %constant.863, f32[] %constant.124, /*index=15*/f32[8,1024]{1,0} %broadcast.421, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.582, f32[8,1024]{1,0} %broadcast.421, /*index=20*/f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.582, f32[1024]{0} %param.29, f32[1024]{0} %param.28, /*index=25*/f32[1024]{0} %param.27, f32[128,1024]{1,0} %param.26, f32[384]{0} %param.25, f32[1024,384]{1,0} %param.24, f32[512]{0} %param.31, /*index=30*/f32[1024,512]{1,0} %param.30, f32[1024]{0} %param.34, f32[1024]{0} %param.33, f32[512,1024]{1,0} %param.32, f16[4,1024,1024]{2,1,0} %convert.64, /*index=35*/s32[4,1024]{1,0} %param.13, f16[4,1024,1024]{2,1,0} %add.388), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %opt-barrier.4 = (f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) opt-barrier((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %tuple.4), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.148 = f16[4,1024,1024]{2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=34, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %reshape.2402 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %get-tuple-element.148)
  %get-tuple-element.149 = f32[1024,384]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=28, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.234 = f16[1024,384]{1,0} convert(f32[1024,384]{1,0} %get-tuple-element.149), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.310 = f16[4096,384]{1,0} dot(f16[4096,1024]{1,0} %reshape.2402, f16[1024,384]{1,0} %convert.234), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2405 = f16[4,1024,384]{2,1,0} reshape(f16[4096,384]{1,0} %dot.310), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %get-tuple-element.150 = f32[384]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=27, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.235 = f16[384]{0} convert(f32[384]{0} %get-tuple-element.150), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1105 = f16[4,1024,384]{2,1,0} broadcast(f16[384]{0} %convert.235), dimensions={2}
  %add.389 = f16[4,1024,384]{2,1,0} add(f16[4,1024,384]{2,1,0} %reshape.2405, f16[4,1024,384]{2,1,0} %broadcast.1105), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %reshape.2411 = f16[4,1024,128,3]{3,2,1,0} reshape(f16[4,1024,384]{2,1,0} %add.389), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/reshape[new_sizes=(8, 1024, 1024, 3) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=161}
  %slice.30 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.2411), slice={[0:4], [0:1024], [0:128], [1:2]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/slice[start_indices=(0, 0, 0, 1) limit_indices=(8, 1024, 1024, 2) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %reshape.2414 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %slice.30), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/reshape[new_sizes=(8, 1024, 32, 32) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=170}
  %transpose.367 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.2414), dimensions={0,2,3,1}
  %slice.31 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.2411), slice={[0:4], [0:1024], [0:128], [0:1]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/slice[start_indices=(0, 0, 0, 0) limit_indices=(8, 1024, 1024, 1) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %get-tuple-element.151 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=0, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1106 = f16[4,1024,128,1]{3,2,1,0} broadcast(f16[] %get-tuple-element.151), dimensions={}
  %divide.206 = f16[4,1024,128,1]{3,2,1,0} divide(f16[4,1024,128,1]{3,2,1,0} %slice.31, f16[4,1024,128,1]{3,2,1,0} %broadcast.1106), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %reshape.2423 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %divide.206), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %transpose.368 = f16[4,4,1024,32]{3,1,2,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.2423), dimensions={0,2,1,3}
  %slice.32 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.2411), slice={[0:4], [0:1024], [0:128], [2:3]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/slice[start_indices=(0, 0, 0, 2) limit_indices=(8, 1024, 1024, 3) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %reshape.2426 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %slice.32), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/reshape[new_sizes=(8, 1024, 32, 32) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=173}
  %transpose.369 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.2426), dimensions={0,2,3,1}
  %dot.311 = f16[4,4,1024,1024]{3,2,1,0} dot(f16[4,4,1024,32]{3,1,2,0} %transpose.368, f16[4,4,32,1024]{2,1,3,0} %transpose.369), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/dot_general[dimension_numbers=(((3,), (3,)), ((0, 2), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %get-tuple-element.152 = s32[4,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=35, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.153 = s32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=1, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.756 = s32[4,1024]{1,0} broadcast(s32[] %get-tuple-element.153), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %compare.38 = pred[4,1024]{1,0} compare(s32[4,1024]{1,0} %get-tuple-element.152, s32[4,1024]{1,0} %broadcast.756), direction=GT, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %reshape.2427 = pred[4,1,1,1024]{3,2,1,0} reshape(pred[4,1024]{1,0} %compare.38), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %get-tuple-element.154 = f16[8,1,1,1024]{3,2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=3, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.852 = f16[4,1,1,1024]{3,2,1,0} dynamic-slice(f16[8,1,1,1024]{3,2,1,0} %get-tuple-element.154, s32[] %reshape.1424, s32[] %constant.59, s32[] %constant.59, s32[] %constant.59), dynamic_slice_sizes={4,1,1,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %get-tuple-element.155 = f16[8,1,1,1024]{3,2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.855 = f16[4,1,1,1024]{3,2,1,0} dynamic-slice(f16[8,1,1,1024]{3,2,1,0} %get-tuple-element.155, s32[] %reshape.1424, s32[] %constant.59, s32[] %constant.59, s32[] %constant.59), dynamic_slice_sizes={4,1,1,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %select.26 = f16[4,1,1,1024]{3,2,1,0} select(pred[4,1,1,1024]{3,2,1,0} %reshape.2427, f16[4,1,1,1024]{3,2,1,0} %dynamic-slice.852, f16[4,1,1,1024]{3,2,1,0} %dynamic-slice.855), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %reshape.2432 = f16[4,1024]{1,0} reshape(f16[4,1,1,1024]{3,2,1,0} %select.26), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=100}
  %broadcast.1107 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,1024]{1,0} %reshape.2432), dimensions={0,3}
  %add.390 = f16[4,4,1024,1024]{3,2,1,0} add(f16[4,4,1024,1024]{3,2,1,0} %dot.311, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1107), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=100}
  %reduce.87 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %add.390, f16[] %constant.179), dimensions={3}, to_apply=%region_125.3653, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.758 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %reduce.87), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.243 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %add.390, f16[4,4,1024,1024]{3,2,1,0} %broadcast.758), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.15 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.243), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.236 = f32[4,4,1024,1024]{3,2,1,0} convert(f16[4,4,1024,1024]{3,2,1,0} %exponential.15), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %reduce.88 = f32[4,4,1024]{2,1,0} reduce(f32[4,4,1024,1024]{3,2,1,0} %convert.236, f32[] %constant.69), dimensions={3}, to_apply=%region_126.3665, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.237 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %reduce.88), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.759 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %convert.237), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.207 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %exponential.15, f16[4,4,1024,1024]{3,2,1,0} %broadcast.759), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %dot.312 = f16[4,4,32,1024]{3,2,1,0} dot(f16[4,4,32,1024]{2,1,3,0} %transpose.367, f16[4,4,1024,1024]{3,2,1,0} %divide.207), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/dot_general[dimension_numbers=(((1,), (3,)), ((0, 2), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %transpose.370 = f16[4,1024,4,32]{1,3,2,0} transpose(f16[4,4,32,1024]{3,2,1,0} %dot.312), dimensions={0,3,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %reshape.2438 = f16[4096,128]{1,0} reshape(f16[4,1024,4,32]{1,3,2,0} %transpose.370)
  %get-tuple-element.156 = f32[128,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=26, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.238 = f16[128,1024]{1,0} convert(f32[128,1024]{1,0} %get-tuple-element.156), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.313 = f16[4096,1024]{1,0} dot(f16[4096,128]{1,0} %reshape.2438, f16[128,1024]{1,0} %convert.238), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.36 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.313), channel_id=37, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.31, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2441 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.36), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %get-tuple-element.157 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=25, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.239 = f16[1024]{0} convert(f32[1024]{0} %get-tuple-element.157), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1108 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.239), dimensions={2}
  %add.391 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %reshape.2441, f16[4,1024,1024]{2,1,0} %broadcast.1108), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %add.392 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.391, f16[4,1024,1024]{2,1,0} %get-tuple-element.148), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=233}
  %convert.240 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.392), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %reduce.89 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.240, f32[] %constant.69), dimensions={2}, to_apply=%region_127.3690, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.158 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=4, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.762 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.158), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.208 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %reduce.89, f32[4,1024]{1,0} %broadcast.762), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.764 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.208), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.244 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.240, f32[4,1024,1024]{2,1,0} %broadcast.764), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %get-tuple-element.159 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=5, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.765 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.159), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.569 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.240, f32[4,1024,1024]{2,1,0} %convert.240), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.90 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.569, f32[] %constant.69), dimensions={2}, to_apply=%region_128.3703, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %get-tuple-element.160 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=6, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.766 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.160), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.209 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %reduce.90, f32[4,1024]{1,0} %broadcast.766), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.570 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.208, f32[4,1024]{1,0} %divide.208), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.245 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.209, f32[4,1024]{1,0} %multiply.570), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.21 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.765, f32[4,1024]{1,0} %subtract.245), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.161 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=7, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.767 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.161), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.393 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.21, f32[4,1024]{1,0} %broadcast.767), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.2446 = f32[4,1024,1]{2,1,0} reshape(f32[4,1024]{1,0} %add.393), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.33 = f32[4,1024,1]{2,1,0} rsqrt(f32[4,1024,1]{2,1,0} %reshape.2446), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.2447 = f32[4,1024]{1,0} reshape(f32[4,1024,1]{2,1,0} %rsqrt.33), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.768 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %reshape.2447), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %get-tuple-element.162 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=24, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1109 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %get-tuple-element.162), dimensions={2}
  %multiply.571 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.768, f32[4,1024,1024]{2,1,0} %broadcast.1109), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.572 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.244, f32[4,1024,1024]{2,1,0} %multiply.571), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %get-tuple-element.163 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=23, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1110 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %get-tuple-element.163), dimensions={2}
  %add.394 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.572, f32[4,1024,1024]{2,1,0} %broadcast.1110), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %convert.241 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.394), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %reshape.2452 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %convert.241)
  %get-tuple-element.164 = f32[1024,512]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=30, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.242 = f16[1024,512]{1,0} convert(f32[1024,512]{1,0} %get-tuple-element.164), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.314 = f16[4096,512]{1,0} dot(f16[4096,1024]{1,0} %reshape.2452, f16[1024,512]{1,0} %convert.242), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2454 = f16[4,1024,512]{2,1,0} reshape(f16[4096,512]{1,0} %dot.314), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %get-tuple-element.165 = f32[512]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=29, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.243 = f16[512]{0} convert(f32[512]{0} %get-tuple-element.165), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1112 = f16[4,1024,512]{2,1,0} broadcast(f16[512]{0} %convert.243), dimensions={2}
  %add.395 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %reshape.2454, f16[4,1024,512]{2,1,0} %broadcast.1112), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %get-tuple-element.166 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=8, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1114 = f16[4,1024,512]{2,1,0} broadcast(f16[] %get-tuple-element.166), dimensions={}
  %divide.210 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %add.395, f16[4,1024,512]{2,1,0} %broadcast.1114), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.244 = f32[4,1024,512]{2,1,0} convert(f16[4,1024,512]{2,1,0} %divide.210), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %clamp.10 = f32[4,1024,512]{2,1,0} clamp(f32[4,1024,512]{2,1,0} %broadcast.952, f32[4,1024,512]{2,1,0} %convert.244, f32[4,1024,512]{2,1,0} %broadcast.954), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.573 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.10, f32[4,1024,512]{2,1,0} %clamp.10), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.574 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %multiply.573, f32[4,1024,512]{2,1,0} %broadcast.956), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.396 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.574, f32[4,1024,512]{2,1,0} %broadcast.957), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.575 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.396, f32[4,1024,512]{2,1,0} %multiply.573), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.397 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.575, f32[4,1024,512]{2,1,0} %broadcast.958), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.576 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.397, f32[4,1024,512]{2,1,0} %multiply.573), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.398 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.576, f32[4,1024,512]{2,1,0} %broadcast.959), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.577 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.398, f32[4,1024,512]{2,1,0} %multiply.573), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.399 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.577, f32[4,1024,512]{2,1,0} %broadcast.960), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.578 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.399, f32[4,1024,512]{2,1,0} %multiply.573), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.400 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.578, f32[4,1024,512]{2,1,0} %broadcast.961), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.579 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.400, f32[4,1024,512]{2,1,0} %multiply.573), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.401 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.579, f32[4,1024,512]{2,1,0} %broadcast.962), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.580 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.401, f32[4,1024,512]{2,1,0} %multiply.573), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.402 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.580, f32[4,1024,512]{2,1,0} %broadcast.963), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.581 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.10, f32[4,1024,512]{2,1,0} %add.402), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.403 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.574, f32[4,1024,512]{2,1,0} %broadcast.964), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.582 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.403, f32[4,1024,512]{2,1,0} %multiply.573), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.404 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.582, f32[4,1024,512]{2,1,0} %broadcast.965), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.583 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.404, f32[4,1024,512]{2,1,0} %multiply.573), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.405 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.583, f32[4,1024,512]{2,1,0} %broadcast.966), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.584 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.405, f32[4,1024,512]{2,1,0} %multiply.573), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.406 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.584, f32[4,1024,512]{2,1,0} %broadcast.968), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.585 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.406, f32[4,1024,512]{2,1,0} %multiply.573), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.407 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.585, f32[4,1024,512]{2,1,0} %broadcast.970), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.211 = f32[4,1024,512]{2,1,0} divide(f32[4,1024,512]{2,1,0} %multiply.581, f32[4,1024,512]{2,1,0} %add.407), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.245 = f16[4,1024,512]{2,1,0} convert(f32[4,1024,512]{2,1,0} %divide.211), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %get-tuple-element.167 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=9, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1115 = f16[4,1024,512]{2,1,0} broadcast(f16[] %get-tuple-element.167), dimensions={}
  %add.408 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %convert.245, f16[4,1024,512]{2,1,0} %broadcast.1115), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.586 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %add.395, f16[4,1024,512]{2,1,0} %add.408), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %get-tuple-element.168 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=10, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1116 = f16[4,1024,512]{2,1,0} broadcast(f16[] %get-tuple-element.168), dimensions={}
  %divide.212 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %multiply.586, f16[4,1024,512]{2,1,0} %broadcast.1116), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %reshape.2471 = f16[4096,512]{1,0} reshape(f16[4,1024,512]{2,1,0} %divide.212)
  %get-tuple-element.169 = f32[512,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=33, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.246 = f16[512,1024]{1,0} convert(f32[512,1024]{1,0} %get-tuple-element.169), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.315 = f16[4096,1024]{1,0} dot(f16[4096,512]{1,0} %reshape.2471, f16[512,1024]{1,0} %convert.246), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.37 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.315), channel_id=38, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.32, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2473 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.37), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %get-tuple-element.170 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=32, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.247 = f16[1024]{0} convert(f32[1024]{0} %get-tuple-element.170), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1117 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.247), dimensions={2}
  %add.409 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %reshape.2473, f16[4,1024,1024]{2,1,0} %broadcast.1117), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %add.410 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.409, f16[4,1024,1024]{2,1,0} %convert.241), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=310}
  %convert.248 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.410), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %reduce.91 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.248, f32[] %constant.69), dimensions={2}, to_apply=%region_129.3789, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.171 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=11, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.778 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.171), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.213 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %reduce.91, f32[4,1024]{1,0} %broadcast.778), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.779 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.213), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.258 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.248, f32[4,1024,1024]{2,1,0} %broadcast.779), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %get-tuple-element.172 = f16[4,1024,1024]{2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=36, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.249 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %get-tuple-element.172), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %multiply.588 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.258, f32[4,1024,1024]{2,1,0} %convert.249), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %get-tuple-element.173 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=31, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1118 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %get-tuple-element.173), dimensions={2}
  %multiply.589 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %multiply.588, f32[4,1024,1024]{2,1,0} %broadcast.1118), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.92 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.589, f32[] %constant.69), dimensions={2}, to_apply=%region_133.3870, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reshape.2478 = f32[4,1024,1]{2,1,0} reshape(f32[4,1024]{1,0} %reduce.92), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reshape[new_sizes=(8, 1024, 1) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %get-tuple-element.174 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=12, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.781 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.174), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.590 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.248, f32[4,1024,1024]{2,1,0} %convert.248), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.93 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.590, f32[] %constant.69), dimensions={2}, to_apply=%region_130.3802, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %get-tuple-element.175 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=13, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.782 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.175), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.214 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %reduce.93, f32[4,1024]{1,0} %broadcast.782), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.591 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.213, f32[4,1024]{1,0} %divide.213), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.259 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.214, f32[4,1024]{1,0} %multiply.591), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.22 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.781, f32[4,1024]{1,0} %subtract.259), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.176 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=14, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.783 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.176), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.411 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.22, f32[4,1024]{1,0} %broadcast.783), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.2479 = f32[4,1024,1]{2,1,0} reshape(f32[4,1024]{1,0} %add.411), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.34 = f32[4,1024,1]{2,1,0} rsqrt(f32[4,1024,1]{2,1,0} %reshape.2479), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %divide.215 = f32[4,1024,1]{2,1,0} divide(f32[4,1024,1]{2,1,0} %rsqrt.34, f32[4,1024,1]{2,1,0} %reshape.2479), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.592 = f32[4,1024,1]{2,1,0} multiply(f32[4,1024,1]{2,1,0} %divide.215, f32[4,1024,1]{2,1,0} %broadcast.1046), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.593 = f32[4,1024,1]{2,1,0} multiply(f32[4,1024,1]{2,1,0} %reshape.2478, f32[4,1024,1]{2,1,0} %multiply.592), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.2480 = f32[4,1024]{1,0} reshape(f32[4,1024,1]{2,1,0} %multiply.593), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reshape[new_sizes=(8, 1024) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=132}
  %compare.39 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %subtract.259, f32[4,1024]{1,0} %maximum.22), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.177 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=20, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.901 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.177, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.178 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=19, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.904 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.178, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.27 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.39, f32[4,1024]{1,0} %dynamic-slice.901, f32[4,1024]{1,0} %dynamic-slice.904), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %compare.40 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %broadcast.781, f32[4,1024]{1,0} %maximum.22), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.179 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=22, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.907 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.179, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.180 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=21, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.910 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.180, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.28 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.40, f32[4,1024]{1,0} %dynamic-slice.907, f32[4,1024]{1,0} %dynamic-slice.910), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.216 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %select.27, f32[4,1024]{1,0} %select.28), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.595 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reshape.2480, f32[4,1024]{1,0} %divide.216), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.217 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %multiply.595, f32[4,1024]{1,0} %broadcast.782), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.596 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.217, f32[4,1024]{1,0} %broadcast.1048), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %broadcast.784 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.596), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.597 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.248, f32[4,1024,1024]{2,1,0} %broadcast.784), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %negate.32 = f32[4,1024]{1,0} negate(f32[4,1024]{1,0} %multiply.595), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.598 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.213, f32[4,1024]{1,0} %broadcast.1048), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.599 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %negate.32, f32[4,1024]{1,0} %multiply.598), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reshape.2491 = f32[4,1024]{1,0} reshape(f32[4,1024,1]{2,1,0} %rsqrt.34), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.785 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %reshape.2491), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.600 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.785, f32[4,1024,1024]{2,1,0} %broadcast.1118), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.601 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.249, f32[4,1024,1024]{2,1,0} %multiply.600), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %negate.33 = f32[4,1024,1024]{2,1,0} negate(f32[4,1024,1024]{2,1,0} %multiply.601), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reduce.94 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %negate.33, f32[] %constant.69), dimensions={2}, to_apply=%region_134.3887, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %add.412 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %multiply.599, f32[4,1024]{1,0} %reduce.94), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=131}
  %divide.218 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %add.412, f32[4,1024]{1,0} %broadcast.778), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.786 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.218), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/broadcast_in_dim[shape=(8, 1024, 1024) broadcast_dimensions=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %add.413 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.597, f32[4,1024,1024]{2,1,0} %broadcast.786), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %convert.250 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.413), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %convert.251 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %multiply.601), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %add.414 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %convert.250, f16[4,1024,1024]{2,1,0} %convert.251), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reshape.2492 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %add.414)
  %dot.316 = f16[4096,512]{1,0} dot(f16[4096,1024]{1,0} %reshape.2492, f16[512,1024]{1,0} %convert.246), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %broadcast.1119 = f16[4096,512]{1,0} broadcast(f16[] %get-tuple-element.168), dimensions={}
  %divide.219 = f16[4096,512]{1,0} divide(f16[4096,512]{1,0} %dot.316, f16[4096,512]{1,0} %broadcast.1119), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %reshape.2499 = f16[4,1024,512]{2,1,0} reshape(f16[4096,512]{1,0} %divide.219), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.602 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %add.395, f16[4,1024,512]{2,1,0} %reshape.2499), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.603 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.602, f16[4,1024,512]{2,1,0} %broadcast.1050), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.604 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %divide.210, f16[4,1024,512]{2,1,0} %divide.210), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %negate.34 = f16[4,1024,512]{2,1,0} negate(f16[4,1024,512]{2,1,0} %multiply.604), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/neg" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %exponential.16 = f16[4,1024,512]{2,1,0} exponential(f16[4,1024,512]{2,1,0} %negate.34), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/exp" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.605 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.603, f16[4,1024,512]{2,1,0} %exponential.16), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.220 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %multiply.605, f16[4,1024,512]{2,1,0} %broadcast.1114), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.606 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %reshape.2499, f16[4,1024,512]{2,1,0} %add.408), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.415 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %divide.220, f16[4,1024,512]{2,1,0} %multiply.606), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %reshape.2501 = f16[4096,512]{1,0} reshape(f16[4,1024,512]{2,1,0} %add.415)
  %dot.317 = f16[4096,1024]{1,0} dot(f16[4096,512]{1,0} %reshape.2501, f16[1024,512]{1,0} %convert.242), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.38 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.317), channel_id=39, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.33, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2503 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.38), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.416 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.414, f16[4,1024,1024]{2,1,0} %reshape.2503), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.252 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.416), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %multiply.609 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.244, f32[4,1024,1024]{2,1,0} %convert.252), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.610 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %multiply.609, f32[4,1024,1024]{2,1,0} %broadcast.1109), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.95 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.610, f32[] %constant.69), dimensions={2}, to_apply=%region_139.3950, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reshape.2504 = f32[4,1024,1]{2,1,0} reshape(f32[4,1024]{1,0} %reduce.95), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reshape[new_sizes=(8, 1024, 1) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %divide.221 = f32[4,1024,1]{2,1,0} divide(f32[4,1024,1]{2,1,0} %rsqrt.33, f32[4,1024,1]{2,1,0} %reshape.2446), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.611 = f32[4,1024,1]{2,1,0} multiply(f32[4,1024,1]{2,1,0} %divide.221, f32[4,1024,1]{2,1,0} %broadcast.1046), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.612 = f32[4,1024,1]{2,1,0} multiply(f32[4,1024,1]{2,1,0} %reshape.2504, f32[4,1024,1]{2,1,0} %multiply.611), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.2505 = f32[4,1024]{1,0} reshape(f32[4,1024,1]{2,1,0} %multiply.612), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reshape[new_sizes=(8, 1024) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=132}
  %compare.41 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %subtract.245, f32[4,1024]{1,0} %maximum.21), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.181 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=16, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.920 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.181, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.182 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=15, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.923 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.182, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.29 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.41, f32[4,1024]{1,0} %dynamic-slice.920, f32[4,1024]{1,0} %dynamic-slice.923), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %compare.42 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %broadcast.765, f32[4,1024]{1,0} %maximum.21), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.183 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=18, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.926 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.183, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.184 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.4), index=17, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.929 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.184, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.30 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.42, f32[4,1024]{1,0} %dynamic-slice.926, f32[4,1024]{1,0} %dynamic-slice.929), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.222 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %select.29, f32[4,1024]{1,0} %select.30), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.613 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reshape.2505, f32[4,1024]{1,0} %divide.222), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.223 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %multiply.613, f32[4,1024]{1,0} %broadcast.766), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.614 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.223, f32[4,1024]{1,0} %broadcast.1048), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %broadcast.788 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.614), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.615 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.240, f32[4,1024,1024]{2,1,0} %broadcast.788), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %negate.35 = f32[4,1024]{1,0} negate(f32[4,1024]{1,0} %multiply.613), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.616 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.208, f32[4,1024]{1,0} %broadcast.1048), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.617 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %negate.35, f32[4,1024]{1,0} %multiply.616), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.618 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.252, f32[4,1024,1024]{2,1,0} %multiply.571), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %negate.36 = f32[4,1024,1024]{2,1,0} negate(f32[4,1024,1024]{2,1,0} %multiply.618), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reduce.96 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %negate.36, f32[] %constant.69), dimensions={2}, to_apply=%region_140.3967, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %add.417 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %multiply.617, f32[4,1024]{1,0} %reduce.96), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=131}
  %divide.224 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %add.417, f32[4,1024]{1,0} %broadcast.762), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.789 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.224), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/broadcast_in_dim[shape=(8, 1024, 1024) broadcast_dimensions=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %add.418 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.615, f32[4,1024,1024]{2,1,0} %broadcast.789), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %convert.253 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.418), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %convert.254 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %multiply.618), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %add.419 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %convert.253, f16[4,1024,1024]{2,1,0} %convert.254), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reshape.2514 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %add.419)
  %dot.318 = f16[4096,128]{1,0} dot(f16[4096,1024]{1,0} %reshape.2514, f16[128,1024]{1,0} %convert.238), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2516 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4096,128]{1,0} %dot.318), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/reshape[new_sizes=(8, 1024, 32, 32) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=208}
  %transpose.371 = f16[4,4,1024,32]{3,1,2,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.2516), dimensions={0,2,1,3}
  %dot.319 = f16[4,4,1024,1024]{3,2,1,0} dot(f16[4,4,1024,32]{3,1,2,0} %transpose.371, f16[4,4,32,1024]{2,1,3,0} %transpose.367), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/dot_general[dimension_numbers=(((2,), (3,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %multiply.619 = f16[4,4,1024]{2,1,0} multiply(f16[4,4,1024]{2,1,0} %convert.237, f16[4,4,1024]{2,1,0} %convert.237), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.225 = f16[4,4,1024]{2,1,0} divide(f16[4,4,1024]{2,1,0} %broadcast.1051, f16[4,4,1024]{2,1,0} %multiply.619), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.790 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %divide.225), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.620 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %dot.319, f16[4,4,1024,1024]{3,2,1,0} %broadcast.790), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.621 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %multiply.620, f16[4,4,1024,1024]{3,2,1,0} %exponential.15), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %reduce.97 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %multiply.621, f16[] %constant.89), dimensions={3}, to_apply=%region_142.3998, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %negate.37 = f16[4,4,1024]{2,1,0} negate(f16[4,4,1024]{2,1,0} %reduce.97), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.791 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %negate.37), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/broadcast_in_dim[shape=(8, 32, 1024, 1024) broadcast_dimensions=(0, 1, 2)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.226 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %dot.319, f16[4,4,1024,1024]{3,2,1,0} %broadcast.759), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %add.420 = f16[4,4,1024,1024]{3,2,1,0} add(f16[4,4,1024,1024]{3,2,1,0} %broadcast.791, f16[4,4,1024,1024]{3,2,1,0} %divide.226), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.622 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %add.420, f16[4,4,1024,1024]{3,2,1,0} %exponential.15), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %dot.320 = f16[4,4,1024,32]{3,2,1,0} dot(f16[4,4,1024,1024]{3,2,1,0} %multiply.622, f16[4,4,1024,32]{3,1,2,0} %transpose.368), lhs_batch_dims={0,1}, lhs_contracting_dims={2}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/dot_general[dimension_numbers=(((2,), (1,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %transpose.372 = f16[4,1024,4,32]{3,1,2,0} transpose(f16[4,4,1024,32]{3,2,1,0} %dot.320), dimensions={0,2,1,3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/transpose[permutation=(0, 2, 1, 3)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %reshape.2517 = f16[4,1024,128,1]{3,2,1,0} reshape(f16[4,1024,4,32]{3,1,2,0} %transpose.372), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=173}
  %pad.12 = f16[4,1024,128,3]{3,2,1,0} pad(f16[4,1024,128,1]{3,2,1,0} %reshape.2517, f16[] %constant.89), padding=0_0x0_0x0_0x2_0, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (2, 0, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.373 = f16[4,4,1024,32]{3,1,2,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.2426), dimensions={0,2,1,3}
  %dot.321 = f16[4,4,1024,32]{3,2,1,0} dot(f16[4,4,1024,1024]{3,2,1,0} %multiply.622, f16[4,4,1024,32]{3,1,2,0} %transpose.373), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/dot_general[dimension_numbers=(((3,), (1,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %broadcast.1120 = f16[4,4,1024,32]{3,2,1,0} broadcast(f16[] %get-tuple-element.151), dimensions={}
  %divide.227 = f16[4,4,1024,32]{3,2,1,0} divide(f16[4,4,1024,32]{3,2,1,0} %dot.321, f16[4,4,1024,32]{3,2,1,0} %broadcast.1120), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %transpose.374 = f16[4,1024,4,32]{3,1,2,0} transpose(f16[4,4,1024,32]{3,2,1,0} %divide.227), dimensions={0,2,1,3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %reshape.2524 = f16[4,1024,128,1]{3,2,1,0} reshape(f16[4,1024,4,32]{3,1,2,0} %transpose.374), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=167}
  %pad.13 = f16[4,1024,128,3]{3,2,1,0} pad(f16[4,1024,128,1]{3,2,1,0} %reshape.2524, f16[] %constant.89), padding=0_0x0_0x0_0x0_2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 2, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %add.421 = f16[4,1024,128,3]{3,2,1,0} add(f16[4,1024,128,3]{3,2,1,0} %pad.12, f16[4,1024,128,3]{3,2,1,0} %pad.13), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.375 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.2516), dimensions={0,2,3,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/transpose[permutation=(0, 2, 3, 1)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %dot.322 = f16[4,4,32,1024]{3,2,1,0} dot(f16[4,4,32,1024]{2,1,3,0} %transpose.375, f16[4,4,1024,1024]{3,2,1,0} %divide.207), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/dot_general[dimension_numbers=(((3,), (2,)), ((0, 1), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %transpose.376 = f16[4,1024,4,32]{1,3,2,0} transpose(f16[4,4,32,1024]{3,2,1,0} %dot.322), dimensions={0,3,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %reshape.2527 = f16[4,1024,128,1]{3,2,1,0} reshape(f16[4,1024,4,32]{1,3,2,0} %transpose.376), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=170}
  %pad.14 = f16[4,1024,128,3]{3,2,1,0} pad(f16[4,1024,128,1]{3,2,1,0} %reshape.2527, f16[] %constant.89), padding=0_0x0_0x0_0x1_1, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (1, 1, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %add.422 = f16[4,1024,128,3]{3,2,1,0} add(f16[4,1024,128,3]{3,2,1,0} %add.421, f16[4,1024,128,3]{3,2,1,0} %pad.14), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %reshape.2530 = f16[4096,384]{1,0} reshape(f16[4,1024,128,3]{3,2,1,0} %add.422)
  %dot.323 = f16[4096,1024]{1,0} dot(f16[4096,384]{1,0} %reshape.2530, f16[1024,384]{1,0} %convert.234), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.39 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.323), channel_id=40, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.34, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2532 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.39), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.423 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.419, f16[4,1024,1024]{2,1,0} %reshape.2532), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %tuple.5 = (f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) tuple(f16[] %constant.862, s32[] %constant.59, f16[8,1,1,1024]{3,2,1,0} %broadcast.580, f16[8,1,1,1024]{3,2,1,0} %broadcast.581, f32[] %constant.863, /*index=5*/f32[] %constant.69, f32[] %constant.863, f32[] %constant.124, f16[] %constant.864, f16[] %constant.337, /*index=10*/f16[] %constant.865, f32[] %constant.863, f32[] %constant.69, f32[] %constant.863, f32[] %constant.124, /*index=15*/f32[8,1024]{1,0} %broadcast.421, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.582, f32[8,1024]{1,0} %broadcast.421, /*index=20*/f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.419, f32[8,1024]{1,0} %broadcast.582, f32[1024]{0} %param.17, f32[1024]{0} %param.16, /*index=25*/f32[1024]{0} %param.15, f32[128,1024]{1,0} %param.14, f32[384]{0} %param.12, f32[1024,384]{1,0} %param.11, f32[512]{0} %param.19, /*index=30*/f32[1024,512]{1,0} %param.18, f32[1024]{0} %param.22, f32[1024]{0} %param.21, f32[512,1024]{1,0} %param.20, f16[4,1024,1024]{2,1,0} %convert.48, /*index=35*/s32[4,1024]{1,0} %param.13, f16[4,1024,1024]{2,1,0} %add.423), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %opt-barrier.5 = (f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) opt-barrier((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %tuple.5), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.185 = f16[4,1024,1024]{2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=34, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %reshape.2533 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %get-tuple-element.185)
  %get-tuple-element.186 = f32[1024,384]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=28, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.255 = f16[1024,384]{1,0} convert(f32[1024,384]{1,0} %get-tuple-element.186), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.324 = f16[4096,384]{1,0} dot(f16[4096,1024]{1,0} %reshape.2533, f16[1024,384]{1,0} %convert.255), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2535 = f16[4,1024,384]{2,1,0} reshape(f16[4096,384]{1,0} %dot.324), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %get-tuple-element.187 = f32[384]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=27, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.256 = f16[384]{0} convert(f32[384]{0} %get-tuple-element.187), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1122 = f16[4,1024,384]{2,1,0} broadcast(f16[384]{0} %convert.256), dimensions={2}
  %add.424 = f16[4,1024,384]{2,1,0} add(f16[4,1024,384]{2,1,0} %reshape.2535, f16[4,1024,384]{2,1,0} %broadcast.1122), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %reshape.2540 = f16[4,1024,128,3]{3,2,1,0} reshape(f16[4,1024,384]{2,1,0} %add.424), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/reshape[new_sizes=(8, 1024, 1024, 3) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=161}
  %slice.33 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.2540), slice={[0:4], [0:1024], [0:128], [1:2]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/slice[start_indices=(0, 0, 0, 1) limit_indices=(8, 1024, 1024, 2) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %reshape.2543 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %slice.33), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/reshape[new_sizes=(8, 1024, 32, 32) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=170}
  %transpose.377 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.2543), dimensions={0,2,3,1}
  %slice.34 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.2540), slice={[0:4], [0:1024], [0:128], [0:1]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/slice[start_indices=(0, 0, 0, 0) limit_indices=(8, 1024, 1024, 1) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %get-tuple-element.188 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=0, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1124 = f16[4,1024,128,1]{3,2,1,0} broadcast(f16[] %get-tuple-element.188), dimensions={}
  %divide.228 = f16[4,1024,128,1]{3,2,1,0} divide(f16[4,1024,128,1]{3,2,1,0} %slice.34, f16[4,1024,128,1]{3,2,1,0} %broadcast.1124), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %reshape.2550 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %divide.228), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %transpose.378 = f16[4,4,1024,32]{3,1,2,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.2550), dimensions={0,2,1,3}
  %slice.35 = f16[4,1024,128,1]{3,2,1,0} slice(f16[4,1024,128,3]{3,2,1,0} %reshape.2540), slice={[0:4], [0:1024], [0:128], [2:3]}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/slice[start_indices=(0, 0, 0, 2) limit_indices=(8, 1024, 1024, 3) strides=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %reshape.2553 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4,1024,128,1]{3,2,1,0} %slice.35), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/reshape[new_sizes=(8, 1024, 32, 32) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=173}
  %transpose.379 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.2553), dimensions={0,2,3,1}
  %dot.325 = f16[4,4,1024,1024]{3,2,1,0} dot(f16[4,4,1024,32]{3,1,2,0} %transpose.378, f16[4,4,32,1024]{2,1,3,0} %transpose.379), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/dot_general[dimension_numbers=(((3,), (3,)), ((0, 2), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %get-tuple-element.189 = s32[4,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=35, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.190 = s32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=1, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.799 = s32[4,1024]{1,0} broadcast(s32[] %get-tuple-element.190), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %compare.43 = pred[4,1024]{1,0} compare(s32[4,1024]{1,0} %get-tuple-element.189, s32[4,1024]{1,0} %broadcast.799), direction=GT, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %reshape.2554 = pred[4,1,1,1024]{3,2,1,0} reshape(pred[4,1024]{1,0} %compare.43), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/gt" source_file="/code/alpa/alpa/model/bert_model.py" source_line=182}
  %get-tuple-element.191 = f16[8,1,1,1024]{3,2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=3, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.962 = f16[4,1,1,1024]{3,2,1,0} dynamic-slice(f16[8,1,1,1024]{3,2,1,0} %get-tuple-element.191, s32[] %reshape.1424, s32[] %constant.59, s32[] %constant.59, s32[] %constant.59), dynamic_slice_sizes={4,1,1,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %get-tuple-element.192 = f16[8,1,1,1024]{3,2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.965 = f16[4,1,1,1024]{3,2,1,0} dynamic-slice(f16[8,1,1,1024]{3,2,1,0} %get-tuple-element.192, s32[] %reshape.1424, s32[] %constant.59, s32[] %constant.59, s32[] %constant.59), dynamic_slice_sizes={4,1,1,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %select.31 = f16[4,1,1,1024]{3,2,1,0} select(pred[4,1,1,1024]{3,2,1,0} %reshape.2554, f16[4,1,1,1024]{3,2,1,0} %dynamic-slice.962, f16[4,1,1,1024]{3,2,1,0} %dynamic-slice.965), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/select_n" source_file="/code/alpa/alpa/model/bert_model.py" source_line=181}
  %reshape.2559 = f16[4,1024]{1,0} reshape(f16[4,1,1,1024]{3,2,1,0} %select.31), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=100}
  %broadcast.1125 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,1024]{1,0} %reshape.2559), dimensions={0,3}
  %add.425 = f16[4,4,1024,1024]{3,2,1,0} add(f16[4,4,1024,1024]{3,2,1,0} %dot.325, f16[4,4,1024,1024]{3,2,1,0} %broadcast.1125), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=100}
  %reduce.98 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %add.425, f16[] %constant.179), dimensions={3}, to_apply=%region_145.4112, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/reduce_max[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.801 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %reduce.98), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %subtract.277 = f16[4,4,1024,1024]{3,2,1,0} subtract(f16[4,4,1024,1024]{3,2,1,0} %add.425, f16[4,4,1024,1024]{3,2,1,0} %broadcast.801), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %exponential.17 = f16[4,4,1024,1024]{3,2,1,0} exponential(f16[4,4,1024,1024]{3,2,1,0} %subtract.277), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/exp" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.257 = f32[4,4,1024,1024]{3,2,1,0} convert(f16[4,4,1024,1024]{3,2,1,0} %exponential.17), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %reduce.99 = f32[4,4,1024]{2,1,0} reduce(f32[4,4,1024,1024]{3,2,1,0} %convert.257, f32[] %constant.69), dimensions={3}, to_apply=%region_146.4124, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %convert.258 = f16[4,4,1024]{2,1,0} convert(f32[4,4,1024]{2,1,0} %reduce.99), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.803 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %convert.258), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.229 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %exponential.17, f16[4,4,1024,1024]{3,2,1,0} %broadcast.803), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %dot.326 = f16[4,4,32,1024]{3,2,1,0} dot(f16[4,4,32,1024]{2,1,3,0} %transpose.377, f16[4,4,1024,1024]{3,2,1,0} %divide.229), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/dot_general[dimension_numbers=(((1,), (3,)), ((0, 2), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %transpose.380 = f16[4,1024,4,32]{1,3,2,0} transpose(f16[4,4,32,1024]{3,2,1,0} %dot.326), dimensions={0,3,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %reshape.2565 = f16[4096,128]{1,0} reshape(f16[4,1024,4,32]{1,3,2,0} %transpose.380)
  %get-tuple-element.193 = f32[128,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=26, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.259 = f16[128,1024]{1,0} convert(f32[128,1024]{1,0} %get-tuple-element.193), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.327 = f16[4096,1024]{1,0} dot(f16[4096,128]{1,0} %reshape.2565, f16[128,1024]{1,0} %convert.259), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.40 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.327), channel_id=41, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.35, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2567 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.40), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %get-tuple-element.194 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=25, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.260 = f16[1024]{0} convert(f32[1024]{0} %get-tuple-element.194), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1126 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.260), dimensions={2}
  %add.426 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %reshape.2567, f16[4,1024,1024]{2,1,0} %broadcast.1126), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %add.427 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.426, f16[4,1024,1024]{2,1,0} %get-tuple-element.185), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=233}
  %convert.261 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.427), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %reduce.100 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.261, f32[] %constant.69), dimensions={2}, to_apply=%region_147.4149, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.195 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=4, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.806 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.195), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.230 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %reduce.100, f32[4,1024]{1,0} %broadcast.806), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.807 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.230), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.278 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.261, f32[4,1024,1024]{2,1,0} %broadcast.807), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %get-tuple-element.196 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=5, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.808 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.196), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.623 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.261, f32[4,1024,1024]{2,1,0} %convert.261), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.101 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.623, f32[] %constant.69), dimensions={2}, to_apply=%region_148.4162, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %get-tuple-element.197 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=6, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.809 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.197), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.231 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %reduce.101, f32[4,1024]{1,0} %broadcast.809), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.624 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.230, f32[4,1024]{1,0} %divide.230), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.279 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.231, f32[4,1024]{1,0} %multiply.624), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.23 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.808, f32[4,1024]{1,0} %subtract.279), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.198 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=7, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.810 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.198), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.428 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.23, f32[4,1024]{1,0} %broadcast.810), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.2571 = f32[4,1024,1]{2,1,0} reshape(f32[4,1024]{1,0} %add.428), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.35 = f32[4,1024,1]{2,1,0} rsqrt(f32[4,1024,1]{2,1,0} %reshape.2571), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.2572 = f32[4,1024]{1,0} reshape(f32[4,1024,1]{2,1,0} %rsqrt.35), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.812 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %reshape.2572), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %get-tuple-element.199 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=24, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1127 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %get-tuple-element.199), dimensions={2}
  %multiply.625 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.812, f32[4,1024,1024]{2,1,0} %broadcast.1127), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.628 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.278, f32[4,1024,1024]{2,1,0} %multiply.625), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %get-tuple-element.200 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=23, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1128 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %get-tuple-element.200), dimensions={2}
  %add.429 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.628, f32[4,1024,1024]{2,1,0} %broadcast.1128), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %convert.262 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.429), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %reshape.2577 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %convert.262)
  %get-tuple-element.201 = f32[1024,512]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=30, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.263 = f16[1024,512]{1,0} convert(f32[1024,512]{1,0} %get-tuple-element.201), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.328 = f16[4096,512]{1,0} dot(f16[4096,1024]{1,0} %reshape.2577, f16[1024,512]{1,0} %convert.263), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2581 = f16[4,1024,512]{2,1,0} reshape(f16[4096,512]{1,0} %dot.328), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %get-tuple-element.202 = f32[512]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=29, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.264 = f16[512]{0} convert(f32[512]{0} %get-tuple-element.202), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1129 = f16[4,1024,512]{2,1,0} broadcast(f16[512]{0} %convert.264), dimensions={2}
  %add.430 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %reshape.2581, f16[4,1024,512]{2,1,0} %broadcast.1129), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %get-tuple-element.203 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=8, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1130 = f16[4,1024,512]{2,1,0} broadcast(f16[] %get-tuple-element.203), dimensions={}
  %divide.232 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %add.430, f16[4,1024,512]{2,1,0} %broadcast.1130), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.265 = f32[4,1024,512]{2,1,0} convert(f16[4,1024,512]{2,1,0} %divide.232), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %clamp.11 = f32[4,1024,512]{2,1,0} clamp(f32[4,1024,512]{2,1,0} %broadcast.952, f32[4,1024,512]{2,1,0} %convert.265, f32[4,1024,512]{2,1,0} %broadcast.954), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.630 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.11, f32[4,1024,512]{2,1,0} %clamp.11), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.632 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %multiply.630, f32[4,1024,512]{2,1,0} %broadcast.956), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.431 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.632, f32[4,1024,512]{2,1,0} %broadcast.957), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.634 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.431, f32[4,1024,512]{2,1,0} %multiply.630), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.432 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.634, f32[4,1024,512]{2,1,0} %broadcast.958), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.636 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.432, f32[4,1024,512]{2,1,0} %multiply.630), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.433 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.636, f32[4,1024,512]{2,1,0} %broadcast.959), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.638 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.433, f32[4,1024,512]{2,1,0} %multiply.630), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.434 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.638, f32[4,1024,512]{2,1,0} %broadcast.960), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.640 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.434, f32[4,1024,512]{2,1,0} %multiply.630), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.435 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.640, f32[4,1024,512]{2,1,0} %broadcast.961), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.642 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.435, f32[4,1024,512]{2,1,0} %multiply.630), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.436 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.642, f32[4,1024,512]{2,1,0} %broadcast.962), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.644 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.436, f32[4,1024,512]{2,1,0} %multiply.630), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.437 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.644, f32[4,1024,512]{2,1,0} %broadcast.963), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.646 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %clamp.11, f32[4,1024,512]{2,1,0} %add.437), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.438 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.632, f32[4,1024,512]{2,1,0} %broadcast.964), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.648 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.438, f32[4,1024,512]{2,1,0} %multiply.630), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.439 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.648, f32[4,1024,512]{2,1,0} %broadcast.965), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.650 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.439, f32[4,1024,512]{2,1,0} %multiply.630), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.440 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.650, f32[4,1024,512]{2,1,0} %broadcast.966), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.652 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.440, f32[4,1024,512]{2,1,0} %multiply.630), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.441 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.652, f32[4,1024,512]{2,1,0} %broadcast.968), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.653 = f32[4,1024,512]{2,1,0} multiply(f32[4,1024,512]{2,1,0} %add.441, f32[4,1024,512]{2,1,0} %multiply.630), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.442 = f32[4,1024,512]{2,1,0} add(f32[4,1024,512]{2,1,0} %multiply.653, f32[4,1024,512]{2,1,0} %broadcast.970), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.233 = f32[4,1024,512]{2,1,0} divide(f32[4,1024,512]{2,1,0} %multiply.646, f32[4,1024,512]{2,1,0} %add.442), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %convert.266 = f16[4,1024,512]{2,1,0} convert(f32[4,1024,512]{2,1,0} %divide.233), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/erf" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %get-tuple-element.204 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=9, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1131 = f16[4,1024,512]{2,1,0} broadcast(f16[] %get-tuple-element.204), dimensions={}
  %add.443 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %convert.266, f16[4,1024,512]{2,1,0} %broadcast.1131), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.654 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %add.430, f16[4,1024,512]{2,1,0} %add.443), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %get-tuple-element.205 = f16[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=10, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1132 = f16[4,1024,512]{2,1,0} broadcast(f16[] %get-tuple-element.205), dimensions={}
  %divide.234 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %multiply.654, f16[4,1024,512]{2,1,0} %broadcast.1132), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %reshape.2598 = f16[4096,512]{1,0} reshape(f16[4,1024,512]{2,1,0} %divide.234)
  %get-tuple-element.206 = f32[512,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=33, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.267 = f16[512,1024]{1,0} convert(f32[512,1024]{1,0} %get-tuple-element.206), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %dot.329 = f16[4096,1024]{1,0} dot(f16[4096,512]{1,0} %reshape.2598, f16[512,1024]{1,0} %convert.267), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.41 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.329), channel_id=42, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.36, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2600 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.41), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/dense/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %get-tuple-element.207 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=32, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.268 = f16[1024]{0} convert(f32[1024]{0} %get-tuple-element.207), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/dense/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1133 = f16[4,1024,1024]{2,1,0} broadcast(f16[1024]{0} %convert.268), dimensions={2}
  %add.444 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %reshape.2600, f16[4,1024,1024]{2,1,0} %broadcast.1133), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/dense/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %add.445 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.444, f16[4,1024,1024]{2,1,0} %convert.262), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/add" source_file="/code/alpa/alpa/model/bert_model.py" source_line=310}
  %convert.269 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.445), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %reduce.102 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %convert.269, f32[] %constant.69), dimensions={2}, to_apply=%region_149.4248, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.208 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=11, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.821 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.208), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %divide.235 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %reduce.102, f32[4,1024]{1,0} %broadcast.821), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.822 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.235), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.292 = f32[4,1024,1024]{2,1,0} subtract(f32[4,1024,1024]{2,1,0} %convert.269, f32[4,1024,1024]{2,1,0} %broadcast.822), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %get-tuple-element.209 = f16[4,1024,1024]{2,1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=36, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %convert.270 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %get-tuple-element.209), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %multiply.656 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.292, f32[4,1024,1024]{2,1,0} %convert.270), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %get-tuple-element.210 = f32[1024]{0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=31, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1135 = f32[4,1024,1024]{2,1,0} broadcast(f32[1024]{0} %get-tuple-element.210), dimensions={2}
  %multiply.657 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %multiply.656, f32[4,1024,1024]{2,1,0} %broadcast.1135), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.103 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.657, f32[] %constant.69), dimensions={2}, to_apply=%region_153.4329, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reshape.2605 = f32[4,1024,1]{2,1,0} reshape(f32[4,1024]{1,0} %reduce.103), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reshape[new_sizes=(8, 1024, 1) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %get-tuple-element.211 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=12, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.824 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.211), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.658 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.269, f32[4,1024,1024]{2,1,0} %convert.269), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.104 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.658, f32[] %constant.69), dimensions={2}, to_apply=%region_150.4261, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %get-tuple-element.212 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=13, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.825 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.212), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %divide.236 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %reduce.104, f32[4,1024]{1,0} %broadcast.825), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.659 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.235, f32[4,1024]{1,0} %divide.235), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.293 = f32[4,1024]{1,0} subtract(f32[4,1024]{1,0} %divide.236, f32[4,1024]{1,0} %multiply.659), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/sub" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.24 = f32[4,1024]{1,0} maximum(f32[4,1024]{1,0} %broadcast.824, f32[4,1024]{1,0} %subtract.293), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/max" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.213 = f32[] get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=14, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.826 = f32[4,1024]{1,0} broadcast(f32[] %get-tuple-element.213), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %add.446 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %maximum.24, f32[4,1024]{1,0} %broadcast.826), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.2606 = f32[4,1024,1]{2,1,0} reshape(f32[4,1024]{1,0} %add.446), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/add" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.36 = f32[4,1024,1]{2,1,0} rsqrt(f32[4,1024,1]{2,1,0} %reshape.2606), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/rsqrt" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %divide.237 = f32[4,1024,1]{2,1,0} divide(f32[4,1024,1]{2,1,0} %rsqrt.36, f32[4,1024,1]{2,1,0} %reshape.2606), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.660 = f32[4,1024,1]{2,1,0} multiply(f32[4,1024,1]{2,1,0} %divide.237, f32[4,1024,1]{2,1,0} %broadcast.1046), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.661 = f32[4,1024,1]{2,1,0} multiply(f32[4,1024,1]{2,1,0} %reshape.2605, f32[4,1024,1]{2,1,0} %multiply.660), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.2607 = f32[4,1024]{1,0} reshape(f32[4,1024,1]{2,1,0} %multiply.661), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reshape[new_sizes=(8, 1024) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=132}
  %compare.44 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %subtract.293, f32[4,1024]{1,0} %maximum.24), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.214 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=20, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.1011 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.214, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.215 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=19, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.1014 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.215, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.32 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.44, f32[4,1024]{1,0} %dynamic-slice.1011, f32[4,1024]{1,0} %dynamic-slice.1014), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %compare.45 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %broadcast.824, f32[4,1024]{1,0} %maximum.24), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.216 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=22, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.1017 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.216, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.217 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=21, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.1020 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.217, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.33 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.45, f32[4,1024]{1,0} %dynamic-slice.1017, f32[4,1024]{1,0} %dynamic-slice.1020), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.238 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %select.32, f32[4,1024]{1,0} %select.33), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.662 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reshape.2607, f32[4,1024]{1,0} %divide.238), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.239 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %multiply.662, f32[4,1024]{1,0} %broadcast.825), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.663 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.239, f32[4,1024]{1,0} %broadcast.1048), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %broadcast.827 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.663), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.664 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.269, f32[4,1024,1024]{2,1,0} %broadcast.827), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %negate.38 = f32[4,1024]{1,0} negate(f32[4,1024]{1,0} %multiply.662), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.665 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.235, f32[4,1024]{1,0} %broadcast.1048), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.666 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %negate.38, f32[4,1024]{1,0} %multiply.665), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %reshape.2618 = f32[4,1024]{1,0} reshape(f32[4,1024,1]{2,1,0} %rsqrt.36), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.828 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %reshape.2618), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.667 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.828, f32[4,1024,1024]{2,1,0} %broadcast.1135), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.668 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.270, f32[4,1024,1024]{2,1,0} %multiply.667), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %negate.39 = f32[4,1024,1024]{2,1,0} negate(f32[4,1024,1024]{2,1,0} %multiply.668), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reduce.105 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %negate.39, f32[] %constant.69), dimensions={2}, to_apply=%region_154.4346, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %add.447 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %multiply.666, f32[4,1024]{1,0} %reduce.105), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=131}
  %divide.240 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %add.447, f32[4,1024]{1,0} %broadcast.821), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.829 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.240), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/broadcast_in_dim[shape=(8, 1024, 1024) broadcast_dimensions=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %add.448 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.664, f32[4,1024,1024]{2,1,0} %broadcast.829), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %convert.271 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.448), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %convert.272 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %multiply.668), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %add.449 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %convert.271, f16[4,1024,1024]{2,1,0} %convert.272), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reshape.2619 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %add.449)
  %dot.330 = f16[4096,512]{1,0} dot(f16[4096,1024]{1,0} %reshape.2619, f16[512,1024]{1,0} %convert.267), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %broadcast.1137 = f16[4096,512]{1,0} broadcast(f16[] %get-tuple-element.205), dimensions={}
  %divide.241 = f16[4096,512]{1,0} divide(f16[4096,512]{1,0} %dot.330, f16[4096,512]{1,0} %broadcast.1137), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %reshape.2625 = f16[4,1024,512]{2,1,0} reshape(f16[4096,512]{1,0} %divide.241), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.669 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %add.430, f16[4,1024,512]{2,1,0} %reshape.2625), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.670 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.669, f16[4,1024,512]{2,1,0} %broadcast.1050), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.671 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %divide.232, f16[4,1024,512]{2,1,0} %divide.232), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %negate.40 = f16[4,1024,512]{2,1,0} negate(f16[4,1024,512]{2,1,0} %multiply.671), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/neg" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %exponential.18 = f16[4,1024,512]{2,1,0} exponential(f16[4,1024,512]{2,1,0} %negate.40), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/exp" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.672 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %multiply.670, f16[4,1024,512]{2,1,0} %exponential.18), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %divide.242 = f16[4,1024,512]{2,1,0} divide(f16[4,1024,512]{2,1,0} %multiply.672, f16[4,1024,512]{2,1,0} %broadcast.1130), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %multiply.673 = f16[4,1024,512]{2,1,0} multiply(f16[4,1024,512]{2,1,0} %reshape.2625, f16[4,1024,512]{2,1,0} %add.443), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/mul" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %add.450 = f16[4,1024,512]{2,1,0} add(f16[4,1024,512]{2,1,0} %divide.242, f16[4,1024,512]{2,1,0} %multiply.673), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %reshape.2627 = f16[4096,512]{1,0} reshape(f16[4,1024,512]{2,1,0} %add.450)
  %dot.331 = f16[4096,1024]{1,0} dot(f16[4096,512]{1,0} %reshape.2627, f16[1024,512]{1,0} %convert.263), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.42 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.331), channel_id=43, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.37, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2629 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.42), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.451 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.449, f16[4,1024,1024]{2,1,0} %reshape.2629), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.273 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.451), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %multiply.674 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.278, f32[4,1024,1024]{2,1,0} %convert.273), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.675 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %multiply.674, f32[4,1024,1024]{2,1,0} %broadcast.1127), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.106 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.675, f32[] %constant.69), dimensions={2}, to_apply=%region_159.4409, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reshape.2630 = f32[4,1024,1]{2,1,0} reshape(f32[4,1024]{1,0} %reduce.106), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reshape[new_sizes=(8, 1024, 1) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %divide.243 = f32[4,1024,1]{2,1,0} divide(f32[4,1024,1]{2,1,0} %rsqrt.35, f32[4,1024,1]{2,1,0} %reshape.2571), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.676 = f32[4,1024,1]{2,1,0} multiply(f32[4,1024,1]{2,1,0} %divide.243, f32[4,1024,1]{2,1,0} %broadcast.1046), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.677 = f32[4,1024,1]{2,1,0} multiply(f32[4,1024,1]{2,1,0} %reshape.2630, f32[4,1024,1]{2,1,0} %multiply.676), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.2631 = f32[4,1024]{1,0} reshape(f32[4,1024,1]{2,1,0} %multiply.677), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reshape[new_sizes=(8, 1024) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=132}
  %compare.46 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %subtract.279, f32[4,1024]{1,0} %maximum.23), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.218 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=16, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.1030 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.218, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.219 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=15, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.1033 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.219, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.34 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.46, f32[4,1024]{1,0} %dynamic-slice.1030, f32[4,1024]{1,0} %dynamic-slice.1033), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %compare.47 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %broadcast.808, f32[4,1024]{1,0} %maximum.23), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.220 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=18, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.1036 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.220, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %get-tuple-element.221 = f32[8,1024]{1,0} get-tuple-element((f16[], s32[], f16[8,1,1,1024]{3,2,1,0}, f16[8,1,1,1024]{3,2,1,0}, f32[], /*index=5*/f32[], f32[], f32[], f16[], f16[], /*index=10*/f16[], f32[], f32[], f32[], f32[], /*index=15*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, /*index=20*/f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[8,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=25*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=30*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f16[4,1024,1024]{2,1,0}, /*index=35*/s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.5), index=17, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %dynamic-slice.1039 = f32[4,1024]{1,0} dynamic-slice(f32[8,1024]{1,0} %get-tuple-element.221, s32[] %reshape.1424, s32[] %constant.59), dynamic_slice_sizes={4,1024}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.35 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.47, f32[4,1024]{1,0} %dynamic-slice.1036, f32[4,1024]{1,0} %dynamic-slice.1039), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.244 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %select.34, f32[4,1024]{1,0} %select.35), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.678 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reshape.2631, f32[4,1024]{1,0} %divide.244), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.245 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %multiply.678, f32[4,1024]{1,0} %broadcast.809), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=83}
  %multiply.680 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.245, f32[4,1024]{1,0} %broadcast.1048), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %broadcast.831 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.680), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.681 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.261, f32[4,1024,1024]{2,1,0} %broadcast.831), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %negate.41 = f32[4,1024]{1,0} negate(f32[4,1024]{1,0} %multiply.678), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.682 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %divide.230, f32[4,1024]{1,0} %broadcast.1048), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.683 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %negate.41, f32[4,1024]{1,0} %multiply.682), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.684 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.273, f32[4,1024,1024]{2,1,0} %multiply.625), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %negate.42 = f32[4,1024,1024]{2,1,0} negate(f32[4,1024,1024]{2,1,0} %multiply.684), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reduce.107 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %negate.42, f32[] %constant.69), dimensions={2}, to_apply=%region_160.4426, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %add.452 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %multiply.683, f32[4,1024]{1,0} %reduce.107), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=131}
  %divide.246 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %add.452, f32[4,1024]{1,0} %broadcast.806), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.832 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %divide.246), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/broadcast_in_dim[shape=(8, 1024, 1024) broadcast_dimensions=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %add.453 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.681, f32[4,1024,1024]{2,1,0} %broadcast.832), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %convert.274 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.453), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %convert.275 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %multiply.684), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %add.454 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %convert.274, f16[4,1024,1024]{2,1,0} %convert.275), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reshape.2641 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %add.454)
  %dot.332 = f16[4096,128]{1,0} dot(f16[4096,1024]{1,0} %reshape.2641, f16[128,1024]{1,0} %convert.259), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/dense/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2643 = f16[4,1024,4,32]{3,2,1,0} reshape(f16[4096,128]{1,0} %dot.332), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/reshape[new_sizes=(8, 1024, 32, 32) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=208}
  %transpose.381 = f16[4,4,1024,32]{3,1,2,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.2643), dimensions={0,2,1,3}
  %dot.333 = f16[4,4,1024,1024]{3,2,1,0} dot(f16[4,4,1024,32]{3,1,2,0} %transpose.381, f16[4,4,32,1024]{2,1,3,0} %transpose.377), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/dot_general[dimension_numbers=(((2,), (3,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %multiply.685 = f16[4,4,1024]{2,1,0} multiply(f16[4,4,1024]{2,1,0} %convert.258, f16[4,4,1024]{2,1,0} %convert.258), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.247 = f16[4,4,1024]{2,1,0} divide(f16[4,4,1024]{2,1,0} %broadcast.1051, f16[4,4,1024]{2,1,0} %multiply.685), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.833 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %divide.247), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.687 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %dot.333, f16[4,4,1024,1024]{3,2,1,0} %broadcast.833), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.688 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %multiply.687, f16[4,4,1024,1024]{3,2,1,0} %exponential.17), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %reduce.108 = f16[4,4,1024]{2,1,0} reduce(f16[4,4,1024,1024]{3,2,1,0} %multiply.688, f16[] %constant.89), dimensions={3}, to_apply=%region_162.4457, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/reduce_sum[axes=(3,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %negate.43 = f16[4,4,1024]{2,1,0} negate(f16[4,4,1024]{2,1,0} %reduce.108), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %broadcast.834 = f16[4,4,1024,1024]{3,2,1,0} broadcast(f16[4,4,1024]{2,1,0} %negate.43), dimensions={0,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/broadcast_in_dim[shape=(8, 32, 1024, 1024) broadcast_dimensions=(0, 1, 2)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %divide.248 = f16[4,4,1024,1024]{3,2,1,0} divide(f16[4,4,1024,1024]{3,2,1,0} %dot.333, f16[4,4,1024,1024]{3,2,1,0} %broadcast.803), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %add.455 = f16[4,4,1024,1024]{3,2,1,0} add(f16[4,4,1024,1024]{3,2,1,0} %broadcast.834, f16[4,4,1024,1024]{3,2,1,0} %divide.248), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %multiply.689 = f16[4,4,1024,1024]{3,2,1,0} multiply(f16[4,4,1024,1024]{3,2,1,0} %add.455, f16[4,4,1024,1024]{3,2,1,0} %exponential.17), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=107}
  %dot.334 = f16[4,4,1024,32]{3,2,1,0} dot(f16[4,4,1024,1024]{3,2,1,0} %multiply.689, f16[4,4,1024,32]{3,1,2,0} %transpose.378), lhs_batch_dims={0,1}, lhs_contracting_dims={2}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/dot_general[dimension_numbers=(((2,), (1,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %transpose.382 = f16[4,1024,4,32]{3,1,2,0} transpose(f16[4,4,1024,32]{3,2,1,0} %dot.334), dimensions={0,2,1,3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/transpose[permutation=(0, 2, 1, 3)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %reshape.2645 = f16[4,1024,128,1]{3,2,1,0} reshape(f16[4,1024,4,32]{3,1,2,0} %transpose.382), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=173}
  %pad.15 = f16[4,1024,128,3]{3,2,1,0} pad(f16[4,1024,128,1]{3,2,1,0} %reshape.2645, f16[] %constant.89), padding=0_0x0_0x0_0x2_0, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (2, 0, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.383 = f16[4,4,1024,32]{3,1,2,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.2553), dimensions={0,2,1,3}
  %dot.335 = f16[4,4,1024,32]{3,2,1,0} dot(f16[4,4,1024,1024]{3,2,1,0} %multiply.689, f16[4,4,1024,32]{3,1,2,0} %transpose.383), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/dot_general[dimension_numbers=(((3,), (1,)), ((0, 1), (0, 2))) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=95}
  %broadcast.1138 = f16[4,4,1024,32]{3,2,1,0} broadcast(f16[] %get-tuple-element.188), dimensions={}
  %divide.249 = f16[4,4,1024,32]{3,2,1,0} divide(f16[4,4,1024,32]{3,2,1,0} %dot.335, f16[4,4,1024,32]{3,2,1,0} %broadcast.1138), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %transpose.384 = f16[4,1024,4,32]{3,1,2,0} transpose(f16[4,4,1024,32]{3,2,1,0} %divide.249), dimensions={0,2,1,3}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/attention.py" source_line=93}
  %reshape.2653 = f16[4,1024,128,1]{3,2,1,0} reshape(f16[4,1024,4,32]{3,1,2,0} %transpose.384), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=167}
  %pad.16 = f16[4,1024,128,3]{3,2,1,0} pad(f16[4,1024,128,1]{3,2,1,0} %reshape.2653, f16[] %constant.89), padding=0_0x0_0x0_0x0_2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 2, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %add.456 = f16[4,1024,128,3]{3,2,1,0} add(f16[4,1024,128,3]{3,2,1,0} %pad.15, f16[4,1024,128,3]{3,2,1,0} %pad.16), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %transpose.385 = f16[4,4,32,1024]{2,1,3,0} transpose(f16[4,1024,4,32]{3,2,1,0} %reshape.2643), dimensions={0,2,3,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/transpose[permutation=(0, 2, 3, 1)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %dot.336 = f16[4,4,32,1024]{3,2,1,0} dot(f16[4,4,32,1024]{2,1,3,0} %transpose.385, f16[4,4,1024,1024]{3,2,1,0} %divide.229), lhs_batch_dims={0,1}, lhs_contracting_dims={3}, rhs_batch_dims={0,1}, rhs_contracting_dims={2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/dot_general[dimension_numbers=(((3,), (2,)), ((0, 1), (0, 1))) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %transpose.386 = f16[4,1024,4,32]{1,3,2,0} transpose(f16[4,4,32,1024]{3,2,1,0} %dot.336), dimensions={0,3,1,2}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/transpose[permutation=(0, 3, 1, 2)]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=206}
  %reshape.2657 = f16[4,1024,128,1]{3,2,1,0} reshape(f16[4,1024,4,32]{1,3,2,0} %transpose.386), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/reshape[new_sizes=(8, 1024, 1024, 1) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=170}
  %pad.17 = f16[4,1024,128,3]{3,2,1,0} pad(f16[4,1024,128,1]{3,2,1,0} %reshape.2657, f16[] %constant.89), padding=0_0x0_0x0_0x1_1, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/pad[padding_config=((0, 0, 0), (0, 0, 0), (0, 0, 0), (1, 1, 0))]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %add.457 = f16[4,1024,128,3]{3,2,1,0} add(f16[4,1024,128,3]{3,2,1,0} %add.456, f16[4,1024,128,3]{3,2,1,0} %pad.17), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/add_any" source_file="/code/alpa/alpa/model/bert_model.py" source_line=163}
  %reshape.2661 = f16[4096,384]{1,0} reshape(f16[4,1024,128,3]{3,2,1,0} %add.457)
  %dot.337 = f16[4096,1024]{1,0} dot(f16[4096,384]{1,0} %reshape.2661, f16[1024,384]{1,0} %convert.255), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.43 = f16[4096,1024]{1,0} all-reduce(f16[4096,1024]{1,0} %dot.337), channel_id=44, replica_groups={{0,1,2,3,4,5,6,7},{8,9,10,11,12,13,14,15}}, use_global_device_ids=true, to_apply=%add.38, sharding={devices=[2,1,8]0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %reshape.2663 = f16[4,1024,1024]{2,1,0} reshape(f16[4096,1024]{1,0} %all-reduce.43), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/dot_general[dimension_numbers=(((2,), (1,)), ((), ())) precision=None preferred_element_type=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %add.458 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %add.454, f16[4,1024,1024]{2,1,0} %reshape.2663), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.276 = f32[4,1024,1024]{2,1,0} convert(f16[4,1024,1024]{2,1,0} %add.458), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=153}
  %reduce.109 = f32[1024]{0} reduce(f32[4,1024,1024]{2,1,0} %convert.276, f32[] %constant.69), dimensions={0,1}, to_apply=%region_165.4507, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %all-reduce.44 = f32[1024]{0} all-reduce(f32[1024]{0} %reduce.109), channel_id=45, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_165.4507, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %broadcast.836 = f32[1024]{0} broadcast(f32[] %constant.843), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.690 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.44, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.87 = f32[1024]{0} parameter(80), sharding={replicated}
  %broadcast.837 = f32[1024]{0} broadcast(f32[] %constant.846), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.691 = f32[1024]{0} multiply(f32[1024]{0} %param.87, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.459 = f32[1024]{0} add(f32[1024]{0} %multiply.690, f32[1024]{0} %multiply.691), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %broadcast.838 = f32[1024]{0} broadcast(f32[] %subtract.94), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %multiply.692 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.44, f32[1024]{0} %all-reduce.44), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %broadcast.839 = f32[1024]{0} broadcast(f32[] %constant.850), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %multiply.693 = f32[1024]{0} multiply(f32[1024]{0} %multiply.692, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.88 = f32[1024]{0} parameter(157), sharding={replicated}
  %broadcast.840 = f32[1024]{0} broadcast(f32[] %constant.853), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %multiply.694 = f32[1024]{0} multiply(f32[1024]{0} %param.88, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.460 = f32[1024]{0} add(f32[1024]{0} %multiply.693, f32[1024]{0} %multiply.694), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %broadcast.841 = f32[1024]{0} broadcast(f32[] %subtract.95), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %divide.250 = f32[1024]{0} divide(f32[1024]{0} %add.460, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.1 = f32[1024]{0} sqrt(f32[1024]{0} %divide.250), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %broadcast.842 = f32[1024]{0} broadcast(f32[] %constant.856), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.461 = f32[1024]{0} add(f32[1024]{0} %sqrt.1, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.695 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.461)
  %divide.251 = f32[1024]{0} divide(f32[1024]{0} %add.459, f32[1024]{0} %multiply.695), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %broadcast.843 = f32[1024]{0} broadcast(f32[] %constant.859), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %multiply.696 = f32[1024]{0} multiply(f32[1024]{0} %divide.251, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.462 = f32[1024]{0} add(f32[1024]{0} %param.10, f32[1024]{0} %multiply.696), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %multiply.697 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %subtract.12, f32[4,1024,1024]{2,1,0} %convert.276), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.698 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.434, f32[4,1024,1024]{2,1,0} %multiply.697), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.110 = f32[1024]{0} reduce(f32[4,1024,1024]{2,1,0} %multiply.698, f32[] %constant.69), dimensions={0,1}, to_apply=%region_166.4517, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %all-reduce.45 = f32[1024]{0} all-reduce(f32[1024]{0} %reduce.110), channel_id=46, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_166.4517, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.701 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.45, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.89 = f32[1024]{0} parameter(81), sharding={replicated}
  %multiply.702 = f32[1024]{0} multiply(f32[1024]{0} %param.89, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.463 = f32[1024]{0} add(f32[1024]{0} %multiply.701, f32[1024]{0} %multiply.702), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.703 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.45, f32[1024]{0} %all-reduce.45), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.704 = f32[1024]{0} multiply(f32[1024]{0} %multiply.703, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.90 = f32[1024]{0} parameter(158), sharding={replicated}
  %multiply.705 = f32[1024]{0} multiply(f32[1024]{0} %param.90, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.465 = f32[1024]{0} add(f32[1024]{0} %multiply.704, f32[1024]{0} %multiply.705), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.252 = f32[1024]{0} divide(f32[1024]{0} %add.465, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.2 = f32[1024]{0} sqrt(f32[1024]{0} %divide.252), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.466 = f32[1024]{0} add(f32[1024]{0} %sqrt.2, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.706 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.466)
  %divide.253 = f32[1024]{0} divide(f32[1024]{0} %add.463, f32[1024]{0} %multiply.706), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.707 = f32[1024]{0} multiply(f32[1024]{0} %divide.253, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.467 = f32[1024]{0} add(f32[1024]{0} %param.9, f32[1024]{0} %multiply.707), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %iota.11 = s32[1,1,1024]{2,1,0} iota(), iota_dimension=2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/position_embeddings.embed_call_one_hot/remat(core_fn)/iota[dtype=int32 shape=(1, 1, 1024) dimension=2]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %multiply.708 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %multiply.697, f32[4,1024,1024]{2,1,0} %broadcast.938), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.111 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %multiply.708, f32[] %constant.69), dimensions={2}, to_apply=%region_167.4526, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reshape.2664 = f32[4,1024,1]{2,1,0} reshape(f32[4,1024]{1,0} %reduce.111), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/reshape[new_sizes=(8, 1024, 1) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %divide.254 = f32[4,1024,1]{2,1,0} divide(f32[4,1024,1]{2,1,0} %rsqrt.12, f32[4,1024,1]{2,1,0} %reshape.1459), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.709 = f32[4,1024,1]{2,1,0} multiply(f32[4,1024,1]{2,1,0} %divide.254, f32[4,1024,1]{2,1,0} %broadcast.1046), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %multiply.710 = f32[4,1024,1]{2,1,0} multiply(f32[4,1024,1]{2,1,0} %reshape.2664, f32[4,1024,1]{2,1,0} %multiply.709), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=139}
  %reshape.2665 = f32[4,1024]{1,0} reshape(f32[4,1024,1]{2,1,0} %multiply.710), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/reshape[new_sizes=(8, 1024) dimensions=None]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=132}
  %compare.48 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %subtract.13, f32[4,1024]{1,0} %maximum.0), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.36 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.48, f32[4,1024]{1,0} %broadcast.928, f32[4,1024]{1,0} %broadcast.930), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %compare.49 = pred[4,1024]{1,0} compare(f32[4,1024]{1,0} %broadcast.930, f32[4,1024]{1,0} %maximum.0), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/eq" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %select.37 = f32[4,1024]{1,0} select(pred[4,1024]{1,0} %compare.49, f32[4,1024]{1,0} %broadcast.1048, f32[4,1024]{1,0} %broadcast.928), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/select_n" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %divide.255 = f32[4,1024]{1,0} divide(f32[4,1024]{1,0} %select.36, f32[4,1024]{1,0} %select.37), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.711 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reshape.2665, f32[4,1024]{1,0} %divide.255), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %constant.2187 = f32[] constant(0.001953125)
  %broadcast.1139 = f32[4,1024]{1,0} broadcast(f32[] %constant.2187), dimensions={}
  %multiply.712 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %multiply.711, f32[4,1024]{1,0} %broadcast.1139), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %broadcast.845 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.712), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.713 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %reshape.1453, f32[4,1024,1024]{2,1,0} %broadcast.845), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %negate.44 = f32[4,1024]{1,0} negate(f32[4,1024]{1,0} %multiply.711), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=93}
  %multiply.714 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %reduce.2, f32[4,1024]{1,0} %broadcast.1139), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.715 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %negate.44, f32[4,1024]{1,0} %multiply.714), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=46}
  %multiply.716 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %convert.276, f32[4,1024,1024]{2,1,0} %multiply.178), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=146}
  %negate.45 = f32[4,1024,1024]{2,1,0} negate(f32[4,1024,1024]{2,1,0} %multiply.716), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/neg" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %reduce.112 = f32[4,1024]{1,0} reduce(f32[4,1024,1024]{2,1,0} %negate.45, f32[] %constant.69), dimensions={2}, to_apply=%region_168.4542, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/reduce_sum[axes=(2,)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %add.468 = f32[4,1024]{1,0} add(f32[4,1024]{1,0} %multiply.715, f32[4,1024]{1,0} %reduce.112), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=131}
  %multiply.717 = f32[4,1024]{1,0} multiply(f32[4,1024]{1,0} %add.468, f32[4,1024]{1,0} %broadcast.936), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/div" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.846 = f32[4,1024,1024]{2,1,0} broadcast(f32[4,1024]{1,0} %multiply.717), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/broadcast_in_dim[shape=(8, 1024, 1024) broadcast_dimensions=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %add.470 = f32[4,1024,1024]{2,1,0} add(f32[4,1024,1024]{2,1,0} %multiply.713, f32[4,1024,1024]{2,1,0} %broadcast.846), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=82}
  %convert.277 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %add.470), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=80}
  %convert.278 = f16[4,1024,1024]{2,1,0} convert(f32[4,1024,1024]{2,1,0} %multiply.716), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %add.471 = f16[4,1024,1024]{2,1,0} add(f16[4,1024,1024]{2,1,0} %convert.277, f16[4,1024,1024]{2,1,0} %convert.278), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/LayerNorm/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=138}
  %tuple.6 = (s32[1,1,1024]{2,1,0}, s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) tuple(s32[1,1,1024]{2,1,0} %iota.11, s32[4,1024]{1,0} %param.7, f16[4,1024,1024]{2,1,0} %add.471), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/position_embeddings.embed_call_one_hot/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %opt-barrier.6 = (s32[1,1,1024]{2,1,0}, s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) opt-barrier((s32[1,1,1024]{2,1,0}, s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %tuple.6), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/position_embeddings.embed_call_one_hot/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.222 = s32[4,1024]{1,0} get-tuple-element((s32[1,1,1024]{2,1,0}, s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.6), index=1, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/position_embeddings.embed_call_one_hot/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.847 = s32[4,1024,1024]{2,1,0} broadcast(s32[4,1024]{1,0} %get-tuple-element.222), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/position_embeddings.embed_call_one_hot/remat(core_fn)/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %get-tuple-element.223 = s32[1,1,1024]{2,1,0} get-tuple-element((s32[1,1,1024]{2,1,0}, s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.6), index=0, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/position_embeddings.embed_call_one_hot/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %reshape.2668 = s32[1024]{0} reshape(s32[1,1,1024]{2,1,0} %get-tuple-element.223), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/position_embeddings.embed_call_one_hot/remat(core_fn)/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %broadcast.1140 = s32[4,1024,1024]{2,1,0} broadcast(s32[1024]{0} %reshape.2668), dimensions={2}
  %compare.50 = pred[4,1024,1024]{2,1,0} compare(s32[4,1024,1024]{2,1,0} %broadcast.847, s32[4,1024,1024]{2,1,0} %broadcast.1140), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/position_embeddings.embed_call_one_hot/remat(core_fn)/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %convert.279 = f16[4,1024,1024]{2,1,0} convert(pred[4,1024,1024]{2,1,0} %compare.50), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/position_embeddings.embed_call_one_hot/remat(core_fn)/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %reshape.2671 = f16[4096,1024]{1,0} reshape(f16[4,1024,1024]{2,1,0} %convert.279)
  %constant.1035 = s32[16]{0} constant({...}), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %dynamic-slice.467 = s32[1]{0} dynamic-slice(s32[16]{0} %constant.1035, u32[] %partition-id), dynamic_slice_sizes={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %reshape.1958 = s32[] reshape(s32[1]{0} %dynamic-slice.467), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %subtract.124 = s32[] subtract(s32[] %reshape.1958, s32[] %reshape.1958), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/div" source_file="/code/alpa/alpa/model/bert_model.py" source_line=285}
  %constant.113 = s32[16]{0} constant({...}), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/position_embeddings.embed_call_one_hot/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %dynamic-slice.34 = s32[1]{0} dynamic-slice(s32[16]{0} %constant.113, u32[] %partition-id), dynamic_slice_sizes={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/position_embeddings.embed_call_one_hot/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %reshape.1449 = s32[] reshape(s32[1]{0} %dynamic-slice.34), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/position_embeddings.embed_call_one_hot/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %subtract.11 = s32[] subtract(s32[] %reshape.1449, s32[] %constant.59), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/position_embeddings.embed_call_one_hot/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %dynamic-slice.1063 = f16[4096,128]{1,0} dynamic-slice(f16[4096,1024]{1,0} %reshape.2671, s32[] %subtract.124, s32[] %subtract.11), dynamic_slice_sizes={4096,128}
  %reshape.2676 = f16[4096,128]{1,0} reshape(f16[4096,128]{1,0} %dynamic-slice.1063)
  %get-tuple-element.224 = f16[4,1024,1024]{2,1,0} get-tuple-element((s32[1,1,1024]{2,1,0}, s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.6), index=2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/position_embeddings.embed_call_one_hot/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %transpose.387 = f16[1024,4,1024]{0,2,1} transpose(f16[4,1024,1024]{2,1,0} %get-tuple-element.224), dimensions={2,0,1}
  %reshape.2677 = f16[1024,4096]{1,0} reshape(f16[1024,4,1024]{0,2,1} %transpose.387)
  %dot.338 = f16[128,1024]{1,0} dot(f16[4096,128]{1,0} %reshape.2676, f16[1024,4096]{1,0} %reshape.2677), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/position_embeddings.embed_call_one_hot/remat(core_fn)/transpose[permutation=(1, 0)]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=237}
  %all-reduce.46 = f16[128,1024]{1,0} all-reduce(f16[128,1024]{1,0} %dot.338), channel_id=47, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%add.39, sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/position_embeddings.embed_call_one_hot/remat(core_fn)/transpose[permutation=(1, 0)]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=237}
  %convert.280 = f32[128,1024]{0,1} convert(f16[128,1024]{1,0} %all-reduce.46), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/position_embeddings.embed_call_one_hot/remat(core_fn)/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=237}
  %broadcast.1141 = f32[128,1024]{1,0} broadcast(f32[] %constant.843), dimensions={}
  %multiply.718 = f32[128,1024]{0,1} multiply(f32[128,1024]{0,1} %convert.280, f32[128,1024]{1,0} %broadcast.1141), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.91 = f32[128,1024]{1,0} parameter(82), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %broadcast.1142 = f32[128,1024]{1,0} broadcast(f32[] %constant.846), dimensions={}
  %multiply.719 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param.91, f32[128,1024]{1,0} %broadcast.1142), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.472 = f32[128,1024]{0,1} add(f32[128,1024]{0,1} %multiply.718, f32[128,1024]{1,0} %multiply.719), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %broadcast.851 = f32[128,1024]{1,0} broadcast(f32[] %subtract.94), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %multiply.720 = f32[128,1024]{0,1} multiply(f32[128,1024]{0,1} %convert.280, f32[128,1024]{0,1} %convert.280), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %broadcast.1143 = f32[128,1024]{1,0} broadcast(f32[] %constant.850), dimensions={}
  %multiply.721 = f32[128,1024]{0,1} multiply(f32[128,1024]{0,1} %multiply.720, f32[128,1024]{1,0} %broadcast.1143), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.92 = f32[128,1024]{1,0} parameter(159), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %broadcast.1144 = f32[128,1024]{1,0} broadcast(f32[] %constant.853), dimensions={}
  %multiply.722 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param.92, f32[128,1024]{1,0} %broadcast.1144), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.473 = f32[128,1024]{0,1} add(f32[128,1024]{0,1} %multiply.721, f32[128,1024]{1,0} %multiply.722), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %broadcast.856 = f32[128,1024]{1,0} broadcast(f32[] %subtract.95), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %divide.256 = f32[128,1024]{0,1} divide(f32[128,1024]{0,1} %add.473, f32[128,1024]{1,0} %broadcast.856), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.3 = f32[128,1024]{0,1} sqrt(f32[128,1024]{0,1} %divide.256), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %broadcast.1146 = f32[128,1024]{1,0} broadcast(f32[] %constant.856), dimensions={}
  %add.474 = f32[128,1024]{0,1} add(f32[128,1024]{0,1} %sqrt.3, f32[128,1024]{1,0} %broadcast.1146), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.723 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %broadcast.851, f32[128,1024]{0,1} %add.474)
  %divide.257 = f32[128,1024]{0,1} divide(f32[128,1024]{0,1} %add.472, f32[128,1024]{1,0} %multiply.723), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %constant.2220 = f32[] constant(0.0001)
  %broadcast.1148 = f32[128,1024]{1,0} broadcast(f32[] %constant.2220), dimensions={}
  %multiply.724 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param.8, f32[128,1024]{1,0} %broadcast.1148), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.475 = f32[128,1024]{0,1} add(f32[128,1024]{0,1} %divide.257, f32[128,1024]{1,0} %multiply.724), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %broadcast.1149 = f32[128,1024]{1,0} broadcast(f32[] %constant.859), dimensions={}
  %multiply.725 = f32[128,1024]{0,1} multiply(f32[128,1024]{0,1} %add.475, f32[128,1024]{1,0} %broadcast.1149), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.476 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %param.8, f32[128,1024]{0,1} %multiply.725), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %transpose.388 = f16[6400,4,1024]{0,2,1} transpose(f16[4,1024,6400]{2,1,0} %add.243), dimensions={2,0,1}
  %reshape.2693 = f16[6400,4096]{1,0} reshape(f16[6400,4,1024]{0,2,1} %transpose.388)
  %dot.339 = f16[6400,1024]{1,0} dot(f16[6400,4096]{1,0} %reshape.2693, f16[4096,1024]{1,0} %reshape.1823), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/dot_general[dimension_numbers=(((0, 1), (0, 1)), ((), ())) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/gpt_model.py" source_line=70}
  %all-reduce.47 = f16[6400,1024]{1,0} all-reduce(f16[6400,1024]{1,0} %dot.339), channel_id=48, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%add.40, sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/dot_general[dimension_numbers=(((0, 1), (0, 1)), ((), ())) precision=None preferred_element_type=None]" source_file="/code/alpa/alpa/model/gpt_model.py" source_line=70}
  %convert.281 = f32[6400,1024]{1,0} convert(f16[6400,1024]{1,0} %all-reduce.47), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/word_embeddings.embed_call_one_hot/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=248}
  %iota.12 = s32[1,1,51200]{2,1,0} iota(), iota_dimension=2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/jvp(FlaxGPTForLMModule)/transformers/embeddings/word_embeddings.embed_call_one_hot/remat(core_fn)/iota[dtype=int32 shape=(1, 1, 51200) dimension=2]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %tuple.7 = (s32[1,1,51200]{2,1,0}, s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) tuple(s32[1,1,51200]{2,1,0} %iota.12, s32[4,1024]{1,0} %param.5, f16[4,1024,1024]{2,1,0} %add.471), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/word_embeddings.embed_call_one_hot/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %opt-barrier.7 = (s32[1,1,51200]{2,1,0}, s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) opt-barrier((s32[1,1,51200]{2,1,0}, s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %tuple.7), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/word_embeddings.embed_call_one_hot/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %get-tuple-element.225 = s32[4,1024]{1,0} get-tuple-element((s32[1,1,51200]{2,1,0}, s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.7), index=1, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/word_embeddings.embed_call_one_hot/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.860 = s32[4,1024,51200]{2,1,0} broadcast(s32[4,1024]{1,0} %get-tuple-element.225), dimensions={0,1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/word_embeddings.embed_call_one_hot/remat(core_fn)/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %get-tuple-element.226 = s32[1,1,51200]{2,1,0} get-tuple-element((s32[1,1,51200]{2,1,0}, s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.7), index=0, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/word_embeddings.embed_call_one_hot/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %reshape.2695 = s32[51200]{0} reshape(s32[1,1,51200]{2,1,0} %get-tuple-element.226), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/word_embeddings.embed_call_one_hot/remat(core_fn)/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %broadcast.1150 = s32[4,1024,51200]{2,1,0} broadcast(s32[51200]{0} %reshape.2695), dimensions={2}
  %compare.51 = pred[4,1024,51200]{2,1,0} compare(s32[4,1024,51200]{2,1,0} %broadcast.860, s32[4,1024,51200]{2,1,0} %broadcast.1150), direction=EQ, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/word_embeddings.embed_call_one_hot/remat(core_fn)/eq" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %convert.282 = f16[4,1024,51200]{2,1,0} convert(pred[4,1024,51200]{2,1,0} %compare.51), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/word_embeddings.embed_call_one_hot/remat(core_fn)/convert_element_type[new_dtype=float16 weak_type=False]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=236}
  %reshape.2698 = f16[4096,51200]{1,0} reshape(f16[4,1024,51200]{2,1,0} %convert.282)
  %constant.60 = s32[16]{0} constant({...}), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %dynamic-slice.4 = s32[1]{0} dynamic-slice(s32[16]{0} %constant.60, u32[] %partition-id), dynamic_slice_sizes={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %reshape.1421 = s32[] reshape(s32[1]{0} %dynamic-slice.4), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %subtract.2 = s32[] subtract(s32[] %reshape.1421, s32[] %constant.59), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/eq" source_file="/code/alpa/playground/alpa_micro_benchmark/test_export_hlo.py" source_line=87}
  %dynamic-slice.1094 = f16[4096,6400]{1,0} dynamic-slice(f16[4096,51200]{1,0} %reshape.2698, s32[] %subtract.124, s32[] %subtract.2), dynamic_slice_sizes={4096,6400}
  %reshape.2703 = f16[4096,6400]{1,0} reshape(f16[4096,6400]{1,0} %dynamic-slice.1094)
  %get-tuple-element.227 = f16[4,1024,1024]{2,1,0} get-tuple-element((s32[1,1,51200]{2,1,0}, s32[4,1024]{1,0}, f16[4,1024,1024]{2,1,0}) %opt-barrier.7), index=2, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/word_embeddings.embed_call_one_hot/remat(core_fn)/optimization_barrier" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %transpose.389 = f16[1024,4,1024]{0,2,1} transpose(f16[4,1024,1024]{2,1,0} %get-tuple-element.227), dimensions={2,0,1}
  %reshape.2704 = f16[1024,4096]{1,0} reshape(f16[1024,4,1024]{0,2,1} %transpose.389)
  %dot.340 = f16[6400,1024]{1,0} dot(f16[4096,6400]{1,0} %reshape.2703, f16[1024,4096]{1,0} %reshape.2704), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/word_embeddings.embed_call_one_hot/remat(core_fn)/transpose[permutation=(1, 0)]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=237}
  %all-reduce.48 = f16[6400,1024]{1,0} all-reduce(f16[6400,1024]{1,0} %dot.340), channel_id=49, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%add.41, sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/word_embeddings.embed_call_one_hot/remat(core_fn)/transpose[permutation=(1, 0)]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=237}
  %convert.283 = f32[6400,1024]{0,1} convert(f16[6400,1024]{1,0} %all-reduce.48), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/word_embeddings.embed_call_one_hot/remat(core_fn)/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/code/alpa/alpa/monkey_patch.py" source_line=237}
  %add.478 = f32[6400,1024]{1,0} add(f32[6400,1024]{1,0} %convert.281, f32[6400,1024]{0,1} %convert.283), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/embeddings/word_embeddings.embed_call_one_hot/add_any" source_file="/python3.9-env/lib/python3.9/site-packages/flax/core/lift.py" source_line=1084}
  %broadcast.1151 = f32[6400,1024]{1,0} broadcast(f32[] %constant.843), dimensions={}
  %multiply.726 = f32[6400,1024]{1,0} multiply(f32[6400,1024]{1,0} %add.478, f32[6400,1024]{1,0} %broadcast.1151), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.93 = f32[6400,1024]{1,0} parameter(83), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %broadcast.1152 = f32[6400,1024]{1,0} broadcast(f32[] %constant.846), dimensions={}
  %multiply.727 = f32[6400,1024]{1,0} multiply(f32[6400,1024]{1,0} %param.93, f32[6400,1024]{1,0} %broadcast.1152), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.479 = f32[6400,1024]{1,0} add(f32[6400,1024]{1,0} %multiply.726, f32[6400,1024]{1,0} %multiply.727), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %broadcast.864 = f32[6400,1024]{1,0} broadcast(f32[] %subtract.94), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %multiply.728 = f32[6400,1024]{1,0} multiply(f32[6400,1024]{1,0} %add.478, f32[6400,1024]{1,0} %add.478), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %broadcast.1153 = f32[6400,1024]{1,0} broadcast(f32[] %constant.850), dimensions={}
  %multiply.729 = f32[6400,1024]{1,0} multiply(f32[6400,1024]{1,0} %multiply.728, f32[6400,1024]{1,0} %broadcast.1153), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.94 = f32[6400,1024]{1,0} parameter(160), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %broadcast.1154 = f32[6400,1024]{1,0} broadcast(f32[] %constant.853), dimensions={}
  %multiply.730 = f32[6400,1024]{1,0} multiply(f32[6400,1024]{1,0} %param.94, f32[6400,1024]{1,0} %broadcast.1154), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.480 = f32[6400,1024]{1,0} add(f32[6400,1024]{1,0} %multiply.729, f32[6400,1024]{1,0} %multiply.730), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %broadcast.868 = f32[6400,1024]{1,0} broadcast(f32[] %subtract.95), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %divide.258 = f32[6400,1024]{1,0} divide(f32[6400,1024]{1,0} %add.480, f32[6400,1024]{1,0} %broadcast.868), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.4 = f32[6400,1024]{1,0} sqrt(f32[6400,1024]{1,0} %divide.258), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %broadcast.1155 = f32[6400,1024]{1,0} broadcast(f32[] %constant.856), dimensions={}
  %add.481 = f32[6400,1024]{1,0} add(f32[6400,1024]{1,0} %sqrt.4, f32[6400,1024]{1,0} %broadcast.1155), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.731 = f32[6400,1024]{1,0} multiply(f32[6400,1024]{1,0} %broadcast.864, f32[6400,1024]{1,0} %add.481)
  %divide.259 = f32[6400,1024]{1,0} divide(f32[6400,1024]{1,0} %add.479, f32[6400,1024]{1,0} %multiply.731), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %broadcast.1156 = f32[6400,1024]{1,0} broadcast(f32[] %constant.2220), dimensions={}
  %multiply.732 = f32[6400,1024]{1,0} multiply(f32[6400,1024]{1,0} %param.6, f32[6400,1024]{1,0} %broadcast.1156), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.482 = f32[6400,1024]{1,0} add(f32[6400,1024]{1,0} %divide.259, f32[6400,1024]{1,0} %multiply.732), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %broadcast.1157 = f32[6400,1024]{1,0} broadcast(f32[] %constant.859), dimensions={}
  %multiply.733 = f32[6400,1024]{1,0} multiply(f32[6400,1024]{1,0} %add.482, f32[6400,1024]{1,0} %broadcast.1157), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.483 = f32[6400,1024]{1,0} add(f32[6400,1024]{1,0} %param.6, f32[6400,1024]{1,0} %multiply.733), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reduce.113 = f32[1024]{0} reduce(f32[4,1024,1024]{2,1,0} %convert.273, f32[] %constant.69), dimensions={0,1}, to_apply=%region_157.4390, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %all-reduce.49 = f32[1024]{0} all-reduce(f32[1024]{0} %reduce.113), channel_id=50, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_157.4390, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %multiply.734 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.49, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.95 = f32[1024]{0} parameter(84), sharding={replicated}
  %multiply.735 = f32[1024]{0} multiply(f32[1024]{0} %param.95, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.484 = f32[1024]{0} add(f32[1024]{0} %multiply.734, f32[1024]{0} %multiply.735), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.736 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.49, f32[1024]{0} %all-reduce.49), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.737 = f32[1024]{0} multiply(f32[1024]{0} %multiply.736, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.96 = f32[1024]{0} parameter(161), sharding={replicated}
  %multiply.738 = f32[1024]{0} multiply(f32[1024]{0} %param.96, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.485 = f32[1024]{0} add(f32[1024]{0} %multiply.737, f32[1024]{0} %multiply.738), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.260 = f32[1024]{0} divide(f32[1024]{0} %add.485, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.5 = f32[1024]{0} sqrt(f32[1024]{0} %divide.260), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.486 = f32[1024]{0} add(f32[1024]{0} %sqrt.5, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.739 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.486)
  %divide.261 = f32[1024]{0} divide(f32[1024]{0} %add.484, f32[1024]{0} %multiply.739), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.740 = f32[1024]{0} multiply(f32[1024]{0} %divide.261, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.487 = f32[1024]{0} add(f32[1024]{0} %param.17, f32[1024]{0} %multiply.740), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %multiply.741 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.812, f32[4,1024,1024]{2,1,0} %multiply.674), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.114 = f32[1024]{0} reduce(f32[4,1024,1024]{2,1,0} %multiply.741, f32[] %constant.69), dimensions={0,1}, to_apply=%region_158.4400, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %all-reduce.50 = f32[1024]{0} all-reduce(f32[1024]{0} %reduce.114), channel_id=51, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_158.4400, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.742 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.50, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.97 = f32[1024]{0} parameter(85), sharding={replicated}
  %multiply.743 = f32[1024]{0} multiply(f32[1024]{0} %param.97, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.488 = f32[1024]{0} add(f32[1024]{0} %multiply.742, f32[1024]{0} %multiply.743), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.744 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.50, f32[1024]{0} %all-reduce.50), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.745 = f32[1024]{0} multiply(f32[1024]{0} %multiply.744, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.98 = f32[1024]{0} parameter(162), sharding={replicated}
  %multiply.746 = f32[1024]{0} multiply(f32[1024]{0} %param.98, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.489 = f32[1024]{0} add(f32[1024]{0} %multiply.745, f32[1024]{0} %multiply.746), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.262 = f32[1024]{0} divide(f32[1024]{0} %add.489, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.6 = f32[1024]{0} sqrt(f32[1024]{0} %divide.262), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.490 = f32[1024]{0} add(f32[1024]{0} %sqrt.6, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.747 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.490)
  %divide.263 = f32[1024]{0} divide(f32[1024]{0} %add.488, f32[1024]{0} %multiply.747), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.748 = f32[1024]{0} multiply(f32[1024]{0} %divide.263, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.491 = f32[1024]{0} add(f32[1024]{0} %param.16, f32[1024]{0} %multiply.748), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reduce.115 = f16[1024]{0} reduce(f16[4,1024,1024]{2,1,0} %add.454, f16[] %constant.89), dimensions={0,1}, to_apply=%region_161.4439, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %all-reduce.51 = f16[1024]{0} all-reduce(f16[1024]{0} %reduce.115), channel_id=52, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_161.4439, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %convert.284 = f32[1024]{0} convert(f16[1024]{0} %all-reduce.51), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.749 = f32[1024]{0} multiply(f32[1024]{0} %convert.284, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.99 = f32[1024]{0} parameter(86), sharding={replicated}
  %multiply.750 = f32[1024]{0} multiply(f32[1024]{0} %param.99, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.492 = f32[1024]{0} add(f32[1024]{0} %multiply.749, f32[1024]{0} %multiply.750), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.751 = f32[1024]{0} multiply(f32[1024]{0} %convert.284, f32[1024]{0} %convert.284), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.752 = f32[1024]{0} multiply(f32[1024]{0} %multiply.751, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.100 = f32[1024]{0} parameter(163), sharding={replicated}
  %multiply.753 = f32[1024]{0} multiply(f32[1024]{0} %param.100, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.493 = f32[1024]{0} add(f32[1024]{0} %multiply.752, f32[1024]{0} %multiply.753), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.264 = f32[1024]{0} divide(f32[1024]{0} %add.493, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.7 = f32[1024]{0} sqrt(f32[1024]{0} %divide.264), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.494 = f32[1024]{0} add(f32[1024]{0} %sqrt.7, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.754 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.494)
  %divide.265 = f32[1024]{0} divide(f32[1024]{0} %add.492, f32[1024]{0} %multiply.754), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.755 = f32[1024]{0} multiply(f32[1024]{0} %divide.265, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.495 = f32[1024]{0} add(f32[1024]{0} %param.15, f32[1024]{0} %multiply.755), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %transpose.390 = f16[1024,4,1024]{0,2,1} transpose(f16[4,1024,1024]{2,1,0} %add.454), dimensions={2,0,1}
  %reshape.2726 = f16[1024,4096]{1,0} reshape(f16[1024,4,1024]{0,2,1} %transpose.390)
  %dot.341 = f16[128,1024]{1,0} dot(f16[4096,128]{1,0} %reshape.2565, f16[1024,4096]{1,0} %reshape.2726), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.52 = f16[128,1024]{1,0} all-reduce(f16[128,1024]{1,0} %dot.341), channel_id=53, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%add.42, sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.285 = f32[128,1024]{0,1} convert(f16[128,1024]{1,0} %all-reduce.52), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.756 = f32[128,1024]{0,1} multiply(f32[128,1024]{0,1} %convert.285, f32[128,1024]{1,0} %broadcast.1141), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.101 = f32[128,1024]{1,0} parameter(87), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.757 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param.101, f32[128,1024]{1,0} %broadcast.1142), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.497 = f32[128,1024]{0,1} add(f32[128,1024]{0,1} %multiply.756, f32[128,1024]{1,0} %multiply.757), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.758 = f32[128,1024]{0,1} multiply(f32[128,1024]{0,1} %convert.285, f32[128,1024]{0,1} %convert.285), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.759 = f32[128,1024]{0,1} multiply(f32[128,1024]{0,1} %multiply.758, f32[128,1024]{1,0} %broadcast.1143), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.102 = f32[128,1024]{1,0} parameter(164), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.760 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param.102, f32[128,1024]{1,0} %broadcast.1144), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.498 = f32[128,1024]{0,1} add(f32[128,1024]{0,1} %multiply.759, f32[128,1024]{1,0} %multiply.760), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.266 = f32[128,1024]{0,1} divide(f32[128,1024]{0,1} %add.498, f32[128,1024]{1,0} %broadcast.856), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.8 = f32[128,1024]{0,1} sqrt(f32[128,1024]{0,1} %divide.266), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.499 = f32[128,1024]{0,1} add(f32[128,1024]{0,1} %sqrt.8, f32[128,1024]{1,0} %broadcast.1146), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.761 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %broadcast.851, f32[128,1024]{0,1} %add.499)
  %divide.267 = f32[128,1024]{0,1} divide(f32[128,1024]{0,1} %add.497, f32[128,1024]{1,0} %multiply.761), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.762 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param.14, f32[128,1024]{1,0} %broadcast.1148), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.500 = f32[128,1024]{0,1} add(f32[128,1024]{0,1} %divide.267, f32[128,1024]{1,0} %multiply.762), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.763 = f32[128,1024]{0,1} multiply(f32[128,1024]{0,1} %add.500, f32[128,1024]{1,0} %broadcast.1149), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.501 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %param.14, f32[128,1024]{0,1} %multiply.763), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reshape.2729 = f16[4,1024,384]{2,1,0} reshape(f16[4,1024,128,3]{3,2,1,0} %add.457), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/reshape[new_sizes=(8, 1024, 3072) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=161}
  %reduce.116 = f16[384]{0} reduce(f16[4,1024,384]{2,1,0} %reshape.2729, f16[] %constant.89), dimensions={0,1}, to_apply=%region_164.4495, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %all-reduce.53 = f16[384]{0} all-reduce(f16[384]{0} %reduce.116), channel_id=54, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_164.4495, sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %convert.286 = f32[384]{0} convert(f16[384]{0} %all-reduce.53), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1158 = f32[384]{0} broadcast(f32[] %constant.843), dimensions={}
  %multiply.764 = f32[384]{0} multiply(f32[384]{0} %convert.286, f32[384]{0} %broadcast.1158), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.103 = f32[384]{0} parameter(88), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %broadcast.1160 = f32[384]{0} broadcast(f32[] %constant.846), dimensions={}
  %multiply.765 = f32[384]{0} multiply(f32[384]{0} %param.103, f32[384]{0} %broadcast.1160), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.502 = f32[384]{0} add(f32[384]{0} %multiply.764, f32[384]{0} %multiply.765), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %broadcast.875 = f32[384]{0} broadcast(f32[] %subtract.94), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %multiply.766 = f32[384]{0} multiply(f32[384]{0} %convert.286, f32[384]{0} %convert.286), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %broadcast.1162 = f32[384]{0} broadcast(f32[] %constant.850), dimensions={}
  %multiply.767 = f32[384]{0} multiply(f32[384]{0} %multiply.766, f32[384]{0} %broadcast.1162), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.104 = f32[384]{0} parameter(165), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %broadcast.1163 = f32[384]{0} broadcast(f32[] %constant.853), dimensions={}
  %multiply.768 = f32[384]{0} multiply(f32[384]{0} %param.104, f32[384]{0} %broadcast.1163), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.503 = f32[384]{0} add(f32[384]{0} %multiply.767, f32[384]{0} %multiply.768), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %broadcast.878 = f32[384]{0} broadcast(f32[] %subtract.95), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %divide.268 = f32[384]{0} divide(f32[384]{0} %add.503, f32[384]{0} %broadcast.878), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.9 = f32[384]{0} sqrt(f32[384]{0} %divide.268), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %broadcast.1164 = f32[384]{0} broadcast(f32[] %constant.856), dimensions={}
  %add.504 = f32[384]{0} add(f32[384]{0} %sqrt.9, f32[384]{0} %broadcast.1164), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.769 = f32[384]{0} multiply(f32[384]{0} %broadcast.875, f32[384]{0} %add.504)
  %divide.269 = f32[384]{0} divide(f32[384]{0} %add.502, f32[384]{0} %multiply.769), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %broadcast.1165 = f32[384]{0} broadcast(f32[] %constant.859), dimensions={}
  %multiply.770 = f32[384]{0} multiply(f32[384]{0} %divide.269, f32[384]{0} %broadcast.1165), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.505 = f32[384]{0} add(f32[384]{0} %param.12, f32[384]{0} %multiply.770), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %transpose.391 = f16[384,4,1024]{0,2,1} transpose(f16[4,1024,384]{2,1,0} %reshape.2729), dimensions={2,0,1}
  %reshape.2745 = f16[384,4096]{1,0} reshape(f16[384,4,1024]{0,2,1} %transpose.391)
  %dot.342 = f16[1024,384]{1,0} dot(f16[4096,1024]{1,0} %reshape.2533, f16[384,4096]{1,0} %reshape.2745), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.54 = f16[1024,384]{1,0} all-reduce(f16[1024,384]{1,0} %dot.342), channel_id=55, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%add.43, sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.287 = f32[1024,384]{0,1} convert(f16[1024,384]{1,0} %all-reduce.54), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/attention/self/qvk_combined/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1166 = f32[1024,384]{1,0} broadcast(f32[] %constant.843), dimensions={}
  %multiply.771 = f32[1024,384]{0,1} multiply(f32[1024,384]{0,1} %convert.287, f32[1024,384]{1,0} %broadcast.1166), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.105 = f32[1024,384]{1,0} parameter(89), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %broadcast.1167 = f32[1024,384]{1,0} broadcast(f32[] %constant.846), dimensions={}
  %multiply.772 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param.105, f32[1024,384]{1,0} %broadcast.1167), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.506 = f32[1024,384]{0,1} add(f32[1024,384]{0,1} %multiply.771, f32[1024,384]{1,0} %multiply.772), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %broadcast.883 = f32[1024,384]{1,0} broadcast(f32[] %subtract.94), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %multiply.773 = f32[1024,384]{0,1} multiply(f32[1024,384]{0,1} %convert.287, f32[1024,384]{0,1} %convert.287), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %broadcast.1168 = f32[1024,384]{1,0} broadcast(f32[] %constant.850), dimensions={}
  %multiply.774 = f32[1024,384]{0,1} multiply(f32[1024,384]{0,1} %multiply.773, f32[1024,384]{1,0} %broadcast.1168), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.106 = f32[1024,384]{1,0} parameter(166), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %broadcast.1169 = f32[1024,384]{1,0} broadcast(f32[] %constant.853), dimensions={}
  %multiply.775 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param.106, f32[1024,384]{1,0} %broadcast.1169), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.507 = f32[1024,384]{0,1} add(f32[1024,384]{0,1} %multiply.774, f32[1024,384]{1,0} %multiply.775), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %broadcast.889 = f32[1024,384]{1,0} broadcast(f32[] %subtract.95), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %divide.270 = f32[1024,384]{0,1} divide(f32[1024,384]{0,1} %add.507, f32[1024,384]{1,0} %broadcast.889), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.10 = f32[1024,384]{0,1} sqrt(f32[1024,384]{0,1} %divide.270), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %broadcast.1170 = f32[1024,384]{1,0} broadcast(f32[] %constant.856), dimensions={}
  %add.508 = f32[1024,384]{0,1} add(f32[1024,384]{0,1} %sqrt.10, f32[1024,384]{1,0} %broadcast.1170), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.776 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %broadcast.883, f32[1024,384]{0,1} %add.508)
  %divide.271 = f32[1024,384]{0,1} divide(f32[1024,384]{0,1} %add.506, f32[1024,384]{1,0} %multiply.776), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %broadcast.1171 = f32[1024,384]{1,0} broadcast(f32[] %constant.2220), dimensions={}
  %multiply.777 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param.11, f32[1024,384]{1,0} %broadcast.1171), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.509 = f32[1024,384]{0,1} add(f32[1024,384]{0,1} %divide.271, f32[1024,384]{1,0} %multiply.777), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %broadcast.1172 = f32[1024,384]{1,0} broadcast(f32[] %constant.859), dimensions={}
  %multiply.778 = f32[1024,384]{0,1} multiply(f32[1024,384]{0,1} %add.509, f32[1024,384]{1,0} %broadcast.1172), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.510 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %param.11, f32[1024,384]{0,1} %multiply.778), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reduce.117 = f16[512]{0} reduce(f16[4,1024,512]{2,1,0} %add.450, f16[] %constant.89), dimensions={0,1}, to_apply=%region_156.4378, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %all-reduce.55 = f16[512]{0} all-reduce(f16[512]{0} %reduce.117), channel_id=56, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_156.4378, sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %convert.288 = f32[512]{0} convert(f16[512]{0} %all-reduce.55), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1173 = f32[512]{0} broadcast(f32[] %constant.843), dimensions={}
  %multiply.780 = f32[512]{0} multiply(f32[512]{0} %convert.288, f32[512]{0} %broadcast.1173), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.107 = f32[512]{0} parameter(90), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %broadcast.1174 = f32[512]{0} broadcast(f32[] %constant.846), dimensions={}
  %multiply.781 = f32[512]{0} multiply(f32[512]{0} %param.107, f32[512]{0} %broadcast.1174), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.511 = f32[512]{0} add(f32[512]{0} %multiply.780, f32[512]{0} %multiply.781), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %broadcast.898 = f32[512]{0} broadcast(f32[] %subtract.94), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %multiply.782 = f32[512]{0} multiply(f32[512]{0} %convert.288, f32[512]{0} %convert.288), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %broadcast.1175 = f32[512]{0} broadcast(f32[] %constant.850), dimensions={}
  %multiply.783 = f32[512]{0} multiply(f32[512]{0} %multiply.782, f32[512]{0} %broadcast.1175), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.108 = f32[512]{0} parameter(167), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %broadcast.1176 = f32[512]{0} broadcast(f32[] %constant.853), dimensions={}
  %multiply.784 = f32[512]{0} multiply(f32[512]{0} %param.108, f32[512]{0} %broadcast.1176), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.512 = f32[512]{0} add(f32[512]{0} %multiply.783, f32[512]{0} %multiply.784), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %broadcast.901 = f32[512]{0} broadcast(f32[] %subtract.95), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %divide.272 = f32[512]{0} divide(f32[512]{0} %add.512, f32[512]{0} %broadcast.901), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.11 = f32[512]{0} sqrt(f32[512]{0} %divide.272), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %broadcast.1178 = f32[512]{0} broadcast(f32[] %constant.856), dimensions={}
  %add.514 = f32[512]{0} add(f32[512]{0} %sqrt.11, f32[512]{0} %broadcast.1178), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.785 = f32[512]{0} multiply(f32[512]{0} %broadcast.898, f32[512]{0} %add.514)
  %divide.273 = f32[512]{0} divide(f32[512]{0} %add.511, f32[512]{0} %multiply.785), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %broadcast.1181 = f32[512]{0} broadcast(f32[] %constant.859), dimensions={}
  %multiply.787 = f32[512]{0} multiply(f32[512]{0} %divide.273, f32[512]{0} %broadcast.1181), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.515 = f32[512]{0} add(f32[512]{0} %param.19, f32[512]{0} %multiply.787), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %transpose.392 = f16[512,4,1024]{0,2,1} transpose(f16[4,1024,512]{2,1,0} %add.450), dimensions={2,0,1}
  %reshape.2778 = f16[512,4096]{1,0} reshape(f16[512,4,1024]{0,2,1} %transpose.392)
  %dot.343 = f16[1024,512]{1,0} dot(f16[4096,1024]{1,0} %reshape.2577, f16[512,4096]{1,0} %reshape.2778), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.56 = f16[1024,512]{1,0} all-reduce(f16[1024,512]{1,0} %dot.343), channel_id=57, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%add.44, sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.289 = f32[1024,512]{0,1} convert(f16[1024,512]{1,0} %all-reduce.56), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/intermediate/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1183 = f32[1024,512]{1,0} broadcast(f32[] %constant.843), dimensions={}
  %multiply.788 = f32[1024,512]{0,1} multiply(f32[1024,512]{0,1} %convert.289, f32[1024,512]{1,0} %broadcast.1183), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.109 = f32[1024,512]{1,0} parameter(91), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %broadcast.1184 = f32[1024,512]{1,0} broadcast(f32[] %constant.846), dimensions={}
  %multiply.789 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param.109, f32[1024,512]{1,0} %broadcast.1184), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.516 = f32[1024,512]{0,1} add(f32[1024,512]{0,1} %multiply.788, f32[1024,512]{1,0} %multiply.789), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %broadcast.908 = f32[1024,512]{1,0} broadcast(f32[] %subtract.94), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %multiply.790 = f32[1024,512]{0,1} multiply(f32[1024,512]{0,1} %convert.289, f32[1024,512]{0,1} %convert.289), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %broadcast.1185 = f32[1024,512]{1,0} broadcast(f32[] %constant.850), dimensions={}
  %multiply.791 = f32[1024,512]{0,1} multiply(f32[1024,512]{0,1} %multiply.790, f32[1024,512]{1,0} %broadcast.1185), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.110 = f32[1024,512]{1,0} parameter(168), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %broadcast.1187 = f32[1024,512]{1,0} broadcast(f32[] %constant.853), dimensions={}
  %multiply.792 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param.110, f32[1024,512]{1,0} %broadcast.1187), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.517 = f32[1024,512]{0,1} add(f32[1024,512]{0,1} %multiply.791, f32[1024,512]{1,0} %multiply.792), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %broadcast.911 = f32[1024,512]{1,0} broadcast(f32[] %subtract.95), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %divide.274 = f32[1024,512]{0,1} divide(f32[1024,512]{0,1} %add.517, f32[1024,512]{1,0} %broadcast.911), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.12 = f32[1024,512]{0,1} sqrt(f32[1024,512]{0,1} %divide.274), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %broadcast.1189 = f32[1024,512]{1,0} broadcast(f32[] %constant.856), dimensions={}
  %add.518 = f32[1024,512]{0,1} add(f32[1024,512]{0,1} %sqrt.12, f32[1024,512]{1,0} %broadcast.1189), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.793 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %broadcast.908, f32[1024,512]{0,1} %add.518)
  %divide.275 = f32[1024,512]{0,1} divide(f32[1024,512]{0,1} %add.516, f32[1024,512]{1,0} %multiply.793), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %broadcast.1190 = f32[1024,512]{1,0} broadcast(f32[] %constant.2220), dimensions={}
  %multiply.794 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param.18, f32[1024,512]{1,0} %broadcast.1190), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.519 = f32[1024,512]{0,1} add(f32[1024,512]{0,1} %divide.275, f32[1024,512]{1,0} %multiply.794), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %broadcast.1191 = f32[1024,512]{1,0} broadcast(f32[] %constant.859), dimensions={}
  %multiply.795 = f32[1024,512]{0,1} multiply(f32[1024,512]{0,1} %add.519, f32[1024,512]{1,0} %broadcast.1191), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.520 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %param.18, f32[1024,512]{0,1} %multiply.795), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reduce.118 = f32[1024]{0} reduce(f32[4,1024,1024]{2,1,0} %convert.270, f32[] %constant.69), dimensions={0,1}, to_apply=%region_151.4310, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %all-reduce.57 = f32[1024]{0} all-reduce(f32[1024]{0} %reduce.118), channel_id=58, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_151.4310, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %multiply.796 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.57, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.111 = f32[1024]{0} parameter(92), sharding={replicated}
  %multiply.797 = f32[1024]{0} multiply(f32[1024]{0} %param.111, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.521 = f32[1024]{0} add(f32[1024]{0} %multiply.796, f32[1024]{0} %multiply.797), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.798 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.57, f32[1024]{0} %all-reduce.57), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.801 = f32[1024]{0} multiply(f32[1024]{0} %multiply.798, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.112 = f32[1024]{0} parameter(169), sharding={replicated}
  %multiply.802 = f32[1024]{0} multiply(f32[1024]{0} %param.112, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.523 = f32[1024]{0} add(f32[1024]{0} %multiply.801, f32[1024]{0} %multiply.802), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.276 = f32[1024]{0} divide(f32[1024]{0} %add.523, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.13 = f32[1024]{0} sqrt(f32[1024]{0} %divide.276), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.524 = f32[1024]{0} add(f32[1024]{0} %sqrt.13, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.803 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.524)
  %divide.277 = f32[1024]{0} divide(f32[1024]{0} %add.521, f32[1024]{0} %multiply.803), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.804 = f32[1024]{0} multiply(f32[1024]{0} %divide.277, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.525 = f32[1024]{0} add(f32[1024]{0} %param.23, f32[1024]{0} %multiply.804), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %multiply.805 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.828, f32[4,1024,1024]{2,1,0} %multiply.656), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.119 = f32[1024]{0} reduce(f32[4,1024,1024]{2,1,0} %multiply.805, f32[] %constant.69), dimensions={0,1}, to_apply=%region_152.4320, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %all-reduce.58 = f32[1024]{0} all-reduce(f32[1024]{0} %reduce.119), channel_id=59, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_152.4320, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.806 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.58, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.113 = f32[1024]{0} parameter(93), sharding={replicated}
  %multiply.807 = f32[1024]{0} multiply(f32[1024]{0} %param.113, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.526 = f32[1024]{0} add(f32[1024]{0} %multiply.806, f32[1024]{0} %multiply.807), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.808 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.58, f32[1024]{0} %all-reduce.58), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.809 = f32[1024]{0} multiply(f32[1024]{0} %multiply.808, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.114 = f32[1024]{0} parameter(170), sharding={replicated}
  %multiply.810 = f32[1024]{0} multiply(f32[1024]{0} %param.114, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.527 = f32[1024]{0} add(f32[1024]{0} %multiply.809, f32[1024]{0} %multiply.810), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.278 = f32[1024]{0} divide(f32[1024]{0} %add.527, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.14 = f32[1024]{0} sqrt(f32[1024]{0} %divide.278), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.528 = f32[1024]{0} add(f32[1024]{0} %sqrt.14, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.811 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.528)
  %divide.279 = f32[1024]{0} divide(f32[1024]{0} %add.526, f32[1024]{0} %multiply.811), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.812 = f32[1024]{0} multiply(f32[1024]{0} %divide.279, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.529 = f32[1024]{0} add(f32[1024]{0} %param.22, f32[1024]{0} %multiply.812), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reduce.120 = f16[1024]{0} reduce(f16[4,1024,1024]{2,1,0} %add.449, f16[] %constant.89), dimensions={0,1}, to_apply=%region_155.4359, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %all-reduce.59 = f16[1024]{0} all-reduce(f16[1024]{0} %reduce.120), channel_id=60, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_155.4359, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %convert.290 = f32[1024]{0} convert(f16[1024]{0} %all-reduce.59), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.813 = f32[1024]{0} multiply(f32[1024]{0} %convert.290, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.115 = f32[1024]{0} parameter(94), sharding={replicated}
  %multiply.814 = f32[1024]{0} multiply(f32[1024]{0} %param.115, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.530 = f32[1024]{0} add(f32[1024]{0} %multiply.813, f32[1024]{0} %multiply.814), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.815 = f32[1024]{0} multiply(f32[1024]{0} %convert.290, f32[1024]{0} %convert.290), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.816 = f32[1024]{0} multiply(f32[1024]{0} %multiply.815, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.116 = f32[1024]{0} parameter(171), sharding={replicated}
  %multiply.817 = f32[1024]{0} multiply(f32[1024]{0} %param.116, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.531 = f32[1024]{0} add(f32[1024]{0} %multiply.816, f32[1024]{0} %multiply.817), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.280 = f32[1024]{0} divide(f32[1024]{0} %add.531, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.15 = f32[1024]{0} sqrt(f32[1024]{0} %divide.280), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.532 = f32[1024]{0} add(f32[1024]{0} %sqrt.15, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.820 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.532)
  %divide.281 = f32[1024]{0} divide(f32[1024]{0} %add.530, f32[1024]{0} %multiply.820), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.822 = f32[1024]{0} multiply(f32[1024]{0} %divide.281, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.533 = f32[1024]{0} add(f32[1024]{0} %param.21, f32[1024]{0} %multiply.822), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %transpose.393 = f16[1024,4,1024]{0,2,1} transpose(f16[4,1024,1024]{2,1,0} %add.449), dimensions={2,0,1}
  %reshape.2797 = f16[1024,4096]{1,0} reshape(f16[1024,4,1024]{0,2,1} %transpose.393)
  %dot.344 = f16[512,1024]{1,0} dot(f16[4096,512]{1,0} %reshape.2598, f16[1024,4096]{1,0} %reshape.2797), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.60 = f16[512,1024]{1,0} all-reduce(f16[512,1024]{1,0} %dot.344), channel_id=61, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%add.45, sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.291 = f32[512,1024]{0,1} convert(f16[512,1024]{1,0} %all-reduce.60), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/0/remat(core_fn)/0/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %broadcast.1192 = f32[512,1024]{1,0} broadcast(f32[] %constant.843), dimensions={}
  %multiply.824 = f32[512,1024]{0,1} multiply(f32[512,1024]{0,1} %convert.291, f32[512,1024]{1,0} %broadcast.1192), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.117 = f32[512,1024]{1,0} parameter(95), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %broadcast.1193 = f32[512,1024]{1,0} broadcast(f32[] %constant.846), dimensions={}
  %multiply.826 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param.117, f32[512,1024]{1,0} %broadcast.1193), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.534 = f32[512,1024]{0,1} add(f32[512,1024]{0,1} %multiply.824, f32[512,1024]{1,0} %multiply.826), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %broadcast.917 = f32[512,1024]{1,0} broadcast(f32[] %subtract.94), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %multiply.828 = f32[512,1024]{0,1} multiply(f32[512,1024]{0,1} %convert.291, f32[512,1024]{0,1} %convert.291), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %broadcast.1194 = f32[512,1024]{1,0} broadcast(f32[] %constant.850), dimensions={}
  %multiply.830 = f32[512,1024]{0,1} multiply(f32[512,1024]{0,1} %multiply.828, f32[512,1024]{1,0} %broadcast.1194), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.118 = f32[512,1024]{1,0} parameter(172), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %broadcast.1196 = f32[512,1024]{1,0} broadcast(f32[] %constant.853), dimensions={}
  %multiply.832 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param.118, f32[512,1024]{1,0} %broadcast.1196), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.535 = f32[512,1024]{0,1} add(f32[512,1024]{0,1} %multiply.830, f32[512,1024]{1,0} %multiply.832), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %broadcast.922 = f32[512,1024]{1,0} broadcast(f32[] %subtract.95), dimensions={}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %divide.282 = f32[512,1024]{0,1} divide(f32[512,1024]{0,1} %add.535, f32[512,1024]{1,0} %broadcast.922), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.16 = f32[512,1024]{0,1} sqrt(f32[512,1024]{0,1} %divide.282), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %broadcast.1198 = f32[512,1024]{1,0} broadcast(f32[] %constant.856), dimensions={}
  %add.536 = f32[512,1024]{0,1} add(f32[512,1024]{0,1} %sqrt.16, f32[512,1024]{1,0} %broadcast.1198), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.834 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %broadcast.917, f32[512,1024]{0,1} %add.536)
  %divide.283 = f32[512,1024]{0,1} divide(f32[512,1024]{0,1} %add.534, f32[512,1024]{1,0} %multiply.834), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %broadcast.1199 = f32[512,1024]{1,0} broadcast(f32[] %constant.2220), dimensions={}
  %multiply.836 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param.20, f32[512,1024]{1,0} %broadcast.1199), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.537 = f32[512,1024]{0,1} add(f32[512,1024]{0,1} %divide.283, f32[512,1024]{1,0} %multiply.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %broadcast.1200 = f32[512,1024]{1,0} broadcast(f32[] %constant.859), dimensions={}
  %multiply.838 = f32[512,1024]{0,1} multiply(f32[512,1024]{0,1} %add.537, f32[512,1024]{1,0} %broadcast.1200), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.539 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %param.20, f32[512,1024]{0,1} %multiply.838), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reduce.121 = f32[1024]{0} reduce(f32[4,1024,1024]{2,1,0} %convert.252, f32[] %constant.69), dimensions={0,1}, to_apply=%region_137.3931, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %all-reduce.61 = f32[1024]{0} all-reduce(f32[1024]{0} %reduce.121), channel_id=62, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_137.3931, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %multiply.840 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.61, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.119 = f32[1024]{0} parameter(96), sharding={replicated}
  %multiply.842 = f32[1024]{0} multiply(f32[1024]{0} %param.119, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.540 = f32[1024]{0} add(f32[1024]{0} %multiply.840, f32[1024]{0} %multiply.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.844 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.61, f32[1024]{0} %all-reduce.61), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.845 = f32[1024]{0} multiply(f32[1024]{0} %multiply.844, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.120 = f32[1024]{0} parameter(173), sharding={replicated}
  %multiply.846 = f32[1024]{0} multiply(f32[1024]{0} %param.120, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.541 = f32[1024]{0} add(f32[1024]{0} %multiply.845, f32[1024]{0} %multiply.846), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.284 = f32[1024]{0} divide(f32[1024]{0} %add.541, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.17 = f32[1024]{0} sqrt(f32[1024]{0} %divide.284), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.542 = f32[1024]{0} add(f32[1024]{0} %sqrt.17, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.848 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.542)
  %divide.285 = f32[1024]{0} divide(f32[1024]{0} %add.540, f32[1024]{0} %multiply.848), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.849 = f32[1024]{0} multiply(f32[1024]{0} %divide.285, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.543 = f32[1024]{0} add(f32[1024]{0} %param.29, f32[1024]{0} %multiply.849), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %multiply.850 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.768, f32[4,1024,1024]{2,1,0} %multiply.609), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.122 = f32[1024]{0} reduce(f32[4,1024,1024]{2,1,0} %multiply.850, f32[] %constant.69), dimensions={0,1}, to_apply=%region_138.3941, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %all-reduce.62 = f32[1024]{0} all-reduce(f32[1024]{0} %reduce.122), channel_id=63, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_138.3941, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.851 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.62, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.121 = f32[1024]{0} parameter(97), sharding={replicated}
  %multiply.852 = f32[1024]{0} multiply(f32[1024]{0} %param.121, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.544 = f32[1024]{0} add(f32[1024]{0} %multiply.851, f32[1024]{0} %multiply.852), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.853 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.62, f32[1024]{0} %all-reduce.62), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.854 = f32[1024]{0} multiply(f32[1024]{0} %multiply.853, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.122 = f32[1024]{0} parameter(174), sharding={replicated}
  %multiply.855 = f32[1024]{0} multiply(f32[1024]{0} %param.122, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.545 = f32[1024]{0} add(f32[1024]{0} %multiply.854, f32[1024]{0} %multiply.855), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.286 = f32[1024]{0} divide(f32[1024]{0} %add.545, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.18 = f32[1024]{0} sqrt(f32[1024]{0} %divide.286), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.546 = f32[1024]{0} add(f32[1024]{0} %sqrt.18, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.856 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.546)
  %divide.287 = f32[1024]{0} divide(f32[1024]{0} %add.544, f32[1024]{0} %multiply.856), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.857 = f32[1024]{0} multiply(f32[1024]{0} %divide.287, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.547 = f32[1024]{0} add(f32[1024]{0} %param.28, f32[1024]{0} %multiply.857), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reduce.123 = f16[1024]{0} reduce(f16[4,1024,1024]{2,1,0} %add.419, f16[] %constant.89), dimensions={0,1}, to_apply=%region_141.3980, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %all-reduce.63 = f16[1024]{0} all-reduce(f16[1024]{0} %reduce.123), channel_id=64, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_141.3980, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %convert.292 = f32[1024]{0} convert(f16[1024]{0} %all-reduce.63), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.858 = f32[1024]{0} multiply(f32[1024]{0} %convert.292, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.123 = f32[1024]{0} parameter(98), sharding={replicated}
  %multiply.859 = f32[1024]{0} multiply(f32[1024]{0} %param.123, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.548 = f32[1024]{0} add(f32[1024]{0} %multiply.858, f32[1024]{0} %multiply.859), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.860 = f32[1024]{0} multiply(f32[1024]{0} %convert.292, f32[1024]{0} %convert.292), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.861 = f32[1024]{0} multiply(f32[1024]{0} %multiply.860, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.124 = f32[1024]{0} parameter(175), sharding={replicated}
  %multiply.862 = f32[1024]{0} multiply(f32[1024]{0} %param.124, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.549 = f32[1024]{0} add(f32[1024]{0} %multiply.861, f32[1024]{0} %multiply.862), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.288 = f32[1024]{0} divide(f32[1024]{0} %add.549, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.19 = f32[1024]{0} sqrt(f32[1024]{0} %divide.288), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.550 = f32[1024]{0} add(f32[1024]{0} %sqrt.19, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.863 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.550)
  %divide.289 = f32[1024]{0} divide(f32[1024]{0} %add.548, f32[1024]{0} %multiply.863), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.864 = f32[1024]{0} multiply(f32[1024]{0} %divide.289, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.551 = f32[1024]{0} add(f32[1024]{0} %param.27, f32[1024]{0} %multiply.864), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %transpose.394 = f16[1024,4,1024]{0,2,1} transpose(f16[4,1024,1024]{2,1,0} %add.419), dimensions={2,0,1}
  %reshape.2819 = f16[1024,4096]{1,0} reshape(f16[1024,4,1024]{0,2,1} %transpose.394)
  %dot.345 = f16[128,1024]{1,0} dot(f16[4096,128]{1,0} %reshape.2438, f16[1024,4096]{1,0} %reshape.2819), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.64 = f16[128,1024]{1,0} all-reduce(f16[128,1024]{1,0} %dot.345), channel_id=65, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%add.46, sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.293 = f32[128,1024]{0,1} convert(f16[128,1024]{1,0} %all-reduce.64), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.865 = f32[128,1024]{0,1} multiply(f32[128,1024]{0,1} %convert.293, f32[128,1024]{1,0} %broadcast.1141), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.125 = f32[128,1024]{1,0} parameter(99), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.866 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param.125, f32[128,1024]{1,0} %broadcast.1142), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.552 = f32[128,1024]{0,1} add(f32[128,1024]{0,1} %multiply.865, f32[128,1024]{1,0} %multiply.866), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.867 = f32[128,1024]{0,1} multiply(f32[128,1024]{0,1} %convert.293, f32[128,1024]{0,1} %convert.293), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.868 = f32[128,1024]{0,1} multiply(f32[128,1024]{0,1} %multiply.867, f32[128,1024]{1,0} %broadcast.1143), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.126 = f32[128,1024]{1,0} parameter(176), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.869 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param.126, f32[128,1024]{1,0} %broadcast.1144), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.553 = f32[128,1024]{0,1} add(f32[128,1024]{0,1} %multiply.868, f32[128,1024]{1,0} %multiply.869), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.290 = f32[128,1024]{0,1} divide(f32[128,1024]{0,1} %add.553, f32[128,1024]{1,0} %broadcast.856), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.20 = f32[128,1024]{0,1} sqrt(f32[128,1024]{0,1} %divide.290), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.555 = f32[128,1024]{0,1} add(f32[128,1024]{0,1} %sqrt.20, f32[128,1024]{1,0} %broadcast.1146), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.870 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %broadcast.851, f32[128,1024]{0,1} %add.555)
  %divide.291 = f32[128,1024]{0,1} divide(f32[128,1024]{0,1} %add.552, f32[128,1024]{1,0} %multiply.870), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.872 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param.26, f32[128,1024]{1,0} %broadcast.1148), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.556 = f32[128,1024]{0,1} add(f32[128,1024]{0,1} %divide.291, f32[128,1024]{1,0} %multiply.872), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.873 = f32[128,1024]{0,1} multiply(f32[128,1024]{0,1} %add.556, f32[128,1024]{1,0} %broadcast.1149), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.557 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %param.26, f32[128,1024]{0,1} %multiply.873), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reshape.2822 = f16[4,1024,384]{2,1,0} reshape(f16[4,1024,128,3]{3,2,1,0} %add.422), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/reshape[new_sizes=(8, 1024, 3072) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=161}
  %reduce.124 = f16[384]{0} reduce(f16[4,1024,384]{2,1,0} %reshape.2822, f16[] %constant.89), dimensions={0,1}, to_apply=%region_144.4036, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %all-reduce.65 = f16[384]{0} all-reduce(f16[384]{0} %reduce.124), channel_id=66, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_144.4036, sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %convert.294 = f32[384]{0} convert(f16[384]{0} %all-reduce.65), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.874 = f32[384]{0} multiply(f32[384]{0} %convert.294, f32[384]{0} %broadcast.1158), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.127 = f32[384]{0} parameter(100), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.875 = f32[384]{0} multiply(f32[384]{0} %param.127, f32[384]{0} %broadcast.1160), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.558 = f32[384]{0} add(f32[384]{0} %multiply.874, f32[384]{0} %multiply.875), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.876 = f32[384]{0} multiply(f32[384]{0} %convert.294, f32[384]{0} %convert.294), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.877 = f32[384]{0} multiply(f32[384]{0} %multiply.876, f32[384]{0} %broadcast.1162), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.128 = f32[384]{0} parameter(177), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.879 = f32[384]{0} multiply(f32[384]{0} %param.128, f32[384]{0} %broadcast.1163), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.559 = f32[384]{0} add(f32[384]{0} %multiply.877, f32[384]{0} %multiply.879), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.292 = f32[384]{0} divide(f32[384]{0} %add.559, f32[384]{0} %broadcast.878), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.21 = f32[384]{0} sqrt(f32[384]{0} %divide.292), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.560 = f32[384]{0} add(f32[384]{0} %sqrt.21, f32[384]{0} %broadcast.1164), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.880 = f32[384]{0} multiply(f32[384]{0} %broadcast.875, f32[384]{0} %add.560)
  %divide.293 = f32[384]{0} divide(f32[384]{0} %add.558, f32[384]{0} %multiply.880), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.881 = f32[384]{0} multiply(f32[384]{0} %divide.293, f32[384]{0} %broadcast.1165), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.561 = f32[384]{0} add(f32[384]{0} %param.25, f32[384]{0} %multiply.881), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %transpose.395 = f16[384,4,1024]{0,2,1} transpose(f16[4,1024,384]{2,1,0} %reshape.2822), dimensions={2,0,1}
  %reshape.2823 = f16[384,4096]{1,0} reshape(f16[384,4,1024]{0,2,1} %transpose.395)
  %dot.346 = f16[1024,384]{1,0} dot(f16[4096,1024]{1,0} %reshape.2402, f16[384,4096]{1,0} %reshape.2823), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.66 = f16[1024,384]{1,0} all-reduce(f16[1024,384]{1,0} %dot.346), channel_id=67, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%add.47, sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.295 = f32[1024,384]{0,1} convert(f16[1024,384]{1,0} %all-reduce.66), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/attention/self/qvk_combined/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.882 = f32[1024,384]{0,1} multiply(f32[1024,384]{0,1} %convert.295, f32[1024,384]{1,0} %broadcast.1166), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.129 = f32[1024,384]{1,0} parameter(101), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.883 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param.129, f32[1024,384]{1,0} %broadcast.1167), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.562 = f32[1024,384]{0,1} add(f32[1024,384]{0,1} %multiply.882, f32[1024,384]{1,0} %multiply.883), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.884 = f32[1024,384]{0,1} multiply(f32[1024,384]{0,1} %convert.295, f32[1024,384]{0,1} %convert.295), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.885 = f32[1024,384]{0,1} multiply(f32[1024,384]{0,1} %multiply.884, f32[1024,384]{1,0} %broadcast.1168), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.130 = f32[1024,384]{1,0} parameter(178), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.886 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param.130, f32[1024,384]{1,0} %broadcast.1169), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.563 = f32[1024,384]{0,1} add(f32[1024,384]{0,1} %multiply.885, f32[1024,384]{1,0} %multiply.886), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.294 = f32[1024,384]{0,1} divide(f32[1024,384]{0,1} %add.563, f32[1024,384]{1,0} %broadcast.889), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.22 = f32[1024,384]{0,1} sqrt(f32[1024,384]{0,1} %divide.294), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.564 = f32[1024,384]{0,1} add(f32[1024,384]{0,1} %sqrt.22, f32[1024,384]{1,0} %broadcast.1170), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.887 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %broadcast.883, f32[1024,384]{0,1} %add.564)
  %divide.295 = f32[1024,384]{0,1} divide(f32[1024,384]{0,1} %add.562, f32[1024,384]{1,0} %multiply.887), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.888 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param.24, f32[1024,384]{1,0} %broadcast.1171), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.565 = f32[1024,384]{0,1} add(f32[1024,384]{0,1} %divide.295, f32[1024,384]{1,0} %multiply.888), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.889 = f32[1024,384]{0,1} multiply(f32[1024,384]{0,1} %add.565, f32[1024,384]{1,0} %broadcast.1172), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.566 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %param.24, f32[1024,384]{0,1} %multiply.889), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reduce.125 = f16[512]{0} reduce(f16[4,1024,512]{2,1,0} %add.415, f16[] %constant.89), dimensions={0,1}, to_apply=%region_136.3919, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %all-reduce.67 = f16[512]{0} all-reduce(f16[512]{0} %reduce.125), channel_id=68, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_136.3919, sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %convert.296 = f32[512]{0} convert(f16[512]{0} %all-reduce.67), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.890 = f32[512]{0} multiply(f32[512]{0} %convert.296, f32[512]{0} %broadcast.1173), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.131 = f32[512]{0} parameter(102), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.893 = f32[512]{0} multiply(f32[512]{0} %param.131, f32[512]{0} %broadcast.1174), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.567 = f32[512]{0} add(f32[512]{0} %multiply.890, f32[512]{0} %multiply.893), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.894 = f32[512]{0} multiply(f32[512]{0} %convert.296, f32[512]{0} %convert.296), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.895 = f32[512]{0} multiply(f32[512]{0} %multiply.894, f32[512]{0} %broadcast.1175), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.132 = f32[512]{0} parameter(179), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.896 = f32[512]{0} multiply(f32[512]{0} %param.132, f32[512]{0} %broadcast.1176), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.568 = f32[512]{0} add(f32[512]{0} %multiply.895, f32[512]{0} %multiply.896), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.296 = f32[512]{0} divide(f32[512]{0} %add.568, f32[512]{0} %broadcast.901), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.23 = f32[512]{0} sqrt(f32[512]{0} %divide.296), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.569 = f32[512]{0} add(f32[512]{0} %sqrt.23, f32[512]{0} %broadcast.1178), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.897 = f32[512]{0} multiply(f32[512]{0} %broadcast.898, f32[512]{0} %add.569)
  %divide.297 = f32[512]{0} divide(f32[512]{0} %add.567, f32[512]{0} %multiply.897), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.898 = f32[512]{0} multiply(f32[512]{0} %divide.297, f32[512]{0} %broadcast.1181), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.570 = f32[512]{0} add(f32[512]{0} %param.31, f32[512]{0} %multiply.898), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %transpose.396 = f16[512,4,1024]{0,2,1} transpose(f16[4,1024,512]{2,1,0} %add.415), dimensions={2,0,1}
  %reshape.2825 = f16[512,4096]{1,0} reshape(f16[512,4,1024]{0,2,1} %transpose.396)
  %dot.347 = f16[1024,512]{1,0} dot(f16[4096,1024]{1,0} %reshape.2452, f16[512,4096]{1,0} %reshape.2825), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.68 = f16[1024,512]{1,0} all-reduce(f16[1024,512]{1,0} %dot.347), channel_id=69, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%add.48, sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.297 = f32[1024,512]{0,1} convert(f16[1024,512]{1,0} %all-reduce.68), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/intermediate/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.899 = f32[1024,512]{0,1} multiply(f32[1024,512]{0,1} %convert.297, f32[1024,512]{1,0} %broadcast.1183), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.133 = f32[1024,512]{1,0} parameter(103), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.900 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param.133, f32[1024,512]{1,0} %broadcast.1184), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.571 = f32[1024,512]{0,1} add(f32[1024,512]{0,1} %multiply.899, f32[1024,512]{1,0} %multiply.900), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.901 = f32[1024,512]{0,1} multiply(f32[1024,512]{0,1} %convert.297, f32[1024,512]{0,1} %convert.297), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.902 = f32[1024,512]{0,1} multiply(f32[1024,512]{0,1} %multiply.901, f32[1024,512]{1,0} %broadcast.1185), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.134 = f32[1024,512]{1,0} parameter(180), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.903 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param.134, f32[1024,512]{1,0} %broadcast.1187), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.574 = f32[1024,512]{0,1} add(f32[1024,512]{0,1} %multiply.902, f32[1024,512]{1,0} %multiply.903), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.298 = f32[1024,512]{0,1} divide(f32[1024,512]{0,1} %add.574, f32[1024,512]{1,0} %broadcast.911), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.24 = f32[1024,512]{0,1} sqrt(f32[1024,512]{0,1} %divide.298), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.575 = f32[1024,512]{0,1} add(f32[1024,512]{0,1} %sqrt.24, f32[1024,512]{1,0} %broadcast.1189), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.904 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %broadcast.908, f32[1024,512]{0,1} %add.575)
  %divide.299 = f32[1024,512]{0,1} divide(f32[1024,512]{0,1} %add.571, f32[1024,512]{1,0} %multiply.904), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.905 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param.30, f32[1024,512]{1,0} %broadcast.1190), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.576 = f32[1024,512]{0,1} add(f32[1024,512]{0,1} %divide.299, f32[1024,512]{1,0} %multiply.905), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.906 = f32[1024,512]{0,1} multiply(f32[1024,512]{0,1} %add.576, f32[1024,512]{1,0} %broadcast.1191), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.577 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %param.30, f32[1024,512]{0,1} %multiply.906), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reduce.126 = f32[1024]{0} reduce(f32[4,1024,1024]{2,1,0} %convert.249, f32[] %constant.69), dimensions={0,1}, to_apply=%region_131.3851, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %all-reduce.69 = f32[1024]{0} all-reduce(f32[1024]{0} %reduce.126), channel_id=70, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_131.3851, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %multiply.907 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.69, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.135 = f32[1024]{0} parameter(104), sharding={replicated}
  %multiply.908 = f32[1024]{0} multiply(f32[1024]{0} %param.135, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.578 = f32[1024]{0} add(f32[1024]{0} %multiply.907, f32[1024]{0} %multiply.908), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.909 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.69, f32[1024]{0} %all-reduce.69), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.910 = f32[1024]{0} multiply(f32[1024]{0} %multiply.909, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.136 = f32[1024]{0} parameter(181), sharding={replicated}
  %multiply.911 = f32[1024]{0} multiply(f32[1024]{0} %param.136, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.580 = f32[1024]{0} add(f32[1024]{0} %multiply.910, f32[1024]{0} %multiply.911), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.300 = f32[1024]{0} divide(f32[1024]{0} %add.580, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.25 = f32[1024]{0} sqrt(f32[1024]{0} %divide.300), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.581 = f32[1024]{0} add(f32[1024]{0} %sqrt.25, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.912 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.581)
  %divide.301 = f32[1024]{0} divide(f32[1024]{0} %add.578, f32[1024]{0} %multiply.912), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.913 = f32[1024]{0} multiply(f32[1024]{0} %divide.301, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.582 = f32[1024]{0} add(f32[1024]{0} %param.35, f32[1024]{0} %multiply.913), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %multiply.914 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.785, f32[4,1024,1024]{2,1,0} %multiply.588), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.127 = f32[1024]{0} reduce(f32[4,1024,1024]{2,1,0} %multiply.914, f32[] %constant.69), dimensions={0,1}, to_apply=%region_132.3861, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %all-reduce.70 = f32[1024]{0} all-reduce(f32[1024]{0} %reduce.127), channel_id=71, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_132.3861, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.915 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.70, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.137 = f32[1024]{0} parameter(105), sharding={replicated}
  %multiply.916 = f32[1024]{0} multiply(f32[1024]{0} %param.137, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.583 = f32[1024]{0} add(f32[1024]{0} %multiply.915, f32[1024]{0} %multiply.916), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.917 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.70, f32[1024]{0} %all-reduce.70), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.918 = f32[1024]{0} multiply(f32[1024]{0} %multiply.917, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.138 = f32[1024]{0} parameter(182), sharding={replicated}
  %multiply.919 = f32[1024]{0} multiply(f32[1024]{0} %param.138, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.584 = f32[1024]{0} add(f32[1024]{0} %multiply.918, f32[1024]{0} %multiply.919), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.302 = f32[1024]{0} divide(f32[1024]{0} %add.584, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.26 = f32[1024]{0} sqrt(f32[1024]{0} %divide.302), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.585 = f32[1024]{0} add(f32[1024]{0} %sqrt.26, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.920 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.585)
  %divide.303 = f32[1024]{0} divide(f32[1024]{0} %add.583, f32[1024]{0} %multiply.920), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.921 = f32[1024]{0} multiply(f32[1024]{0} %divide.303, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.586 = f32[1024]{0} add(f32[1024]{0} %param.34, f32[1024]{0} %multiply.921), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reduce.128 = f16[1024]{0} reduce(f16[4,1024,1024]{2,1,0} %add.414, f16[] %constant.89), dimensions={0,1}, to_apply=%region_135.3900, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %all-reduce.71 = f16[1024]{0} all-reduce(f16[1024]{0} %reduce.128), channel_id=72, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_135.3900, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %convert.298 = f32[1024]{0} convert(f16[1024]{0} %all-reduce.71), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.922 = f32[1024]{0} multiply(f32[1024]{0} %convert.298, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.139 = f32[1024]{0} parameter(106), sharding={replicated}
  %multiply.923 = f32[1024]{0} multiply(f32[1024]{0} %param.139, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.587 = f32[1024]{0} add(f32[1024]{0} %multiply.922, f32[1024]{0} %multiply.923), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.924 = f32[1024]{0} multiply(f32[1024]{0} %convert.298, f32[1024]{0} %convert.298), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.925 = f32[1024]{0} multiply(f32[1024]{0} %multiply.924, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.140 = f32[1024]{0} parameter(183), sharding={replicated}
  %multiply.926 = f32[1024]{0} multiply(f32[1024]{0} %param.140, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.588 = f32[1024]{0} add(f32[1024]{0} %multiply.925, f32[1024]{0} %multiply.926), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.304 = f32[1024]{0} divide(f32[1024]{0} %add.588, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.27 = f32[1024]{0} sqrt(f32[1024]{0} %divide.304), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.589 = f32[1024]{0} add(f32[1024]{0} %sqrt.27, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.927 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.589)
  %divide.305 = f32[1024]{0} divide(f32[1024]{0} %add.587, f32[1024]{0} %multiply.927), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.928 = f32[1024]{0} multiply(f32[1024]{0} %divide.305, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.590 = f32[1024]{0} add(f32[1024]{0} %param.33, f32[1024]{0} %multiply.928), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %transpose.397 = f16[1024,4,1024]{0,2,1} transpose(f16[4,1024,1024]{2,1,0} %add.414), dimensions={2,0,1}
  %reshape.2827 = f16[1024,4096]{1,0} reshape(f16[1024,4,1024]{0,2,1} %transpose.397)
  %dot.348 = f16[512,1024]{1,0} dot(f16[4096,512]{1,0} %reshape.2471, f16[1024,4096]{1,0} %reshape.2827), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.72 = f16[512,1024]{1,0} all-reduce(f16[512,1024]{1,0} %dot.348), channel_id=73, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%add.49, sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.299 = f32[512,1024]{0,1} convert(f16[512,1024]{1,0} %all-reduce.72), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/1/remat(core_fn)/1/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.929 = f32[512,1024]{0,1} multiply(f32[512,1024]{0,1} %convert.299, f32[512,1024]{1,0} %broadcast.1192), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.141 = f32[512,1024]{1,0} parameter(107), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.930 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param.141, f32[512,1024]{1,0} %broadcast.1193), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.592 = f32[512,1024]{0,1} add(f32[512,1024]{0,1} %multiply.929, f32[512,1024]{1,0} %multiply.930), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.931 = f32[512,1024]{0,1} multiply(f32[512,1024]{0,1} %convert.299, f32[512,1024]{0,1} %convert.299), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.932 = f32[512,1024]{0,1} multiply(f32[512,1024]{0,1} %multiply.931, f32[512,1024]{1,0} %broadcast.1194), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.142 = f32[512,1024]{1,0} parameter(184), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.933 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param.142, f32[512,1024]{1,0} %broadcast.1196), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.593 = f32[512,1024]{0,1} add(f32[512,1024]{0,1} %multiply.932, f32[512,1024]{1,0} %multiply.933), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.306 = f32[512,1024]{0,1} divide(f32[512,1024]{0,1} %add.593, f32[512,1024]{1,0} %broadcast.922), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.28 = f32[512,1024]{0,1} sqrt(f32[512,1024]{0,1} %divide.306), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.594 = f32[512,1024]{0,1} add(f32[512,1024]{0,1} %sqrt.28, f32[512,1024]{1,0} %broadcast.1198), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.934 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %broadcast.917, f32[512,1024]{0,1} %add.594)
  %divide.307 = f32[512,1024]{0,1} divide(f32[512,1024]{0,1} %add.592, f32[512,1024]{1,0} %multiply.934), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.935 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param.32, f32[512,1024]{1,0} %broadcast.1199), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.595 = f32[512,1024]{0,1} add(f32[512,1024]{0,1} %divide.307, f32[512,1024]{1,0} %multiply.935), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.936 = f32[512,1024]{0,1} multiply(f32[512,1024]{0,1} %add.595, f32[512,1024]{1,0} %broadcast.1200), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.596 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %param.32, f32[512,1024]{0,1} %multiply.936), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reduce.129 = f32[1024]{0} reduce(f32[4,1024,1024]{2,1,0} %convert.231, f32[] %constant.69), dimensions={0,1}, to_apply=%region_117.3472, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %all-reduce.73 = f32[1024]{0} all-reduce(f32[1024]{0} %reduce.129), channel_id=74, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_117.3472, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %multiply.937 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.73, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.143 = f32[1024]{0} parameter(108), sharding={replicated}
  %multiply.938 = f32[1024]{0} multiply(f32[1024]{0} %param.143, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.597 = f32[1024]{0} add(f32[1024]{0} %multiply.937, f32[1024]{0} %multiply.938), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.939 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.73, f32[1024]{0} %all-reduce.73), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.940 = f32[1024]{0} multiply(f32[1024]{0} %multiply.939, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.144 = f32[1024]{0} parameter(185), sharding={replicated}
  %multiply.941 = f32[1024]{0} multiply(f32[1024]{0} %param.144, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.599 = f32[1024]{0} add(f32[1024]{0} %multiply.940, f32[1024]{0} %multiply.941), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.308 = f32[1024]{0} divide(f32[1024]{0} %add.599, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.29 = f32[1024]{0} sqrt(f32[1024]{0} %divide.308), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.600 = f32[1024]{0} add(f32[1024]{0} %sqrt.29, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.942 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.600)
  %divide.309 = f32[1024]{0} divide(f32[1024]{0} %add.597, f32[1024]{0} %multiply.942), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.943 = f32[1024]{0} multiply(f32[1024]{0} %divide.309, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.601 = f32[1024]{0} add(f32[1024]{0} %param.41, f32[1024]{0} %multiply.943), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %multiply.944 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.725, f32[4,1024,1024]{2,1,0} %multiply.555), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.130 = f32[1024]{0} reduce(f32[4,1024,1024]{2,1,0} %multiply.944, f32[] %constant.69), dimensions={0,1}, to_apply=%region_118.3482, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %all-reduce.74 = f32[1024]{0} all-reduce(f32[1024]{0} %reduce.130), channel_id=75, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_118.3482, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.945 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.74, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.145 = f32[1024]{0} parameter(109), sharding={replicated}
  %multiply.946 = f32[1024]{0} multiply(f32[1024]{0} %param.145, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.602 = f32[1024]{0} add(f32[1024]{0} %multiply.945, f32[1024]{0} %multiply.946), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.947 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.74, f32[1024]{0} %all-reduce.74), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.948 = f32[1024]{0} multiply(f32[1024]{0} %multiply.947, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.146 = f32[1024]{0} parameter(186), sharding={replicated}
  %multiply.949 = f32[1024]{0} multiply(f32[1024]{0} %param.146, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.603 = f32[1024]{0} add(f32[1024]{0} %multiply.948, f32[1024]{0} %multiply.949), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.310 = f32[1024]{0} divide(f32[1024]{0} %add.603, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.30 = f32[1024]{0} sqrt(f32[1024]{0} %divide.310), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.604 = f32[1024]{0} add(f32[1024]{0} %sqrt.30, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.950 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.604)
  %divide.311 = f32[1024]{0} divide(f32[1024]{0} %add.602, f32[1024]{0} %multiply.950), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.951 = f32[1024]{0} multiply(f32[1024]{0} %divide.311, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.605 = f32[1024]{0} add(f32[1024]{0} %param.40, f32[1024]{0} %multiply.951), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reduce.131 = f16[1024]{0} reduce(f16[4,1024,1024]{2,1,0} %add.384, f16[] %constant.89), dimensions={0,1}, to_apply=%region_121.3521, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %all-reduce.75 = f16[1024]{0} all-reduce(f16[1024]{0} %reduce.131), channel_id=76, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_121.3521, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %convert.300 = f32[1024]{0} convert(f16[1024]{0} %all-reduce.75), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.952 = f32[1024]{0} multiply(f32[1024]{0} %convert.300, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.147 = f32[1024]{0} parameter(110), sharding={replicated}
  %multiply.953 = f32[1024]{0} multiply(f32[1024]{0} %param.147, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.606 = f32[1024]{0} add(f32[1024]{0} %multiply.952, f32[1024]{0} %multiply.953), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.954 = f32[1024]{0} multiply(f32[1024]{0} %convert.300, f32[1024]{0} %convert.300), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.955 = f32[1024]{0} multiply(f32[1024]{0} %multiply.954, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.148 = f32[1024]{0} parameter(187), sharding={replicated}
  %multiply.956 = f32[1024]{0} multiply(f32[1024]{0} %param.148, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.607 = f32[1024]{0} add(f32[1024]{0} %multiply.955, f32[1024]{0} %multiply.956), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.312 = f32[1024]{0} divide(f32[1024]{0} %add.607, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.31 = f32[1024]{0} sqrt(f32[1024]{0} %divide.312), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.608 = f32[1024]{0} add(f32[1024]{0} %sqrt.31, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.957 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.608)
  %divide.313 = f32[1024]{0} divide(f32[1024]{0} %add.606, f32[1024]{0} %multiply.957), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.958 = f32[1024]{0} multiply(f32[1024]{0} %divide.313, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.609 = f32[1024]{0} add(f32[1024]{0} %param.39, f32[1024]{0} %multiply.958), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %transpose.398 = f16[1024,4,1024]{0,2,1} transpose(f16[4,1024,1024]{2,1,0} %add.384), dimensions={2,0,1}
  %reshape.2829 = f16[1024,4096]{1,0} reshape(f16[1024,4,1024]{0,2,1} %transpose.398)
  %dot.349 = f16[128,1024]{1,0} dot(f16[4096,128]{1,0} %reshape.2302, f16[1024,4096]{1,0} %reshape.2829), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.76 = f16[128,1024]{1,0} all-reduce(f16[128,1024]{1,0} %dot.349), channel_id=77, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%add.50, sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.301 = f32[128,1024]{0,1} convert(f16[128,1024]{1,0} %all-reduce.76), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.959 = f32[128,1024]{0,1} multiply(f32[128,1024]{0,1} %convert.301, f32[128,1024]{1,0} %broadcast.1141), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.149 = f32[128,1024]{1,0} parameter(111), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.960 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param.149, f32[128,1024]{1,0} %broadcast.1142), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.610 = f32[128,1024]{0,1} add(f32[128,1024]{0,1} %multiply.959, f32[128,1024]{1,0} %multiply.960), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.961 = f32[128,1024]{0,1} multiply(f32[128,1024]{0,1} %convert.301, f32[128,1024]{0,1} %convert.301), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.962 = f32[128,1024]{0,1} multiply(f32[128,1024]{0,1} %multiply.961, f32[128,1024]{1,0} %broadcast.1143), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.150 = f32[128,1024]{1,0} parameter(188), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.963 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param.150, f32[128,1024]{1,0} %broadcast.1144), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.611 = f32[128,1024]{0,1} add(f32[128,1024]{0,1} %multiply.962, f32[128,1024]{1,0} %multiply.963), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.314 = f32[128,1024]{0,1} divide(f32[128,1024]{0,1} %add.611, f32[128,1024]{1,0} %broadcast.856), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.32 = f32[128,1024]{0,1} sqrt(f32[128,1024]{0,1} %divide.314), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.612 = f32[128,1024]{0,1} add(f32[128,1024]{0,1} %sqrt.32, f32[128,1024]{1,0} %broadcast.1146), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.964 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %broadcast.851, f32[128,1024]{0,1} %add.612)
  %divide.315 = f32[128,1024]{0,1} divide(f32[128,1024]{0,1} %add.610, f32[128,1024]{1,0} %multiply.964), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.965 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param.38, f32[128,1024]{1,0} %broadcast.1148), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.614 = f32[128,1024]{0,1} add(f32[128,1024]{0,1} %divide.315, f32[128,1024]{1,0} %multiply.965), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.966 = f32[128,1024]{0,1} multiply(f32[128,1024]{0,1} %add.614, f32[128,1024]{1,0} %broadcast.1149), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.615 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %param.38, f32[128,1024]{0,1} %multiply.966), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reshape.2831 = f16[4,1024,384]{2,1,0} reshape(f16[4,1024,128,3]{3,2,1,0} %add.387), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/reshape[new_sizes=(8, 1024, 3072) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=161}
  %reduce.132 = f16[384]{0} reduce(f16[4,1024,384]{2,1,0} %reshape.2831, f16[] %constant.89), dimensions={0,1}, to_apply=%region_124.3577, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %all-reduce.77 = f16[384]{0} all-reduce(f16[384]{0} %reduce.132), channel_id=78, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_124.3577, sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %convert.302 = f32[384]{0} convert(f16[384]{0} %all-reduce.77), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.967 = f32[384]{0} multiply(f32[384]{0} %convert.302, f32[384]{0} %broadcast.1158), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.151 = f32[384]{0} parameter(112), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.968 = f32[384]{0} multiply(f32[384]{0} %param.151, f32[384]{0} %broadcast.1160), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.616 = f32[384]{0} add(f32[384]{0} %multiply.967, f32[384]{0} %multiply.968), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.969 = f32[384]{0} multiply(f32[384]{0} %convert.302, f32[384]{0} %convert.302), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.970 = f32[384]{0} multiply(f32[384]{0} %multiply.969, f32[384]{0} %broadcast.1162), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.152 = f32[384]{0} parameter(189), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.972 = f32[384]{0} multiply(f32[384]{0} %param.152, f32[384]{0} %broadcast.1163), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.617 = f32[384]{0} add(f32[384]{0} %multiply.970, f32[384]{0} %multiply.972), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.316 = f32[384]{0} divide(f32[384]{0} %add.617, f32[384]{0} %broadcast.878), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.33 = f32[384]{0} sqrt(f32[384]{0} %divide.316), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.618 = f32[384]{0} add(f32[384]{0} %sqrt.33, f32[384]{0} %broadcast.1164), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.973 = f32[384]{0} multiply(f32[384]{0} %broadcast.875, f32[384]{0} %add.618)
  %divide.317 = f32[384]{0} divide(f32[384]{0} %add.616, f32[384]{0} %multiply.973), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.974 = f32[384]{0} multiply(f32[384]{0} %divide.317, f32[384]{0} %broadcast.1165), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.619 = f32[384]{0} add(f32[384]{0} %param.37, f32[384]{0} %multiply.974), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %transpose.399 = f16[384,4,1024]{0,2,1} transpose(f16[4,1024,384]{2,1,0} %reshape.2831), dimensions={2,0,1}
  %reshape.2832 = f16[384,4096]{1,0} reshape(f16[384,4,1024]{0,2,1} %transpose.399)
  %dot.350 = f16[1024,384]{1,0} dot(f16[4096,1024]{1,0} %reshape.2263, f16[384,4096]{1,0} %reshape.2832), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.78 = f16[1024,384]{1,0} all-reduce(f16[1024,384]{1,0} %dot.350), channel_id=79, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%add.51, sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.303 = f32[1024,384]{0,1} convert(f16[1024,384]{1,0} %all-reduce.78), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/attention/self/qvk_combined/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.975 = f32[1024,384]{0,1} multiply(f32[1024,384]{0,1} %convert.303, f32[1024,384]{1,0} %broadcast.1166), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.153 = f32[1024,384]{1,0} parameter(113), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.976 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param.153, f32[1024,384]{1,0} %broadcast.1167), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.620 = f32[1024,384]{0,1} add(f32[1024,384]{0,1} %multiply.975, f32[1024,384]{1,0} %multiply.976), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.977 = f32[1024,384]{0,1} multiply(f32[1024,384]{0,1} %convert.303, f32[1024,384]{0,1} %convert.303), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.979 = f32[1024,384]{0,1} multiply(f32[1024,384]{0,1} %multiply.977, f32[1024,384]{1,0} %broadcast.1168), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.154 = f32[1024,384]{1,0} parameter(190), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.980 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param.154, f32[1024,384]{1,0} %broadcast.1169), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.621 = f32[1024,384]{0,1} add(f32[1024,384]{0,1} %multiply.979, f32[1024,384]{1,0} %multiply.980), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.318 = f32[1024,384]{0,1} divide(f32[1024,384]{0,1} %add.621, f32[1024,384]{1,0} %broadcast.889), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.34 = f32[1024,384]{0,1} sqrt(f32[1024,384]{0,1} %divide.318), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.623 = f32[1024,384]{0,1} add(f32[1024,384]{0,1} %sqrt.34, f32[1024,384]{1,0} %broadcast.1170), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.981 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %broadcast.883, f32[1024,384]{0,1} %add.623)
  %divide.319 = f32[1024,384]{0,1} divide(f32[1024,384]{0,1} %add.620, f32[1024,384]{1,0} %multiply.981), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.982 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param.36, f32[1024,384]{1,0} %broadcast.1171), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.624 = f32[1024,384]{0,1} add(f32[1024,384]{0,1} %divide.319, f32[1024,384]{1,0} %multiply.982), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.983 = f32[1024,384]{0,1} multiply(f32[1024,384]{0,1} %add.624, f32[1024,384]{1,0} %broadcast.1172), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.625 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %param.36, f32[1024,384]{0,1} %multiply.983), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reduce.133 = f16[512]{0} reduce(f16[4,1024,512]{2,1,0} %add.380, f16[] %constant.89), dimensions={0,1}, to_apply=%region_116.3460, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %all-reduce.79 = f16[512]{0} all-reduce(f16[512]{0} %reduce.133), channel_id=80, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_116.3460, sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %convert.304 = f32[512]{0} convert(f16[512]{0} %all-reduce.79), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.984 = f32[512]{0} multiply(f32[512]{0} %convert.304, f32[512]{0} %broadcast.1173), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.155 = f32[512]{0} parameter(114), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.985 = f32[512]{0} multiply(f32[512]{0} %param.155, f32[512]{0} %broadcast.1174), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.626 = f32[512]{0} add(f32[512]{0} %multiply.984, f32[512]{0} %multiply.985), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.986 = f32[512]{0} multiply(f32[512]{0} %convert.304, f32[512]{0} %convert.304), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.987 = f32[512]{0} multiply(f32[512]{0} %multiply.986, f32[512]{0} %broadcast.1175), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.156 = f32[512]{0} parameter(191), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.988 = f32[512]{0} multiply(f32[512]{0} %param.156, f32[512]{0} %broadcast.1176), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.627 = f32[512]{0} add(f32[512]{0} %multiply.987, f32[512]{0} %multiply.988), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.320 = f32[512]{0} divide(f32[512]{0} %add.627, f32[512]{0} %broadcast.901), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.35 = f32[512]{0} sqrt(f32[512]{0} %divide.320), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.629 = f32[512]{0} add(f32[512]{0} %sqrt.35, f32[512]{0} %broadcast.1178), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.989 = f32[512]{0} multiply(f32[512]{0} %broadcast.898, f32[512]{0} %add.629)
  %divide.321 = f32[512]{0} divide(f32[512]{0} %add.626, f32[512]{0} %multiply.989), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.990 = f32[512]{0} multiply(f32[512]{0} %divide.321, f32[512]{0} %broadcast.1181), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.631 = f32[512]{0} add(f32[512]{0} %param.43, f32[512]{0} %multiply.990), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %transpose.400 = f16[512,4,1024]{0,2,1} transpose(f16[4,1024,512]{2,1,0} %add.380), dimensions={2,0,1}
  %reshape.2834 = f16[512,4096]{1,0} reshape(f16[512,4,1024]{0,2,1} %transpose.400)
  %dot.351 = f16[1024,512]{1,0} dot(f16[4096,1024]{1,0} %reshape.2315, f16[512,4096]{1,0} %reshape.2834), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.80 = f16[1024,512]{1,0} all-reduce(f16[1024,512]{1,0} %dot.351), channel_id=81, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%add.52, sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.305 = f32[1024,512]{0,1} convert(f16[1024,512]{1,0} %all-reduce.80), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/intermediate/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.993 = f32[1024,512]{0,1} multiply(f32[1024,512]{0,1} %convert.305, f32[1024,512]{1,0} %broadcast.1183), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.157 = f32[1024,512]{1,0} parameter(115), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.994 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param.157, f32[1024,512]{1,0} %broadcast.1184), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.633 = f32[1024,512]{0,1} add(f32[1024,512]{0,1} %multiply.993, f32[1024,512]{1,0} %multiply.994), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.995 = f32[1024,512]{0,1} multiply(f32[1024,512]{0,1} %convert.305, f32[1024,512]{0,1} %convert.305), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.996 = f32[1024,512]{0,1} multiply(f32[1024,512]{0,1} %multiply.995, f32[1024,512]{1,0} %broadcast.1185), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.158 = f32[1024,512]{1,0} parameter(192), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.997 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param.158, f32[1024,512]{1,0} %broadcast.1187), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.635 = f32[1024,512]{0,1} add(f32[1024,512]{0,1} %multiply.996, f32[1024,512]{1,0} %multiply.997), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.322 = f32[1024,512]{0,1} divide(f32[1024,512]{0,1} %add.635, f32[1024,512]{1,0} %broadcast.911), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.36 = f32[1024,512]{0,1} sqrt(f32[1024,512]{0,1} %divide.322), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.637 = f32[1024,512]{0,1} add(f32[1024,512]{0,1} %sqrt.36, f32[1024,512]{1,0} %broadcast.1189), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.998 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %broadcast.908, f32[1024,512]{0,1} %add.637)
  %divide.323 = f32[1024,512]{0,1} divide(f32[1024,512]{0,1} %add.633, f32[1024,512]{1,0} %multiply.998), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.999 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param.42, f32[1024,512]{1,0} %broadcast.1190), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.639 = f32[1024,512]{0,1} add(f32[1024,512]{0,1} %divide.323, f32[1024,512]{1,0} %multiply.999), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.1000 = f32[1024,512]{0,1} multiply(f32[1024,512]{0,1} %add.639, f32[1024,512]{1,0} %broadcast.1191), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.641 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %param.42, f32[1024,512]{0,1} %multiply.1000), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reduce.134 = f32[1024]{0} reduce(f32[4,1024,1024]{2,1,0} %convert.228, f32[] %constant.69), dimensions={0,1}, to_apply=%region_111.3392, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %all-reduce.81 = f32[1024]{0} all-reduce(f32[1024]{0} %reduce.134), channel_id=82, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_111.3392, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %multiply.1001 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.81, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.159 = f32[1024]{0} parameter(116), sharding={replicated}
  %multiply.1002 = f32[1024]{0} multiply(f32[1024]{0} %param.159, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.643 = f32[1024]{0} add(f32[1024]{0} %multiply.1001, f32[1024]{0} %multiply.1002), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1003 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.81, f32[1024]{0} %all-reduce.81), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1004 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1003, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.160 = f32[1024]{0} parameter(193), sharding={replicated}
  %multiply.1005 = f32[1024]{0} multiply(f32[1024]{0} %param.160, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.645 = f32[1024]{0} add(f32[1024]{0} %multiply.1004, f32[1024]{0} %multiply.1005), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.324 = f32[1024]{0} divide(f32[1024]{0} %add.645, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.37 = f32[1024]{0} sqrt(f32[1024]{0} %divide.324), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.647 = f32[1024]{0} add(f32[1024]{0} %sqrt.37, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1006 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.647)
  %divide.325 = f32[1024]{0} divide(f32[1024]{0} %add.643, f32[1024]{0} %multiply.1006), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1007 = f32[1024]{0} multiply(f32[1024]{0} %divide.325, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.649 = f32[1024]{0} add(f32[1024]{0} %param.47, f32[1024]{0} %multiply.1007), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %multiply.1008 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.744, f32[4,1024,1024]{2,1,0} %multiply.537), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.135 = f32[1024]{0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1008, f32[] %constant.69), dimensions={0,1}, to_apply=%region_112.3402, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %all-reduce.82 = f32[1024]{0} all-reduce(f32[1024]{0} %reduce.135), channel_id=83, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_112.3402, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.1009 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.82, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.161 = f32[1024]{0} parameter(117), sharding={replicated}
  %multiply.1012 = f32[1024]{0} multiply(f32[1024]{0} %param.161, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.651 = f32[1024]{0} add(f32[1024]{0} %multiply.1009, f32[1024]{0} %multiply.1012), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1014 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.82, f32[1024]{0} %all-reduce.82), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1016 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1014, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.162 = f32[1024]{0} parameter(194), sharding={replicated}
  %multiply.1018 = f32[1024]{0} multiply(f32[1024]{0} %param.162, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.652 = f32[1024]{0} add(f32[1024]{0} %multiply.1016, f32[1024]{0} %multiply.1018), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.326 = f32[1024]{0} divide(f32[1024]{0} %add.652, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.38 = f32[1024]{0} sqrt(f32[1024]{0} %divide.326), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.653 = f32[1024]{0} add(f32[1024]{0} %sqrt.38, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1020 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.653)
  %divide.327 = f32[1024]{0} divide(f32[1024]{0} %add.651, f32[1024]{0} %multiply.1020), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1022 = f32[1024]{0} multiply(f32[1024]{0} %divide.327, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.655 = f32[1024]{0} add(f32[1024]{0} %param.46, f32[1024]{0} %multiply.1022), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reduce.136 = f16[1024]{0} reduce(f16[4,1024,1024]{2,1,0} %add.379, f16[] %constant.89), dimensions={0,1}, to_apply=%region_115.3441, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %all-reduce.83 = f16[1024]{0} all-reduce(f16[1024]{0} %reduce.136), channel_id=84, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_115.3441, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %convert.306 = f32[1024]{0} convert(f16[1024]{0} %all-reduce.83), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1024 = f32[1024]{0} multiply(f32[1024]{0} %convert.306, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.163 = f32[1024]{0} parameter(118), sharding={replicated}
  %multiply.1026 = f32[1024]{0} multiply(f32[1024]{0} %param.163, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.656 = f32[1024]{0} add(f32[1024]{0} %multiply.1024, f32[1024]{0} %multiply.1026), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1028 = f32[1024]{0} multiply(f32[1024]{0} %convert.306, f32[1024]{0} %convert.306), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1030 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1028, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.164 = f32[1024]{0} parameter(195), sharding={replicated}
  %multiply.1032 = f32[1024]{0} multiply(f32[1024]{0} %param.164, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.657 = f32[1024]{0} add(f32[1024]{0} %multiply.1030, f32[1024]{0} %multiply.1032), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.328 = f32[1024]{0} divide(f32[1024]{0} %add.657, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.39 = f32[1024]{0} sqrt(f32[1024]{0} %divide.328), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.658 = f32[1024]{0} add(f32[1024]{0} %sqrt.39, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1034 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.658)
  %divide.329 = f32[1024]{0} divide(f32[1024]{0} %add.656, f32[1024]{0} %multiply.1034), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1036 = f32[1024]{0} multiply(f32[1024]{0} %divide.329, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.659 = f32[1024]{0} add(f32[1024]{0} %param.45, f32[1024]{0} %multiply.1036), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %transpose.401 = f16[1024,4,1024]{0,2,1} transpose(f16[4,1024,1024]{2,1,0} %add.379), dimensions={2,0,1}
  %reshape.2836 = f16[1024,4096]{1,0} reshape(f16[1024,4,1024]{0,2,1} %transpose.401)
  %dot.352 = f16[512,1024]{1,0} dot(f16[4096,512]{1,0} %reshape.2336, f16[1024,4096]{1,0} %reshape.2836), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.84 = f16[512,1024]{1,0} all-reduce(f16[512,1024]{1,0} %dot.352), channel_id=85, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%add.53, sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.307 = f32[512,1024]{0,1} convert(f16[512,1024]{1,0} %all-reduce.84), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/2/remat(core_fn)/2/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1037 = f32[512,1024]{0,1} multiply(f32[512,1024]{0,1} %convert.307, f32[512,1024]{1,0} %broadcast.1192), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.165 = f32[512,1024]{1,0} parameter(119), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1038 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param.165, f32[512,1024]{1,0} %broadcast.1193), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.660 = f32[512,1024]{0,1} add(f32[512,1024]{0,1} %multiply.1037, f32[512,1024]{1,0} %multiply.1038), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1040 = f32[512,1024]{0,1} multiply(f32[512,1024]{0,1} %convert.307, f32[512,1024]{0,1} %convert.307), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1041 = f32[512,1024]{0,1} multiply(f32[512,1024]{0,1} %multiply.1040, f32[512,1024]{1,0} %broadcast.1194), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.166 = f32[512,1024]{1,0} parameter(196), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1042 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param.166, f32[512,1024]{1,0} %broadcast.1196), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.661 = f32[512,1024]{0,1} add(f32[512,1024]{0,1} %multiply.1041, f32[512,1024]{1,0} %multiply.1042), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.330 = f32[512,1024]{0,1} divide(f32[512,1024]{0,1} %add.661, f32[512,1024]{1,0} %broadcast.922), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.40 = f32[512,1024]{0,1} sqrt(f32[512,1024]{0,1} %divide.330), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.662 = f32[512,1024]{0,1} add(f32[512,1024]{0,1} %sqrt.40, f32[512,1024]{1,0} %broadcast.1198), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1043 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %broadcast.917, f32[512,1024]{0,1} %add.662)
  %divide.331 = f32[512,1024]{0,1} divide(f32[512,1024]{0,1} %add.660, f32[512,1024]{1,0} %multiply.1043), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1044 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param.44, f32[512,1024]{1,0} %broadcast.1199), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.663 = f32[512,1024]{0,1} add(f32[512,1024]{0,1} %divide.331, f32[512,1024]{1,0} %multiply.1044), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.1045 = f32[512,1024]{0,1} multiply(f32[512,1024]{0,1} %add.663, f32[512,1024]{1,0} %broadcast.1200), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.666 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %param.44, f32[512,1024]{0,1} %multiply.1045), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reduce.137 = f32[1024]{0} reduce(f32[4,1024,1024]{2,1,0} %convert.210, f32[] %constant.69), dimensions={0,1}, to_apply=%region_97.3013, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %all-reduce.85 = f32[1024]{0} all-reduce(f32[1024]{0} %reduce.137), channel_id=86, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_97.3013, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %multiply.1046 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.85, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.167 = f32[1024]{0} parameter(120), sharding={replicated}
  %multiply.1047 = f32[1024]{0} multiply(f32[1024]{0} %param.167, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.667 = f32[1024]{0} add(f32[1024]{0} %multiply.1046, f32[1024]{0} %multiply.1047), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1048 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.85, f32[1024]{0} %all-reduce.85), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1049 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1048, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.168 = f32[1024]{0} parameter(197), sharding={replicated}
  %multiply.1050 = f32[1024]{0} multiply(f32[1024]{0} %param.168, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.668 = f32[1024]{0} add(f32[1024]{0} %multiply.1049, f32[1024]{0} %multiply.1050), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.332 = f32[1024]{0} divide(f32[1024]{0} %add.668, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.41 = f32[1024]{0} sqrt(f32[1024]{0} %divide.332), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.669 = f32[1024]{0} add(f32[1024]{0} %sqrt.41, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1051 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.669)
  %divide.333 = f32[1024]{0} divide(f32[1024]{0} %add.667, f32[1024]{0} %multiply.1051), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1052 = f32[1024]{0} multiply(f32[1024]{0} %divide.333, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.670 = f32[1024]{0} add(f32[1024]{0} %param.53, f32[1024]{0} %multiply.1052), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %multiply.1053 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.682, f32[4,1024,1024]{2,1,0} %multiply.503), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.138 = f32[1024]{0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1053, f32[] %constant.69), dimensions={0,1}, to_apply=%region_98.3023, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %all-reduce.86 = f32[1024]{0} all-reduce(f32[1024]{0} %reduce.138), channel_id=87, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_98.3023, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.1054 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.86, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.169 = f32[1024]{0} parameter(121), sharding={replicated}
  %multiply.1055 = f32[1024]{0} multiply(f32[1024]{0} %param.169, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.672 = f32[1024]{0} add(f32[1024]{0} %multiply.1054, f32[1024]{0} %multiply.1055), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1056 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.86, f32[1024]{0} %all-reduce.86), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1057 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1056, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.170 = f32[1024]{0} parameter(198), sharding={replicated}
  %multiply.1058 = f32[1024]{0} multiply(f32[1024]{0} %param.170, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.673 = f32[1024]{0} add(f32[1024]{0} %multiply.1057, f32[1024]{0} %multiply.1058), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.334 = f32[1024]{0} divide(f32[1024]{0} %add.673, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.42 = f32[1024]{0} sqrt(f32[1024]{0} %divide.334), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.674 = f32[1024]{0} add(f32[1024]{0} %sqrt.42, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1059 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.674)
  %divide.335 = f32[1024]{0} divide(f32[1024]{0} %add.672, f32[1024]{0} %multiply.1059), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1060 = f32[1024]{0} multiply(f32[1024]{0} %divide.335, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.675 = f32[1024]{0} add(f32[1024]{0} %param.52, f32[1024]{0} %multiply.1060), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reduce.139 = f16[1024]{0} reduce(f16[4,1024,1024]{2,1,0} %add.349, f16[] %constant.89), dimensions={0,1}, to_apply=%region_101.3062, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %all-reduce.87 = f16[1024]{0} all-reduce(f16[1024]{0} %reduce.139), channel_id=88, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_101.3062, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %convert.308 = f32[1024]{0} convert(f16[1024]{0} %all-reduce.87), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1061 = f32[1024]{0} multiply(f32[1024]{0} %convert.308, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.171 = f32[1024]{0} parameter(122), sharding={replicated}
  %multiply.1062 = f32[1024]{0} multiply(f32[1024]{0} %param.171, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.676 = f32[1024]{0} add(f32[1024]{0} %multiply.1061, f32[1024]{0} %multiply.1062), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1064 = f32[1024]{0} multiply(f32[1024]{0} %convert.308, f32[1024]{0} %convert.308), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1065 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1064, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.172 = f32[1024]{0} parameter(199), sharding={replicated}
  %multiply.1066 = f32[1024]{0} multiply(f32[1024]{0} %param.172, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.677 = f32[1024]{0} add(f32[1024]{0} %multiply.1065, f32[1024]{0} %multiply.1066), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.336 = f32[1024]{0} divide(f32[1024]{0} %add.677, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.43 = f32[1024]{0} sqrt(f32[1024]{0} %divide.336), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.678 = f32[1024]{0} add(f32[1024]{0} %sqrt.43, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1067 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.678)
  %divide.337 = f32[1024]{0} divide(f32[1024]{0} %add.676, f32[1024]{0} %multiply.1067), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1068 = f32[1024]{0} multiply(f32[1024]{0} %divide.337, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.679 = f32[1024]{0} add(f32[1024]{0} %param.51, f32[1024]{0} %multiply.1068), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %transpose.402 = f16[1024,4,1024]{0,2,1} transpose(f16[4,1024,1024]{2,1,0} %add.349), dimensions={2,0,1}
  %reshape.2838 = f16[1024,4096]{1,0} reshape(f16[1024,4,1024]{0,2,1} %transpose.402)
  %dot.353 = f16[128,1024]{1,0} dot(f16[4096,128]{1,0} %reshape.2164, f16[1024,4096]{1,0} %reshape.2838), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.88 = f16[128,1024]{1,0} all-reduce(f16[128,1024]{1,0} %dot.353), channel_id=89, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%add.54, sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.309 = f32[128,1024]{0,1} convert(f16[128,1024]{1,0} %all-reduce.88), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1069 = f32[128,1024]{0,1} multiply(f32[128,1024]{0,1} %convert.309, f32[128,1024]{1,0} %broadcast.1141), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.173 = f32[128,1024]{1,0} parameter(123), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1071 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param.173, f32[128,1024]{1,0} %broadcast.1142), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.680 = f32[128,1024]{0,1} add(f32[128,1024]{0,1} %multiply.1069, f32[128,1024]{1,0} %multiply.1071), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1072 = f32[128,1024]{0,1} multiply(f32[128,1024]{0,1} %convert.309, f32[128,1024]{0,1} %convert.309), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1073 = f32[128,1024]{0,1} multiply(f32[128,1024]{0,1} %multiply.1072, f32[128,1024]{1,0} %broadcast.1143), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.174 = f32[128,1024]{1,0} parameter(200), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1074 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param.174, f32[128,1024]{1,0} %broadcast.1144), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.681 = f32[128,1024]{0,1} add(f32[128,1024]{0,1} %multiply.1073, f32[128,1024]{1,0} %multiply.1074), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.338 = f32[128,1024]{0,1} divide(f32[128,1024]{0,1} %add.681, f32[128,1024]{1,0} %broadcast.856), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.44 = f32[128,1024]{0,1} sqrt(f32[128,1024]{0,1} %divide.338), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.682 = f32[128,1024]{0,1} add(f32[128,1024]{0,1} %sqrt.44, f32[128,1024]{1,0} %broadcast.1146), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1075 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %broadcast.851, f32[128,1024]{0,1} %add.682)
  %divide.339 = f32[128,1024]{0,1} divide(f32[128,1024]{0,1} %add.680, f32[128,1024]{1,0} %multiply.1075), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1076 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param.50, f32[128,1024]{1,0} %broadcast.1148), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.684 = f32[128,1024]{0,1} add(f32[128,1024]{0,1} %divide.339, f32[128,1024]{1,0} %multiply.1076), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.1077 = f32[128,1024]{0,1} multiply(f32[128,1024]{0,1} %add.684, f32[128,1024]{1,0} %broadcast.1149), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.685 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %param.50, f32[128,1024]{0,1} %multiply.1077), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reshape.2840 = f16[4,1024,384]{2,1,0} reshape(f16[4,1024,128,3]{3,2,1,0} %add.352), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/reshape[new_sizes=(8, 1024, 3072) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=161}
  %reduce.140 = f16[384]{0} reduce(f16[4,1024,384]{2,1,0} %reshape.2840, f16[] %constant.89), dimensions={0,1}, to_apply=%region_104.3118, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %all-reduce.89 = f16[384]{0} all-reduce(f16[384]{0} %reduce.140), channel_id=90, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_104.3118, sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %convert.310 = f32[384]{0} convert(f16[384]{0} %all-reduce.89), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1078 = f32[384]{0} multiply(f32[384]{0} %convert.310, f32[384]{0} %broadcast.1158), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.175 = f32[384]{0} parameter(124), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1079 = f32[384]{0} multiply(f32[384]{0} %param.175, f32[384]{0} %broadcast.1160), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.686 = f32[384]{0} add(f32[384]{0} %multiply.1078, f32[384]{0} %multiply.1079), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1080 = f32[384]{0} multiply(f32[384]{0} %convert.310, f32[384]{0} %convert.310), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1081 = f32[384]{0} multiply(f32[384]{0} %multiply.1080, f32[384]{0} %broadcast.1162), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.176 = f32[384]{0} parameter(201), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1082 = f32[384]{0} multiply(f32[384]{0} %param.176, f32[384]{0} %broadcast.1163), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.687 = f32[384]{0} add(f32[384]{0} %multiply.1081, f32[384]{0} %multiply.1082), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.340 = f32[384]{0} divide(f32[384]{0} %add.687, f32[384]{0} %broadcast.878), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.45 = f32[384]{0} sqrt(f32[384]{0} %divide.340), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.688 = f32[384]{0} add(f32[384]{0} %sqrt.45, f32[384]{0} %broadcast.1164), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1085 = f32[384]{0} multiply(f32[384]{0} %broadcast.875, f32[384]{0} %add.688)
  %divide.341 = f32[384]{0} divide(f32[384]{0} %add.686, f32[384]{0} %multiply.1085), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1086 = f32[384]{0} multiply(f32[384]{0} %divide.341, f32[384]{0} %broadcast.1165), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.689 = f32[384]{0} add(f32[384]{0} %param.49, f32[384]{0} %multiply.1086), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %transpose.403 = f16[384,4,1024]{0,2,1} transpose(f16[4,1024,384]{2,1,0} %reshape.2840), dimensions={2,0,1}
  %reshape.2841 = f16[384,4096]{1,0} reshape(f16[384,4,1024]{0,2,1} %transpose.403)
  %dot.354 = f16[1024,384]{1,0} dot(f16[4096,1024]{1,0} %reshape.2131, f16[384,4096]{1,0} %reshape.2841), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.90 = f16[1024,384]{1,0} all-reduce(f16[1024,384]{1,0} %dot.354), channel_id=91, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%add.55, sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.311 = f32[1024,384]{0,1} convert(f16[1024,384]{1,0} %all-reduce.90), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/attention/self/qvk_combined/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1087 = f32[1024,384]{0,1} multiply(f32[1024,384]{0,1} %convert.311, f32[1024,384]{1,0} %broadcast.1166), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.177 = f32[1024,384]{1,0} parameter(125), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1088 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param.177, f32[1024,384]{1,0} %broadcast.1167), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.691 = f32[1024,384]{0,1} add(f32[1024,384]{0,1} %multiply.1087, f32[1024,384]{1,0} %multiply.1088), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1089 = f32[1024,384]{0,1} multiply(f32[1024,384]{0,1} %convert.311, f32[1024,384]{0,1} %convert.311), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1090 = f32[1024,384]{0,1} multiply(f32[1024,384]{0,1} %multiply.1089, f32[1024,384]{1,0} %broadcast.1168), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.178 = f32[1024,384]{1,0} parameter(202), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1091 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param.178, f32[1024,384]{1,0} %broadcast.1169), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.692 = f32[1024,384]{0,1} add(f32[1024,384]{0,1} %multiply.1090, f32[1024,384]{1,0} %multiply.1091), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.342 = f32[1024,384]{0,1} divide(f32[1024,384]{0,1} %add.692, f32[1024,384]{1,0} %broadcast.889), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.46 = f32[1024,384]{0,1} sqrt(f32[1024,384]{0,1} %divide.342), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.693 = f32[1024,384]{0,1} add(f32[1024,384]{0,1} %sqrt.46, f32[1024,384]{1,0} %broadcast.1170), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1092 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %broadcast.883, f32[1024,384]{0,1} %add.693)
  %divide.343 = f32[1024,384]{0,1} divide(f32[1024,384]{0,1} %add.691, f32[1024,384]{1,0} %multiply.1092), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1093 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param.48, f32[1024,384]{1,0} %broadcast.1171), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.694 = f32[1024,384]{0,1} add(f32[1024,384]{0,1} %divide.343, f32[1024,384]{1,0} %multiply.1093), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.1094 = f32[1024,384]{0,1} multiply(f32[1024,384]{0,1} %add.694, f32[1024,384]{1,0} %broadcast.1172), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.695 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %param.48, f32[1024,384]{0,1} %multiply.1094), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reduce.141 = f16[512]{0} reduce(f16[4,1024,512]{2,1,0} %add.345, f16[] %constant.89), dimensions={0,1}, to_apply=%region_96.3001, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %all-reduce.91 = f16[512]{0} all-reduce(f16[512]{0} %reduce.141), channel_id=92, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_96.3001, sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %convert.312 = f32[512]{0} convert(f16[512]{0} %all-reduce.91), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1095 = f32[512]{0} multiply(f32[512]{0} %convert.312, f32[512]{0} %broadcast.1173), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.179 = f32[512]{0} parameter(126), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1096 = f32[512]{0} multiply(f32[512]{0} %param.179, f32[512]{0} %broadcast.1174), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.696 = f32[512]{0} add(f32[512]{0} %multiply.1095, f32[512]{0} %multiply.1096), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1097 = f32[512]{0} multiply(f32[512]{0} %convert.312, f32[512]{0} %convert.312), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1098 = f32[512]{0} multiply(f32[512]{0} %multiply.1097, f32[512]{0} %broadcast.1175), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.180 = f32[512]{0} parameter(203), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1099 = f32[512]{0} multiply(f32[512]{0} %param.180, f32[512]{0} %broadcast.1176), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.697 = f32[512]{0} add(f32[512]{0} %multiply.1098, f32[512]{0} %multiply.1099), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.344 = f32[512]{0} divide(f32[512]{0} %add.697, f32[512]{0} %broadcast.901), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.47 = f32[512]{0} sqrt(f32[512]{0} %divide.344), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.698 = f32[512]{0} add(f32[512]{0} %sqrt.47, f32[512]{0} %broadcast.1178), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1100 = f32[512]{0} multiply(f32[512]{0} %broadcast.898, f32[512]{0} %add.698)
  %divide.345 = f32[512]{0} divide(f32[512]{0} %add.696, f32[512]{0} %multiply.1100), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1101 = f32[512]{0} multiply(f32[512]{0} %divide.345, f32[512]{0} %broadcast.1181), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.699 = f32[512]{0} add(f32[512]{0} %param.55, f32[512]{0} %multiply.1101), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %transpose.404 = f16[512,4,1024]{0,2,1} transpose(f16[4,1024,512]{2,1,0} %add.345), dimensions={2,0,1}
  %reshape.2843 = f16[512,4096]{1,0} reshape(f16[512,4,1024]{0,2,1} %transpose.404)
  %dot.355 = f16[1024,512]{1,0} dot(f16[4096,1024]{1,0} %reshape.2176, f16[512,4096]{1,0} %reshape.2843), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.92 = f16[1024,512]{1,0} all-reduce(f16[1024,512]{1,0} %dot.355), channel_id=93, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%add.56, sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.313 = f32[1024,512]{0,1} convert(f16[1024,512]{1,0} %all-reduce.92), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/intermediate/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1102 = f32[1024,512]{0,1} multiply(f32[1024,512]{0,1} %convert.313, f32[1024,512]{1,0} %broadcast.1183), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.181 = f32[1024,512]{1,0} parameter(127), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1103 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param.181, f32[1024,512]{1,0} %broadcast.1184), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.700 = f32[1024,512]{0,1} add(f32[1024,512]{0,1} %multiply.1102, f32[1024,512]{1,0} %multiply.1103), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1104 = f32[1024,512]{0,1} multiply(f32[1024,512]{0,1} %convert.313, f32[1024,512]{0,1} %convert.313), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1105 = f32[1024,512]{0,1} multiply(f32[1024,512]{0,1} %multiply.1104, f32[1024,512]{1,0} %broadcast.1185), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.182 = f32[1024,512]{1,0} parameter(204), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1106 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param.182, f32[1024,512]{1,0} %broadcast.1187), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.701 = f32[1024,512]{0,1} add(f32[1024,512]{0,1} %multiply.1105, f32[1024,512]{1,0} %multiply.1106), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.346 = f32[1024,512]{0,1} divide(f32[1024,512]{0,1} %add.701, f32[1024,512]{1,0} %broadcast.911), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.48 = f32[1024,512]{0,1} sqrt(f32[1024,512]{0,1} %divide.346), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.702 = f32[1024,512]{0,1} add(f32[1024,512]{0,1} %sqrt.48, f32[1024,512]{1,0} %broadcast.1189), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1107 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %broadcast.908, f32[1024,512]{0,1} %add.702)
  %divide.347 = f32[1024,512]{0,1} divide(f32[1024,512]{0,1} %add.700, f32[1024,512]{1,0} %multiply.1107), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1108 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param.54, f32[1024,512]{1,0} %broadcast.1190), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.703 = f32[1024,512]{0,1} add(f32[1024,512]{0,1} %divide.347, f32[1024,512]{1,0} %multiply.1108), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.1109 = f32[1024,512]{0,1} multiply(f32[1024,512]{0,1} %add.703, f32[1024,512]{1,0} %broadcast.1191), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.704 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %param.54, f32[1024,512]{0,1} %multiply.1109), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reduce.142 = f32[1024]{0} reduce(f32[4,1024,1024]{2,1,0} %convert.207, f32[] %constant.69), dimensions={0,1}, to_apply=%region_91.2933, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %all-reduce.93 = f32[1024]{0} all-reduce(f32[1024]{0} %reduce.142), channel_id=94, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_91.2933, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %multiply.1110 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.93, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.183 = f32[1024]{0} parameter(128), sharding={replicated}
  %multiply.1111 = f32[1024]{0} multiply(f32[1024]{0} %param.183, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.706 = f32[1024]{0} add(f32[1024]{0} %multiply.1110, f32[1024]{0} %multiply.1111), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1112 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.93, f32[1024]{0} %all-reduce.93), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1113 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1112, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.184 = f32[1024]{0} parameter(205), sharding={replicated}
  %multiply.1114 = f32[1024]{0} multiply(f32[1024]{0} %param.184, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.707 = f32[1024]{0} add(f32[1024]{0} %multiply.1113, f32[1024]{0} %multiply.1114), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.348 = f32[1024]{0} divide(f32[1024]{0} %add.707, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.49 = f32[1024]{0} sqrt(f32[1024]{0} %divide.348), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.708 = f32[1024]{0} add(f32[1024]{0} %sqrt.49, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1115 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.708)
  %divide.349 = f32[1024]{0} divide(f32[1024]{0} %add.706, f32[1024]{0} %multiply.1115), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1116 = f32[1024]{0} multiply(f32[1024]{0} %divide.349, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.709 = f32[1024]{0} add(f32[1024]{0} %param.59, f32[1024]{0} %multiply.1116), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %multiply.1117 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.701, f32[4,1024,1024]{2,1,0} %multiply.484), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.143 = f32[1024]{0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1117, f32[] %constant.69), dimensions={0,1}, to_apply=%region_92.2943, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %all-reduce.94 = f32[1024]{0} all-reduce(f32[1024]{0} %reduce.143), channel_id=95, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_92.2943, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.1118 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.94, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.185 = f32[1024]{0} parameter(129), sharding={replicated}
  %multiply.1119 = f32[1024]{0} multiply(f32[1024]{0} %param.185, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.710 = f32[1024]{0} add(f32[1024]{0} %multiply.1118, f32[1024]{0} %multiply.1119), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1120 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.94, f32[1024]{0} %all-reduce.94), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1121 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1120, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.186 = f32[1024]{0} parameter(206), sharding={replicated}
  %multiply.1122 = f32[1024]{0} multiply(f32[1024]{0} %param.186, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.711 = f32[1024]{0} add(f32[1024]{0} %multiply.1121, f32[1024]{0} %multiply.1122), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.350 = f32[1024]{0} divide(f32[1024]{0} %add.711, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.50 = f32[1024]{0} sqrt(f32[1024]{0} %divide.350), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.712 = f32[1024]{0} add(f32[1024]{0} %sqrt.50, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1123 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.712)
  %divide.351 = f32[1024]{0} divide(f32[1024]{0} %add.710, f32[1024]{0} %multiply.1123), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1124 = f32[1024]{0} multiply(f32[1024]{0} %divide.351, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.713 = f32[1024]{0} add(f32[1024]{0} %param.58, f32[1024]{0} %multiply.1124), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reduce.144 = f16[1024]{0} reduce(f16[4,1024,1024]{2,1,0} %add.344, f16[] %constant.89), dimensions={0,1}, to_apply=%region_95.2982, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %all-reduce.95 = f16[1024]{0} all-reduce(f16[1024]{0} %reduce.144), channel_id=96, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_95.2982, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %convert.314 = f32[1024]{0} convert(f16[1024]{0} %all-reduce.95), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1125 = f32[1024]{0} multiply(f32[1024]{0} %convert.314, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.187 = f32[1024]{0} parameter(130), sharding={replicated}
  %multiply.1126 = f32[1024]{0} multiply(f32[1024]{0} %param.187, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.715 = f32[1024]{0} add(f32[1024]{0} %multiply.1125, f32[1024]{0} %multiply.1126), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1127 = f32[1024]{0} multiply(f32[1024]{0} %convert.314, f32[1024]{0} %convert.314), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1128 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1127, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.188 = f32[1024]{0} parameter(207), sharding={replicated}
  %multiply.1129 = f32[1024]{0} multiply(f32[1024]{0} %param.188, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.716 = f32[1024]{0} add(f32[1024]{0} %multiply.1128, f32[1024]{0} %multiply.1129), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.352 = f32[1024]{0} divide(f32[1024]{0} %add.716, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.51 = f32[1024]{0} sqrt(f32[1024]{0} %divide.352), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.717 = f32[1024]{0} add(f32[1024]{0} %sqrt.51, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1130 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.717)
  %divide.353 = f32[1024]{0} divide(f32[1024]{0} %add.715, f32[1024]{0} %multiply.1130), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1131 = f32[1024]{0} multiply(f32[1024]{0} %divide.353, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.718 = f32[1024]{0} add(f32[1024]{0} %param.57, f32[1024]{0} %multiply.1131), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %transpose.405 = f16[1024,4,1024]{0,2,1} transpose(f16[4,1024,1024]{2,1,0} %add.344), dimensions={2,0,1}
  %reshape.2845 = f16[1024,4096]{1,0} reshape(f16[1024,4,1024]{0,2,1} %transpose.405)
  %dot.356 = f16[512,1024]{1,0} dot(f16[4096,512]{1,0} %reshape.2200, f16[1024,4096]{1,0} %reshape.2845), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.96 = f16[512,1024]{1,0} all-reduce(f16[512,1024]{1,0} %dot.356), channel_id=97, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%add.57, sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.315 = f32[512,1024]{0,1} convert(f16[512,1024]{1,0} %all-reduce.96), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/3/remat(core_fn)/3/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1132 = f32[512,1024]{0,1} multiply(f32[512,1024]{0,1} %convert.315, f32[512,1024]{1,0} %broadcast.1192), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.189 = f32[512,1024]{1,0} parameter(131), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1133 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param.189, f32[512,1024]{1,0} %broadcast.1193), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.719 = f32[512,1024]{0,1} add(f32[512,1024]{0,1} %multiply.1132, f32[512,1024]{1,0} %multiply.1133), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1134 = f32[512,1024]{0,1} multiply(f32[512,1024]{0,1} %convert.315, f32[512,1024]{0,1} %convert.315), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1135 = f32[512,1024]{0,1} multiply(f32[512,1024]{0,1} %multiply.1134, f32[512,1024]{1,0} %broadcast.1194), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.190 = f32[512,1024]{1,0} parameter(208), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1136 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param.190, f32[512,1024]{1,0} %broadcast.1196), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.720 = f32[512,1024]{0,1} add(f32[512,1024]{0,1} %multiply.1135, f32[512,1024]{1,0} %multiply.1136), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.354 = f32[512,1024]{0,1} divide(f32[512,1024]{0,1} %add.720, f32[512,1024]{1,0} %broadcast.922), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.52 = f32[512,1024]{0,1} sqrt(f32[512,1024]{0,1} %divide.354), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.721 = f32[512,1024]{0,1} add(f32[512,1024]{0,1} %sqrt.52, f32[512,1024]{1,0} %broadcast.1198), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1137 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %broadcast.917, f32[512,1024]{0,1} %add.721)
  %divide.355 = f32[512,1024]{0,1} divide(f32[512,1024]{0,1} %add.719, f32[512,1024]{1,0} %multiply.1137), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1138 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param.56, f32[512,1024]{1,0} %broadcast.1199), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.722 = f32[512,1024]{0,1} add(f32[512,1024]{0,1} %divide.355, f32[512,1024]{1,0} %multiply.1138), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.1139 = f32[512,1024]{0,1} multiply(f32[512,1024]{0,1} %add.722, f32[512,1024]{1,0} %broadcast.1200), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.723 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %param.56, f32[512,1024]{0,1} %multiply.1139), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reduce.145 = f32[1024]{0} reduce(f32[4,1024,1024]{2,1,0} %convert.189, f32[] %constant.69), dimensions={0,1}, to_apply=%region_77.2554, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %all-reduce.97 = f32[1024]{0} all-reduce(f32[1024]{0} %reduce.145), channel_id=98, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_77.2554, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %multiply.1140 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.97, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.191 = f32[1024]{0} parameter(132), sharding={replicated}
  %multiply.1141 = f32[1024]{0} multiply(f32[1024]{0} %param.191, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.724 = f32[1024]{0} add(f32[1024]{0} %multiply.1140, f32[1024]{0} %multiply.1141), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1142 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.97, f32[1024]{0} %all-reduce.97), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1143 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1142, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.192 = f32[1024]{0} parameter(209), sharding={replicated}
  %multiply.1144 = f32[1024]{0} multiply(f32[1024]{0} %param.192, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.725 = f32[1024]{0} add(f32[1024]{0} %multiply.1143, f32[1024]{0} %multiply.1144), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.356 = f32[1024]{0} divide(f32[1024]{0} %add.725, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.53 = f32[1024]{0} sqrt(f32[1024]{0} %divide.356), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.726 = f32[1024]{0} add(f32[1024]{0} %sqrt.53, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1145 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.726)
  %divide.357 = f32[1024]{0} divide(f32[1024]{0} %add.724, f32[1024]{0} %multiply.1145), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1146 = f32[1024]{0} multiply(f32[1024]{0} %divide.357, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.727 = f32[1024]{0} add(f32[1024]{0} %param.65, f32[1024]{0} %multiply.1146), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %multiply.1147 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.643, f32[4,1024,1024]{2,1,0} %multiply.448), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.146 = f32[1024]{0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1147, f32[] %constant.69), dimensions={0,1}, to_apply=%region_78.2564, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %all-reduce.98 = f32[1024]{0} all-reduce(f32[1024]{0} %reduce.146), channel_id=99, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_78.2564, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.1148 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.98, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.193 = f32[1024]{0} parameter(133), sharding={replicated}
  %multiply.1149 = f32[1024]{0} multiply(f32[1024]{0} %param.193, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.728 = f32[1024]{0} add(f32[1024]{0} %multiply.1148, f32[1024]{0} %multiply.1149), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1150 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.98, f32[1024]{0} %all-reduce.98), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1151 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1150, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.194 = f32[1024]{0} parameter(210), sharding={replicated}
  %multiply.1152 = f32[1024]{0} multiply(f32[1024]{0} %param.194, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.729 = f32[1024]{0} add(f32[1024]{0} %multiply.1151, f32[1024]{0} %multiply.1152), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.358 = f32[1024]{0} divide(f32[1024]{0} %add.729, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.54 = f32[1024]{0} sqrt(f32[1024]{0} %divide.358), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.731 = f32[1024]{0} add(f32[1024]{0} %sqrt.54, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1153 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.731)
  %divide.359 = f32[1024]{0} divide(f32[1024]{0} %add.728, f32[1024]{0} %multiply.1153), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1154 = f32[1024]{0} multiply(f32[1024]{0} %divide.359, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.732 = f32[1024]{0} add(f32[1024]{0} %param.64, f32[1024]{0} %multiply.1154), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reduce.147 = f16[1024]{0} reduce(f16[4,1024,1024]{2,1,0} %add.314, f16[] %constant.89), dimensions={0,1}, to_apply=%region_81.2603, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %all-reduce.99 = f16[1024]{0} all-reduce(f16[1024]{0} %reduce.147), channel_id=100, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_81.2603, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %convert.316 = f32[1024]{0} convert(f16[1024]{0} %all-reduce.99), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1155 = f32[1024]{0} multiply(f32[1024]{0} %convert.316, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.195 = f32[1024]{0} parameter(134), sharding={replicated}
  %multiply.1156 = f32[1024]{0} multiply(f32[1024]{0} %param.195, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.733 = f32[1024]{0} add(f32[1024]{0} %multiply.1155, f32[1024]{0} %multiply.1156), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1157 = f32[1024]{0} multiply(f32[1024]{0} %convert.316, f32[1024]{0} %convert.316), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1158 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1157, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.196 = f32[1024]{0} parameter(211), sharding={replicated}
  %multiply.1159 = f32[1024]{0} multiply(f32[1024]{0} %param.196, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.734 = f32[1024]{0} add(f32[1024]{0} %multiply.1158, f32[1024]{0} %multiply.1159), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.360 = f32[1024]{0} divide(f32[1024]{0} %add.734, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.55 = f32[1024]{0} sqrt(f32[1024]{0} %divide.360), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.735 = f32[1024]{0} add(f32[1024]{0} %sqrt.55, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1160 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.735)
  %divide.361 = f32[1024]{0} divide(f32[1024]{0} %add.733, f32[1024]{0} %multiply.1160), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1161 = f32[1024]{0} multiply(f32[1024]{0} %divide.361, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.736 = f32[1024]{0} add(f32[1024]{0} %param.63, f32[1024]{0} %multiply.1161), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %transpose.406 = f16[1024,4,1024]{0,2,1} transpose(f16[4,1024,1024]{2,1,0} %add.314), dimensions={2,0,1}
  %reshape.2847 = f16[1024,4096]{1,0} reshape(f16[1024,4,1024]{0,2,1} %transpose.406)
  %dot.357 = f16[128,1024]{1,0} dot(f16[4096,128]{1,0} %reshape.2037, f16[1024,4096]{1,0} %reshape.2847), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.100 = f16[128,1024]{1,0} all-reduce(f16[128,1024]{1,0} %dot.357), channel_id=101, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%add.58, sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.317 = f32[128,1024]{0,1} convert(f16[128,1024]{1,0} %all-reduce.100), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1162 = f32[128,1024]{0,1} multiply(f32[128,1024]{0,1} %convert.317, f32[128,1024]{1,0} %broadcast.1141), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.197 = f32[128,1024]{1,0} parameter(135), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1164 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param.197, f32[128,1024]{1,0} %broadcast.1142), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.737 = f32[128,1024]{0,1} add(f32[128,1024]{0,1} %multiply.1162, f32[128,1024]{1,0} %multiply.1164), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1165 = f32[128,1024]{0,1} multiply(f32[128,1024]{0,1} %convert.317, f32[128,1024]{0,1} %convert.317), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1166 = f32[128,1024]{0,1} multiply(f32[128,1024]{0,1} %multiply.1165, f32[128,1024]{1,0} %broadcast.1143), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.198 = f32[128,1024]{1,0} parameter(212), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1167 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param.198, f32[128,1024]{1,0} %broadcast.1144), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.738 = f32[128,1024]{0,1} add(f32[128,1024]{0,1} %multiply.1166, f32[128,1024]{1,0} %multiply.1167), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.362 = f32[128,1024]{0,1} divide(f32[128,1024]{0,1} %add.738, f32[128,1024]{1,0} %broadcast.856), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.56 = f32[128,1024]{0,1} sqrt(f32[128,1024]{0,1} %divide.362), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.739 = f32[128,1024]{0,1} add(f32[128,1024]{0,1} %sqrt.56, f32[128,1024]{1,0} %broadcast.1146), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1168 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %broadcast.851, f32[128,1024]{0,1} %add.739)
  %divide.363 = f32[128,1024]{0,1} divide(f32[128,1024]{0,1} %add.737, f32[128,1024]{1,0} %multiply.1168), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1169 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param.62, f32[128,1024]{1,0} %broadcast.1148), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.740 = f32[128,1024]{0,1} add(f32[128,1024]{0,1} %divide.363, f32[128,1024]{1,0} %multiply.1169), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.1171 = f32[128,1024]{0,1} multiply(f32[128,1024]{0,1} %add.740, f32[128,1024]{1,0} %broadcast.1149), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.741 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %param.62, f32[128,1024]{0,1} %multiply.1171), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reshape.2849 = f16[4,1024,384]{2,1,0} reshape(f16[4,1024,128,3]{3,2,1,0} %add.317), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/reshape[new_sizes=(8, 1024, 3072) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=161}
  %reduce.148 = f16[384]{0} reduce(f16[4,1024,384]{2,1,0} %reshape.2849, f16[] %constant.89), dimensions={0,1}, to_apply=%region_84.2659, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %all-reduce.101 = f16[384]{0} all-reduce(f16[384]{0} %reduce.148), channel_id=102, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_84.2659, sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %convert.318 = f32[384]{0} convert(f16[384]{0} %all-reduce.101), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1172 = f32[384]{0} multiply(f32[384]{0} %convert.318, f32[384]{0} %broadcast.1158), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.199 = f32[384]{0} parameter(136), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1173 = f32[384]{0} multiply(f32[384]{0} %param.199, f32[384]{0} %broadcast.1160), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.742 = f32[384]{0} add(f32[384]{0} %multiply.1172, f32[384]{0} %multiply.1173), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1174 = f32[384]{0} multiply(f32[384]{0} %convert.318, f32[384]{0} %convert.318), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1175 = f32[384]{0} multiply(f32[384]{0} %multiply.1174, f32[384]{0} %broadcast.1162), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.200 = f32[384]{0} parameter(213), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1176 = f32[384]{0} multiply(f32[384]{0} %param.200, f32[384]{0} %broadcast.1163), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.743 = f32[384]{0} add(f32[384]{0} %multiply.1175, f32[384]{0} %multiply.1176), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.364 = f32[384]{0} divide(f32[384]{0} %add.743, f32[384]{0} %broadcast.878), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.57 = f32[384]{0} sqrt(f32[384]{0} %divide.364), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.744 = f32[384]{0} add(f32[384]{0} %sqrt.57, f32[384]{0} %broadcast.1164), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1177 = f32[384]{0} multiply(f32[384]{0} %broadcast.875, f32[384]{0} %add.744)
  %divide.365 = f32[384]{0} divide(f32[384]{0} %add.742, f32[384]{0} %multiply.1177), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1178 = f32[384]{0} multiply(f32[384]{0} %divide.365, f32[384]{0} %broadcast.1165), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.745 = f32[384]{0} add(f32[384]{0} %param.61, f32[384]{0} %multiply.1178), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %transpose.407 = f16[384,4,1024]{0,2,1} transpose(f16[4,1024,384]{2,1,0} %reshape.2849), dimensions={2,0,1}
  %reshape.2850 = f16[384,4096]{1,0} reshape(f16[384,4,1024]{0,2,1} %transpose.407)
  %dot.358 = f16[1024,384]{1,0} dot(f16[4096,1024]{1,0} %reshape.2004, f16[384,4096]{1,0} %reshape.2850), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.102 = f16[1024,384]{1,0} all-reduce(f16[1024,384]{1,0} %dot.358), channel_id=103, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%add.59, sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.319 = f32[1024,384]{0,1} convert(f16[1024,384]{1,0} %all-reduce.102), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/attention/self/qvk_combined/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1179 = f32[1024,384]{0,1} multiply(f32[1024,384]{0,1} %convert.319, f32[1024,384]{1,0} %broadcast.1166), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.201 = f32[1024,384]{1,0} parameter(137), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1180 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param.201, f32[1024,384]{1,0} %broadcast.1167), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.747 = f32[1024,384]{0,1} add(f32[1024,384]{0,1} %multiply.1179, f32[1024,384]{1,0} %multiply.1180), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1181 = f32[1024,384]{0,1} multiply(f32[1024,384]{0,1} %convert.319, f32[1024,384]{0,1} %convert.319), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1182 = f32[1024,384]{0,1} multiply(f32[1024,384]{0,1} %multiply.1181, f32[1024,384]{1,0} %broadcast.1168), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.202 = f32[1024,384]{1,0} parameter(214), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1185 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param.202, f32[1024,384]{1,0} %broadcast.1169), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.748 = f32[1024,384]{0,1} add(f32[1024,384]{0,1} %multiply.1182, f32[1024,384]{1,0} %multiply.1185), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.366 = f32[1024,384]{0,1} divide(f32[1024,384]{0,1} %add.748, f32[1024,384]{1,0} %broadcast.889), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.58 = f32[1024,384]{0,1} sqrt(f32[1024,384]{0,1} %divide.366), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.749 = f32[1024,384]{0,1} add(f32[1024,384]{0,1} %sqrt.58, f32[1024,384]{1,0} %broadcast.1170), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1186 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %broadcast.883, f32[1024,384]{0,1} %add.749)
  %divide.367 = f32[1024,384]{0,1} divide(f32[1024,384]{0,1} %add.747, f32[1024,384]{1,0} %multiply.1186), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1187 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param.60, f32[1024,384]{1,0} %broadcast.1171), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.750 = f32[1024,384]{0,1} add(f32[1024,384]{0,1} %divide.367, f32[1024,384]{1,0} %multiply.1187), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.1188 = f32[1024,384]{0,1} multiply(f32[1024,384]{0,1} %add.750, f32[1024,384]{1,0} %broadcast.1172), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.751 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %param.60, f32[1024,384]{0,1} %multiply.1188), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reduce.149 = f16[512]{0} reduce(f16[4,1024,512]{2,1,0} %add.310, f16[] %constant.89), dimensions={0,1}, to_apply=%region_76.2542, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %all-reduce.103 = f16[512]{0} all-reduce(f16[512]{0} %reduce.149), channel_id=104, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_76.2542, sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %convert.320 = f32[512]{0} convert(f16[512]{0} %all-reduce.103), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1189 = f32[512]{0} multiply(f32[512]{0} %convert.320, f32[512]{0} %broadcast.1173), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.203 = f32[512]{0} parameter(138), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1190 = f32[512]{0} multiply(f32[512]{0} %param.203, f32[512]{0} %broadcast.1174), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.752 = f32[512]{0} add(f32[512]{0} %multiply.1189, f32[512]{0} %multiply.1190), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1191 = f32[512]{0} multiply(f32[512]{0} %convert.320, f32[512]{0} %convert.320), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1192 = f32[512]{0} multiply(f32[512]{0} %multiply.1191, f32[512]{0} %broadcast.1175), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.204 = f32[512]{0} parameter(215), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1193 = f32[512]{0} multiply(f32[512]{0} %param.204, f32[512]{0} %broadcast.1176), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.753 = f32[512]{0} add(f32[512]{0} %multiply.1192, f32[512]{0} %multiply.1193), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.368 = f32[512]{0} divide(f32[512]{0} %add.753, f32[512]{0} %broadcast.901), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.59 = f32[512]{0} sqrt(f32[512]{0} %divide.368), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.754 = f32[512]{0} add(f32[512]{0} %sqrt.59, f32[512]{0} %broadcast.1178), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1194 = f32[512]{0} multiply(f32[512]{0} %broadcast.898, f32[512]{0} %add.754)
  %divide.369 = f32[512]{0} divide(f32[512]{0} %add.752, f32[512]{0} %multiply.1194), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1195 = f32[512]{0} multiply(f32[512]{0} %divide.369, f32[512]{0} %broadcast.1181), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.755 = f32[512]{0} add(f32[512]{0} %param.67, f32[512]{0} %multiply.1195), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %transpose.408 = f16[512,4,1024]{0,2,1} transpose(f16[4,1024,512]{2,1,0} %add.310), dimensions={2,0,1}
  %reshape.2852 = f16[512,4096]{1,0} reshape(f16[512,4,1024]{0,2,1} %transpose.408)
  %dot.359 = f16[1024,512]{1,0} dot(f16[4096,1024]{1,0} %reshape.2050, f16[512,4096]{1,0} %reshape.2852), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.104 = f16[1024,512]{1,0} all-reduce(f16[1024,512]{1,0} %dot.359), channel_id=105, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%add.60, sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.321 = f32[1024,512]{0,1} convert(f16[1024,512]{1,0} %all-reduce.104), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/intermediate/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1196 = f32[1024,512]{0,1} multiply(f32[1024,512]{0,1} %convert.321, f32[1024,512]{1,0} %broadcast.1183), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.205 = f32[1024,512]{1,0} parameter(139), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1197 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param.205, f32[1024,512]{1,0} %broadcast.1184), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.756 = f32[1024,512]{0,1} add(f32[1024,512]{0,1} %multiply.1196, f32[1024,512]{1,0} %multiply.1197), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1198 = f32[1024,512]{0,1} multiply(f32[1024,512]{0,1} %convert.321, f32[1024,512]{0,1} %convert.321), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1199 = f32[1024,512]{0,1} multiply(f32[1024,512]{0,1} %multiply.1198, f32[1024,512]{1,0} %broadcast.1185), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.206 = f32[1024,512]{1,0} parameter(216), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1200 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param.206, f32[1024,512]{1,0} %broadcast.1187), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.757 = f32[1024,512]{0,1} add(f32[1024,512]{0,1} %multiply.1199, f32[1024,512]{1,0} %multiply.1200), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.370 = f32[1024,512]{0,1} divide(f32[1024,512]{0,1} %add.757, f32[1024,512]{1,0} %broadcast.911), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.60 = f32[1024,512]{0,1} sqrt(f32[1024,512]{0,1} %divide.370), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.758 = f32[1024,512]{0,1} add(f32[1024,512]{0,1} %sqrt.60, f32[1024,512]{1,0} %broadcast.1189), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1201 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %broadcast.908, f32[1024,512]{0,1} %add.758)
  %divide.371 = f32[1024,512]{0,1} divide(f32[1024,512]{0,1} %add.756, f32[1024,512]{1,0} %multiply.1201), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1204 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param.66, f32[1024,512]{1,0} %broadcast.1190), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.759 = f32[1024,512]{0,1} add(f32[1024,512]{0,1} %divide.371, f32[1024,512]{1,0} %multiply.1204), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.1206 = f32[1024,512]{0,1} multiply(f32[1024,512]{0,1} %add.759, f32[1024,512]{1,0} %broadcast.1191), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.760 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %param.66, f32[1024,512]{0,1} %multiply.1206), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reduce.150 = f32[1024]{0} reduce(f32[4,1024,1024]{2,1,0} %convert.186, f32[] %constant.69), dimensions={0,1}, to_apply=%region_71.2474, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %all-reduce.105 = f32[1024]{0} all-reduce(f32[1024]{0} %reduce.150), channel_id=106, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_71.2474, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %multiply.1208 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.105, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.207 = f32[1024]{0} parameter(140), sharding={replicated}
  %multiply.1210 = f32[1024]{0} multiply(f32[1024]{0} %param.207, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.761 = f32[1024]{0} add(f32[1024]{0} %multiply.1208, f32[1024]{0} %multiply.1210), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1212 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.105, f32[1024]{0} %all-reduce.105), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1214 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1212, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.208 = f32[1024]{0} parameter(217), sharding={replicated}
  %multiply.1216 = f32[1024]{0} multiply(f32[1024]{0} %param.208, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.762 = f32[1024]{0} add(f32[1024]{0} %multiply.1214, f32[1024]{0} %multiply.1216), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.372 = f32[1024]{0} divide(f32[1024]{0} %add.762, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.61 = f32[1024]{0} sqrt(f32[1024]{0} %divide.372), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.763 = f32[1024]{0} add(f32[1024]{0} %sqrt.61, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1218 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.763)
  %divide.373 = f32[1024]{0} divide(f32[1024]{0} %add.761, f32[1024]{0} %multiply.1218), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1220 = f32[1024]{0} multiply(f32[1024]{0} %divide.373, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.766 = f32[1024]{0} add(f32[1024]{0} %param.71, f32[1024]{0} %multiply.1220), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %multiply.1222 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.658, f32[4,1024,1024]{2,1,0} %multiply.430), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.151 = f32[1024]{0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1222, f32[] %constant.69), dimensions={0,1}, to_apply=%region_72.2484, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %all-reduce.106 = f32[1024]{0} all-reduce(f32[1024]{0} %reduce.151), channel_id=107, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_72.2484, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.1224 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.106, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.209 = f32[1024]{0} parameter(141), sharding={replicated}
  %multiply.1226 = f32[1024]{0} multiply(f32[1024]{0} %param.209, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.767 = f32[1024]{0} add(f32[1024]{0} %multiply.1224, f32[1024]{0} %multiply.1226), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1228 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.106, f32[1024]{0} %all-reduce.106), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1229 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1228, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.210 = f32[1024]{0} parameter(218), sharding={replicated}
  %multiply.1230 = f32[1024]{0} multiply(f32[1024]{0} %param.210, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.768 = f32[1024]{0} add(f32[1024]{0} %multiply.1229, f32[1024]{0} %multiply.1230), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.374 = f32[1024]{0} divide(f32[1024]{0} %add.768, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.62 = f32[1024]{0} sqrt(f32[1024]{0} %divide.374), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.769 = f32[1024]{0} add(f32[1024]{0} %sqrt.62, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1232 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.769)
  %divide.375 = f32[1024]{0} divide(f32[1024]{0} %add.767, f32[1024]{0} %multiply.1232), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1233 = f32[1024]{0} multiply(f32[1024]{0} %divide.375, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.770 = f32[1024]{0} add(f32[1024]{0} %param.70, f32[1024]{0} %multiply.1233), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reduce.152 = f16[1024]{0} reduce(f16[4,1024,1024]{2,1,0} %add.309, f16[] %constant.89), dimensions={0,1}, to_apply=%region_75.2523, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %all-reduce.107 = f16[1024]{0} all-reduce(f16[1024]{0} %reduce.152), channel_id=108, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_75.2523, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %convert.322 = f32[1024]{0} convert(f16[1024]{0} %all-reduce.107), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1234 = f32[1024]{0} multiply(f32[1024]{0} %convert.322, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.211 = f32[1024]{0} parameter(142), sharding={replicated}
  %multiply.1235 = f32[1024]{0} multiply(f32[1024]{0} %param.211, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.772 = f32[1024]{0} add(f32[1024]{0} %multiply.1234, f32[1024]{0} %multiply.1235), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1236 = f32[1024]{0} multiply(f32[1024]{0} %convert.322, f32[1024]{0} %convert.322), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1237 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1236, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.212 = f32[1024]{0} parameter(219), sharding={replicated}
  %multiply.1238 = f32[1024]{0} multiply(f32[1024]{0} %param.212, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.773 = f32[1024]{0} add(f32[1024]{0} %multiply.1237, f32[1024]{0} %multiply.1238), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.376 = f32[1024]{0} divide(f32[1024]{0} %add.773, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.63 = f32[1024]{0} sqrt(f32[1024]{0} %divide.376), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.774 = f32[1024]{0} add(f32[1024]{0} %sqrt.63, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1239 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.774)
  %divide.377 = f32[1024]{0} divide(f32[1024]{0} %add.772, f32[1024]{0} %multiply.1239), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1240 = f32[1024]{0} multiply(f32[1024]{0} %divide.377, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.775 = f32[1024]{0} add(f32[1024]{0} %param.69, f32[1024]{0} %multiply.1240), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %transpose.409 = f16[1024,4,1024]{0,2,1} transpose(f16[4,1024,1024]{2,1,0} %add.309), dimensions={2,0,1}
  %reshape.2854 = f16[1024,4096]{1,0} reshape(f16[1024,4,1024]{0,2,1} %transpose.409)
  %dot.360 = f16[512,1024]{1,0} dot(f16[4096,512]{1,0} %reshape.2069, f16[1024,4096]{1,0} %reshape.2854), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.108 = f16[512,1024]{1,0} all-reduce(f16[512,1024]{1,0} %dot.360), channel_id=109, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%add.61, sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.323 = f32[512,1024]{0,1} convert(f16[512,1024]{1,0} %all-reduce.108), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/4/remat(core_fn)/4/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1241 = f32[512,1024]{0,1} multiply(f32[512,1024]{0,1} %convert.323, f32[512,1024]{1,0} %broadcast.1192), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.213 = f32[512,1024]{1,0} parameter(143), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1242 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param.213, f32[512,1024]{1,0} %broadcast.1193), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.776 = f32[512,1024]{0,1} add(f32[512,1024]{0,1} %multiply.1241, f32[512,1024]{1,0} %multiply.1242), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1243 = f32[512,1024]{0,1} multiply(f32[512,1024]{0,1} %convert.323, f32[512,1024]{0,1} %convert.323), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1244 = f32[512,1024]{0,1} multiply(f32[512,1024]{0,1} %multiply.1243, f32[512,1024]{1,0} %broadcast.1194), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.214 = f32[512,1024]{1,0} parameter(220), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1245 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param.214, f32[512,1024]{1,0} %broadcast.1196), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.777 = f32[512,1024]{0,1} add(f32[512,1024]{0,1} %multiply.1244, f32[512,1024]{1,0} %multiply.1245), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.378 = f32[512,1024]{0,1} divide(f32[512,1024]{0,1} %add.777, f32[512,1024]{1,0} %broadcast.922), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.64 = f32[512,1024]{0,1} sqrt(f32[512,1024]{0,1} %divide.378), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.778 = f32[512,1024]{0,1} add(f32[512,1024]{0,1} %sqrt.64, f32[512,1024]{1,0} %broadcast.1198), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1246 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %broadcast.917, f32[512,1024]{0,1} %add.778)
  %divide.379 = f32[512,1024]{0,1} divide(f32[512,1024]{0,1} %add.776, f32[512,1024]{1,0} %multiply.1246), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1247 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param.68, f32[512,1024]{1,0} %broadcast.1199), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.779 = f32[512,1024]{0,1} add(f32[512,1024]{0,1} %divide.379, f32[512,1024]{1,0} %multiply.1247), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.1248 = f32[512,1024]{0,1} multiply(f32[512,1024]{0,1} %add.779, f32[512,1024]{1,0} %broadcast.1200), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.780 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %param.68, f32[512,1024]{0,1} %multiply.1248), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reduce.153 = f32[1024]{0} reduce(f32[4,1024,1024]{2,1,0} %convert.168, f32[] %constant.69), dimensions={0,1}, to_apply=%region_57.2095, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %all-reduce.109 = f32[1024]{0} all-reduce(f32[1024]{0} %reduce.153), channel_id=110, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_57.2095, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %multiply.1249 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.109, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.215 = f32[1024]{0} parameter(144), sharding={replicated}
  %multiply.1250 = f32[1024]{0} multiply(f32[1024]{0} %param.215, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.781 = f32[1024]{0} add(f32[1024]{0} %multiply.1249, f32[1024]{0} %multiply.1250), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1251 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.109, f32[1024]{0} %all-reduce.109), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1252 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1251, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.216 = f32[1024]{0} parameter(221), sharding={replicated}
  %multiply.1253 = f32[1024]{0} multiply(f32[1024]{0} %param.216, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.782 = f32[1024]{0} add(f32[1024]{0} %multiply.1252, f32[1024]{0} %multiply.1253), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.380 = f32[1024]{0} divide(f32[1024]{0} %add.782, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.65 = f32[1024]{0} sqrt(f32[1024]{0} %divide.380), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.784 = f32[1024]{0} add(f32[1024]{0} %sqrt.65, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1254 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.784)
  %divide.381 = f32[1024]{0} divide(f32[1024]{0} %add.781, f32[1024]{0} %multiply.1254), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1256 = f32[1024]{0} multiply(f32[1024]{0} %divide.381, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.785 = f32[1024]{0} add(f32[1024]{0} %param.77, f32[1024]{0} %multiply.1256), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %multiply.1257 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.597, f32[4,1024,1024]{2,1,0} %multiply.398), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.154 = f32[1024]{0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1257, f32[] %constant.69), dimensions={0,1}, to_apply=%region_58.2105, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %all-reduce.110 = f32[1024]{0} all-reduce(f32[1024]{0} %reduce.154), channel_id=111, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_58.2105, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.1258 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.110, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.217 = f32[1024]{0} parameter(145), sharding={replicated}
  %multiply.1259 = f32[1024]{0} multiply(f32[1024]{0} %param.217, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.786 = f32[1024]{0} add(f32[1024]{0} %multiply.1258, f32[1024]{0} %multiply.1259), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1260 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.110, f32[1024]{0} %all-reduce.110), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1261 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1260, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.218 = f32[1024]{0} parameter(222), sharding={replicated}
  %multiply.1263 = f32[1024]{0} multiply(f32[1024]{0} %param.218, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.787 = f32[1024]{0} add(f32[1024]{0} %multiply.1261, f32[1024]{0} %multiply.1263), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.382 = f32[1024]{0} divide(f32[1024]{0} %add.787, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.66 = f32[1024]{0} sqrt(f32[1024]{0} %divide.382), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.788 = f32[1024]{0} add(f32[1024]{0} %sqrt.66, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1264 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.788)
  %divide.383 = f32[1024]{0} divide(f32[1024]{0} %add.786, f32[1024]{0} %multiply.1264), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1265 = f32[1024]{0} multiply(f32[1024]{0} %divide.383, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.789 = f32[1024]{0} add(f32[1024]{0} %param.76, f32[1024]{0} %multiply.1265), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reduce.155 = f16[1024]{0} reduce(f16[4,1024,1024]{2,1,0} %add.279, f16[] %constant.89), dimensions={0,1}, to_apply=%region_61.2144, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %all-reduce.111 = f16[1024]{0} all-reduce(f16[1024]{0} %reduce.155), channel_id=112, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_61.2144, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %convert.324 = f32[1024]{0} convert(f16[1024]{0} %all-reduce.111), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1266 = f32[1024]{0} multiply(f32[1024]{0} %convert.324, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.219 = f32[1024]{0} parameter(146), sharding={replicated}
  %multiply.1267 = f32[1024]{0} multiply(f32[1024]{0} %param.219, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.791 = f32[1024]{0} add(f32[1024]{0} %multiply.1266, f32[1024]{0} %multiply.1267), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1268 = f32[1024]{0} multiply(f32[1024]{0} %convert.324, f32[1024]{0} %convert.324), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1269 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1268, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.220 = f32[1024]{0} parameter(223), sharding={replicated}
  %multiply.1270 = f32[1024]{0} multiply(f32[1024]{0} %param.220, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.792 = f32[1024]{0} add(f32[1024]{0} %multiply.1269, f32[1024]{0} %multiply.1270), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.384 = f32[1024]{0} divide(f32[1024]{0} %add.792, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.67 = f32[1024]{0} sqrt(f32[1024]{0} %divide.384), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.793 = f32[1024]{0} add(f32[1024]{0} %sqrt.67, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1271 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.793)
  %divide.385 = f32[1024]{0} divide(f32[1024]{0} %add.791, f32[1024]{0} %multiply.1271), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1272 = f32[1024]{0} multiply(f32[1024]{0} %divide.385, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.794 = f32[1024]{0} add(f32[1024]{0} %param.75, f32[1024]{0} %multiply.1272), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %transpose.410 = f16[1024,4,1024]{0,2,1} transpose(f16[4,1024,1024]{2,1,0} %add.279), dimensions={2,0,1}
  %reshape.2856 = f16[1024,4096]{1,0} reshape(f16[1024,4,1024]{0,2,1} %transpose.410)
  %dot.361 = f16[128,1024]{1,0} dot(f16[4096,128]{1,0} %reshape.1898, f16[1024,4096]{1,0} %reshape.2856), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.112 = f16[128,1024]{1,0} all-reduce(f16[128,1024]{1,0} %dot.361), channel_id=113, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%add.62, sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.325 = f32[128,1024]{0,1} convert(f16[128,1024]{1,0} %all-reduce.112), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1273 = f32[128,1024]{0,1} multiply(f32[128,1024]{0,1} %convert.325, f32[128,1024]{1,0} %broadcast.1141), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.221 = f32[128,1024]{1,0} parameter(147), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1274 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param.221, f32[128,1024]{1,0} %broadcast.1142), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.795 = f32[128,1024]{0,1} add(f32[128,1024]{0,1} %multiply.1273, f32[128,1024]{1,0} %multiply.1274), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1277 = f32[128,1024]{0,1} multiply(f32[128,1024]{0,1} %convert.325, f32[128,1024]{0,1} %convert.325), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1278 = f32[128,1024]{0,1} multiply(f32[128,1024]{0,1} %multiply.1277, f32[128,1024]{1,0} %broadcast.1143), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.222 = f32[128,1024]{1,0} parameter(224), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1279 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param.222, f32[128,1024]{1,0} %broadcast.1144), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.796 = f32[128,1024]{0,1} add(f32[128,1024]{0,1} %multiply.1278, f32[128,1024]{1,0} %multiply.1279), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.386 = f32[128,1024]{0,1} divide(f32[128,1024]{0,1} %add.796, f32[128,1024]{1,0} %broadcast.856), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.68 = f32[128,1024]{0,1} sqrt(f32[128,1024]{0,1} %divide.386), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.797 = f32[128,1024]{0,1} add(f32[128,1024]{0,1} %sqrt.68, f32[128,1024]{1,0} %broadcast.1146), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1280 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %broadcast.851, f32[128,1024]{0,1} %add.797)
  %divide.387 = f32[128,1024]{0,1} divide(f32[128,1024]{0,1} %add.795, f32[128,1024]{1,0} %multiply.1280), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1281 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param.74, f32[128,1024]{1,0} %broadcast.1148), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.798 = f32[128,1024]{0,1} add(f32[128,1024]{0,1} %divide.387, f32[128,1024]{1,0} %multiply.1281), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.1282 = f32[128,1024]{0,1} multiply(f32[128,1024]{0,1} %add.798, f32[128,1024]{1,0} %broadcast.1149), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.799 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %param.74, f32[128,1024]{0,1} %multiply.1282), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reshape.2858 = f16[4,1024,384]{2,1,0} reshape(f16[4,1024,128,3]{3,2,1,0} %add.282), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/reshape[new_sizes=(8, 1024, 3072) dimensions=None]" source_file="/code/alpa/alpa/model/bert_model.py" source_line=161}
  %reduce.156 = f16[384]{0} reduce(f16[4,1024,384]{2,1,0} %reshape.2858, f16[] %constant.89), dimensions={0,1}, to_apply=%region_64.2200, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %all-reduce.113 = f16[384]{0} all-reduce(f16[384]{0} %reduce.156), channel_id=114, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_64.2200, sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %convert.326 = f32[384]{0} convert(f16[384]{0} %all-reduce.113), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1283 = f32[384]{0} multiply(f32[384]{0} %convert.326, f32[384]{0} %broadcast.1158), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.223 = f32[384]{0} parameter(148), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1284 = f32[384]{0} multiply(f32[384]{0} %param.223, f32[384]{0} %broadcast.1160), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.800 = f32[384]{0} add(f32[384]{0} %multiply.1283, f32[384]{0} %multiply.1284), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1285 = f32[384]{0} multiply(f32[384]{0} %convert.326, f32[384]{0} %convert.326), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1286 = f32[384]{0} multiply(f32[384]{0} %multiply.1285, f32[384]{0} %broadcast.1162), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.224 = f32[384]{0} parameter(225), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1287 = f32[384]{0} multiply(f32[384]{0} %param.224, f32[384]{0} %broadcast.1163), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.801 = f32[384]{0} add(f32[384]{0} %multiply.1286, f32[384]{0} %multiply.1287), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.388 = f32[384]{0} divide(f32[384]{0} %add.801, f32[384]{0} %broadcast.878), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.69 = f32[384]{0} sqrt(f32[384]{0} %divide.388), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.802 = f32[384]{0} add(f32[384]{0} %sqrt.69, f32[384]{0} %broadcast.1164), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1288 = f32[384]{0} multiply(f32[384]{0} %broadcast.875, f32[384]{0} %add.802)
  %divide.389 = f32[384]{0} divide(f32[384]{0} %add.800, f32[384]{0} %multiply.1288), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1289 = f32[384]{0} multiply(f32[384]{0} %divide.389, f32[384]{0} %broadcast.1165), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.803 = f32[384]{0} add(f32[384]{0} %param.73, f32[384]{0} %multiply.1289), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %transpose.411 = f16[384,4,1024]{0,2,1} transpose(f16[4,1024,384]{2,1,0} %reshape.2858), dimensions={2,0,1}
  %reshape.2859 = f16[384,4096]{1,0} reshape(f16[384,4,1024]{0,2,1} %transpose.411)
  %dot.362 = f16[1024,384]{1,0} dot(f16[4096,1024]{1,0} %reshape.1859, f16[384,4096]{1,0} %reshape.2859), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.114 = f16[1024,384]{1,0} all-reduce(f16[1024,384]{1,0} %dot.362), channel_id=115, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%add.63, sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.327 = f32[1024,384]{0,1} convert(f16[1024,384]{1,0} %all-reduce.114), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/attention/self/qvk_combined/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1290 = f32[1024,384]{0,1} multiply(f32[1024,384]{0,1} %convert.327, f32[1024,384]{1,0} %broadcast.1166), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.225 = f32[1024,384]{1,0} parameter(149), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1291 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param.225, f32[1024,384]{1,0} %broadcast.1167), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.804 = f32[1024,384]{0,1} add(f32[1024,384]{0,1} %multiply.1290, f32[1024,384]{1,0} %multiply.1291), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1292 = f32[1024,384]{0,1} multiply(f32[1024,384]{0,1} %convert.327, f32[1024,384]{0,1} %convert.327), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1293 = f32[1024,384]{0,1} multiply(f32[1024,384]{0,1} %multiply.1292, f32[1024,384]{1,0} %broadcast.1168), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.226 = f32[1024,384]{1,0} parameter(226), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1294 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param.226, f32[1024,384]{1,0} %broadcast.1169), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.806 = f32[1024,384]{0,1} add(f32[1024,384]{0,1} %multiply.1293, f32[1024,384]{1,0} %multiply.1294), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.390 = f32[1024,384]{0,1} divide(f32[1024,384]{0,1} %add.806, f32[1024,384]{1,0} %broadcast.889), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.70 = f32[1024,384]{0,1} sqrt(f32[1024,384]{0,1} %divide.390), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.807 = f32[1024,384]{0,1} add(f32[1024,384]{0,1} %sqrt.70, f32[1024,384]{1,0} %broadcast.1170), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1295 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %broadcast.883, f32[1024,384]{0,1} %add.807)
  %divide.391 = f32[1024,384]{0,1} divide(f32[1024,384]{0,1} %add.804, f32[1024,384]{1,0} %multiply.1295), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1296 = f32[1024,384]{1,0} multiply(f32[1024,384]{1,0} %param.72, f32[1024,384]{1,0} %broadcast.1171), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.808 = f32[1024,384]{0,1} add(f32[1024,384]{0,1} %divide.391, f32[1024,384]{1,0} %multiply.1296), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.1297 = f32[1024,384]{0,1} multiply(f32[1024,384]{0,1} %add.808, f32[1024,384]{1,0} %broadcast.1172), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.809 = f32[1024,384]{1,0} add(f32[1024,384]{1,0} %param.72, f32[1024,384]{0,1} %multiply.1297), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reduce.157 = f16[512]{0} reduce(f16[4,1024,512]{2,1,0} %add.275, f16[] %constant.89), dimensions={0,1}, to_apply=%region_56.2083, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %all-reduce.115 = f16[512]{0} all-reduce(f16[512]{0} %reduce.157), channel_id=116, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_56.2083, sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %convert.328 = f32[512]{0} convert(f16[512]{0} %all-reduce.115), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1298 = f32[512]{0} multiply(f32[512]{0} %convert.328, f32[512]{0} %broadcast.1173), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.227 = f32[512]{0} parameter(150), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1299 = f32[512]{0} multiply(f32[512]{0} %param.227, f32[512]{0} %broadcast.1174), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.810 = f32[512]{0} add(f32[512]{0} %multiply.1298, f32[512]{0} %multiply.1299), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1300 = f32[512]{0} multiply(f32[512]{0} %convert.328, f32[512]{0} %convert.328), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1301 = f32[512]{0} multiply(f32[512]{0} %multiply.1300, f32[512]{0} %broadcast.1175), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.228 = f32[512]{0} parameter(227), sharding={devices=[8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1302 = f32[512]{0} multiply(f32[512]{0} %param.228, f32[512]{0} %broadcast.1176), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.811 = f32[512]{0} add(f32[512]{0} %multiply.1301, f32[512]{0} %multiply.1302), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.392 = f32[512]{0} divide(f32[512]{0} %add.811, f32[512]{0} %broadcast.901), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.71 = f32[512]{0} sqrt(f32[512]{0} %divide.392), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.812 = f32[512]{0} add(f32[512]{0} %sqrt.71, f32[512]{0} %broadcast.1178), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1303 = f32[512]{0} multiply(f32[512]{0} %broadcast.898, f32[512]{0} %add.812)
  %divide.393 = f32[512]{0} divide(f32[512]{0} %add.810, f32[512]{0} %multiply.1303), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1304 = f32[512]{0} multiply(f32[512]{0} %divide.393, f32[512]{0} %broadcast.1181), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.813 = f32[512]{0} add(f32[512]{0} %param.79, f32[512]{0} %multiply.1304), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %transpose.412 = f16[512,4,1024]{0,2,1} transpose(f16[4,1024,512]{2,1,0} %add.275), dimensions={2,0,1}
  %reshape.2861 = f16[512,4096]{1,0} reshape(f16[512,4,1024]{0,2,1} %transpose.412)
  %dot.363 = f16[1024,512]{1,0} dot(f16[4096,1024]{1,0} %reshape.1911, f16[512,4096]{1,0} %reshape.2861), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.116 = f16[1024,512]{1,0} all-reduce(f16[1024,512]{1,0} %dot.363), channel_id=117, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%add.64, sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.329 = f32[1024,512]{0,1} convert(f16[1024,512]{1,0} %all-reduce.116), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/intermediate/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1305 = f32[1024,512]{0,1} multiply(f32[1024,512]{0,1} %convert.329, f32[1024,512]{1,0} %broadcast.1183), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.229 = f32[1024,512]{1,0} parameter(151), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1306 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param.229, f32[1024,512]{1,0} %broadcast.1184), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.815 = f32[1024,512]{0,1} add(f32[1024,512]{0,1} %multiply.1305, f32[1024,512]{1,0} %multiply.1306), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1307 = f32[1024,512]{0,1} multiply(f32[1024,512]{0,1} %convert.329, f32[1024,512]{0,1} %convert.329), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1308 = f32[1024,512]{0,1} multiply(f32[1024,512]{0,1} %multiply.1307, f32[1024,512]{1,0} %broadcast.1185), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.230 = f32[1024,512]{1,0} parameter(228), sharding={devices=[1,8,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1309 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param.230, f32[1024,512]{1,0} %broadcast.1187), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.816 = f32[1024,512]{0,1} add(f32[1024,512]{0,1} %multiply.1308, f32[1024,512]{1,0} %multiply.1309), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.394 = f32[1024,512]{0,1} divide(f32[1024,512]{0,1} %add.816, f32[1024,512]{1,0} %broadcast.911), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.72 = f32[1024,512]{0,1} sqrt(f32[1024,512]{0,1} %divide.394), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.817 = f32[1024,512]{0,1} add(f32[1024,512]{0,1} %sqrt.72, f32[1024,512]{1,0} %broadcast.1189), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1310 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %broadcast.908, f32[1024,512]{0,1} %add.817)
  %divide.395 = f32[1024,512]{0,1} divide(f32[1024,512]{0,1} %add.815, f32[1024,512]{1,0} %multiply.1310), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1311 = f32[1024,512]{1,0} multiply(f32[1024,512]{1,0} %param.78, f32[1024,512]{1,0} %broadcast.1190), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.818 = f32[1024,512]{0,1} add(f32[1024,512]{0,1} %divide.395, f32[1024,512]{1,0} %multiply.1311), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.1312 = f32[1024,512]{0,1} multiply(f32[1024,512]{0,1} %add.818, f32[1024,512]{1,0} %broadcast.1191), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.819 = f32[1024,512]{1,0} add(f32[1024,512]{1,0} %param.78, f32[1024,512]{0,1} %multiply.1312), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reduce.158 = f32[1024]{0} reduce(f32[4,1024,1024]{2,1,0} %convert.165, f32[] %constant.69), dimensions={0,1}, to_apply=%region_51.2015, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %all-reduce.117 = f32[1024]{0} all-reduce(f32[1024]{0} %reduce.158), channel_id=118, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_51.2015, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=150}
  %multiply.1313 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.117, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.231 = f32[1024]{0} parameter(152), sharding={replicated}
  %multiply.1314 = f32[1024]{0} multiply(f32[1024]{0} %param.231, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.821 = f32[1024]{0} add(f32[1024]{0} %multiply.1313, f32[1024]{0} %multiply.1314), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1315 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.117, f32[1024]{0} %all-reduce.117), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1316 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1315, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.232 = f32[1024]{0} parameter(229), sharding={replicated}
  %multiply.1317 = f32[1024]{0} multiply(f32[1024]{0} %param.232, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.823 = f32[1024]{0} add(f32[1024]{0} %multiply.1316, f32[1024]{0} %multiply.1317), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.396 = f32[1024]{0} divide(f32[1024]{0} %add.823, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.73 = f32[1024]{0} sqrt(f32[1024]{0} %divide.396), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.825 = f32[1024]{0} add(f32[1024]{0} %sqrt.73, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1318 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.825)
  %divide.397 = f32[1024]{0} divide(f32[1024]{0} %add.821, f32[1024]{0} %multiply.1318), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1319 = f32[1024]{0} multiply(f32[1024]{0} %divide.397, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.827 = f32[1024]{0} add(f32[1024]{0} %param.83, f32[1024]{0} %multiply.1319), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %multiply.1320 = f32[4,1024,1024]{2,1,0} multiply(f32[4,1024,1024]{2,1,0} %broadcast.620, f32[4,1024,1024]{2,1,0} %multiply.380), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/mul" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %reduce.159 = f32[1024]{0} reduce(f32[4,1024,1024]{2,1,0} %multiply.1320, f32[] %constant.69), dimensions={0,1}, to_apply=%region_52.2025, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %all-reduce.118 = f32[1024]{0} all-reduce(f32[1024]{0} %reduce.159), channel_id=119, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_52.2025, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/LayerNorm/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/normalization.py" source_line=144}
  %multiply.1321 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.118, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.233 = f32[1024]{0} parameter(153), sharding={replicated}
  %multiply.1322 = f32[1024]{0} multiply(f32[1024]{0} %param.233, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.829 = f32[1024]{0} add(f32[1024]{0} %multiply.1321, f32[1024]{0} %multiply.1322), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1323 = f32[1024]{0} multiply(f32[1024]{0} %all-reduce.118, f32[1024]{0} %all-reduce.118), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1324 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1323, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.234 = f32[1024]{0} parameter(230), sharding={replicated}
  %multiply.1325 = f32[1024]{0} multiply(f32[1024]{0} %param.234, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.831 = f32[1024]{0} add(f32[1024]{0} %multiply.1324, f32[1024]{0} %multiply.1325), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.398 = f32[1024]{0} divide(f32[1024]{0} %add.831, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.74 = f32[1024]{0} sqrt(f32[1024]{0} %divide.398), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.833 = f32[1024]{0} add(f32[1024]{0} %sqrt.74, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1326 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.833)
  %divide.399 = f32[1024]{0} divide(f32[1024]{0} %add.829, f32[1024]{0} %multiply.1326), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1327 = f32[1024]{0} multiply(f32[1024]{0} %divide.399, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.835 = f32[1024]{0} add(f32[1024]{0} %param.82, f32[1024]{0} %multiply.1327), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %reduce.160 = f16[1024]{0} reduce(f16[4,1024,1024]{2,1,0} %add.274, f16[] %constant.89), dimensions={0,1}, to_apply=%region_55.2064, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %all-reduce.119 = f16[1024]{0} all-reduce(f16[1024]{0} %reduce.160), channel_id=120, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%region_55.2064, sharding={replicated}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/dense/reduce_sum[axes=(0, 1)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=200}
  %convert.330 = f32[1024]{0} convert(f16[1024]{0} %all-reduce.119), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1328 = f32[1024]{0} multiply(f32[1024]{0} %convert.330, f32[1024]{0} %broadcast.836), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.235 = f32[1024]{0} parameter(154), sharding={replicated}
  %multiply.1329 = f32[1024]{0} multiply(f32[1024]{0} %param.235, f32[1024]{0} %broadcast.837), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.837 = f32[1024]{0} add(f32[1024]{0} %multiply.1328, f32[1024]{0} %multiply.1329), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1330 = f32[1024]{0} multiply(f32[1024]{0} %convert.330, f32[1024]{0} %convert.330), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1331 = f32[1024]{0} multiply(f32[1024]{0} %multiply.1330, f32[1024]{0} %broadcast.839), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.236 = f32[1024]{0} parameter(231), sharding={replicated}
  %multiply.1332 = f32[1024]{0} multiply(f32[1024]{0} %param.236, f32[1024]{0} %broadcast.840), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.839 = f32[1024]{0} add(f32[1024]{0} %multiply.1331, f32[1024]{0} %multiply.1332), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.400 = f32[1024]{0} divide(f32[1024]{0} %add.839, f32[1024]{0} %broadcast.841), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.75 = f32[1024]{0} sqrt(f32[1024]{0} %divide.400), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.841 = f32[1024]{0} add(f32[1024]{0} %sqrt.75, f32[1024]{0} %broadcast.842), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1333 = f32[1024]{0} multiply(f32[1024]{0} %broadcast.838, f32[1024]{0} %add.841)
  %divide.401 = f32[1024]{0} divide(f32[1024]{0} %add.837, f32[1024]{0} %multiply.1333), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1334 = f32[1024]{0} multiply(f32[1024]{0} %divide.401, f32[1024]{0} %broadcast.843), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.843 = f32[1024]{0} add(f32[1024]{0} %param.81, f32[1024]{0} %multiply.1334), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  %transpose.413 = f16[1024,4,1024]{0,2,1} transpose(f16[4,1024,1024]{2,1,0} %add.274), dimensions={2,0,1}
  %reshape.2864 = f16[1024,4096]{1,0} reshape(f16[1024,4,1024]{0,2,1} %transpose.413)
  %dot.364 = f16[512,1024]{1,0} dot(f16[4096,512]{1,0} %reshape.1930, f16[1024,4096]{1,0} %reshape.2864), lhs_contracting_dims={0}, rhs_contracting_dims={1}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %all-reduce.120 = f16[512,1024]{1,0} all-reduce(f16[512,1024]{1,0} %dot.364), channel_id=121, replica_groups={{0,8},{1,9},{2,10},{3,11},{4,12},{5,13},{6,14},{7,15}}, use_global_device_ids=true, to_apply=%add.65, sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}, metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/dense/transpose[permutation=(1, 0)]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/linear.py" source_line=196}
  %convert.331 = f32[512,1024]{0,1} convert(f16[512,1024]{1,0} %all-reduce.120), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/transpose(jvp(FlaxGPTForLMModule))/transformers/encoder/layer/5/remat(core_fn)/5/output/dense/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/python3.9-env/lib/python3.9/site-packages/flax/linen/dtypes.py" source_line=97}
  %multiply.1335 = f32[512,1024]{0,1} multiply(f32[512,1024]{0,1} %convert.331, f32[512,1024]{1,0} %broadcast.1192), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %param.237 = f32[512,1024]{1,0} parameter(155), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1336 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param.237, f32[512,1024]{1,0} %broadcast.1193), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %add.844 = f32[512,1024]{0,1} add(f32[512,1024]{0,1} %multiply.1335, f32[512,1024]{1,0} %multiply.1336), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=84}
  %multiply.1337 = f32[512,1024]{0,1} multiply(f32[512,1024]{0,1} %convert.331, f32[512,1024]{0,1} %convert.331), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=98}
  %multiply.1338 = f32[512,1024]{0,1} multiply(f32[512,1024]{0,1} %multiply.1337, f32[512,1024]{1,0} %broadcast.1194), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %param.238 = f32[512,1024]{1,0} parameter(232), sharding={devices=[8,1,2]0,8,1,9,2,10,3,11,4,12,5,13,6,14,7,15 last_tile_dim_replicate}
  %multiply.1339 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param.238, f32[512,1024]{1,0} %broadcast.1196), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %add.845 = f32[512,1024]{0,1} add(f32[512,1024]{0,1} %multiply.1338, f32[512,1024]{1,0} %multiply.1339), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=107}
  %divide.402 = f32[512,1024]{0,1} divide(f32[512,1024]{0,1} %add.845, f32[512,1024]{1,0} %broadcast.922), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=114}
  %sqrt.76 = f32[512,1024]{0,1} sqrt(f32[512,1024]{0,1} %divide.402), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/sqrt" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %add.847 = f32[512,1024]{0,1} add(f32[512,1024]{0,1} %sqrt.76, f32[512,1024]{1,0} %broadcast.1198), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1340 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %broadcast.917, f32[512,1024]{0,1} %add.847)
  %divide.403 = f32[512,1024]{0,1} divide(f32[512,1024]{0,1} %add.844, f32[512,1024]{1,0} %multiply.1340), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/div" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=340}
  %multiply.1341 = f32[512,1024]{1,0} multiply(f32[512,1024]{1,0} %param.80, f32[512,1024]{1,0} %broadcast.1199), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %add.848 = f32[512,1024]{0,1} add(f32[512,1024]{0,1} %divide.403, f32[512,1024]{1,0} %multiply.1341), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=657}
  %multiply.1342 = f32[512,1024]{0,1} multiply(f32[512,1024]{0,1} %add.848, f32[512,1024]{1,0} %broadcast.1200), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/mul" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/transform.py" source_line=405}
  %add.849 = f32[512,1024]{1,0} add(f32[512,1024]{1,0} %param.80, f32[512,1024]{0,1} %multiply.1342), metadata={op_name="parallelize(train_step_shard_parallel)/jit(main)/add" source_file="/python3.9-env/lib/python3.9/site-packages/optax/_src/update.py" source_line=43}
  ROOT %tuple.8 = (s32[], f32[6400]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, /*index=5*/f32[6400,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, /*index=10*/f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, f32[1024,512]{1,0}, f32[1024]{0}, /*index=15*/f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=20*/f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, /*index=25*/f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, /*index=30*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, /*index=35*/f32[1024,384]{1,0}, f32[512]{0}, f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, /*index=40*/f32[1024]{0}, f32[512,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=45*/f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, f32[1024,512]{1,0}, /*index=50*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, f32[1024]{0}, /*index=55*/f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, f32[384]{0}, f32[1024,384]{1,0}, /*index=60*/f32[512]{0}, f32[1024,512]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=65*/f32[512,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{1,0}, /*index=70*/f32[384]{0}, f32[1024,384]{1,0}, f32[512]{0}, f32[1024,512]{1,0}, f32[1024]{0}, /*index=75*/f32[1024]{0}, f32[1024]{0}, f32[512,1024]{1,0}, s32[], f32[6400]{0}, /*index=80*/f32[1024]{0}, f32[1024]{0}, f32[128,1024]{0,1}, f32[6400,1024]{1,0}, f32[1024]{0}, /*index=85*/f32[1024]{0}, f32[1024]{0}, f32[128,1024]{0,1}, f32[384]{0}, f32[1024,384]{0,1}, /*index=90*/f32[512]{0}, f32[1024,512]{0,1}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=95*/f32[512,1024]{0,1}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{0,1}, /*index=100*/f32[384]{0}, f32[1024,384]{0,1}, f32[512]{0}, f32[1024,512]{0,1}, f32[1024]{0}, /*index=105*/f32[1024]{0}, f32[1024]{0}, f32[512,1024]{0,1}, f32[1024]{0}, f32[1024]{0}, /*index=110*/f32[1024]{0}, f32[128,1024]{0,1}, f32[384]{0}, f32[1024,384]{0,1}, f32[512]{0}, /*index=115*/f32[1024,512]{0,1}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{0,1}, /*index=120*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{0,1}, f32[384]{0}, /*index=125*/f32[1024,384]{0,1}, f32[512]{0}, f32[1024,512]{0,1}, f32[1024]{0}, f32[1024]{0}, /*index=130*/f32[1024]{0}, f32[512,1024]{0,1}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=135*/f32[128,1024]{0,1}, f32[384]{0}, f32[1024,384]{0,1}, f32[512]{0}, f32[1024,512]{0,1}, /*index=140*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{0,1}, f32[1024]{0}, /*index=145*/f32[1024]{0}, f32[1024]{0}, f32[128,1024]{0,1}, f32[384]{0}, f32[1024,384]{0,1}, /*index=150*/f32[512]{0}, f32[1024,512]{0,1}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=155*/f32[512,1024]{0,1}, f32[6400]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{0,1}, /*index=160*/f32[6400,1024]{1,0}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{0,1}, /*index=165*/f32[384]{0}, f32[1024,384]{0,1}, f32[512]{0}, f32[1024,512]{0,1}, f32[1024]{0}, /*index=170*/f32[1024]{0}, f32[1024]{0}, f32[512,1024]{0,1}, f32[1024]{0}, f32[1024]{0}, /*index=175*/f32[1024]{0}, f32[128,1024]{0,1}, f32[384]{0}, f32[1024,384]{0,1}, f32[512]{0}, /*index=180*/f32[1024,512]{0,1}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{0,1}, /*index=185*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{0,1}, f32[384]{0}, /*index=190*/f32[1024,384]{0,1}, f32[512]{0}, f32[1024,512]{0,1}, f32[1024]{0}, f32[1024]{0}, /*index=195*/f32[1024]{0}, f32[512,1024]{0,1}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=200*/f32[128,1024]{0,1}, f32[384]{0}, f32[1024,384]{0,1}, f32[512]{0}, f32[1024,512]{0,1}, /*index=205*/f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[512,1024]{0,1}, f32[1024]{0}, /*index=210*/f32[1024]{0}, f32[1024]{0}, f32[128,1024]{0,1}, f32[384]{0}, f32[1024,384]{0,1}, /*index=215*/f32[512]{0}, f32[1024,512]{0,1}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, /*index=220*/f32[512,1024]{0,1}, f32[1024]{0}, f32[1024]{0}, f32[1024]{0}, f32[128,1024]{0,1}, /*index=225*/f32[384]{0}, f32[1024,384]{0,1}, f32[512]{0}, f32[1024,512]{0,1}, f32[1024]{0}, /*index=230*/f32[1024]{0}, f32[1024]{0}, f32[512,1024]{0,1}) tuple(s32[] %add.92, f32[6400]{0} %add.248, f32[1024]{0} %add.462, f32[1024]{0} %add.467, f32[128,1024]{1,0} %add.476, /*index=5*/f32[6400,1024]{1,0} %add.483, f32[1024]{0} %add.487, f32[1024]{0} %add.491, f32[1024]{0} %add.495, f32[128,1024]{1,0} %add.501, /*index=10*/f32[384]{0} %add.505, f32[1024,384]{1,0} %add.510, f32[512]{0} %add.515, f32[1024,512]{1,0} %add.520, f32[1024]{0} %add.525, /*index=15*/f32[1024]{0} %add.529, f32[1024]{0} %add.533, f32[512,1024]{1,0} %add.539, f32[1024]{0} %add.543, f32[1024]{0} %add.547, /*index=20*/f32[1024]{0} %add.551, f32[128,1024]{1,0} %add.557, f32[384]{0} %add.561, f32[1024,384]{1,0} %add.566, f32[512]{0} %add.570, /*index=25*/f32[1024,512]{1,0} %add.577, f32[1024]{0} %add.582, f32[1024]{0} %add.586, f32[1024]{0} %add.590, f32[512,1024]{1,0} %add.596, /*index=30*/f32[1024]{0} %add.601, f32[1024]{0} %add.605, f32[1024]{0} %add.609, f32[128,1024]{1,0} %add.615, f32[384]{0} %add.619, /*index=35*/f32[1024,384]{1,0} %add.625, f32[512]{0} %add.631, f32[1024,512]{1,0} %add.641, f32[1024]{0} %add.649, f32[1024]{0} %add.655, /*index=40*/f32[1024]{0} %add.659, f32[512,1024]{1,0} %add.666, f32[1024]{0} %add.670, f32[1024]{0} %add.675, f32[1024]{0} %add.679, /*index=45*/f32[128,1024]{1,0} %add.685, f32[384]{0} %add.689, f32[1024,384]{1,0} %add.695, f32[512]{0} %add.699, f32[1024,512]{1,0} %add.704, /*index=50*/f32[1024]{0} %add.709, f32[1024]{0} %add.713, f32[1024]{0} %add.718, f32[512,1024]{1,0} %add.723, f32[1024]{0} %add.727, /*index=55*/f32[1024]{0} %add.732, f32[1024]{0} %add.736, f32[128,1024]{1,0} %add.741, f32[384]{0} %add.745, f32[1024,384]{1,0} %add.751, /*index=60*/f32[512]{0} %add.755, f32[1024,512]{1,0} %add.760, f32[1024]{0} %add.766, f32[1024]{0} %add.770, f32[1024]{0} %add.775, /*index=65*/f32[512,1024]{1,0} %add.780, f32[1024]{0} %add.785, f32[1024]{0} %add.789, f32[1024]{0} %add.794, f32[128,1024]{1,0} %add.799, /*index=70*/f32[384]{0} %add.803, f32[1024,384]{1,0} %add.809, f32[512]{0} %add.813, f32[1024,512]{1,0} %add.819, f32[1024]{0} %add.827, /*index=75*/f32[1024]{0} %add.835, f32[1024]{0} %add.843, f32[512,1024]{1,0} %add.849, s32[] %select.5, f32[6400]{0} %add.244, /*index=80*/f32[1024]{0} %add.459, f32[1024]{0} %add.463, f32[128,1024]{0,1} %add.472, f32[6400,1024]{1,0} %add.479, f32[1024]{0} %add.484, /*index=85*/f32[1024]{0} %add.488, f32[1024]{0} %add.492, f32[128,1024]{0,1} %add.497, f32[384]{0} %add.502, f32[1024,384]{0,1} %add.506, /*index=90*/f32[512]{0} %add.511, f32[1024,512]{0,1} %add.516, f32[1024]{0} %add.521, f32[1024]{0} %add.526, f32[1024]{0} %add.530, /*index=95*/f32[512,1024]{0,1} %add.534, f32[1024]{0} %add.540, f32[1024]{0} %add.544, f32[1024]{0} %add.548, f32[128,1024]{0,1} %add.552, /*index=100*/f32[384]{0} %add.558, f32[1024,384]{0,1} %add.562, f32[512]{0} %add.567, f32[1024,512]{0,1} %add.571, f32[1024]{0} %add.578, /*index=105*/f32[1024]{0} %add.583, f32[1024]{0} %add.587, f32[512,1024]{0,1} %add.592, f32[1024]{0} %add.597, f32[1024]{0} %add.602, /*index=110*/f32[1024]{0} %add.606, f32[128,1024]{0,1} %add.610, f32[384]{0} %add.616, f32[1024,384]{0,1} %add.620, f32[512]{0} %add.626, /*index=115*/f32[1024,512]{0,1} %add.633, f32[1024]{0} %add.643, f32[1024]{0} %add.651, f32[1024]{0} %add.656, f32[512,1024]{0,1} %add.660, /*index=120*/f32[1024]{0} %add.667, f32[1024]{0} %add.672, f32[1024]{0} %add.676, f32[128,1024]{0,1} %add.680, f32[384]{0} %add.686, /*index=125*/f32[1024,384]{0,1} %add.691, f32[512]{0} %add.696, f32[1024,512]{0,1} %add.700, f32[1024]{0} %add.706, f32[1024]{0} %add.710, /*index=130*/f32[1024]{0} %add.715, f32[512,1024]{0,1} %add.719, f32[1024]{0} %add.724, f32[1024]{0} %add.728, f32[1024]{0} %add.733, /*index=135*/f32[128,1024]{0,1} %add.737, f32[384]{0} %add.742, f32[1024,384]{0,1} %add.747, f32[512]{0} %add.752, f32[1024,512]{0,1} %add.756, /*index=140*/f32[1024]{0} %add.761, f32[1024]{0} %add.767, f32[1024]{0} %add.772, f32[512,1024]{0,1} %add.776, f32[1024]{0} %add.781, /*index=145*/f32[1024]{0} %add.786, f32[1024]{0} %add.791, f32[128,1024]{0,1} %add.795, f32[384]{0} %add.800, f32[1024,384]{0,1} %add.804, /*index=150*/f32[512]{0} %add.810, f32[1024,512]{0,1} %add.815, f32[1024]{0} %add.821, f32[1024]{0} %add.829, f32[1024]{0} %add.837, /*index=155*/f32[512,1024]{0,1} %add.844, f32[6400]{0} %add.246, f32[1024]{0} %add.460, f32[1024]{0} %add.465, f32[128,1024]{0,1} %add.473, /*index=160*/f32[6400,1024]{1,0} %add.480, f32[1024]{0} %add.485, f32[1024]{0} %add.489, f32[1024]{0} %add.493, f32[128,1024]{0,1} %add.498, /*index=165*/f32[384]{0} %add.503, f32[1024,384]{0,1} %add.507, f32[512]{0} %add.512, f32[1024,512]{0,1} %add.517, f32[1024]{0} %add.523, /*index=170*/f32[1024]{0} %add.527, f32[1024]{0} %add.531, f32[512,1024]{0,1} %add.535, f32[1024]{0} %add.541, f32[1024]{0} %add.545, /*index=175*/f32[1024]{0} %add.549, f32[128,1024]{0,1} %add.553, f32[384]{0} %add.559, f32[1024,384]{0,1} %add.563, f32[512]{0} %add.568, /*index=180*/f32[1024,512]{0,1} %add.574, f32[1024]{0} %add.580, f32[1024]{0} %add.584, f32[1024]{0} %add.588, f32[512,1024]{0,1} %add.593, /*index=185*/f32[1024]{0} %add.599, f32[1024]{0} %add.603, f32[1024]{0} %add.607, f32[128,1024]{0,1} %add.611, f32[384]{0} %add.617, /*index=190*/f32[1024,384]{0,1} %add.621, f32[512]{0} %add.627, f32[1024,512]{0,1} %add.635, f32[1024]{0} %add.645, f32[1024]{0} %add.652, /*index=195*/f32[1024]{0} %add.657, f32[512,1024]{0,1} %add.661, f32[1024]{0} %add.668, f32[1024]{0} %add.673, f32[1024]{0} %add.677, /*index=200*/f32[128,1024]{0,1} %add.681, f32[384]{0} %add.687, f32[1024,384]{0,1} %add.692, f32[512]{0} %add.697, f32[1024,512]{0,1} %add.701, /*index=205*/f32[1024]{0} %add.707, f32[1024]{0} %add.711, f32[1024]{0} %add.716, f32[512,1024]{0,1} %add.720, f32[1024]{0} %add.725, /*index=210*/f32[1024]{0} %add.729, f32[1024]{0} %add.734, f32[128,1024]{0,1} %add.738, f32[384]{0} %add.743, f32[1024,384]{0,1} %add.748, /*index=215*/f32[512]{0} %add.753, f32[1024,512]{0,1} %add.757, f32[1024]{0} %add.762, f32[1024]{0} %add.768, f32[1024]{0} %add.773, /*index=220*/f32[512,1024]{0,1} %add.777, f32[1024]{0} %add.782, f32[1024]{0} %add.787, f32[1024]{0} %add.792, f32[128,1024]{0,1} %add.796, /*index=225*/f32[384]{0} %add.801, f32[1024,384]{0,1} %add.806, f32[512]{0} %add.811, f32[1024,512]{0,1} %add.816, f32[1024]{0} %add.823, /*index=230*/f32[1024]{0} %add.831, f32[1024]{0} %add.839, f32[512,1024]{0,1} %add.845)
}

